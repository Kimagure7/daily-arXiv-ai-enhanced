<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 13]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Navigating Ideation Space: Decomposed Conceptual Representations for Positioning Scientific Ideas](https://arxiv.org/abs/2601.08901)
*Yuexi Shen,Minqian Liu,Dawei Zhou,Lifu Huang*

Main category: cs.IR

TL;DR: 提出Ideation Space框架，将科学知识分解为研究问题、方法和核心发现三个维度，实现细粒度文献检索和科学创新性评估


<details>
  <summary>Details</summary>
Motivation: 现有嵌入方法将不同概念方面混为单一表示，无法支持细粒度文献检索；基于LLM的评估器存在奉承偏见，无法提供区分性的创新性评估。需要解决从快速增长文献中识别概念相关先前工作并评估新想法差异的挑战。

Method: 引入Ideation Space结构化表示，通过对比学习将科学知识分解为研究问题、方法和核心发现三个维度；提出分层子空间检索框架进行高效定向文献检索，以及分解创新性评估算法识别想法的创新方面。

Result: Recall@30达到0.329（比基线提升16.7%），ideation transition检索的Hit Rate@30达到0.643，创新性评估与专家判断的相关性达到0.37，均显著优于基线方法。

Conclusion: 该工作为加速和评估科学发现提供了有前景的范式，通过结构化表示和分解方法解决了现有文献检索和创新性评估的局限性。

Abstract: Scientific discovery is a cumulative process and requires new ideas to be situated within an ever-expanding landscape of existing knowledge. An emerging and critical challenge is how to identify conceptually relevant prior work from rapidly growing literature, and assess how a new idea differentiates from existing research. Current embedding approaches typically conflate distinct conceptual aspects into single representations and cannot support fine-grained literature retrieval; meanwhile, LLM-based evaluators are subject to sycophancy biases, failing to provide discriminative novelty assessment. To tackle these challenges, we introduce the Ideation Space, a structured representation that decomposes scientific knowledge into three distinct dimensions, i.e., research problem, methodology, and core findings, each learned through contrastive training. This framework enables principled measurement of conceptual distance between ideas, and modeling of ideation transitions that capture the logical connections within a proposed idea. Building upon this representation, we propose a Hierarchical Sub-Space Retrieval framework for efficient, targeted literature retrieval, and a Decomposed Novelty Assessment algorithm that identifies which aspects of an idea are novel. Extensive experiments demonstrate substantial improvements, where our approach achieves Recall@30 of 0.329 (16.7% over baselines), our ideation transition retrieval reaches Hit Rate@30 of 0.643, and novelty assessment attains 0.37 correlation with expert judgments. In summary, our work provides a promising paradigm for future research on accelerating and evaluating scientific discovery.

</details>


### [2] [Fine Grained Evaluation of LLMs-as-Judges](https://arxiv.org/abs/2601.08919)
*Sourav Saha,Mandar Mitra*

Main category: cs.IR

TL;DR: LLMs作为信息检索相关性评估者的研究，扩展了现有工作，使用INEX维基百科测试集，让LLMs不仅判断文档相关性，还高亮相关段落，以评估LLMs作为评估者的质量。


<details>
  <summary>Details</summary>
Motivation: 近期研究关注LLMs作为"评委"替代人类评估文本/图像处理系统输出的质量。在信息检索领域，已有研究探讨LLMs作为相关性评估者的效果。本研究旨在扩展这些研究，探索LLMs在更细粒度评估中的表现。

Method: 使用INEX创建的维基百科测试集，提示LLMs不仅判断文档是否相关，还要高亮文档中相关的段落。这与人类评估者的任务类似，他们也被要求高亮文档中响应查询信息需求的段落。通过这种方法，可以在文档层面和段落层面评估LLMs作为评估者的质量。

Result: 研究发现LLMs作为评估者在人类监督下表现最佳。研究不仅评估了文档层面的相关性判断，还量化了LLMs"正确判断出于正确原因"的频率。

Conclusion: LLMs作为信息检索相关性评估者具有潜力，但在人类监督下表现最好。研究提供了评估LLMs评估质量的新方法，不仅关注文档层面的判断，还关注其判断的正确理由。

Abstract: A good deal of recent research has focused on how Large Language Models
  (LLMs) may be used as `judges' in place of humans to evaluate the quality
  of the output produced by various text / image processing systems. Within
  this broader context, a number of studies have investigated the specific
  question of how effectively LLMs can be used as relevance assessors for
  the standard ad hoc task in Information Retrieval (IR). We extend these
  studies by looking at additional questions. Most importantly, we use a
  Wikipedia based test collection created by the INEX initiative, and
  prompt LLMs to not only judge whether documents are relevant /
  non-relevant, but to highlight relevant passages in documents that it
  regards as useful. The human relevance assessors involved in creating
  this collection were given analogous instructions, i.e., they were asked
  to highlight all passages within a document that respond to the
  information need expressed in a query. This enables us to evaluate the
  quality of LLMs as judges not only at the document level, but to also
  quantify how often these `judges' are right for the right reasons.
  Our findings suggest that LLMs-as-judges work best under human
  supervision.

</details>


### [3] [LLMs Meet Isolation Kernel: Lightweight, Learning-free Binary Embeddings for Fast Retrieval](https://arxiv.org/abs/2601.09159)
*Zhibo Zhang,Yang Xu,Kai Ming Ting,Cam-Tu Nguyen*

Main category: cs.IR

TL;DR: 提出IKE方法，将LLM高维嵌入转换为二进制嵌入，显著降低存储和检索开销，同时保持检索精度。


<details>
  <summary>Details</summary>
Motivation: LLM嵌入通常维度很高，导致存储和检索开销大。现有方法如MRL和CSR虽然有所缓解，但仍存在检索精度下降的问题。

Method: 提出Isolation Kernel Embedding (IKE)，一种无需学习的方法，使用Isolation Kernel将LLM嵌入转换为二进制嵌入。IKE基于多样化（随机）分区的集成，能够在LLM嵌入空间中稳健估计理想核。

Result: 在多个文本检索数据集上的实验表明，IKE相比LLM嵌入可实现高达16.7倍的检索加速和16倍的内存使用降低，同时保持相当或更好的精度。相比CSR和其他压缩方法，IKE在检索效率和效果之间达到最佳平衡。

Conclusion: IKE提供了一种轻量级的二进制编码方法，显著降低了LLM嵌入的存储和计算开销，同时保持了检索精度，在效率和效果之间取得了良好平衡。

Abstract: Large language models (LLMs) have recently enabled remarkable progress in text representation. However, their embeddings are typically high-dimensional, leading to substantial storage and retrieval overhead. Although recent approaches such as Matryoshka Representation Learning (MRL) and Contrastive Sparse Representation (CSR) alleviate these issues to some extent, they still suffer from retrieval accuracy degradation. This paper proposes \emph{Isolation Kernel Embedding} or IKE, a learning-free method that transforms an LLM embedding into a binary embedding using Isolation Kernel (IK). IKE is an ensemble of diverse (random) partitions, enabling robust estimation of ideal kernel in the LLM embedding space, thus reducing retrieval accuracy loss as the ensemble grows. Lightweight and based on binary encoding, it offers low memory footprint and fast bitwise computation, lowering retrieval latency. Experiments on multiple text retrieval datasets demonstrate that IKE offers up to 16.7x faster retrieval and 16x lower memory usage than LLM embeddings, while maintaining comparable or better accuracy. Compared to CSR and other compression methods, IKE consistently achieves the best balance between retrieval efficiency and effectiveness.

</details>


### [4] [Why not Collaborative Filtering in Dual View? Bridging Sparse and Dense Models](https://arxiv.org/abs/2601.09286)
*Hanze Guo,Jianxun Lian,Xiao Zhou*

Main category: cs.IR

TL;DR: SaD框架通过稀疏与稠密双视角对齐，解决了传统协同过滤在冷门物品建模中的信噪比瓶颈问题，实现了SOTA性能


<details>
  <summary>Details</summary>
Motivation: 传统基于稠密嵌入的协同过滤方法在处理冷门物品时存在信噪比瓶颈，因为参数化稠密模型在数据极度稀疏情况下信噪比会下降，限制了推荐系统的性能

Method: 提出SaD统一框架，结合稠密嵌入的语义表达能力和稀疏交互模式的结构可靠性，通过轻量级双向对齐机制：稠密视角通过注入语义相关性丰富稀疏视角，稀疏视角通过显式结构信号正则化稠密模型

Result: 理论证明双视角对齐能获得严格更优的全局信噪比；实验表明即使简单的矩阵分解式稠密模型也能达到SOTA性能；在真实基准测试中持续优于强基线，在BarsMatch排行榜排名第一

Conclusion: SaD框架展示了从双视角利用协同过滤的持久力量，具有即插即用特性，可无缝应用于广泛的现有推荐模型，为解决冷启动和稀疏性问题提供了有效方案

Abstract: Collaborative Filtering (CF) remains the cornerstone of modern recommender systems, with dense embedding--based methods dominating current practice. However, these approaches suffer from a critical limitation: our theoretical analysis reveals a fundamental signal-to-noise ratio (SNR) ceiling when modeling unpopular items, where parameter-based dense models experience diminishing SNR under severe data sparsity. To overcome this bottleneck, we propose SaD (Sparse and Dense), a unified framework that integrates the semantic expressiveness of dense embeddings with the structural reliability of sparse interaction patterns. We theoretically show that aligning these dual views yields a strictly superior global SNR. Concretely, SaD introduces a lightweight bidirectional alignment mechanism: the dense view enriches the sparse view by injecting semantic correlations, while the sparse view regularizes the dense model through explicit structural signals. Extensive experiments demonstrate that, under this dual-view alignment, even a simple matrix factorization--style dense model can achieve state-of-the-art performance. Moreover, SaD is plug-and-play and can be seamlessly applied to a wide range of existing recommender models, highlighting the enduring power of collaborative filtering when leveraged from dual perspectives. Further evaluations on real-world benchmarks show that SaD consistently outperforms strong baselines, ranking first on the BarsMatch leaderboard. The code is publicly available at https://github.com/harris26-G/SaD.

</details>


### [5] [On-Device Large Language Models for Sequential Recommendation](https://arxiv.org/abs/2601.09306)
*Xin Xia,Hongzhi Yin,Shane Culpepper*

Main category: cs.IR

TL;DR: OD-LLM是一个专门为顺序推荐任务设计的LLM任务自适应压缩框架，通过低秩结构压缩和tokenization归一化技术，实现在设备上高效部署LLM推荐模型，模型大小减半时性能无损失。


<details>
  <summary>Details</summary>
Motivation: 设备端推荐在延迟、隐私和网络不稳定场景中至关重要，但LLM模型虽然推荐能力强，却因内存占用大和计算开销高而难以在资源受限设备上部署。

Method: 提出OD-LLM框架，集成两种互补压缩策略：1）使用SVD的低秩结构压缩算法减少参数冗余；2）新颖的tokenization归一化技术增强低秩分解效果；3）采用渐进对齐算法逐层优化参数以减少高压缩率下的性能损失。

Result: 在顺序推荐基准测试中，OD-LLM在模型大小减半的情况下，相比原始推荐模型没有性能损失，证明了其有效性和可扩展性。

Conclusion: OD-LLM为实时设备端推荐提供了一种实用替代方案，能够替代昂贵的远程执行LLM，在保持性能的同时显著减少模型大小和计算开销。

Abstract: On-device recommendation is critical for a number of real-world applications, especially in scenarios that have agreements on execution latency, user privacy, and robust functionality when internet connectivity is unstable or even impossible. While large language models (LLMs) can now provide exceptional capabilities that model user behavior for sequential recommendation tasks, their substantial memory footprint and computational overhead make the deployment on resource-constrained devices a high risk proposition. In this paper, we propose OD-LLM, the first task-adaptive compression framework explicitly designed to provide efficient and accurate on-device deployment of LLMs for sequential recommendation tasks. OD-LLM uniquely integrates two complementary compression strategies: a low-rank structural compression algorithm which uses Singular Value Decomposition (SVD) to significantly reduce parameter redundancy in the model, and a novel tokenization normalization technique that better complements the low-rank decomposition process being used. Additionally, to minimize any potential performance degradation when using higher compression ratios, a novel progressive alignment algorithm is used to iteratively refine the parameters required layerwise in the target model. Empirical evaluations conducted on sequential recommendation benchmarks show that OD-LLM exhibits no loss in effectiveness when compared to the original recommendation model, when the deployed model size is halved. These promising results demonstrate the efficacy and scalability of OD-LLM, making this novel solution a practical alternative for real-time, on-device solutions wishing to replace expensive, remotely executed LLMs.

</details>


### [6] [LISP -- A Rich Interaction Dataset and Loggable Interactive Search Platform](https://arxiv.org/abs/2601.09366)
*Jana Isabelle Friese,Andreas Konstantin Kruff,Philipp Schaer,Norbert Fuhr,Nicola Ferro*

Main category: cs.IR

TL;DR: 提出了一个可重复使用的数据集和基础设施，用于研究交互式信息检索中的人类搜索行为，包含61名参与者的详细交互日志和用户特征，支持可重复研究和用户模拟器开发。


<details>
  <summary>Details</summary>
Motivation: 在交互式信息检索领域，需要能够研究个体和情境因素如何影响搜索行为的数据集，同时支持可重复研究和用户模拟器的开发与验证。

Method: 收集了61名参与者（122个会话）的详细交互日志，结合用户特征（感知速度、主题兴趣、搜索专业知识、人口统计信息），提供完整的研究设置文档、基于网络的感知速度测试框架，以及进行类似用户研究的框架。

Result: 创建了一个包含详细交互数据和用户特征的综合数据集，通过示例分析展示了数据集的潜力，并将所有资源作为开放获取发布，支持IIR社区的可重复研究和资源共享。

Conclusion: 该工作为研究人类搜索行为中的个体差异提供了宝贵资源，促进了交互式信息检索领域的可重复研究，并为开发考虑用户变异性的模拟器提供了基础。

Abstract: We present a reusable dataset and accompanying infrastructure for studying human search behavior in Interactive Information Retrieval (IIR). The dataset combines detailed interaction logs from 61 participants (122 sessions) with user characteristics, including perceptual speed, topic-specific interest, search expertise, and demographic information. To facilitate reproducibility and reuse, we provide a fully documented study setup, a web-based perceptual speed test, and a framework for conducting similar user studies. Our work allows researchers to investigate individual and contextual factors affecting search behavior, and to develop or validate user simulators that account for such variability. We illustrate the datasets potential through an illustrative analysis and release all resources as open-access, supporting reproducible research and resource sharing in the IIR community.

</details>


### [7] [Dissecting Judicial Reasoning in U.S. Copyright Damage Awards](https://arxiv.org/abs/2601.09459)
*Pei-Chi Lo,Thomas Y. Lu*

Main category: cs.IR

TL;DR: 本文提出了一种基于修辞结构理论和智能工作流的LLM方法，用于分析版权损害赔偿判决中的司法推理模式，揭示了不同巡回法院在因素权衡上的差异。


<details>
  <summary>Details</summary>
Motivation: 版权损害赔偿判决中的司法推理存在不一致性，不同法院对1976年版权法的解释和因素权衡差异很大，导致诉讼结果不可预测，且缺乏对法律决策经验基础的清晰认识。

Method: 提出了一种基于话语分析的大型语言模型方法，整合修辞结构理论和智能工作流，通过三阶段流程（数据集构建、话语分析、智能特征提取）从司法意见中提取和量化推理模式。

Result: 话语增强的LLM分析优于传统方法，揭示了不同巡回法院在因素权衡上未被量化的差异，为计算法律分析提供了方法学进步。

Conclusion: 该方法为法律从业者提供了预测工具，为学者研究法律原则应用提供了新视角，为政策制定者应对版权法不一致性提供了实证基础。

Abstract: Judicial reasoning in copyright damage awards poses a core challenge for computational legal analysis. Although federal courts follow the 1976 Copyright Act, their interpretations and factor weightings vary widely across jurisdictions. This inconsistency creates unpredictability for litigants and obscures the empirical basis of legal decisions. This research introduces a novel discourse-based Large Language Model (LLM) methodology that integrates Rhetorical Structure Theory (RST) with an agentic workflow to extract and quantify previously opaque reasoning patterns from judicial opinions. Our framework addresses a major gap in empirical legal scholarship by parsing opinions into hierarchical discourse structures and using a three-stage pipeline, i.e., Dataset Construction, Discourse Analysis, and Agentic Feature Extraction. This pipeline identifies reasoning components and extract feature labels with corresponding discourse subtrees. In analyzing copyright damage rulings, we show that discourse-augmented LLM analysis outperforms traditional methods while uncovering unquantified variations in factor weighting across circuits. These findings offer both methodological advances in computational legal analysis and practical insights into judicial reasoning, with implications for legal practitioners seeking predictive tools, scholars studying legal principle application, and policymakers confronting inconsistencies in copyright law.

</details>


### [8] [Bridging Semantic Understanding and Popularity Bias with LLMs](https://arxiv.org/abs/2601.09478)
*Renqiang Luo,Dong Zhang,Yupeng Gao,Wen Shi,Mingliang Hou,Jiaying Liu,Zhe Wang,Shuo Yu*

Main category: cs.IR

TL;DR: FairLRM是一个基于大语言模型的推荐框架，通过结构化指令提示将流行度偏差分解为用户侧和物品侧组件，从语义层面理解并解决推荐系统中的流行度偏差问题，显著提升了公平性和推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统去偏方法大多将流行度偏差的语义理解简化为多样性增强或长尾覆盖问题，忽视了偏差本身的因果起源，这种浅层理解限制了去偏效果和推荐准确性。

Method: 提出FairLRM框架，基于大语言模型进行推荐，将流行度偏差分解为物品侧和用户侧组件，使用结构化指令提示增强模型对全局物品分布和个体用户偏好的理解，从语义层面解释和解决偏差。

Result: 实证评估表明，FairLRM显著提升了公平性和推荐准确性，提供了更具语义感知和可信赖的流行度偏差理解方法。

Conclusion: FairLRM通过大语言模型从语义层面理解流行度偏差，超越了传统的表面特征处理方法，为推荐系统提供了一种更深入、更有效的去偏解决方案。

Abstract: Semantic understanding of popularity bias is a crucial yet underexplored challenge in recommender systems, where popular items are often favored at the expense of niche content. Most existing debiasing methods treat the semantic understanding of popularity bias as a matter of diversity enhancement or long-tail coverage, neglecting the deeper semantic layer that embodies the causal origins of the bias itself. Consequently, such shallow interpretations limit both their debiasing effectiveness and recommendation accuracy. In this paper, we propose FairLRM, a novel framework that bridges the gap in the semantic understanding of popularity bias with Recommendation via Large Language Model (RecLLM). FairLRM decomposes popularity bias into item-side and user-side components, using structured instruction-based prompts to enhance the model's comprehension of both global item distributions and individual user preferences. Unlike traditional methods that rely on surface-level features such as "diversity" or "debiasing", FairLRM improves the model's ability to semantically interpret and address the underlying bias. Through empirical evaluation, we show that FairLRM significantly enhances both fairness and recommendation accuracy, providing a more semantically aware and trustworthy approach to enhance the semantic understanding of popularity bias. The implementation is available at https://github.com/LuoRenqiang/FairLRM.

</details>


### [9] [Unifying Search and Recommendation in LLMs via Gradient Multi-Subspace Tuning](https://arxiv.org/abs/2601.09496)
*Jujia Zhao,Zihan Wang,Shuaiqun Pan,Suzan Verberne,Zhaochun Ren*

Main category: cs.IR

TL;DR: GEMS提出了一种参数高效微调框架，通过多子空间分解和零空间投影来解决搜索与推荐统一建模中的梯度冲突和通用知识保持问题。


<details>
  <summary>Details</summary>
Motivation: 搜索和推荐在在线平台中具有互补作用，统一建模具有重要价值。现有基于LLM的统一方法依赖全参数微调，计算成本高且可扩展性差。参数高效微调面临梯度冲突和通用知识偏移两大挑战。

Method: GEMS框架包含两个核心技术：1）多子空间分解，将共享和任务特定的优化信号解耦到互补的低秩子空间，减少破坏性梯度干扰；2）零空间投影，将参数更新约束在与通用知识空间正交的子空间，减轻用户意图理解的偏移。

Result: 在基准数据集上的广泛实验表明，GEMS在搜索和推荐任务上均持续优于最先进的基线方法，实现了卓越的有效性。

Conclusion: GEMS通过创新的参数高效微调方法，成功解决了搜索与推荐统一建模中的关键挑战，为LLM在搜索推荐领域的应用提供了更实用、可扩展的解决方案。

Abstract: Search and recommendation (S&R) are core to online platforms, addressing explicit intent through queries and modeling implicit intent from behaviors, respectively. Their complementary roles motivate a unified modeling paradigm. Early studies to unify S&R adopt shared encoders with task-specific heads, while recent efforts reframe item ranking in both S&R as conditional generation. The latter holds particular promise, enabling end-to-end optimization and leveraging the semantic understanding of LLMs. However, existing methods rely on full fine-tuning, which is computationally expensive and limits scalability. Parameter-efficient fine-tuning (PEFT) offers a more practical alternative but faces two critical challenges in unifying S&R: (1) gradient conflicts across tasks due to divergent optimization objectives, and (2) shifts in user intent understanding caused by overfitting to fine-tuning data, which distort general-domain knowledge and weaken LLM reasoning. To address the above issues, we propose Gradient Multi-Subspace Tuning (GEMS), a novel framework that unifies S&R with LLMs while alleviating gradient conflicts and preserving general-domain knowledge. GEMS introduces (1) \textbf{Multi-Subspace Decomposition}, which disentangles shared and task-specific optimization signals into complementary low-rank subspaces, thereby reducing destructive gradient interference, and (2) \textbf{Null-Space Projection}, which constrains parameter updates to a subspace orthogonal to the general-domain knowledge space, mitigating shifts in user intent understanding. Extensive experiments on benchmark datasets show that GEMS consistently outperforms the state-of-the-art baselines across both search and recommendation tasks, achieving superior effectiveness.

</details>


### [10] [TEMPO: A Realistic Multi-Domain Benchmark for Temporal Reasoning-Intensive Retrieval](https://arxiv.org/abs/2601.09523)
*Abdelrahman Abdallah,Mohammed Ali,Muhammad Abdul-Mageed,Adam Jatowt*

Main category: cs.IR

TL;DR: TEMPO是首个结合时间推理与推理密集型检索的基准测试，包含1,730个复杂查询，覆盖13个领域，评估系统在时间演化推理方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有时间QA基准主要关注新闻语料中的简单事实查询，而推理密集型检索基准缺乏时间基础。现实世界的信息需求通常需要推理时间演化并综合跨时间段的证据。

Method: 构建包含1,730个复杂查询的基准，要求深度时间推理（如追踪变化、识别趋势、比较跨时期证据）；采用分步检索规划，包含3,976个分解步骤，每个步骤映射黄金文档；引入新的时间度量指标Temporal Coverage@k和Temporal Precision@k。

Result: 评估12个检索系统显示重大挑战：最佳模型(DiVeR)仅达到32.0 NDCG@10和71.4% Temporal Coverage@10，表明在检索时间完整证据方面存在困难。

Conclusion: TEMPO为改进检索和RAG系统中的时间推理提供了一个具有挑战性的基准，揭示了当前系统在时间推理方面的显著不足。

Abstract: Existing temporal QA benchmarks focus on simple fact-seeking queries from news corpora, while reasoning-intensive retrieval benchmarks lack temporal grounding. However, real-world information needs often require reasoning about temporal evolution and synthesizing evidence across time periods. We introduce TEMPO, the first benchmark combining temporal reasoning with reasoning-intensive retrieval across 13 domains. TEMPO features: (1) 1,730 complex queries requiring deep temporal reasoning such as tracking changes, identifying trends, or comparing cross-period evidence; (2) step-wise retrieval planning with 3,976 decomposed steps and gold documents mapped to each step for multi-hop evaluation; and (3) novel temporal metrics including Temporal Coverage@k and Temporal Precision@k measuring whether results span required time periods. Evaluation of 12 retrieval systems reveals substantial challenges: the best model (DiVeR) achieves only 32.0 NDCG@10 and 71.4\% Temporal Coverage@10, demonstrating difficulty in retrieving temporally complete evidence. We believe TEMPO provides a challenging benchmark for improving temporal reasoning in retrieval and RAG systems. Our code and data are available at https://github.com/tempo-bench/Tempo. See also our official website: https://tempo-bench.github.io/.

</details>


### [11] [SpatCode: Rotary-based Unified Encoding Framework for Efficient Spatiotemporal Vector Retrieval](https://arxiv.org/abs/2601.09530)
*Bingde Hu,Enhao Pan,Wanjing Zhou,Yang Gao,Zunlei Feng,Hao Zhong*

Main category: cs.IR

TL;DR: 提出统一时空向量检索框架，通过旋转编码、增量更新和加权检索算法，实现高效、可扩展的时空信息检索。


<details>
  <summary>Details</summary>
Motivation: 现有时空检索方法通常是传统向量搜索系统的扩展，依赖外部过滤器或专用索引来处理时空约束，导致效率低下、架构复杂，且在处理异构模态时灵活性有限。

Method: 1) 基于旋转的统一编码方法：将时间和位置嵌入旋转位置向量，实现一致的时空表示；2) 循环增量更新机制：支持高效的滑动窗口更新，无需全局重新编码或索引重建；3) 加权兴趣检索算法：自适应平衡模态权重，实现上下文感知和个性化检索。

Result: 在多个真实世界数据集上的实验表明，该框架在检索准确性和效率方面显著优于现有最先进基线方法，同时在动态数据演化下保持鲁棒性。

Conclusion: 所提出的方法为智能系统中的可扩展时空信息检索提供了有效且实用的解决方案，通过统一框架解决了现有方法的效率、复杂性和灵活性限制。

Abstract: Spatiotemporal vector retrieval has emerged as a critical paradigm in modern information retrieval, enabling efficient access to massive, heterogeneous data that evolve over both time and space. However, existing spatiotemporal retrieval methods are often extensions of conventional vector search systems that rely on external filters or specialized indices to incorporate temporal and spatial constraints, leading to inefficiency, architectural complexity, and limited flexibility in handling heterogeneous modalities. To overcome these challenges, we present a unified spatiotemporal vector retrieval framework that integrates temporal, spatial, and semantic cues within a coherent similarity space while maintaining scalability and adaptability to continuous data streams. Specifically, we propose (1) a Rotary-based Unified Encoding Method that embeds time and location into rotational position vectors for consistent spatiotemporal representation; (2) a Circular Incremental Update Mechanism that supports efficient sliding-window updates without global re-encoding or index reconstruction; and (3) a Weighted Interest-based Retrieval Algorithm that adaptively balances modality weights for context-aware and personalized retrieval. Extensive experiments across multiple real-world datasets demonstrate that our framework substantially outperforms state-of-the-art baselines in both retrieval accuracy and efficiency, while maintaining robustness under dynamic data evolution. These results highlight the effectiveness and practicality of the proposed approach for scalable spatiotemporal information retrieval in intelligent systems.

</details>


### [12] [Examining DOM Coordinate Effectiveness For Page Segmentation](https://arxiv.org/abs/2601.09543)
*Jason Carpenter,Faaiq Bilal,Eman Ramadan,Zhi-Li Zhang*

Main category: cs.IR

TL;DR: 该研究发现DOM坐标比视觉坐标在网页分割中表现更好，简单向量优于复杂向量，通过优化匹配可实现74%的分割准确率


<details>
  <summary>Details</summary>
Motivation: 网页数据规模庞大且非结构化，现有基于DOM的方法使用视觉坐标或树结构坐标，但这些向量的构建和组件价值缺乏深入分析，需要理解DOM坐标对网页分割的影响

Method: 提出并详细检查DOM坐标，比较不同坐标向量（视觉坐标vs DOM坐标）在网页分割中的表现，分析简单向量与复杂向量的效果，探索向量、聚类算法和页面的最佳匹配

Result: 视觉坐标比DOM坐标平均差20-30%；简单向量（单坐标）在68.2%的页面中表现优于复杂向量；通过优化匹配可实现74%的整体分割准确率，比朴素向量应用提高20%

Conclusion: 挑战了当前分割向量创建的正统观念，表明无需包含视觉坐标，DOM坐标优化可实现更好的网页分割，强调了为网页分割找到最佳匹配机制的重要性

Abstract: Web pages form a cornerstone of available data for daily human consumption and with the rise of LLM-based search and learning systems a treasure trove of valuable data. The scale of this data and its unstructured format still continue to grow requiring ever more robust automated extraction and retrieval mechanisms. Existing work, leveraging the web pages Document Object Model (DOM), often derives clustering vectors from coordinates informed by the DOM such as visual placement or tree structure. The construction and component value of these vectors often go unexamined. Our work proposes and examines DOM coordinates in a detail to understand their impact on web page segmentation. Our work finds that there is no one-size-fits-all vector, and that visual coordinates under-perform compared to DOM coordinates by about 20-30% on average. This challenges the necessity of including visual coordinates in clustering vectors. Further, our work finds that simple vectors, comprised of single coordinates, fare better than complex vectors constituting 68.2% of the top performing vectors of the pages examined. Finally, we find that if a vector, clustering algorithm, and page are properly matched, one can achieve overall high segmentation accuracy at 74%. This constitutes a 20% improvement over a naive application of vectors. Conclusively, our results challenge the current orthodoxy for segmentation vector creation, opens up the possibility to optimize page segmentation via clustering on DOM coordinates, and highlights the importance of finding mechanisms to match the best approach for web page segmentation.

</details>


### [13] [MM-BRIGHT: A Multi-Task Multimodal Benchmark for Reasoning-Intensive Retrieval](https://arxiv.org/abs/2601.09562)
*Abdelrahman Abdallah,Mohamed Darwish Mounis,Mahmoud Abdalla,Mahmoud SalahEldin Kasem,Mostafa Farouk Senussi,Mohamed Mahmoud,Mohammed Ali,Adam Jatowt,Hyun-Soo Kang*

Main category: cs.IR

TL;DR: MM-BRIGHT是首个针对推理密集型检索的多模态基准测试，包含2,803个真实世界查询，涵盖29个技术领域和4种复杂度递增的任务，现有最先进模型在所有任务上都表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有检索基准主要基于文本查询，而现实世界查询常包含图表、截图等多模态元素，需要深度推理才能找到相关文档，当前缺乏针对此类推理密集型多模态检索的评估基准。

Method: 构建包含2,803个真实世界查询的多模态基准测试，涵盖29个技术领域，设计了四种复杂度递增的任务：文本到文本、多模态到文本、多模态到图像、多模态到多模态检索。

Result: 现有模型在所有任务上都表现不佳：BM25在纯文本检索上仅获得8.5 nDCG@10，最佳多模态模型Nomic-Vision在多模态到文本检索上仅27.6 nDCG@10，甚至不如最佳纯文本模型DiVeR（32.2）。

Conclusion: MM-BRIGHT揭示了现有模型在多模态推理密集型检索方面的显著不足，为下一代更好整合视觉推理的检索模型提供了重要测试平台。

Abstract: Existing retrieval benchmarks primarily consist of text-based queries where keyword or semantic matching is usually sufficient. Many real-world queries contain multimodal elements, particularly, images such as diagrams, charts, and screenshots that require intensive reasoning to identify relevant documents. To address this gap, we introduce MM-BRIGHT, the first multimodal benchmark for reasoning-intensive retrieval. Our dataset consists of 2,803 real-world queries spanning 29 diverse technical domains, with four tasks of increasing complexity: text-to-text, multimodal-to-text, multimodal-to-image, and multimodal-to-multimodal retrieval. Extensive evaluation reveals that state-of-the-art models struggle across all tasks: BM25 achieves only 8.5 nDCG@10 on text-only retrieval, while the best multimodal model Nomic-Vision reaches just 27.6 nDCG@10 on multimodal-to-text retrieval actually underperforming the best text-only model (DiVeR: 32.2). These results highlight substantial headroom and position MM-BRIGHT as a testbed for next-generation retrieval models that better integrate visual reasoning. Our code and data are available at https://github.com/mm-bright/MM-BRIGHT. See also our official website: https://mm-bright.github.io/.

</details>
