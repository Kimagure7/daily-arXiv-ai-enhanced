<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 11]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Embedding the Teacher: Distilling vLLM Preferences for Scalable Image Retrieval](https://arxiv.org/abs/2510.12014)
*Eric He,Akash Gupta,Adian Liusie,Vatsal Raina,Piotr Molenda,Shirom Chabra,Vyas Raina*

Main category: cs.IR

TL;DR: 提出了一种将大型视觉语言模型的偏好排序能力蒸馏到嵌入式系统中的框架，用于个性化文本-图像检索，在保持推理效率的同时提升对抽象和角色驱动属性的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的嵌入式检索方法（如CLIP）主要基于字面文本-图像对训练，难以处理产品推荐中常见的抽象或角色驱动属性；而视觉语言模型虽然能灵活对齐文本和图像，但受限于上下文窗口无法直接处理大规模检索。

Method: 通过将强大的视觉语言模型的偏好排序能力蒸馏到嵌入式系统中，转移其细致的对齐能力，同时保持嵌入式方法在推理时的可扩展性。

Result: 在角色驱动的产品推荐任务上的实验表明，该方法显著优于现有的嵌入式基线方法。

Conclusion: 该方法为个性化文本-图像检索提供了一个高效的解决方案，结合了视觉语言模型的灵活性和嵌入式系统的高效性。

Abstract: Text--image retrieval is necessary for applications such as product
recommendation. Embedding-based approaches like CLIP enable efficient
large-scale retrieval via vector similarity search, but they are primarily
trained on literal caption-like text--image pairs and often fail to capture
abstract or persona-driven attributes common in product recommendation
applications (e.g., ``a gift for a mother who loves gardening''). In contrast,
state-of-the-art vision--language models (vLLMs) can align text with images in
a flexible manner, but their limited context window prevents them from directly
handling retrieval over large catalogs. We propose a framework that distills
the preference rankings of a powerful vLLM into an embedding-based system,
transferring its nuanced alignment abilities while maintaining the
inference-time scalability of an embedding-based approach. Experiments on
persona-driven product recommendation tasks demonstrate that our method
significantly outperforms existing embedding-based baselines, providing an
efficient solution for personalized text--image retrieval.

</details>


### [2] [MIARec: Mutual-influence-aware Heterogeneous Network Embedding for Scientific Paper Recommendation](https://arxiv.org/abs/2510.12054)
*Wenjin Xie,Tao Jia*

Main category: cs.IR

TL;DR: MIARec模型通过重力方法衡量学者间的相互学术影响力，并将其融入图表示学习的消息传播中，结合多通道聚合方法，在科学论文推荐任务中表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有图推荐方法忽略了学术网络中普遍存在的不对称学术影响力，需要更准确地建模学者间的相互影响关系。

Method: 使用重力方法测量学者间的相互学术影响力，在消息传播过程中进行特征聚合，并采用多通道聚合方法捕捉单关系子网络的个体嵌入及其相互依赖嵌入。

Result: 在真实数据集上的实验表明，MIARec模型在三个主要评估指标上均优于基线模型。

Conclusion: MIARec模型通过考虑学术影响力的不对称性，能够更全面地理解异质学术网络，有效提升科学论文推荐性能。

Abstract: With the rapid expansion of scientific literature, scholars increasingly
demand precise and high-quality paper recommendations. Among various
recommendation methodologies, graph-based approaches have garnered attention by
effectively exploiting the structural characteristics inherent in scholarly
networks. However, these methods often overlook the asymmetric academic
influence that is prevalent in scholarly networks when learning graph
representations. To address this limitation, this study proposes the
Mutual-Influence-Aware Recommendation (MIARec) model, which employs a
gravity-based approach to measure the mutual academic influence between
scholars and incorporates this influence into the feature aggregation process
during message propagation in graph representation learning. Additionally, the
model utilizes a multi-channel aggregation method to capture both individual
embeddings of distinct single relational sub-networks and their interdependent
embeddings, thereby enabling a more comprehensive understanding of the
heterogeneous scholarly network. Extensive experiments conducted on real-world
datasets demonstrate that the MIARec model outperforms baseline models across
three primary evaluation metrics, indicating its effectiveness in scientific
paper recommendation tasks.

</details>


### [3] [Reinforced Preference Optimization for Recommendation](https://arxiv.org/abs/2510.12211)
*Junfei Tan,Yuxin Chen,An Zhang,Junguang Jiang,Bin Liu,Ziru Xu,Han Zhu,Jian Xu,Bo Zheng,Xiang Wang*

Main category: cs.IR

TL;DR: 提出ReRe框架，通过强化学习改进基于LLM的生成式推荐系统，解决负样本建模不足和奖励稀疏问题，在多个数据集上表现优于传统和LLM推荐方法。


<details>
  <summary>Details</summary>
Motivation: 当前生成式推荐系统存在两个核心限制：缺乏高质量负样本建模和依赖隐式奖励。强化学习提供自然解决方案，但在生成式推荐中应用仍面临挑战。

Method: 提出ReRe框架，包含约束束搜索提高采样效率和多样化硬负样本，同时用辅助排序奖励增强基于规则的准确度奖励，提供更细粒度的监督。

Result: 在三个真实世界数据集上的实验表明，ReRe在排序性能上持续优于传统和基于LLM的推荐方法，且在不同骨干模型和规模上都能稳健泛化。

Conclusion: ReRe不仅提升了基础模型和SFT初始化模型的性能，还系统地探索了推荐中RLVR的设计空间，为未来研究提供见解。

Abstract: Recent breakthroughs in large language models (LLMs) have fundamentally
shifted recommender systems from discriminative to generative paradigms, where
user behavior modeling is achieved by generating target items conditioned on
historical interactions. Yet current generative recommenders still suffer from
two core limitations: the lack of high-quality negative modeling and the
reliance on implicit rewards. Reinforcement learning with verifiable rewards
(RLVR) offers a natural solution by enabling on-policy sampling of harder
negatives and grounding optimization in explicit reward signals. However,
applying RLVR to generative recommenders remains non-trivial. Its unique
generation space often leads to invalid or repetitive items that undermine
sampling efficiency, and ranking supervision is sparse since most items receive
identical zero rewards. To address these challenges, we propose Reinforced
Preference Optimization for Recommendation (ReRe), a reinforcement-based
paradigm tailored to LLM-based recommenders, an important direction in
generative recommendation. ReRe incorporates constrained beam search to improve
sampling efficiency and diversify hard negatives, while augmenting rule-based
accuracy rewards with auxiliary ranking rewards for finer-grained supervision.
Extensive experiments on three real-world datasets demonstrate that ReRe
consistently outperforms both traditional and LLM-based recommenders in ranking
performance. Further analysis shows that ReRe not only enhances performance
across both base and SFT-initialized models but also generalizes robustly
across different backbone families and scales. Beyond empirical gains, we
systematically investigate the design space of RLVR in recommendation across
generation, sampling strategy, reward modeling, and optimization algorithm,
offering insights for future research.

</details>


### [4] [An Empirical Study for Representations of Videos in Video Question Answering via MLLMs](https://arxiv.org/abs/2510.12299)
*Zhi Li,Yanan Wang,Hao Niu,Julio Vizcarra,Masato Taya*

Main category: cs.IR

TL;DR: 该论文对多模态大语言模型在视频问答任务中的视频表示方法进行了全面的实证研究，评估了单模态和多模态输入在准确性和计算效率之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视频问答方面取得了显著进展，但尚不清楚哪种视频表示方法最有效，以及不同模态如何在任务准确性和计算效率之间取得平衡。

Method: 系统评估了单模态输入（仅问题、字幕、视觉帧、音频信号）以及多模态组合，在两个广泛使用的基准测试VideoMME和LongVideoBench上进行实验。

Result: 视觉帧显著提高准确性但带来GPU内存和推理延迟的高昂成本，而字幕提供了轻量级但有效的替代方案，尤其适用于长视频。

Conclusion: 研究发现视频表示方法在有效性和效率之间存在明确的权衡，为设计资源感知的MLLM视频问答系统提供了实用见解。

Abstract: Multimodal large language models have recently achieved remarkable progress
in video question answering (VideoQA) by jointly processing visual, textual,
and audio information. However, it remains unclear which video representations
are most effective for MLLMs, and how different modalities balance task
accuracy against computational efficiency. In this work, we present a
comprehensive empirical study of video representation methods for VideoQA with
MLLMs. We systematically evaluate single modality inputs question only,
subtitles, visual frames, and audio signals as well as multimodal combinations,
on two widely used benchmarks: VideoMME and LongVideoBench. Our results show
that visual frames substantially enhance accuracy but impose heavy costs in GPU
memory and inference latency, while subtitles provide a lightweight yet
effective alternative, particularly for long videos. These findings highlight
clear trade-offs between effectiveness and efficiency and provide practical
insights for designing resource-aware MLLM-based VideoQA systems.

</details>


### [5] [Causal Inspired Multi Modal Recommendation](https://arxiv.org/abs/2510.12325)
*Jie Yang,Chenyang Gu,Zixuan Liu*

Main category: cs.IR

TL;DR: 提出一种因果启发的多模态推荐框架，通过双通道跨模态扩散模块识别隐藏模态混杂因子，使用后门调整和前门调整来消除模态混杂和交互偏差，显著提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推荐系统存在两个关键偏差：模态混杂（多个模态被相同潜在因素驱动导致虚假特征-偏好关联）和交互偏差（真实用户偏好与曝光效应和偶然点击的噪声混合）。

Method: 引入双通道跨模态扩散模块识别隐藏模态混杂因子，使用后门调整（分层匹配和向量量化码本）阻断混杂路径，应用前门调整结合因果拓扑重构构建去混杂因果子图。

Result: 在三个真实电商数据集上的广泛实验表明，该方法显著优于现有最先进基线，同时保持强可解释性。

Conclusion: 提出的因果启发的多模态推荐框架有效解决了模态混杂和交互偏差问题，在提升推荐性能的同时保持了模型的可解释性。

Abstract: Multimodal recommender systems enhance personalized recommendations in
e-commerce and online advertising by integrating visual, textual, and user-item
interaction data. However, existing methods often overlook two critical biases:
(i) modal confounding, where latent factors (e.g., brand style or product
category) simultaneously drive multiple modalities and influence user
preference, leading to spurious feature-preference associations; (ii)
interaction bias, where genuine user preferences are mixed with noise from
exposure effects and accidental clicks. To address these challenges, we propose
a Causal-inspired multimodal Recommendation framework. Specifically, we
introduce a dual-channel cross-modal diffusion module to identify hidden modal
confounders, utilize back-door adjustment with hierarchical matching and
vector-quantized codebooks to block confounding paths, and apply front-door
adjustment combined with causal topology reconstruction to build a deconfounded
causal subgraph. Extensive experiments on three real-world e-commerce datasets
demonstrate that our method significantly outperforms state-of-the-art
baselines while maintaining strong interpretability.

</details>


### [6] [Simple Projection Variants Improve ColBERT Performance](https://arxiv.org/abs/2510.12327)
*Benjamin Clavié,Sean Lee,Rikiya Takehi,Aamir Shakir,Makoto P. Kato*

Main category: cs.IR

TL;DR: 研究发现ColBERT等多向量稠密检索方法使用的单层线性投影存在局限性，通过替换为更复杂的FFN块（如深度非线性FFN、GLU块和跳跃连接）可以显著提升检索性能，最佳变体在多个基准测试中平均提升超过2 NDCG@10点。


<details>
  <summary>Details</summary>
Motivation: 探索MaxSim操作符对多向量模型训练梯度流的影响，揭示单层线性投影在ColBERT等模型中的固有局限性，并寻求通过更先进的投影块设计来改进检索性能。

Method: 设计和系统评估替代投影块，包括深度非线性FFN块、GLU块和跳跃连接等变体，并进行参数消融研究分析影响性能的关键因素。

Result: 许多投影变体优于原始线性投影，最佳变体在跨领域检索基准上平均提升超过2 NDCG@10点，其中放大中间投影和残差连接特别重要。

Conclusion: 替换ColBERT模型的线性层为更复杂的投影块是一个稳健、即插即用的升级方案，能显著提升检索性能，且效果在不同随机种子下保持一致性。

Abstract: Multi-vector dense retrieval methods like ColBERT systematically use a
single-layer linear projection to reduce the dimensionality of individual
vectors. In this study, we explore the implications of the MaxSim operator on
the gradient flows of the training of multi-vector models and show that such a
simple linear projection has inherent, if non-critical, limitations in this
setting. We then discuss the theoretical improvements that could result from
replacing this single-layer projection with well-studied alternative
feedforward linear networks (FFN), such as deeper, non-linear FFN blocks, GLU
blocks, and skip-connections, could alleviate these limitations. Through the
design and systematic evaluation of alternate projection blocks, we show that
better-designed final projections positively impact the downstream performance
of ColBERT models. We highlight that many projection variants outperform the
original linear projections, with the best-performing variants increasing
average performance on a range of retrieval benchmarks across domains by over 2
NDCG@10 points. We then conduct further exploration on the individual
parameters of these projections block in order to understand what drives this
empirical performance, highlighting the particular importance of upscaled
intermediate projections and residual connections. As part of these ablation
studies, we show that numerous suboptimal projection variants still outperform
the traditional single-layer projection across multiple benchmarks, confirming
our hypothesis. Finally, we observe that this effect is consistent across
random seeds, further confirming that replacing the linear layer of ColBERT
models is a robust, drop-in upgrade.

</details>


### [7] [A Hierarchical Quantized Tokenization Framework for Task-Adaptive Graph Representation Learning](https://arxiv.org/abs/2510.12369)
*Yang Xiang,Li Fan,Chenke Yin,Chengtao Ji*

Main category: cs.IR

TL;DR: 提出了一种用于图结构的分层量化框架，通过自加权机制实现任务自适应的多尺度聚合，在保持编码器冻结的同时通过轻量级门控过程调节信息流。


<details>
  <summary>Details</summary>
Motivation: 现有图标记化方法（线性化、连续、量化）在适应性和效率方面存在局限，特别是量化方法以固定或任务无关的方式组织层次信息，无法动态重新加权不同层级的贡献。

Method: 分层量化框架，引入自加权机制进行任务自适应多尺度聚合，保持编码器冻结，通过轻量级门控过程调节信息流。

Result: 在节点分类和链接预测基准数据集上的实验表明，在可比较的计算预算下相对于强基线模型取得了一致的改进。

Conclusion: 该方法实现了参数高效的适应性，能够适应多样化的下游任务，同时保持计算效率。

Abstract: Recent progress in language and vision foundation models demonstrates the
importance of discrete token interfaces that transform complex inputs into
compact sequences for large-scale modeling. Extending this paradigm to graphs
requires a tokenization scheme that handles non-Euclidean structures and
multi-scale dependencies efficiently. Existing approaches to graph
tokenization, linearized, continuous, and quantized, remain limited in
adaptability and efficiency. In particular, most current quantization-based
tokenizers organize hierarchical information in fixed or task-agnostic ways,
which may either over-represent or under-utilize structural cues, and lack the
ability to dynamically reweight contributions from different levels without
retraining the encoder. This work presents a hierarchical quantization
framework that introduces a self-weighted mechanism for task-adaptive
aggregation across multiple scales. The proposed method maintains a frozen
encoder while modulating information flow through a lightweight gating process,
enabling parameter-efficient adaptation to diverse downstream tasks.
Experiments on benchmark datasets for node classification and link prediction
demonstrate consistent improvements over strong baselines under comparable
computational budgets.

</details>


### [8] [Leveraging Language Semantics for Collaborative Filtering with TextGCN and TextGCN-MLP: Zero-Shot vs In-Domain Performance](https://arxiv.org/abs/2510.12461)
*Andrei Chernov,Haroon Wahab,Oleg Novitskij*

Main category: cs.IR

TL;DR: 提出TextGCN方法，直接在LLM生成的物品标题嵌入上应用参数无关的图卷积层，结合语言语义和图消息传递，在零样本推荐中达到最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注微调LLM生成推荐或将LLM嵌入集成到下游模型中，本文探索后一种方向，旨在更好地结合语言语义和图结构。

Method: 使用参数无关的图卷积层直接处理LLM生成的物品标题嵌入，而不是学习传统的基于ID的嵌入。还提出了TextGCN-MLP扩展，使用可训练的多层感知机和对比损失。

Result: TextGCN在零样本推荐中达到最先进性能，显著优于先前方法。TextGCN-MLP在领域内基准测试中达到最佳性能，但零样本性能低于TextGCN。

Conclusion: TextGCN架构通过结合语言语义和图消息传递实现了优秀的零样本推荐性能，但存在领域内专业化和零样本泛化之间的权衡。

Abstract: In recent years, various approaches have been proposed to leverage large
language models (LLMs) for incorporating textual information about items into
recommender systems. Existing methods primarily focus on either fine-tuning
LLMs to generate recommendations or integrating LLM-based embeddings into
downstream models. In this work, we follow the latter direction and propose
\textbf{TextGCN}, which applies parameter-free graph convolution layers
directly over LLM-based item-title embeddings, instead of learning ID-based
embeddings as in traditional methods. By combining language semantics with
graph message passing, this architecture achieves state-of-the-art zero-shot
performance, significantly outperforming prior approaches. Furthermore, we
introduce \textbf{TextGCN-MLP}, which extends TextGCN with a trainable
multilayer perceptron trained using a contrastive loss, achieving
state-of-the-art in-domain performance on recommendation benchmarks. However,
the zero-shot performance of TextGCN-MLP remains lower than that of TextGCN,
highlighting the trade-off between in-domain specialization and zero-shot
generalization. We release our code on github at
\href{https://github.com/ChernovAndrey/TFCE}{github.com/ChernovAndrey/TFCE}.

</details>


### [9] [SMILE: SeMantic Ids Enhanced CoLd Item Representation for Click-through Rate Prediction in E-commerce SEarch](https://arxiv.org/abs/2510.12604)
*Qihang Zhao,Zhongbo Sun,Xiaoyang Zheng,Xian Guo,Siyuan Wang,Zihan Liang,Mingcan Peng,Ben Chen,Chenyi Lei*

Main category: cs.IR

TL;DR: SMILE是一种基于语义ID融合对齐的商品表示增强方法，通过RQ-OPQ编码和两步对齐机制解决冷启动商品推荐中的马太效应问题。


<details>
  <summary>Details</summary>
Motivation: 现代搜索和推荐平台中，冷启动商品缺乏协同信息，加剧了现有商品的马太效应，影响平台多样性。现有方法未能考虑协同与内容之间的不对称性以及商品间的细粒度差异。

Method: 使用RQ-OPQ编码量化商品内容和协同信息，进行两步对齐：RQ编码在商品间传递共享协同信号，OPQ编码学习商品的差异化信息。

Result: 在大规模工业数据集上的离线实验显示SMILE的优越性，在线A/B测试证实了统计显著的改进：商品点击率+1.66%，买家数+1.57%，订单量+2.17%。

Conclusion: SMILE通过融合语义ID对齐有效解决了冷启动商品推荐问题，显著提升了平台性能指标。

Abstract: With the rise of modern search and recommendation platforms, insufficient
collaborative information of cold-start items exacerbates the Matthew effect of
existing platform items, challenging platform diversity and becoming a
longstanding issue. Existing methods align items' side content with
collaborative information to transfer collaborative signals from
high-popularity items to cold-start items. However, these methods fail to
account for the asymmetry between collaboration and content, nor the
fine-grained differences among items. To address these issues, we propose
SMILE, an item representation enhancement approach based on fused alignment of
semantic IDs. Specifically, we use RQ-OPQ encoding to quantize item content and
collaborative information, followed by a two-step alignment: RQ encoding
transfers shared collaborative signals across items, while OPQ encoding learns
differentiated information of items. Comprehensive offline experiments on
large-scale industrial datasets demonstrate superiority of SMILE, and rigorous
online A/B tests confirm statistically significant improvements: item CTR
+1.66%, buyers +1.57%, and order volume +2.17%.

</details>


### [10] [The Role of Parametric Injection-A Systematic Study of Parametric Retrieval-Augmented Generation](https://arxiv.org/abs/2510.12668)
*Minghao Tang,Shiyu Ni,Jingtong Wu,Zengxin Han,Keping Bi*

Main category: cs.IR

TL;DR: 本文系统研究了参数化检索增强生成(PRAG)机制，发现参数化文档仅捕获部分语义信息，单独使用性能不如文本级交互。但参数化表示编码了高层文档信息，与文本文档结合使用时能更有效利用相关信息，提高模型对噪声输入的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管PRAG作为新兴的RAG形式受到关注，但其参数注入机制仍缺乏深入理解。本研究旨在阐明参数注入在PRAG中的作用机制。

Method: 通过系统研究PRAG，分析参数化文档与文本文档的交互效果，比较单独使用参数化文档、文本文档以及两者结合时的性能差异。

Result: 参数化文档仅捕获文档的部分语义信息，单独使用时性能不如文本级交互。但参数化表示编码了高层文档信息，与文本文档结合能更有效利用相关信息，提高对噪声输入的鲁棒性，性能优于单独使用任一来源。

Conclusion: 建议联合使用参数化和文本文档，并提倡增加参数化表示的信息含量以推进PRAG的发展。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
retrieving external documents. As an emerging form of RAG, parametric
retrieval-augmented generation (PRAG) encodes documents as model parameters
(i.e., LoRA modules) and injects these representations into the model during
inference, enabling interaction between the LLM and documents at parametric
level. Compared with directly placing documents in the input context, PRAG is
more efficient and has the potential to offer deeper model-document
interaction. Despite its growing attention, the mechanism underlying parametric
injection remains poorly understood. In this work, we present a systematic
study of PRAG to clarify the role of parametric injection, showing that
parameterized documents capture only partial semantic information of documents,
and relying on them alone yields inferior performance compared to interaction
at text level. However, these parametric representations encode high-level
document information that can enhance the model's understanding of documents
within the input context. When combined parameterized documents with textual
documents, the model can leverage relevant information more effectively and
become more robust to noisy inputs, achieving better performance than either
source alone. We recommend jointly using parameterized and textual documents
and advocate for increasing the information content of parametric
representations to advance PRAG.

</details>


### [11] [SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model](https://arxiv.org/abs/2510.12709)
*Lin Lin,Jiefeng Long,Zhihe Wan,Yuchi Wang,Dingkang Yang,Shuang Yang,Yueyang Yao,Xu Chen,Zirui Guo,Shengqiang Li,Weiran Li,Hanyu Li,Yaling Mou,Yan Qiu,Haiyang Yu,Xiao Liang,Hongsheng Li,Chao Feng*

Main category: cs.IR

TL;DR: SAIL-Embedding是一个全模态嵌入基础模型，通过多阶段训练策略和架构设计解决现有方法在真实应用中的局限性，在检索任务中达到SOTA性能，并在推荐场景中显著提升用户体验指标。


<details>
  <summary>Details</summary>
Motivation: 解决现有多模态嵌入模型在真实世界应用中的挑战，包括有限模态支持、训练机制不稳定和工业领域差距等问题。

Method: 提出多阶段训练方案：内容感知渐进训练增强模型对下游任务的适应性；协作感知推荐增强训练通过知识蒸馏和用户历史兴趣挖掘来适应推荐场景；同时开发随机专业化和数据集驱动模式匹配来增强训练灵活性和泛化性。

Result: 在不同检索任务中达到SOTA性能；在抖音精选场景中实现7天LT增益+0.158%和14天LT增益+0.144%；在抖音feed排序模型中，匹配特征带来+0.08% AUC增益。

Conclusion: SAIL-Embedding通过精心设计的训练策略有效解决了多模态嵌入模型在工业应用中的关键挑战，在多个真实场景中显著提升了推荐系统的性能指标。

Abstract: Multimodal embedding models aim to yield informative unified representations
that empower diverse cross-modal tasks. Despite promising developments in the
evolution from CLIP-based dual-tower architectures to large vision-language
models, prior works still face unavoidable challenges in real-world
applications and business scenarios, such as the limited modality support,
unstable training mechanisms, and industrial domain gaps. In this work, we
introduce SAIL-Embedding, an omni-modal embedding foundation model that
addresses these issues through tailored training strategies and architectural
design. In the optimization procedure, we propose a multi-stage training scheme
to boost the multifaceted effectiveness of representation learning.
Specifically, the content-aware progressive training aims to enhance the
model's adaptability to diverse downstream tasks and master enriched
cross-modal proficiency. The collaboration-aware recommendation enhancement
training further adapts multimodal representations for recommendation scenarios
by distilling knowledge from sequence-to-item and ID-to-item embeddings while
mining user historical interests. Concurrently, we develop the stochastic
specialization and dataset-driven pattern matching to strengthen model training
flexibility and generalizability. Experimental results show that SAIL-Embedding
achieves SOTA performance compared to other methods in different retrieval
tasks. In online experiments across various real-world scenarios integrated
with our model, we observe a significant increase in Lifetime (LT), which is a
crucial indicator for the recommendation experience. For instance, the model
delivers the 7-day LT gain of +0.158% and the 14-day LT gain of +0.144% in the
Douyin-Selected scenario. For the Douyin feed rank model, the match features
produced by SAIL-Embedding yield a +0.08% AUC gain.

</details>
