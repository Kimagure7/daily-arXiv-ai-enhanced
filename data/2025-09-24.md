<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 7]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Understand your Users, An Ensemble Learning Framework for Natural Noise Filtering in Recommender Systems](https://arxiv.org/abs/2509.18560)
*Clarita Hawat,Wissam Al Jurdi,Jacques Bou Abdo,Jacques Demerjian,Abdallah Makhoul*

Main category: cs.IR

TL;DR: 该论文提出了一种新的框架来识别推荐系统中的噪声评分，通过区分外部因素、意外偏好和偶然交互三种现象，使用三层模块化方法提高推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 随着网络内容的指数级增长，推荐系统面临噪声定义的挑战，这些噪声与人类偏好和行为的变异性密切相关，需要区分不同类型的变化现象。

Method: 提出一个模块化框架，包含三层：已知自然噪声算法进行项目分类、集成学习模型进行项目精炼评估、基于签名的噪声识别。同时倡导使用定量评估意外性和群体验证的指标。

Result: 该方法能够提供更清洁的训练数据集，从而提高推荐系统的用户满意度和参与度。

Conclusion: 所提出的框架能够有效识别和减少推荐系统中的噪声，通过提高推荐准确性来增强用户体验。

Abstract: The exponential growth of web content is a major key to the success for
Recommender Systems. This paper addresses the challenge of defining noise,
which is inherently related to variability in human preferences and behaviors.
In classifying changes in user tendencies, we distinguish three kinds of
phenomena: external factors that directly influence users' sentiment,
serendipity causing unexpected preference, and incidental interaction perceived
as noise. To overcome these problems, we present a new framework that
identifies noisy ratings. In this context, the proposed framework is modular,
consisting of three layers: known natural noise algorithms for item
classification, an Ensemble learning model for refined evaluation of the items
and signature-based noise identification. We further advocate the metrics that
quantitatively assess serendipity and group validation, offering higher
robustness in recommendation accuracy. Our approach aims to provide a cleaner
training dataset that would inherently improve user satisfaction and engagement
with Recommender Systems.

</details>


### [2] [The Ranking Blind Spot: Decision Hijacking in LLM-based Text Ranking](https://arxiv.org/abs/2509.18575)
*Yaoyao Qian,Yifan Zeng,Yuchao Jiang,Chelsi Jain,Huazheng Wang*

Main category: cs.IR

TL;DR: 本文研究了LLMs在多文档比较任务中的"排序盲点"问题，揭示了恶意内容提供者如何通过决策目标劫持和决策标准劫持来操纵LLM排序系统。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在信息检索任务中的指令跟随能力如何与多文档比较任务交互，识别LLM决策过程中的排序盲点特性。

Method: 通过两种攻击方法：决策目标劫持（改变成对排序系统的评估目标）和决策标准劫持（修改不同排序方案的相关性标准），分析排序盲点对LLM评估系统的影响。

Result: 实验证明这些攻击在多种LLMs中有效，可推广到多种排序方案，且更强的LLMs更容易受到攻击。

Conclusion: LLMs存在排序盲点漏洞，恶意内容提供者可利用此弱点操纵排序结果获得额外曝光，需要加强LLM排序系统的安全性。

Abstract: Large Language Models (LLMs) have demonstrated strong performance in
information retrieval tasks like passage ranking. Our research examines how
instruction-following capabilities in LLMs interact with multi-document
comparison tasks, identifying what we term the "Ranking Blind Spot", a
characteristic of LLM decision processes during comparative evaluation. We
analyze how this ranking blind spot affects LLM evaluation systems through two
approaches: Decision Objective Hijacking, which alters the evaluation goal in
pairwise ranking systems, and Decision Criteria Hijacking, which modifies
relevance standards across ranking schemes. These approaches demonstrate how
content providers could potentially influence LLM-based ranking systems to
affect document positioning. These attacks aim to force the LLM ranker to
prefer a specific passage and rank it at the top. Malicious content providers
can exploit this weakness, which helps them gain additional exposure by
attacking the ranker. In our experiment, We empirically show that the proposed
attacks are effective in various LLMs and can be generalized to multiple
ranking schemes. We apply these attack to realistic examples to show their
effectiveness. We also found stronger LLMs are more vulnerable to these
attacks. Our code is available at:
https://github.com/blindspotorg/RankingBlindSpot

</details>


### [3] [Agentic AutoSurvey: Let LLMs Survey LLMs](https://arxiv.org/abs/2509.18661)
*Yixin Liu,Yonghui Wu,Denghui Zhang,Lichao Sun*

Main category: cs.IR

TL;DR: Agentic AutoSurvey是一个多智能体框架，用于自动生成文献综述，通过四个专门化智能体的协同工作，在快速发展的科学领域中显著提升了综述质量。


<details>
  <summary>Details</summary>
Motivation: 科学文献的指数级增长给研究人员在快速发展的领域中进行知识综合带来了前所未有的挑战，现有方法存在根本性局限性。

Method: 采用四个专门化智能体（论文搜索专家、主题挖掘与聚类、学术综述撰写者、质量评估者）协同工作，处理每个主题75-443篇论文，通过专门的智能体编排实现高引用覆盖率。

Result: 在六个代表性LLM研究主题上的实验表明，多智能体方法相比现有基线有显著改进，评分达到8.18/10（AutoSurvey为4.77/10），处理了847篇论文。

Conclusion: 多智能体架构代表了在快速发展的科学领域中自动文献综述生成的有意义进展，12维评估捕获了组织、综合整合和批判性分析等超越基本指标的维度。

Abstract: The exponential growth of scientific literature poses unprecedented
challenges for researchers attempting to synthesize knowledge across rapidly
evolving fields. We present \textbf{Agentic AutoSurvey}, a multi-agent
framework for automated survey generation that addresses fundamental
limitations in existing approaches. Our system employs four specialized agents
(Paper Search Specialist, Topic Mining \& Clustering, Academic Survey Writer,
and Quality Evaluator) working in concert to generate comprehensive literature
surveys with superior synthesis quality. Through experiments on six
representative LLM research topics from COLM 2024 categories, we demonstrate
that our multi-agent approach achieves significant improvements over existing
baselines, scoring 8.18/10 compared to AutoSurvey's 4.77/10. The multi-agent
architecture processes 75--443 papers per topic (847 total across six topics)
while targeting high citation coverage (often $\geq$80\% on 75--100-paper sets;
lower on very large sets such as RLHF) through specialized agent orchestration.
Our 12-dimension evaluation captures organization, synthesis integration, and
critical analysis beyond basic metrics. These findings demonstrate that
multi-agent architectures represent a meaningful advancement for automated
literature survey generation in rapidly evolving scientific domains.

</details>


### [4] [Robust Denoising Neural Reranker for Recommender Systems](https://arxiv.org/abs/2509.18736)
*Wenyu Mao,Shuchang Liu,Hailan Yang,Xiaobei Wang,Xiaoyu Yang,Xu Gao,Xiang Li,Lantao Hu,Han Li,Kun Gai,An Zhang,Xiang Wang*

Main category: cs.IR

TL;DR: 本文提出DNR框架，将多阶段推荐系统中的重排序任务建模为检索器分数的去噪问题，通过对抗性训练和三个增强目标来提升重排序性能。


<details>
  <summary>Details</summary>
Motivation: 现有两阶段检索-重排序框架中，检索器分数的重要性未被充分探索，而重排序本质上是从检索器分数中去除噪声的问题。

Method: 提出DNR对抗框架，包含去噪重排序器和噪声生成模块，扩展了传统的分数误差最小化目标，增加了去噪目标、对抗性检索器分数生成目标和分布正则化项。

Result: 在三个公开数据集上的大量实验验证了DNR的有效性。

Conclusion: DNR框架通过将重排序建模为去噪问题，显著提升了多阶段推荐系统的性能。

Abstract: For multi-stage recommenders in industry, a user request would first trigger
a simple and efficient retriever module that selects and ranks a list of
relevant items, then calls a slower but more sophisticated deep reranking model
that refines the item arrangement before exposure to the user. The latter model
typically reranks the item list conditioned on the user's history content and
the initial ranking from retrievers. Although this two-stage retrieval-ranking
framework demonstrates practical effectiveness, the significance of retriever
scores from the previous stage has been limitedly explored, which is
informative. In this work, we first theoretically analyze the limitations of
using retriever scores as the rerankers' input directly and argue that the
reranking task is essentially a noise reduction problem from the retriever
scores. Following this notion, we derive an adversarial framework, DNR, that
associates the denoising reranker with a carefully designed noise generation
module. We extend the conventional score error minimization term with three
augmented objectives, including: 1) a denoising objective that aims to denoise
the noisy retriever scores to align with the user feedback; 2) an adversarial
retriever score generation objective that improves the exploration in the
retriever score space; and 3) a distribution regularization term that aims to
align the distribution of generated noisy retriever scores with the real ones.
Extensive experiments are conducted on three public datasets, together with
analytical support, validating the effectiveness of the proposed DNR.

</details>


### [5] [Single-Branch Network Architectures to Close the Modality Gap in Multimodal Recommendation](https://arxiv.org/abs/2509.18807)
*Christian Ganhör,Marta Moscati,Anna Hausberger,Shah Nawaz,Markus Schedl*

Main category: cs.IR

TL;DR: 该论文提出使用单分支神经网络结合权重共享、模态采样和对比损失的方法，在模态缺失情况下提供准确的推荐，通过缩小模态差距来改进混合推荐系统。


<details>
  <summary>Details</summary>
Motivation: 传统混合推荐系统在多模态学习中，当某些模态缺失时推荐质量会下降。本文旨在解决模态缺失场景下的推荐性能问题。

Method: 采用单分支神经网络架构，结合权重共享、模态采样和对比损失技术，与多分支网络进行对比实验。在三个数据集上进行广泛实验，使用6个准确性指标和4个超准确性指标评估不同训练范式。

Result: 单分支网络在热启动场景中表现竞争力，在模态缺失设置中显著优于其他方法，且能实现嵌入空间中项目模态的更紧密接近。

Conclusion: 单分支网络方法能有效缩小模态差距，在模态缺失情况下提供更准确的推荐，且能实现更好的模态嵌入空间表示。

Abstract: Traditional recommender systems rely on collaborative filtering, using past
user-item interactions to help users discover new items in a vast collection.
In cold start, i.e., when interaction histories of users or items are not
available, content-based recommender systems use side information instead.
Hybrid recommender systems (HRSs) often employ multimodal learning to combine
collaborative and side information, which we jointly refer to as modalities.
Though HRSs can provide recommendations when some modalities are missing, their
quality degrades. In this work, we utilize single-branch neural networks
equipped with weight sharing, modality sampling, and contrastive loss to
provide accurate recommendations even in missing modality scenarios by
narrowing the modality gap. We compare these networks with multi-branch
alternatives and conduct extensive experiments on three datasets. Six
accuracy-based and four beyond-accuracy-based metrics help assess the
recommendation quality for the different training paradigms and their
hyperparameters in warm-start and missing modality scenarios. We quantitatively
and qualitatively study the effects of these different aspects on bridging the
modality gap. Our results show that single-branch networks achieve competitive
performance in warm-start scenarios and are significantly better in missing
modality settings. Moreover, our approach leads to closer proximity of an
item's modalities in the embedding space. Our full experimental setup is
available at https://github.com/hcai-mms/single-branch-networks.

</details>


### [6] [RELATE: Relation Extraction in Biomedical Abstracts with LLMs and Ontology Constraints](https://arxiv.org/abs/2509.19057)
*Olawumi Olasunkanmi,Mathew Satursky,Hong Yi,Chris Bizon,Harlin Lee,Stanley Ahalt*

Main category: cs.IR

TL;DR: RELATE是一个三阶段流水线，将LLM提取的生物医学关系映射到标准化本体谓词，解决LLM输出缺乏标准化和本体对齐的问题。


<details>
  <summary>Details</summary>
Motivation: 生物医学知识图谱对药物发现和临床决策支持至关重要，但往往不完整。LLM擅长提取生物医学关系，但其输出缺乏标准化和本体对齐，限制了知识图谱的集成。

Method: 三阶段流水线：1）本体预处理和谓词嵌入；2）基于相似性的检索（使用SapBERT增强）；3）基于LLM的重排序（包含显式否定处理）。

Result: 在ChemProt基准测试中，RELATE达到52%的精确匹配和94%的准确率@10；在2400篇HEAL项目摘要中，有效拒绝无关关联（0.4%）并识别否定断言。

Conclusion: RELATE通过结合向量搜索和上下文LLM推理，提供了一个可扩展、语义准确的框架，将非结构化生物医学文献转换为标准化知识图谱。

Abstract: Biomedical knowledge graphs (KGs) are vital for drug discovery and clinical
decision support but remain incomplete. Large language models (LLMs) excel at
extracting biomedical relations, yet their outputs lack standardization and
alignment with ontologies, limiting KG integration. We introduce RELATE, a
three-stage pipeline that maps LLM-extracted relations to standardized ontology
predicates using ChemProt and the Biolink Model. The pipeline includes: (1)
ontology preprocessing with predicate embeddings, (2) similarity-based
retrieval enhanced with SapBERT, and (3) LLM-based reranking with explicit
negation handling. This approach transforms relation extraction from free-text
outputs to structured, ontology-constrained representations. On the ChemProt
benchmark, RELATE achieves 52% exact match and 94% accuracy@10, and in 2,400
HEAL Project abstracts, it effectively rejects irrelevant associations (0.4%)
and identifies negated assertions. RELATE captures nuanced biomedical
relationships while ensuring quality for KG augmentation. By combining vector
search with contextual LLM reasoning, RELATE provides a scalable, semantically
accurate framework for converting unstructured biomedical literature into
standardized KGs.

</details>


### [7] [A Knowledge Graph and a Tripartite Evaluation Framework Make Retrieval-Augmented Generation Scalable and Transparent](https://arxiv.org/abs/2509.19209)
*Olalekan K. Akindele,Bhupesh Kumar Mishra,Kenneth Y. Wertheim*

Main category: cs.IR

TL;DR: 该研究提出了一个结合知识图谱和向量搜索的RAG聊天机器人，并开发了RAG-Eval评估框架来评估RAG应用的质量，通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在领域特定应用中存在的事实不一致性和准确性挑战，特别是在处理大规模工程邮件数据时，需要设计有效的聊天机器人评估方法。

Method: 采用检索增强生成(RAG)方法，结合知识图谱和向量搜索检索技术，开发了RAG-Eval评估框架，该框架基于链式思维LLM，从查询相关性、事实准确性、覆盖度、连贯性和流畅性等多个维度进行评估。

Result: 实验表明RAG-Eval在摘要评估任务中优于BERTScore和G-EVAL，能够可靠地检测事实差距和查询不匹配问题，为用户提供1-100%的置信度评分。

Conclusion: 该方法为开发准确、用户可验证的聊天机器人提供了可扩展的路径，弥合了高水平对话流畅性和事实准确性之间的差距。

Abstract: Large Language Models (LLMs) have significantly enhanced conversational
Artificial Intelligence(AI) chatbots; however, domain-specific accuracy and the
avoidance of factual inconsistencies remain pressing challenges, particularly
for large datasets. Designing an effective chatbot with appropriate methods and
evaluating its effectiveness is among the challenges in this domain. This study
presents a Retrieval Augmented Generation (RAG) chatbot that harnesses a
knowledge graph and vector search retrieval to deliver precise, context-rich
responses in an exemplary use case from over high-volume engineering
project-related emails, thereby minimising the need for document chunking. A
central innovation of this work is the introduction of RAG Evaluation
(RAG-Eval), a novel chain-of-thought LLM-based tripartite evaluation framework
specifically developed to assess RAG applications. This framework operates in
parallel with the chatbot, jointly assessing the user's query, the retrieved
document, and the generated response, enabling a holistic evaluation across
multiple quality metrics like query relevance, factual accuracy, coverage,
coherence and fluency. The resulting scoring system is provided directly to
users as a confidence score (1 to 100%), enabling quick identification of
possible misaligned or incomplete answers. This proposed approach promotes
transparency and rapid verification by incorporating metadata email IDs,
timestamps into responses. Experimental comparisons against BERTScore and
G-EVAL for summarisation evaluation tasks confirm its effectiveness, and
empirical analysis also shows RAG-Eval reliably detects factual gaps and query
mismatches, thereby fostering trust in high demand, data centric environments.
These findings highlight a scalable path for developing accurate,
user-verifiable chatbots that bridge the gap between high-level conversational
fluency and factual accuracy.

</details>
