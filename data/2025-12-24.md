<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 7]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Towards Analysing Invoices and Receipts with Amazon Textract](https://arxiv.org/abs/2512.19958)
*Sneha Oommen,Gabby Sanchez,Cassandra T. Britto,Di Wang,Jordan Chiou,Maria Spichkova*

Main category: cs.IR

TL;DR: 评估AWS Textract在收据数据提取中的表现，分析其功能、优缺点，并提出改进策略


<details>
  <summary>Details</summary>
Motivation: 评估AWS Textract在真实场景中处理收据数据提取的能力，了解其在处理不同格式和条件收据时的表现，识别实际应用中的限制和问题

Method: 使用包含多种格式和条件的收据数据集，对AWS Textract功能进行定性分析，评估其数据提取能力，特别关注图像质量和布局对结果的影响

Result: Textract在提取收据总额方面表现一致，但存在典型问题和异常，这些问题通常受图像质量和布局影响，需要针对性的缓解策略

Conclusion: AWS Textract在收据数据提取中具有实用价值，特别是在总额检测方面表现稳定，但需要针对图像质量和布局问题制定缓解策略以提高整体性能

Abstract: This paper presents an evaluation of the AWS Textract in the context of extracting data from receipts. We analyse Textract functionalities using a dataset that includes receipts of varied formats and conditions. Our analysis provided a qualitative view of Textract strengths and limitations. While the receipts totals were consistently detected, we also observed typical issues and irregularities that were often influenced by image quality and layout. Based on the analysis of the observations, we propose mitigation strategies.

</details>


### [2] [IGDMRec: Behavior Conditioned Item Graph Diffusion for Multimodal Recommendation](https://arxiv.org/abs/2512.19983)
*Ziyuan Guo,Jie Guo,Zhenghao Chen,Bin Song,Fei Richard Yu*

Main category: cs.IR

TL;DR: 提出IGDMRec方法，利用扩散模型和分类器自由引导，通过整合用户行为信息来去噪语义物品图，提升多模态推荐系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于结构的MRS通过构建语义物品图取得了SOTA性能，但这类图存在噪声问题：1) 多模态信息固有噪声；2) 物品语义与用户-物品共现关系不对齐，导致虚假链接和次优推荐。

Method: 提出IGDMRec方法，包含三个核心组件：1) 行为条件图扩散(BGD)模块，将交互数据作为条件信息指导语义物品图去噪；2) 条件去噪网络(CD-Net)，以可管理复杂度实现去噪过程；3) 对比表示增强方案，利用去噪图和原始图增强物品表示。

Result: 在四个真实世界数据集上的广泛实验表明，IGDMRec优于竞争基线，鲁棒性分析验证了其去噪能力，消融研究验证了关键组件的有效性。

Conclusion: IGDMRec通过扩散模型整合用户行为信息有效去噪语义物品图，提升了多模态推荐系统的性能，为解决语义物品图噪声问题提供了新思路。

Abstract: Multimodal recommender systems (MRSs) are critical for various online platforms, offering users more accurate personalized recommendations by incorporating multimodal information of items. Structure-based MRSs have achieved state-of-the-art performance by constructing semantic item graphs, which explicitly model relationships between items based on modality feature similarity. However, such semantic item graphs are often noisy due to 1) inherent noise in multimodal information and 2) misalignment between item semantics and user-item co-occurrence relationships, which introduces false links and leads to suboptimal recommendations. To address this challenge, we propose Item Graph Diffusion for Multimodal Recommendation (IGDMRec), a novel method that leverages a diffusion model with classifier-free guidance to denoise the semantic item graph by integrating user behavioral information. Specifically, IGDMRec introduces a Behavior-conditioned Graph Diffusion (BGD) module, incorporating interaction data as conditioning information to guide the denoising of the semantic item graph. Additionally, a Conditional Denoising Network (CD-Net) is designed to implement the denoising process with manageable complexity. Finally, we propose a contrastive representation augmentation scheme that leverages both the denoised item graph and the original item graph to enhance item representations. \LL{Extensive experiments on four real-world datasets demonstrate the superiority of IGDMRec over competitive baselines, with robustness analysis validating its denoising capability and ablation studies verifying the effectiveness of its key components.

</details>


### [3] [LLM-Assisted Abstract Screening with OLIVER: Evaluating Calibration and Single-Model vs. Actor-Critic Configurations in Literature Reviews](https://arxiv.org/abs/2512.20022)
*Kian Godhwani,David Benrimoh*

Main category: cs.IR

TL;DR: OLIVER是一个用于LLM辅助文献筛选的开源工具，评估发现单模型性能受综述特性影响大且校准差，而actor-critic框架能显著提升分类质量和置信度校准。


<details>
  <summary>Details</summary>
Motivation: 现有LLM辅助文献筛选研究主要关注早期模型、标准化Cochrane综述和单一模型设置，缺乏对泛化性、配置效果和校准的全面评估。

Method: 开发OLIVER开源流水线，评估多个当代LLM在两个非Cochrane系统综述中的表现，使用准确率、AUC和校准指标，并测试结合两个轻量级模型的actor-critic筛选框架。

Result: 单模型性能差异大：小综述中模型敏感度高但假阳性多、校准差；大综述中特异性高但召回率低。单模型校准普遍弱，而actor-critic框架显著提升区分度和校准，AUC更高。

Conclusion: LLM有潜力加速文献筛选，但单模型性能受综述特性和提示设计影响大且校准有限。Actor-critic框架能改善分类质量和置信度可靠性，同时保持计算效率，实现低成本大规模筛选。

Abstract: Introduction: Recent work suggests large language models (LLMs) can accelerate screening, but prior evaluations focus on earlier LLMs, standardized Cochrane reviews, single-model setups, and accuracy as the primary metric, leaving generalizability, configuration effects, and calibration largely unexamined.
  Methods: We developed OLIVER (Optimized LLM-based Inclusion and Vetting Engine for Reviews), an open-source pipeline for LLM-assisted abstract screening. We evaluated multiple contemporary LLMs across two non-Cochrane systematic reviews and performance was assessed at both the full-text screening and final inclusion stages using accuracy, AUC, and calibration metrics. We further tested an actor-critic screening framework combining two lightweight models under three aggregation rules.
  Results: Across individual models, performance varied widely. In the smaller Review 1 (821 abstracts, 63 final includes), several models achieved high sensitivity for final includes but at the cost of substantial false positives and poor calibration. In the larger Review 2 (7741 abstracts, 71 final includes), most models were highly specific but struggled to recover true includes, with prompt design influencing recall. Calibration was consistently weak across single-model configurations despite high overall accuracy. Actor-critic screening improved discrimination and markedly reduced calibration error in both reviews, yielding higher AUCs.
  Discussion: LLMs may eventually accelerate abstract screening, but single-model performance is highly sensitive to review characteristics, prompting, and calibration is limited. An actor-critic framework improves classification quality and confidence reliability while remaining computationally efficient, enabling large-scale screening at low cost.

</details>


### [4] [VSA:Visual-Structural Alignment for UI-to-Code](https://arxiv.org/abs/2512.20034)
*Xian Wu,Ming Zhang,Zhiyu Fang,Fei Li,Bin Wang,Yong Jiang,Hao Zhou*

Main category: cs.IR

TL;DR: VSA提出多阶段视觉-结构对齐范式，将UI设计图转换为模块化、类型安全的前端组件代码，解决现有设计到代码转换方法生成非结构化、低内聚高耦合代码的问题。


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型的设计到代码转换方法主要生成非结构化、扁平化的代码库，缺乏与React或Angular等组件化库的兼容性，导致代码内聚性低、耦合度高，难以长期维护。

Method: 采用多阶段视觉-结构对齐范式：1) 使用空间感知transformer将视觉输入重建为层次化树状表示；2) 集成算法模式匹配层识别重复UI模式并封装为模块化模板；3) 通过模式驱动的合成引擎，确保LLM生成类型安全、支持属性传递的生产级组件。

Result: 实验结果表明，该框架在代码模块化和架构一致性方面相比最先进基准有显著提升，有效弥合了原始像素与可扩展软件工程之间的差距。

Conclusion: VSA通过视觉-结构对齐的多阶段方法，能够生成组织良好的前端资产，解决了现有设计到代码转换方法在组件化、可维护性方面的不足，为自动化UI开发提供了更实用的解决方案。

Abstract: The automation of user interface development has the potential to accelerate software delivery by mitigating intensive manual implementation. Despite the advancements in Large Multimodal Models for design-to-code translation, existing methodologies predominantly yield unstructured, flat codebases that lack compatibility with component-oriented libraries such as React or Angular. Such outputs typically exhibit low cohesion and high coupling, complicating long-term maintenance. In this paper, we propose \textbf{VSA (VSA)}, a multi-stage paradigm designed to synthesize organized frontend assets through visual-structural alignment. Our approach first employs a spatial-aware transformer to reconstruct the visual input into a hierarchical tree representation. Moving beyond basic layout extraction, we integrate an algorithmic pattern-matching layer to identify recurring UI motifs and encapsulate them into modular templates. These templates are then processed via a schema-driven synthesis engine, ensuring the Large Language Model generates type-safe, prop-drilled components suitable for production environments. Experimental results indicate that our framework yields a substantial improvement in code modularity and architectural consistency over state-of-the-art benchmarks, effectively bridging the gap between raw pixels and scalable software engineering.

</details>


### [5] [Collaborative Group-Aware Hashing for Fast Recommender Systems](https://arxiv.org/abs/2512.20172)
*Yan Zhang,Li Deng,Lixin Duan,Ivor W. Tsang,Guowu Yang*

Main category: cs.IR

TL;DR: 提出CGAH方法，通过整合用户和物品的固有分组信息来缓解稀疏性问题，提升哈希推荐系统的准确率


<details>
  <summary>Details</summary>
Motivation: 现有基于哈希的推荐方法在稀疏场景下准确率低，主要原因是每个比特表示能力有限，且忽略了用户和物品之间的固有关系

Method: 提出协作分组感知哈希(CGAH)方法：1) 将用户和物品的潜在向量分类到不同组中提取固有分组亲和力；2) 将偏好建模为分组亲和力与哈希码相似度的内积；3) 通过学习包含固有分组信息的哈希码来提升稀疏交互数据下的效果

Result: 在三个公共数据集上的实验表明，CGAH和CGAH-CF在不同稀疏设置下均优于最先进的离散协作过滤方法和离散内容感知推荐方法

Conclusion: 通过整合固有分组信息，CGAH方法能够生成比现有离散方法更有效的哈希码，显著提升了稀疏场景下的推荐准确率

Abstract: The fast online recommendation is critical for applications with large-scale databases; meanwhile, it is challenging to provide accurate recommendations in sparse scenarios. Hash technique has shown its superiority for speeding up the online recommendation by bit operations on Hamming distance computations. However, existing hashing-based recommendations suffer from low accuracy, especially with sparse settings, due to the limited representation capability of each bit and neglected inherent relations among users and items. To this end, this paper lodges a Collaborative Group-Aware Hashing (CGAH) method for both collaborative filtering (namely CGAH-CF) and content-aware recommendations (namely CGAH) by integrating the inherent group information to alleviate the sparse issue. Firstly, we extract inherent group affinities of users and items by classifying their latent vectors into different groups. Then, the preference is formulated as the inner product of the group affinity and the similarity of hash codes. By learning hash codes with the inherent group information, CGAH obtains more effective hash codes than other discrete methods with sparse interactive data. Extensive experiments on three public datasets show the superior performance of our proposed CGAH and CGAH-CF over the state-of-the-art discrete collaborative filtering methods and discrete content-aware recommendations under different sparse settings.

</details>


### [6] [Laser: Governing Long-Horizon Agentic Search via Structured Protocol and Context Register](https://arxiv.org/abs/2512.20458)
*Shuting Wang,Qiaolin Xia,Hao Wang,Yu Lu,Bobsimons,Zhicheng Dou*

Main category: cs.IR

TL;DR: Laser是一个用于稳定和扩展代理搜索的通用框架，通过符号化动作协议和紧凑上下文寄存器解决现有代理搜索系统的不稳定推理轨迹和上下文溢出问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型和大推理模型的代理搜索系统主要依赖非结构化的自然语言推理，在上下文中积累原始中间轨迹，导致推理轨迹不稳定、上下文溢出，以及在复杂多跳查询上性能下降。

Method: Laser定义了符号化动作协议，将代理行为组织为三个空间：规划、任务解决和反思。每个动作都有明确的语义和确定性执行格式，支持结构化逻辑推理过程和可靠的动作解析。同时维护紧凑的上下文寄存器，只存储推理过程的关键状态。

Result: 在Qwen2.5/3系列模型上，在具有挑战性的多跳QA数据集上的实验表明，Laser在仅提示和微调设置下都持续优于现有的代理搜索基线方法。

Conclusion: Laser为稳健、可扩展的代理搜索提供了一个原则性和有效的基础，通过结构化推理和紧凑上下文管理解决了现有方法的局限性。

Abstract: Recent advances in Large Language Models (LLMs) and Large Reasoning Models (LRMs) have enabled agentic search systems that interleave multi-step reasoning with external tool use. However, existing frameworks largely rely on unstructured natural-language reasoning and accumulate raw intermediate traces in the context, which often leads to unstable reasoning trajectories, context overflow, and degraded performance on complex multi-hop queries. In this study, we introduce Laser, a general framework for stabilizing and scaling agentic search. Laser defines a symbolic action protocol that organizes agent behaviors into three spaces: planning, task-solving, and retrospection. Each action is specified with explicit semantics and a deterministic execution format, enabling structured and logical reasoning processes and reliable action parsing. This design makes intermediate decisions interpretable and traceable, enhancing explicit retrospection and fine-grained control over reasoning trajectories. In coordination with parsable actions, Laser further maintains a compact context register that stores only essential states of the reasoning process, allowing the agent to reason over long horizons without uncontrolled context expansion. Experiments on Qwen2.5/3-series models across challenging multi-hop QA datasets show that Laser consistently outperforms existing agentic search baselines under both prompting-only and fine-tuning settings, demonstrating that Laser provides a principled and effective foundation for robust, scalable agentic search.

</details>


### [7] [Making Large Language Models Efficient Dense Retrievers](https://arxiv.org/abs/2512.20612)
*Yibin Lei,Shwai He,Ang Li,Andrew Yates*

Main category: cs.IR

TL;DR: EffiR框架通过粗粒度深度缩减和细粒度宽度缩减压缩LLM检索器中的MLP层，在保持性能的同时大幅减少模型大小和推理成本。


<details>
  <summary>Details</summary>
Motivation: 虽然直接微调大型语言模型进行密集检索效果很好，但参数量大导致计算效率低。先前研究显示LLM在生成任务中存在层冗余，但尚不清楚检索任务中是否存在类似冗余，因为检索需要将整个序列编码为固定表示而非迭代生成标记。

Method: 提出EffiR框架：首先分析LLM检索器中的层冗余，发现MLP层更可剪枝而注意力层对语义聚合更关键；然后采用粗到细策略进行大规模MLP压缩（粗粒度深度缩减后接细粒度宽度缩减），并结合检索特定的微调。

Result: 在多种BEIR数据集和LLM骨干网络上，EffiR实现了模型大小和推理成本的大幅减少，同时保持了完整尺寸模型的性能。

Conclusion: LLM检索器存在与生成任务不同的层冗余模式，MLP层更可压缩而注意力层更关键；EffiR框架能有效开发高效检索器，在保持性能的同时显著提升效率。

Abstract: Recent work has shown that directly fine-tuning large language models (LLMs) for dense retrieval yields strong performance, but their substantial parameter counts make them computationally inefficient. While prior studies have revealed significant layer redundancy in LLMs for generative tasks, it remains unclear whether similar redundancy exists when these models are adapted for retrieval tasks, which require encoding entire sequences into fixed representations rather than generating tokens iteratively. To this end, we conduct a comprehensive analysis of layer redundancy in LLM-based dense retrievers. We find that, in contrast to generative settings, MLP layers are substantially more prunable, while attention layers remain critical for semantic aggregation. Building on this insight, we propose EffiR, a framework for developing efficient retrievers that performs large-scale MLP compression through a coarse-to-fine strategy (coarse-grained depth reduction followed by fine-grained width reduction), combined with retrieval-specific fine-tuning. Across diverse BEIR datasets and LLM backbones, EffiR achieves substantial reductions in model size and inference cost while preserving the performance of full-size models.

</details>
