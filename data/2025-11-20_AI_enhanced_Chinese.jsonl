{"id": "2511.14763", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14763", "abs": "https://arxiv.org/abs/2511.14763", "authors": ["Li Cuihong", "Huang Xiaowen", "Yin Chuanhuan", "Sang Jitao"], "title": "Membership Inference Attack against Large Language Model-based Recommendation Systems: A New Distillation-based Paradigm", "comment": null, "summary": "Membership Inference Attack (MIA) aims to determine if a data sample is used in the training dataset of a target model. Traditional MIA obtains feature of target model via shadow models and uses the feature to train attack model, but the scale and complexity of training or fine-tuning data for large language model (LLM)-based recommendation systems make shadow models difficult to construct. Knowledge distillation as a method for extracting knowledge contributes to construct a stronger reference model. Knowledge distillation enables separate distillation for member and non-member data during the distillation process, enhancing the model's discriminative capability between the two in MIA. This paper propose a knowledge distillation-based MIA paradigm to improve the performance of membership inference attacks on LLM-based recommendation systems. Our paradigm introduces knowledge distillation to obtain a reference model, which enhances the reference model's ability to distinguish between member and non-member data. We obtain individual features from the reference model and train our attack model with fused feature. Our paradigm improves the attack performance of MIA compared to shadow model-based attack.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u6210\u5458\u63a8\u7406\u653b\u51fb\u8303\u5f0f\uff0c\u7528\u4e8e\u63d0\u5347\u5bf9\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u8350\u7cfb\u7edf\u7684\u653b\u51fb\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfMIA\u901a\u8fc7\u5f71\u5b50\u6a21\u578b\u83b7\u53d6\u76ee\u6807\u6a21\u578b\u7279\u5f81\uff0c\u4f46LLM\u63a8\u8350\u7cfb\u7edf\u7684\u8bad\u7ec3\u6570\u636e\u89c4\u6a21\u548c\u590d\u6742\u6027\u4f7f\u5f97\u5f71\u5b50\u6a21\u578b\u96be\u4ee5\u6784\u5efa\u3002\u77e5\u8bc6\u84b8\u998f\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u5f3a\u7684\u53c2\u8003\u6a21\u578b\u3002", "method": "\u5f15\u5165\u77e5\u8bc6\u84b8\u998f\u83b7\u53d6\u53c2\u8003\u6a21\u578b\uff0c\u5728\u84b8\u998f\u8fc7\u7a0b\u4e2d\u5206\u522b\u5bf9\u6210\u5458\u548c\u975e\u6210\u5458\u6570\u636e\u8fdb\u884c\u84b8\u998f\uff0c\u589e\u5f3a\u6a21\u578b\u5bf9\u4e24\u8005\u7684\u533a\u5206\u80fd\u529b\u3002\u4ece\u53c2\u8003\u6a21\u578b\u83b7\u53d6\u4e2a\u4f53\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u878d\u5408\u7279\u5f81\u8bad\u7ec3\u653b\u51fb\u6a21\u578b\u3002", "result": "\u76f8\u6bd4\u57fa\u4e8e\u5f71\u5b50\u6a21\u578b\u7684\u653b\u51fb\uff0c\u8be5\u8303\u5f0f\u63d0\u9ad8\u4e86MIA\u7684\u653b\u51fb\u6027\u80fd\u3002", "conclusion": "\u77e5\u8bc6\u84b8\u998f\u80fd\u591f\u6709\u6548\u63d0\u5347\u5bf9LLM\u63a8\u8350\u7cfb\u7edf\u7684\u6210\u5458\u63a8\u7406\u653b\u51fb\u6548\u679c\u3002"}}
{"id": "2511.14764", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14764", "abs": "https://arxiv.org/abs/2511.14764", "authors": ["Mariya Hendriksen", "Svitlana Vakulenko", "Jordan Massiah", "Gabriella Kazai", "Emine Yilmaz"], "title": "Image-Seeking Intent Prediction for Cross-Device Product Search", "comment": "Oral at RecSys Gen AI for E-commerce 2025", "summary": "Large Language Models (LLMs) are transforming personalized search, recommendations, and customer interaction in e-commerce. Customers increasingly shop across multiple devices, from voice-only assistants to multimodal displays, each offering different input and output capabilities. A proactive suggestion to switch devices can greatly improve the user experience, but it must be offered with high precision to avoid unnecessary friction. We address the challenge of predicting when a query requires visual augmentation and a cross-device switch to improve product discovery. We introduce Image-Seeking Intent Prediction, a novel task for LLM-driven e-commerce assistants that anticipates when a spoken product query should proactively trigger a visual on a screen-enabled device. Using large-scale production data from a multi-device retail assistant, including 900K voice queries, associated product retrievals, and behavioral signals such as image carousel engagement, we train IRP (Image Request Predictor), a model that leverages user input query and corresponding retrieved product metadata to anticipate visual intent. Our experiments show that combining query semantics with product data, particularly when improved through lightweight summarization, consistently improves prediction accuracy. Incorporating a differentiable precision-oriented loss further reduces false positives. These results highlight the potential of LLMs to power intelligent, cross-device shopping assistants that anticipate and adapt to user needs, enabling more seamless and personalized e-commerce experiences.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u56fe\u50cf\u5bfb\u6c42\u610f\u56fe\u9884\u6d4b\u4efb\u52a1\uff0c\u4f7f\u7528LLM\u9a71\u52a8\u7684\u7535\u5b50\u5546\u52a1\u52a9\u624b\u9884\u6d4b\u4f55\u65f6\u8bed\u97f3\u4ea7\u54c1\u67e5\u8be2\u9700\u8981\u4e3b\u52a8\u89e6\u53d1\u5c4f\u5e55\u8bbe\u5907\u7684\u89c6\u89c9\u589e\u5f3a\uff0c\u901a\u8fc7\u7ed3\u5408\u67e5\u8be2\u8bed\u4e49\u548c\u4ea7\u54c1\u6570\u636e\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740\u7528\u6237\u5728\u591a\u79cd\u8bbe\u5907\u4e0a\u8d2d\u7269\uff0c\u4e3b\u52a8\u5efa\u8bae\u5207\u6362\u5230\u5177\u6709\u89c6\u89c9\u529f\u80fd\u7684\u8bbe\u5907\u53ef\u4ee5\u663e\u8457\u6539\u5584\u7528\u6237\u4f53\u9a8c\uff0c\u4f46\u9700\u8981\u9ad8\u7cbe\u5ea6\u4ee5\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u5e72\u6270\u3002", "method": "\u8bad\u7ec3IRP\u6a21\u578b\uff0c\u5229\u7528\u7528\u6237\u8f93\u5165\u67e5\u8be2\u548c\u68c0\u7d22\u5230\u7684\u4ea7\u54c1\u5143\u6570\u636e\u6765\u9884\u6d4b\u89c6\u89c9\u610f\u56fe\uff0c\u7ed3\u5408\u67e5\u8be2\u8bed\u4e49\u548c\u4ea7\u54c1\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6458\u8981\u6539\u8fdb\uff0c\u4f7f\u7528\u9762\u5411\u7cbe\u5ea6\u7684\u53ef\u5fae\u5206\u635f\u5931\u51fd\u6570\u51cf\u5c11\u8bef\u62a5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u67e5\u8be2\u8bed\u4e49\u548c\u4ea7\u54c1\u6570\u636e\u80fd\u6301\u7eed\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6458\u8981\u6539\u8fdb\u540e\uff0c\u4f7f\u7528\u7cbe\u5ea6\u5bfc\u5411\u635f\u5931\u8fdb\u4e00\u6b65\u51cf\u5c11\u5047\u9633\u6027\u3002", "conclusion": "LLM\u6709\u6f5c\u529b\u9a71\u52a8\u667a\u80fd\u8de8\u8bbe\u5907\u8d2d\u7269\u52a9\u624b\uff0c\u9884\u6d4b\u5e76\u9002\u5e94\u7528\u6237\u9700\u6c42\uff0c\u5b9e\u73b0\u66f4\u65e0\u7f1d\u548c\u4e2a\u6027\u5316\u7684\u7535\u5b50\u5546\u52a1\u4f53\u9a8c\u3002"}}
{"id": "2511.14765", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14765", "abs": "https://arxiv.org/abs/2511.14765", "authors": ["Mohammad Usman Altam", "Md Imtiaz Habib", "Tuan Hoang"], "title": "Optimizing Agricultural Research: A RAG-Based Approach to Mycorrhizal Fungi Information", "comment": "10 pages, 4 figures, 1 table", "summary": "Retrieval-Augmented Generation (RAG) represents a transformative approach within natural language processing (NLP), combining neural information retrieval with generative language modeling to enhance both contextual accuracy and factual reliability of responses. Unlike conventional Large Language Models (LLMs), which are constrained by static training corpora, RAG-powered systems dynamically integrate domain-specific external knowledge sources, thereby overcoming temporal and disciplinary limitations. In this study, we present the design and evaluation of a RAG-enabled system tailored for Mycophyto, with a focus on advancing agricultural applications related to arbuscular mycorrhizal fungi (AMF). These fungi play a critical role in sustainable agriculture by enhancing nutrient acquisition, improving plant resilience under abiotic and biotic stresses, and contributing to soil health. Our system operationalizes a dual-layered strategy: (i) semantic retrieval and augmentation of domain-specific content from agronomy and biotechnology corpora using vector embeddings, and (ii) structured data extraction to capture predefined experimental metadata such as inoculation methods, spore densities, soil parameters, and yield outcomes. This hybrid approach ensures that generated responses are not only semantically aligned but also supported by structured experimental evidence. To support scalability, embeddings are stored in a high-performance vector database, allowing near real-time retrieval from an evolving literature base. Empirical evaluation demonstrates that the proposed pipeline retrieves and synthesizes highly relevant information regarding AMF interactions with crop systems, such as tomato (Solanum lycopersicum). The framework underscores the potential of AI-driven knowledge discovery to accelerate agroecological innovation and enhance decision-making in sustainable farming systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u9488\u5bf9Mycophyto\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7cfb\u7edf\uff0c\u4e13\u6ce8\u4e8e\u4e1b\u679d\u83cc\u6839\u771f\u83cc(AMF)\u7684\u519c\u4e1a\u5e94\u7528\uff0c\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u68c0\u7d22\u548c\u7ed3\u6784\u5316\u6570\u636e\u63d0\u53d6\uff0c\u63d0\u9ad8\u519c\u4e1a\u77e5\u8bc6\u53d1\u73b0\u548c\u51b3\u7b56\u652f\u6301\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u4f20\u7edf\u5927\u8bed\u8a00\u6a21\u578b\u53d7\u9650\u4e8e\u9759\u6001\u8bad\u7ec3\u8bed\u6599\uff0c\u65e0\u6cd5\u52a8\u6001\u6574\u5408\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u3002RAG\u7cfb\u7edf\u80fd\u591f\u514b\u670d\u65f6\u95f4\u548c\u5b66\u79d1\u9650\u5236\uff0c\u4e3a\u53ef\u6301\u7eed\u519c\u4e1a\u4e2d\u7684AMF\u5e94\u7528\u63d0\u4f9b\u51c6\u786e\u53ef\u9760\u7684\u77e5\u8bc6\u652f\u6301\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u7b56\u7565\uff1a(1)\u4f7f\u7528\u5411\u91cf\u5d4c\u5165\u4ece\u519c\u5b66\u548c\u751f\u7269\u6280\u672f\u8bed\u6599\u5e93\u4e2d\u8fdb\u884c\u8bed\u4e49\u68c0\u7d22\u548c\u589e\u5f3a\uff1b(2)\u7ed3\u6784\u5316\u6570\u636e\u63d0\u53d6\uff0c\u6355\u83b7\u9884\u5b9a\u4e49\u7684\u5b9e\u9a8c\u5143\u6570\u636e\u5982\u63a5\u79cd\u65b9\u6cd5\u3001\u5b62\u5b50\u5bc6\u5ea6\u3001\u571f\u58e4\u53c2\u6570\u548c\u4ea7\u91cf\u7ed3\u679c\u3002\u5d4c\u5165\u5b58\u50a8\u5728\u9ad8\u6548\u5411\u91cf\u6570\u636e\u5e93\u4e2d\uff0c\u652f\u6301\u8fd1\u5b9e\u65f6\u68c0\u7d22\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u7ba1\u9053\u80fd\u591f\u68c0\u7d22\u548c\u5408\u6210\u5173\u4e8eAMF\u4e0e\u4f5c\u7269\u7cfb\u7edf(\u5982\u756a\u8304)\u76f8\u4e92\u4f5c\u7528\u7684\u9ad8\u5ea6\u76f8\u5173\u4fe1\u606f\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u519c\u4e1a\u77e5\u8bc6\u53d1\u73b0\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5f3a\u8c03\u4e86AI\u9a71\u52a8\u77e5\u8bc6\u53d1\u73b0\u5728\u52a0\u901f\u519c\u4e1a\u751f\u6001\u521b\u65b0\u548c\u589e\u5f3a\u53ef\u6301\u7eed\u519c\u4e1a\u7cfb\u7edf\u51b3\u7b56\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u519c\u4e1a\u9886\u57df\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u77e5\u8bc6\u589e\u5f3a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.14766", "categories": ["cs.IR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.14766", "abs": "https://arxiv.org/abs/2511.14766", "authors": ["Yang Li", "Yajiao Wang", "Wenhao Hu", "Zhixiong Zhang", "Mengting Zhang"], "title": "OTCR: Optimal Transmission, Compression and Representation for Multimodal Information Extraction", "comment": "5 pages, 3 figures", "summary": "Multimodal Information Extraction (MIE) requires fusing text and visual cues from visually rich documents. While recent methods have advanced multimodal representation learning, most implicitly assume modality equivalence or treat modalities in a largely uniform manner, still relying on generic fusion paradigms. This often results in indiscriminate incorporation of multimodal signals and insufficient control over task-irrelevant redundancy, which may in turn limit generalization. We revisit MIE from a task-centric view: text should dominate, vision should selectively support. We present OTCR, a two-stage framework. First, Cross-modal Optimal Transport (OT) yields sparse, probabilistic alignments between text tokens and visual patches, with a context-aware gate controlling visual injection. Second, a Variational Information Bottleneck (VIB) compresses fused features, filtering task-irrelevant noise to produce compact, task-adaptive representations. On FUNSD, OTCR achieves 91.95% SER and 91.13% RE, while on XFUND (ZH), it reaches 91.09% SER and 94.20% RE, demonstrating competitive performance across datasets. Feature-level analyses further confirm reduced modality redundancy and strengthened task signals. This work offers an interpretable, information-theoretic paradigm for controllable multimodal fusion in document AI.", "AI": {"tldr": "OTCR\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u591a\u6a21\u6001\u4fe1\u606f\u63d0\u53d6\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u6700\u4f18\u4f20\u8f93\u548c\u53d8\u5206\u4fe1\u606f\u74f6\u9888\u5b9e\u73b0\u6587\u672c\u4e3b\u5bfc\u3001\u89c6\u89c9\u9009\u62e9\u6027\u652f\u6301\u7684\u878d\u5408\u7b56\u7565\uff0c\u5728\u6587\u6863AI\u4e2d\u53d6\u5f97\u4e86\u7ade\u4e89\u6027\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9690\u542b\u5047\u8bbe\u6a21\u6001\u7b49\u4ef7\u6216\u91c7\u7528\u7edf\u4e00\u878d\u5408\u65b9\u5f0f\uff0c\u5bfc\u81f4\u591a\u6a21\u6001\u4fe1\u53f7\u7684\u65e0\u5dee\u522b\u878d\u5408\u548c\u4efb\u52a1\u65e0\u5173\u5197\u4f59\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u3002\u9700\u8981\u4ece\u4efb\u52a1\u4e2d\u5fc3\u89c6\u89d2\u91cd\u65b0\u5ba1\u89c6\u591a\u6a21\u6001\u4fe1\u606f\u63d0\u53d6\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u8de8\u6a21\u6001\u6700\u4f18\u4f20\u8f93\u751f\u6210\u6587\u672ctoken\u548c\u89c6\u89c9patch\u7684\u7a00\u758f\u6982\u7387\u5bf9\u9f50\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u95e8\u63a7\u5236\u89c6\u89c9\u6ce8\u5165\uff1b2) \u53d8\u5206\u4fe1\u606f\u74f6\u9888\u538b\u7f29\u878d\u5408\u7279\u5f81\uff0c\u8fc7\u6ee4\u4efb\u52a1\u65e0\u5173\u566a\u58f0\u3002", "result": "\u5728FUNSD\u4e0a\u8fbe\u523091.95% SER\u548c91.13% RE\uff0c\u5728XFUND(ZH)\u4e0a\u8fbe\u523091.09% SER\u548c94.20% RE\uff0c\u8868\u73b0\u51fa\u8de8\u6570\u636e\u96c6\u7684\u7ade\u4e89\u6027\u6027\u80fd\u3002\u7279\u5f81\u5206\u6790\u786e\u8ba4\u51cf\u5c11\u4e86\u6a21\u6001\u5197\u4f59\u5e76\u589e\u5f3a\u4e86\u4efb\u52a1\u4fe1\u53f7\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u6587\u6863AI\u4e2d\u7684\u53ef\u63a7\u591a\u6a21\u6001\u878d\u5408\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u3001\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u8303\u5f0f\uff0c\u5f3a\u8c03\u6587\u672c\u4e3b\u5bfc\u3001\u89c6\u89c9\u9009\u62e9\u6027\u652f\u6301\u7684\u539f\u5219\u3002"}}
{"id": "2511.14767", "categories": ["cs.IR", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.14767", "abs": "https://arxiv.org/abs/2511.14767", "authors": ["Minh-Thuan Nguyen", "Thien Vo-Thanh", "Thai-Duy Dinh", "Xuan-Quang Phan", "Tan-Ha Mai", "Lam-Son L\u00ea"], "title": "An LLM-Powered Agent for Real-Time Analysis of the Vietnamese IT Job Market", "comment": "Accepted at ACOMPA 2025", "summary": "Individuals entering Vietnam's dynamic Information Technology (IT) job market face a critical gap in reliable career guidance. Existing market reports are often outdated, while the manual analysis of thousands of job postings is impractical for most. To address this challenge, we present the AI Job Market Consultant, a novel conversational agent that delivers deep, data-driven insights directly from the labor market in real-time. The foundation of our system is a custom-built dataset created via an automated pipeline that crawls job portals using Playwright and leverages the Large Language Model (LLM) to intelligently structure unstructured posting data. The core of our system is a tool-augmented AI agent, based on the ReAct agentic framework, which enables the ability of autonomously reasoning, planning, and executing actions through a specialized toolbox for SQL queries, semantic search, and data visualization. Our prototype successfully collected and analyzed 3,745 job postings, demonstrating its ability to answer complex, multi-step queries, generate on-demand visualizations, and provide personalized career advice grounded in real-world data. This work introduces a new paradigm for labor market analysis, showcasing how specialized agentic AI systems can democratize access to timely, trustworthy career intelligence for the next generation of professionals.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2aAI\u804c\u4e1a\u5e02\u573a\u987e\u95ee\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u722c\u53d6\u548cLLM\u5904\u7406\u8d8a\u5357IT\u804c\u4f4d\u6570\u636e\uff0c\u57fa\u4e8eReAct\u6846\u67b6\u7684\u667a\u80fd\u4ee3\u7406\u63d0\u4f9b\u5b9e\u65f6\u6570\u636e\u9a71\u52a8\u7684\u804c\u4e1a\u6307\u5bfc\u3002", "motivation": "\u8d8a\u5357IT\u5c31\u4e1a\u5e02\u573a\u7f3a\u4e4f\u53ef\u9760\u7684\u5b9e\u65f6\u804c\u4e1a\u6307\u5bfc\uff0c\u73b0\u6709\u5e02\u573a\u62a5\u544a\u8fc7\u65f6\uff0c\u624b\u52a8\u5206\u6790\u5927\u91cf\u804c\u4f4d\u4fe1\u606f\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u6784\u5efa\u81ea\u52a8\u5316\u6570\u636e\u91c7\u96c6\u7ba1\u9053\uff08\u4f7f\u7528Playwright\u722c\u53d6\u804c\u4f4d\u95e8\u6237\u7f51\u7ad9\uff09\uff0c\u5229\u7528LLM\u7ed3\u6784\u5316\u975e\u7ed3\u6784\u5316\u6570\u636e\uff0c\u57fa\u4e8eReAct\u6846\u67b6\u5f00\u53d1\u5de5\u5177\u589e\u5f3a\u7684AI\u4ee3\u7406\uff0c\u652f\u6301SQL\u67e5\u8be2\u3001\u8bed\u4e49\u641c\u7d22\u548c\u6570\u636e\u53ef\u89c6\u5316\u3002", "result": "\u6210\u529f\u91c7\u96c6\u5206\u67903,745\u4e2a\u804c\u4f4d\u4fe1\u606f\uff0c\u7cfb\u7edf\u80fd\u591f\u56de\u7b54\u590d\u6742\u591a\u6b65\u9aa4\u67e5\u8be2\u3001\u751f\u6210\u5b9e\u65f6\u53ef\u89c6\u5316\u56fe\u8868\uff0c\u5e76\u63d0\u4f9b\u57fa\u4e8e\u771f\u5b9e\u6570\u636e\u7684\u4e2a\u6027\u5316\u804c\u4e1a\u5efa\u8bae\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5f15\u5165\u4e86\u52b3\u52a8\u529b\u5e02\u573a\u5206\u6790\u7684\u65b0\u8303\u5f0f\uff0c\u5c55\u793a\u4e86\u4e13\u4e1a\u5316AI\u4ee3\u7406\u7cfb\u7edf\u5982\u4f55\u4e3a\u65b0\u4e00\u4ee3\u4e13\u4e1a\u4eba\u58eb\u63d0\u4f9b\u53ca\u65f6\u53ef\u4fe1\u7684\u804c\u4e1a\u667a\u80fd\u670d\u52a1\u3002"}}
{"id": "2511.14768", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14768", "abs": "https://arxiv.org/abs/2511.14768", "authors": ["Bhavika Jain", "Robert Pitsko", "Ananya Drishti", "Mahfuza Farooque"], "title": "Causally-Informed Reinforcement Learning for Adaptive Emotion-Aware Social Media Recommendation", "comment": null, "summary": "Social media recommendation systems play a central role in shaping users' emotional experiences. However, most systems are optimized solely for engagement metrics, such as click rate, viewing time, or scrolling, without accounting for users' emotional states. Repeated exposure to emotionally charged content has been shown to negatively affect users' emotional well-being over time. We propose an Emotion-aware Social Media Recommendation (ESMR) framework that personalizes content based on users' evolving emotional trajectories. ESMR integrates a Transformer-based emotion predictor with a hybrid recommendation policy: a LightGBM model for engagement during stable periods and a reinforcement learning agent with causally informed rewards when negative emotional states persist. Through behaviorally grounded evaluation over 30-day interaction traces, ESMR demonstrates improved emotional recovery, reduced volatility, and strong engagement retention. ESMR offers a path toward emotionally aware recommendations without compromising engagement performance.", "AI": {"tldr": "\u63d0\u51faESMR\u6846\u67b6\uff0c\u901a\u8fc7Transformer\u9884\u6d4b\u7528\u6237\u60c5\u7eea\u8f68\u8ff9\uff0c\u7ed3\u5408LightGBM\u548c\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u60c5\u611f\u611f\u77e5\u7684\u793e\u4ea4\u5a92\u4f53\u63a8\u8350\uff0c\u5728\u4fdd\u6301\u53c2\u4e0e\u5ea6\u7684\u540c\u65f6\u6539\u5584\u7528\u6237\u60c5\u7eea\u5065\u5eb7\u3002", "motivation": "\u73b0\u6709\u793e\u4ea4\u5a92\u4f53\u63a8\u8350\u7cfb\u7edf\u4ec5\u4f18\u5316\u53c2\u4e0e\u5ea6\u6307\u6807\uff0c\u4e0d\u8003\u8651\u7528\u6237\u60c5\u7eea\u72b6\u6001\uff0c\u957f\u671f\u63a5\u89e6\u60c5\u7eea\u5316\u5185\u5bb9\u4f1a\u635f\u5bb3\u7528\u6237\u60c5\u7eea\u5065\u5eb7\u3002", "method": "ESMR\u6846\u67b6\u6574\u5408Transformer\u60c5\u7eea\u9884\u6d4b\u5668\u548c\u6df7\u5408\u63a8\u8350\u7b56\u7565\uff1a\u7a33\u5b9a\u671f\u4f7f\u7528LightGBM\u6a21\u578b\uff0c\u6301\u7eed\u8d1f\u9762\u60c5\u7eea\u65f6\u4f7f\u7528\u56e0\u679c\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u3002", "result": "30\u5929\u4ea4\u4e92\u8f68\u8ff9\u8bc4\u4f30\u663e\u793aESMR\u6539\u5584\u4e86\u60c5\u7eea\u6062\u590d\u3001\u964d\u4f4e\u4e86\u60c5\u7eea\u6ce2\u52a8\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u53c2\u4e0e\u5ea6\u3002", "conclusion": "ESMR\u63d0\u4f9b\u4e86\u5728\u4e0d\u5f71\u54cd\u53c2\u4e0e\u5ea6\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u60c5\u611f\u611f\u77e5\u63a8\u8350\u7684\u8def\u5f84\u3002"}}
{"id": "2511.14769", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14769", "abs": "https://arxiv.org/abs/2511.14769", "authors": ["Yifan Xu", "Vipul Gupta", "Rohit Aggarwal", "Varsha Mahadevan", "Bhaskar Krishnamachari"], "title": "Cluster-based Adaptive Retrieval: Dynamic Context Selection for RAG Applications", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by pulling in external material, document, code, manuals, from vast and ever-growing corpora, to effectively answer user queries. The effectiveness of RAG depends significantly on aligning the number of retrieved documents with query characteristics: narrowly focused queries typically require fewer, highly relevant documents, whereas broader or ambiguous queries benefit from retrieving more extensive supporting information. However, the common static top-k retrieval approach fails to adapt to this variability, resulting in either insufficient context from too few documents or redundant information from too many. Motivated by these challenges, we introduce Cluster-based Adaptive Retrieval (CAR), an algorithm that dynamically determines the optimal number of documents by analyzing the clustering patterns of ordered query-document similarity distances. CAR detects the transition point within similarity distances, where tightly clustered, highly relevant documents shift toward less pertinent candidates, establishing an adaptive cut-off that scales with query complexity. On Coinbase's CDP corpus and the public MultiHop-RAG benchmark, CAR consistently picks the optimal retrieval depth and achieves the highest TES score, outperforming every fixed top-k baseline. In downstream RAG evaluations, CAR cuts LLM token usage by 60%, trims end-to-end latency by 22%, and reduces hallucinations by 10% while fully preserving answer relevance. Since integrating CAR into Coinbase's virtual assistant, we've seen user engagement jump by 200%.", "AI": {"tldr": "CAR\u7b97\u6cd5\u901a\u8fc7\u5206\u6790\u67e5\u8be2-\u6587\u6863\u76f8\u4f3c\u5ea6\u8ddd\u79bb\u7684\u805a\u7c7b\u6a21\u5f0f\uff0c\u52a8\u6001\u786e\u5b9a\u68c0\u7d22\u6587\u6863\u6570\u91cf\uff0c\u89e3\u51b3\u4e86\u9759\u6001top-k\u68c0\u7d22\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u67e5\u8be2\u7279\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u9759\u6001top-k\u68c0\u7d22\u65b9\u6cd5\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u67e5\u8be2\u7279\u6027\uff1a\u7a84\u8303\u56f4\u67e5\u8be2\u9700\u8981\u5c11\u91cf\u9ad8\u76f8\u5173\u6587\u6863\uff0c\u800c\u5bbd\u6cdb\u67e5\u8be2\u9700\u8981\u66f4\u591a\u652f\u6301\u4fe1\u606f\u3002\u9759\u6001\u65b9\u6cd5\u4f1a\u5bfc\u81f4\u4e0a\u4e0b\u6587\u4e0d\u8db3\u6216\u4fe1\u606f\u5197\u4f59\u3002", "method": "CAR\u7b97\u6cd5\u901a\u8fc7\u5206\u6790\u67e5\u8be2-\u6587\u6863\u76f8\u4f3c\u5ea6\u8ddd\u79bb\u7684\u805a\u7c7b\u6a21\u5f0f\uff0c\u68c0\u6d4b\u9ad8\u76f8\u5173\u6587\u6863\u5411\u4f4e\u76f8\u5173\u6587\u6863\u7684\u8fc7\u6e21\u70b9\uff0c\u5efa\u7acb\u81ea\u9002\u5e94\u622a\u65ad\u673a\u5236\u3002", "result": "\u5728Coinbase CDP\u8bed\u6599\u5e93\u548cMultiHop-RAG\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCAR\u59cb\u7ec8\u9009\u62e9\u6700\u4f18\u68c0\u7d22\u6df1\u5ea6\uff0c\u83b7\u5f97\u6700\u9ad8TES\u5206\u6570\u3002\u5728\u4e0b\u6e38RAG\u8bc4\u4f30\u4e2d\uff0cCAR\u5c06LLM token\u4f7f\u7528\u51cf\u5c1160%\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u964d\u4f4e22%\uff0c\u5e7b\u89c9\u51cf\u5c1110%\uff0c\u540c\u65f6\u5b8c\u5168\u4fdd\u6301\u7b54\u6848\u76f8\u5173\u6027\u3002", "conclusion": "CAR\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86RAG\u7cfb\u7edf\u7684\u6548\u7387\u548c\u6548\u679c\uff0c\u5728Coinbase\u865a\u62df\u52a9\u624b\u4e2d\u7684\u96c6\u6210\u4f7f\u7528\u6237\u53c2\u4e0e\u5ea6\u63d0\u5347\u4e86200%\u3002"}}
{"id": "2511.14770", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14770", "abs": "https://arxiv.org/abs/2511.14770", "authors": ["Bo Ma", "LuYao Liu", "ZeHua Hu", "Simon Lau"], "title": "ExplainRec: Towards Explainable Multi-Modal Zero-Shot Recommendation with Preference Attribution and Large Language Models", "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have opened new possibilities for recommendation systems, though current approaches such as TALLRec face challenges in explainability and cold-start scenarios. We present ExplainRec, a framework that extends LLM-based recommendation capabilities through preference attribution, multi-modal fusion, and zero-shot transfer learning. The framework incorporates four technical contributions: preference attribution tuning for explainable recommendations, zero-shot preference transfer for cold-start users and items, multi-modal enhancement leveraging visual and textual content, and multi-task collaborative optimization. Experimental evaluation on MovieLens-25M and Amazon datasets shows that ExplainRec outperforms existing methods, achieving AUC improvements of 0.7\\% on movie recommendation and 0.9\\% on cross-domain tasks, while generating interpretable explanations and handling cold-start scenarios effectively.", "AI": {"tldr": "ExplainRec\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7\u504f\u597d\u5f52\u56e0\u3001\u591a\u6a21\u6001\u878d\u5408\u548c\u96f6\u6837\u672c\u8fc1\u79fb\u5b66\u4e60\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u548c\u51b7\u542f\u52a8\u5904\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709LLM\u63a8\u8350\u65b9\u6cd5\u5982TALLRec\u5728\u53ef\u89e3\u91ca\u6027\u548c\u51b7\u542f\u52a8\u573a\u666f\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u751f\u6210\u53ef\u89e3\u91ca\u63a8\u8350\u5e76\u6709\u6548\u5904\u7406\u51b7\u542f\u52a8\u95ee\u9898\u7684\u6846\u67b6\u3002", "method": "\u91c7\u7528\u504f\u597d\u5f52\u56e0\u8c03\u4f18\u5b9e\u73b0\u53ef\u89e3\u91ca\u63a8\u8350\uff0c\u96f6\u6837\u672c\u504f\u597d\u8fc1\u79fb\u5904\u7406\u51b7\u542f\u52a8\u7528\u6237\u548c\u7269\u54c1\uff0c\u591a\u6a21\u6001\u589e\u5f3a\u5229\u7528\u89c6\u89c9\u548c\u6587\u672c\u5185\u5bb9\uff0c\u591a\u4efb\u52a1\u534f\u540c\u4f18\u5316\u3002", "result": "\u5728MovieLens-25M\u548cAmazon\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cExplainRec\u5728\u7535\u5f71\u63a8\u8350\u4efb\u52a1\u4e0aAUC\u63d0\u53470.7%\uff0c\u8de8\u57df\u4efb\u52a1\u4e0a\u63d0\u53470.9%\uff0c\u540c\u65f6\u80fd\u751f\u6210\u53ef\u89e3\u91ca\u7684\u63a8\u8350\u7406\u7531\u5e76\u6709\u6548\u5904\u7406\u51b7\u542f\u52a8\u573a\u666f\u3002", "conclusion": "ExplainRec\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u504f\u597d\u5f52\u56e0\u3001\u591a\u6a21\u6001\u878d\u5408\u548c\u96f6\u6837\u672c\u8fc1\u79fb\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\u3001\u53ef\u89e3\u91ca\u6027\u548c\u51b7\u542f\u52a8\u5904\u7406\u80fd\u529b\u3002"}}
{"id": "2511.14881", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.14881", "abs": "https://arxiv.org/abs/2511.14881", "authors": ["Bi Xue", "Hong Wu", "Lei Chen", "Chao Yang", "Yiming Ma", "Fei Ding", "Zhen Wang", "Liang Wang", "Xiaoheng Mao", "Ke Huang", "Xialu Li", "Peng Xia", "Rui Jian", "Yanli Zhao", "Yanzun Huang", "Yijie Deng", "Harry Tran", "Ryan Chang", "Min Yu", "Eric Dong", "Jiazhou Wang", "Qianqian Zhang", "Keke Zhai", "Hongzhang Yin", "Pawel Garbacki", "Zheng Fang", "Yiyi Pan", "Min Ni", "Yang Liu"], "title": "SilverTorch: A Unified Model-based System to Democratize Large-Scale Recommendation on GPUs", "comment": null, "summary": "Serving deep learning based recommendation models (DLRM) at scale is challenging. Existing systems rely on CPU-based ANN indexing and filtering services, suffering from non-negligible costs and forgoing joint optimization opportunities. Such inefficiency makes them difficult to support more complex model architectures, such as learned similarities and multi-task retrieval.\n  In this paper, we propose SilverTorch, a model-based system for serving recommendation models on GPUs. SilverTorch unifies model serving by replacing standalone indexing and filtering services with layers of served models. We propose a Bloom index algorithm on GPUs for feature filtering and a tensor-native fused Int8 ANN kernel on GPUs for nearest neighbor search. We further co-design the ANN search index and filtering index to reduce GPU memory utilization and eliminate unnecessary computation. Benefit from SilverTorch's serving paradigm, we introduce a OverArch scoring layer and a Value Model to aggregate results across multi-tasks. These advancements improve the accuracy for retrieval and enable future studies for serving more complex models. For ranking, SilverTorch's design accelerates item embedding calculation by caching the pre-calculated embeddings inside the serving model.\n  Our evaluation on the industry-scale datasets show that SilverTorch achieves up to 5.6x lower latency and 23.7x higher throughput compared to the state-of-the-art approaches. We also demonstrate that SilverTorch's solution is 13.35x more cost-efficient than CPU-based solution while improving accuracy via serving more complex models. SilverTorch serves over hundreds of models online across major products and recommends contents for billions of daily active users.", "AI": {"tldr": "SilverTorch\u662f\u4e00\u4e2a\u5728GPU\u4e0a\u670d\u52a1\u63a8\u8350\u6a21\u578b\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u7edf\u4e00\u6a21\u578b\u670d\u52a1\u3001GPU\u4e0a\u7684Bloom\u7d22\u5f15\u7b97\u6cd5\u548cInt8 ANN\u5185\u6838\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u4e86\u541e\u5410\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eCPU\u7684ANN\u7d22\u5f15\u548c\u8fc7\u6ee4\u670d\u52a1\u5b58\u5728\u663e\u8457\u6210\u672c\u95ee\u9898\uff0c\u4e14\u65e0\u6cd5\u652f\u6301\u66f4\u590d\u6742\u7684\u6a21\u578b\u67b6\u6784\uff0c\u5982\u5b66\u4e60\u76f8\u4f3c\u6027\u548c\u591a\u4efb\u52a1\u68c0\u7d22\u3002", "method": "\u63d0\u51faSilverTorch\u7cfb\u7edf\uff0c\u7528\u6a21\u578b\u5c42\u66ff\u6362\u72ec\u7acb\u7684\u7d22\u5f15\u548c\u8fc7\u6ee4\u670d\u52a1\uff0c\u5305\u62ecGPU\u4e0a\u7684Bloom\u7d22\u5f15\u7b97\u6cd5\u3001\u878d\u5408Int8 ANN\u5185\u6838\uff0c\u4ee5\u53ca\u5171\u540c\u8bbe\u8ba1ANN\u641c\u7d22\u7d22\u5f15\u548c\u8fc7\u6ee4\u7d22\u5f15\u3002", "result": "\u5728\u884c\u4e1a\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\uff0cSilverTorch\u5b9e\u73b0\u4e86\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u4f4e5.6\u500d\u7684\u5ef6\u8fdf\u548c\u9ad823.7\u500d\u7684\u541e\u5410\u91cf\uff0c\u6210\u672c\u6548\u7387\u6bd4\u57fa\u4e8eCPU\u7684\u89e3\u51b3\u65b9\u6848\u9ad813.35\u500d\u3002", "conclusion": "SilverTorch\u901a\u8fc7\u5176\u670d\u52a1\u8303\u5f0f\u63d0\u9ad8\u4e86\u68c0\u7d22\u51c6\u786e\u6027\uff0c\u5e76\u4e3a\u670d\u52a1\u66f4\u590d\u6742\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u80fd\u6027\uff0c\u5df2\u5728\u4e3b\u8981\u4ea7\u54c1\u4e2d\u670d\u52a1\u6570\u767e\u4e2a\u6a21\u578b\uff0c\u4e3a\u6570\u5341\u4ebf\u65e5\u6d3b\u8dc3\u7528\u6237\u63a8\u8350\u5185\u5bb9\u3002"}}
{"id": "2511.15122", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15122", "abs": "https://arxiv.org/abs/2511.15122", "authors": ["Fuwei Zhang", "Xiaoyu Liu", "Dongbo Xi", "Jishen Yin", "Huan Chen", "Peng Yan", "Fuzhen Zhuang", "Zhao Zhang"], "title": "Multi-Aspect Cross-modal Quantization for Generative Recommendation", "comment": "Accepted by AAAI 2026 (Oral)", "summary": "Generative Recommendation (GR) has emerged as a new paradigm in recommender systems. This approach relies on quantized representations to discretize item features, modeling users' historical interactions as sequences of discrete tokens. Based on these tokenized sequences, GR predicts the next item by employing next-token prediction methods. The challenges of GR lie in constructing high-quality semantic identifiers (IDs) that are hierarchically organized, minimally conflicting, and conducive to effective generative model training. However, current approaches remain limited in their ability to harness multimodal information and to capture the deep and intricate interactions among diverse modalities, both of which are essential for learning high-quality semantic IDs and for effectively training GR models. To address this, we propose Multi-Aspect Cross-modal quantization for generative Recommendation (MACRec), which introduces multimodal information and incorporates it into both semantic ID learning and generative model training from different aspects. Specifically, we first introduce cross-modal quantization during the ID learning process, which effectively reduces conflict rates and thus improves codebook usability through the complementary integration of multimodal information. In addition, to further enhance the generative ability of our GR model, we incorporate multi-aspect cross-modal alignments, including the implicit and explicit alignments. Finally, we conduct extensive experiments on three well-known recommendation datasets to demonstrate the effectiveness of our proposed method.", "AI": {"tldr": "\u63d0\u51fa\u4e86MACRec\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u91cf\u5316\u548c\u591a\u89d2\u5ea6\u5bf9\u9f50\u6765\u6539\u8fdb\u751f\u6210\u5f0f\u63a8\u8350\u7cfb\u7edf\uff0c\u89e3\u51b3\u8bed\u4e49ID\u6784\u5efa\u548c\u751f\u6210\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u5f53\u524d\u751f\u6210\u5f0f\u63a8\u8350\u65b9\u6cd5\u5728\u5229\u7528\u591a\u6a21\u6001\u4fe1\u606f\u548c\u6355\u6349\u6df1\u5ea6\u8de8\u6a21\u6001\u4ea4\u4e92\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u8fd9\u5f71\u54cd\u4e86\u9ad8\u8d28\u91cf\u8bed\u4e49ID\u7684\u6784\u5efa\u548c\u751f\u6210\u6a21\u578b\u7684\u6709\u6548\u8bad\u7ec3\u3002", "method": "\u5f15\u5165\u8de8\u6a21\u6001\u91cf\u5316\u6765\u51cf\u5c11\u51b2\u7a81\u7387\uff0c\u5e76\u7ed3\u5408\u9690\u5f0f\u548c\u663e\u5f0f\u7684\u591a\u89d2\u5ea6\u8de8\u6a21\u6001\u5bf9\u9f50\u6765\u589e\u5f3a\u751f\u6210\u80fd\u529b\u3002", "result": "\u5728\u4e09\u4e2a\u77e5\u540d\u63a8\u8350\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "MACRec\u901a\u8fc7\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\u4ece\u4e0d\u540c\u89d2\u5ea6\u6539\u8fdb\u8bed\u4e49ID\u5b66\u4e60\u548c\u751f\u6210\u6a21\u578b\u8bad\u7ec3\uff0c\u63d0\u5347\u4e86\u751f\u6210\u5f0f\u63a8\u8350\u7684\u6027\u80fd\u3002"}}
{"id": "2511.15141", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15141", "abs": "https://arxiv.org/abs/2511.15141", "authors": ["Sunwoo Kim", "Geon Lee", "Kyungho Kim", "Jaemin Yoo", "Kijung Shin"], "title": "ItemRAG: Item-Based Retrieval-Augmented Generation for LLM-Based Recommendation", "comment": null, "summary": "Recently, large language models (LLMs) have been widely used as recommender systems, owing to their strong reasoning capability and their effectiveness in handling cold-start items. To better adapt LLMs for recommendation, retrieval-augmented generation (RAG) has been incorporated. Most existing RAG methods are user-based, retrieving purchase patterns of users similar to the target user and providing them to the LLM. In this work, we propose ItemRAG, an item-based RAG method for LLM-based recommendation that retrieves relevant items (rather than users) from item-item co-purchase histories. ItemRAG helps LLMs capture co-purchase patterns among items, which are beneficial for recommendations. Especially, our retrieval strategy incorporates semantically similar items to better handle cold-start items and uses co-purchase frequencies to improve the relevance of the retrieved items. Through extensive experiments, we demonstrate that ItemRAG consistently (1) improves the zero-shot LLM-based recommender by up to 43% in Hit-Ratio-1 and (2) outperforms user-based RAG baselines under both standard and cold-start item recommendation settings.", "AI": {"tldr": "\u63d0\u51faItemRAG\u65b9\u6cd5\uff0c\u57fa\u4e8e\u7269\u54c1\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\uff0c\u901a\u8fc7\u68c0\u7d22\u7269\u54c1\u95f4\u7684\u5171\u8d2d\u5386\u53f2\u6765\u6539\u8fdbLLM\u63a8\u8350\u7cfb\u7edf\uff0c\u5728\u51b7\u542f\u52a8\u7269\u54c1\u63a8\u8350\u548c\u6807\u51c6\u63a8\u8350\u573a\u666f\u4e0b\u5747\u4f18\u4e8e\u7528\u6237\u57faRAG\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709RAG\u65b9\u6cd5\u591a\u4e3a\u7528\u6237\u57fa\uff0c\u68c0\u7d22\u76f8\u4f3c\u7528\u6237\u7684\u8d2d\u4e70\u6a21\u5f0f\u3002\u672c\u6587\u63d0\u51fa\u7269\u54c1\u57faRAG\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u7d22\u7269\u54c1\u95f4\u7684\u5171\u8d2d\u6a21\u5f0f\u6765\u66f4\u597d\u5730\u9002\u5e94LLM\u63a8\u8350\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u5904\u7406\u51b7\u542f\u52a8\u7269\u54c1\u3002", "method": "ItemRAG\u65b9\u6cd5\u4ece\u7269\u54c1-\u7269\u54c1\u5171\u8d2d\u5386\u53f2\u4e2d\u68c0\u7d22\u76f8\u5173\u7269\u54c1\uff0c\u7ed3\u5408\u8bed\u4e49\u76f8\u4f3c\u7269\u54c1\u5904\u7406\u51b7\u542f\u52a8\u7269\u54c1\uff0c\u5229\u7528\u5171\u8d2d\u9891\u7387\u63d0\u9ad8\u68c0\u7d22\u76f8\u5173\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793aItemRAG\u5c06\u96f6\u6837\u672cLLM\u63a8\u8350\u5668\u7684Hit-Ratio-1\u63d0\u5347\u9ad8\u8fbe43%\uff0c\u5728\u6807\u51c6\u548c\u51b7\u542f\u52a8\u7269\u54c1\u63a8\u8350\u573a\u666f\u4e0b\u5747\u4f18\u4e8e\u7528\u6237\u57faRAG\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ItemRAG\u901a\u8fc7\u7269\u54c1\u57fa\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6709\u6548\u6355\u6349\u7269\u54c1\u95f4\u5171\u8d2d\u6a21\u5f0f\uff0c\u663e\u8457\u63d0\u5347LLM\u63a8\u8350\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u51b7\u542f\u52a8\u7269\u54c1\u63a8\u8350\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2511.15241", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.15241", "abs": "https://arxiv.org/abs/2511.15241", "authors": ["Mi Tian", "Kun Zhang", "Fei Liu", "Jinglong Li", "Yuxin Liao", "Chenxi Bai", "Zhengtao Tan", "Le Wu", "Richang Hong"], "title": "Selective Mixup for Debiasing Question Selection in Computerized Adaptive Testing", "comment": "Accepted by CIKM 2025", "summary": "Computerized Adaptive Testing (CAT) is a widely used technology for evaluating learners' proficiency in online education platforms. By leveraging prior estimates of proficiency to select questions and updating the estimates iteratively based on responses, CAT enables personalized learner modeling and has attracted substantial attention. Despite this progress, most existing works focus primarily on improving diagnostic accuracy, while overlooking the selection bias inherent in the adaptive process. Selection Bias arises because the question selection is strongly influenced by the estimated proficiency, such as assigning easier questions to learners with lower proficiency and harder ones to learners with higher proficiency. Since the selection depends on prior estimation, this bias propagates into the diagnosis model, which is further amplified during iterative updates, leading to misalignment and biased predictions. Moreover, the imbalanced nature of learners' historical interactions often exacerbates the bias in diagnosis models. To address this issue, we propose a debiasing framework consisting of two key modules: Cross-Attribute Examinee Retrieval and Selective Mixup-based Regularization. First, we retrieve balanced examinees with relatively even distributions of correct and incorrect responses and use them as neutral references for biased examinees. Then, mixup is applied between each biased examinee and its matched balanced counterpart under label consistency. This augmentation enriches the diversity of bias-conflicting samples and smooths selection boundaries. Finally, extensive experiments on two benchmark datasets with multiple advanced diagnosis models demonstrate that our method substantially improves both the generalization ability and fairness of question selection in CAT.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8ba1\u7b97\u673a\u81ea\u9002\u5e94\u6d4b\u8bd5(CAT)\u7684\u53bb\u504f\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u53c9\u5c5e\u6027\u8003\u751f\u68c0\u7d22\u548c\u9009\u62e9\u6027Mixup\u6b63\u5219\u5316\u6765\u89e3\u51b3\u81ea\u9002\u5e94\u8fc7\u7a0b\u4e2d\u7684\u9009\u62e9\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684CAT\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u4f46\u5ffd\u89c6\u4e86\u81ea\u9002\u5e94\u8fc7\u7a0b\u4e2d\u56fa\u6709\u7684\u9009\u62e9\u504f\u5dee\u95ee\u9898\u3002\u8fd9\u79cd\u504f\u5dee\u6e90\u4e8e\u95ee\u9898\u9009\u62e9\u5f3a\u70c8\u4f9d\u8d56\u4e8e\u4f30\u8ba1\u7684\u80fd\u529b\u6c34\u5e73\uff0c\u5bfc\u81f4\u504f\u5dee\u5728\u8fed\u4ee3\u66f4\u65b0\u4e2d\u88ab\u653e\u5927\uff0c\u9020\u6210\u9884\u6d4b\u504f\u5dee\u548c\u5bf9\u9f50\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6a21\u5757\u7684\u53bb\u504f\u6846\u67b6\uff1a1) \u4ea4\u53c9\u5c5e\u6027\u8003\u751f\u68c0\u7d22 - \u68c0\u7d22\u5177\u6709\u76f8\u5bf9\u5747\u5300\u6b63\u786e\u548c\u9519\u8bef\u56de\u7b54\u5206\u5e03\u7684\u5e73\u8861\u8003\u751f\u4f5c\u4e3a\u504f\u7f6e\u8003\u751f\u7684\u4e2d\u6027\u53c2\u8003\uff1b2) \u9009\u62e9\u6027Mixup\u6b63\u5219\u5316 - \u5728\u6807\u7b7e\u4e00\u81f4\u6761\u4ef6\u4e0b\uff0c\u5bf9\u6bcf\u4e2a\u504f\u7f6e\u8003\u751f\u4e0e\u5176\u5339\u914d\u7684\u5e73\u8861\u5bf9\u5e94\u9879\u5e94\u7528mixup\uff0c\u4e30\u5bcc\u504f\u5dee\u51b2\u7a81\u6837\u672c\u7684\u591a\u6837\u6027\u5e76\u5e73\u6ed1\u9009\u62e9\u8fb9\u754c\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86CAT\u4e2d\u95ee\u9898\u9009\u62e9\u7684\u6cdb\u5316\u80fd\u529b\u548c\u516c\u5e73\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u53bb\u504f\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86CAT\u4e2d\u7684\u9009\u62e9\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u8bca\u65ad\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd\u548c\u516c\u5e73\u6027\u3002"}}
{"id": "2511.15389", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.15389", "abs": "https://arxiv.org/abs/2511.15389", "authors": ["Suyu Chen", "Yimeng Bai", "Yulong Huang", "Xiaoyan Zhao", "Yang Zhang"], "title": "Unveiling Inference Scaling for Difference-Aware User Modeling in LLM Personalization", "comment": null, "summary": "Large Language Models (LLMs) are increasingly integrated into users' daily lives, driving a growing demand for personalized outputs. Prior work has primarily leveraged a user's own history, often overlooking inter-user differences that are critical for effective personalization. While recent methods have attempted to model such differences, their feature extraction processes typically rely on fixed dimensions and quick, intuitive inference (System-1 thinking), limiting both the coverage and granularity of captured user differences. To address these limitations, we propose Difference-aware Reasoning Personalization (DRP), a framework that reconstructs the difference extraction mechanism by leveraging inference scaling to enhance LLM personalization. DRP autonomously identifies relevant difference feature dimensions and generates structured definitions and descriptions, enabling slow, deliberate reasoning (System-2 thinking) over user differences. Experiments on personalized review generation demonstrate that DRP consistently outperforms baseline methods across multiple metrics.", "AI": {"tldr": "\u63d0\u51fa\u4e86DRP\u6846\u67b6\uff0c\u901a\u8fc7\u63a8\u7406\u6269\u5c55\u91cd\u6784\u5dee\u5f02\u63d0\u53d6\u673a\u5236\uff0c\u5229\u7528\u7cfb\u7edf2\u601d\u7ef4\u8fdb\u884c\u7528\u6237\u5dee\u5f02\u63a8\u7406\uff0c\u5728\u4e2a\u6027\u5316\u8bc4\u8bba\u751f\u6210\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u7528\u6237\u81ea\u8eab\u5386\u53f2\uff0c\u5ffd\u7565\u4e86\u7528\u6237\u95f4\u5dee\u5f02\uff0c\u4e14\u7279\u5f81\u63d0\u53d6\u8fc7\u7a0b\u4f9d\u8d56\u56fa\u5b9a\u7ef4\u5ea6\u548c\u5feb\u901f\u76f4\u89c9\u63a8\u7406\uff0c\u9650\u5236\u4e86\u7528\u6237\u5dee\u5f02\u7684\u8986\u76d6\u8303\u56f4\u548c\u7c92\u5ea6", "method": "DRP\u6846\u67b6\u81ea\u4e3b\u8bc6\u522b\u76f8\u5173\u5dee\u5f02\u7279\u5f81\u7ef4\u5ea6\uff0c\u751f\u6210\u7ed3\u6784\u5316\u5b9a\u4e49\u548c\u63cf\u8ff0\uff0c\u5229\u7528\u7cfb\u7edf2\u601d\u7ef4\u8fdb\u884c\u6162\u901f\u3001\u5ba1\u614e\u7684\u7528\u6237\u5dee\u5f02\u63a8\u7406", "result": "\u5728\u4e2a\u6027\u5316\u8bc4\u8bba\u751f\u6210\u5b9e\u9a8c\u4e2d\uff0cDRP\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "DRP\u901a\u8fc7\u91cd\u6784\u5dee\u5f02\u63d0\u53d6\u673a\u5236\uff0c\u5229\u7528\u63a8\u7406\u6269\u5c55\u589e\u5f3aLLM\u4e2a\u6027\u5316\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u7cfb\u7edf2\u601d\u7ef4\u5728\u7528\u6237\u5dee\u5f02\u5efa\u6a21\u4e2d\u7684\u6709\u6548\u6027"}}
{"id": "2511.15443", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.15443", "abs": "https://arxiv.org/abs/2511.15443", "authors": ["Ao Xie", "Jiahui Chen", "Quanzhi Zhu", "Xiaoze Jiang", "Zhiheng Qin", "Enyun Yu", "Han Li"], "title": "CroPS: Improving Dense Retrieval with Cross-Perspective Positive Samples in Short-Video Search", "comment": "AAAI-2026, Oral", "summary": "Dense retrieval has become a foundational paradigm in modern search systems, especially on short-video platforms. However, most industrial systems adopt a self-reinforcing training pipeline that relies on historically exposed user interactions for supervision. This paradigm inevitably leads to a filter bubble effect, where potentially relevant but previously unseen content is excluded from the training signal, biasing the model toward narrow and conservative retrieval. In this paper, we present CroPS (Cross-Perspective Positive Samples), a novel retrieval data engine designed to alleviate this problem by introducing diverse and semantically meaningful positive examples from multiple perspectives. CroPS enhances training with positive signals derived from user query reformulation behavior (query-level), engagement data in recommendation streams (system-level), and world knowledge synthesized by large language models (knowledge-level). To effectively utilize these heterogeneous signals, we introduce a Hierarchical Label Assignment (HLA) strategy and a corresponding H-InfoNCE loss that together enable fine-grained, relevance-aware optimization. Extensive experiments conducted on Kuaishou Search, a large-scale commercial short-video search platform, demonstrate that CroPS significantly outperforms strong baselines both offline and in live A/B tests, achieving superior retrieval performance and reducing query reformulation rates. CroPS is now fully deployed in Kuaishou Search, serving hundreds of millions of users daily.", "AI": {"tldr": "CroPS\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u68c0\u7d22\u6570\u636e\u5f15\u64ce\uff0c\u901a\u8fc7\u4ece\u67e5\u8be2\u7ea7\u3001\u7cfb\u7edf\u7ea7\u548c\u77e5\u8bc6\u7ea7\u4e09\u4e2a\u89c6\u89d2\u5f15\u5165\u591a\u6837\u5316\u7684\u6b63\u6837\u672c\u6765\u7f13\u89e3\u7a20\u5bc6\u68c0\u7d22\u4e2d\u7684\u8fc7\u6ee4\u6c14\u6ce1\u6548\u5e94\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u5de5\u4e1a\u68c0\u7d22\u7cfb\u7edf\u4f9d\u8d56\u5386\u53f2\u7528\u6237\u4ea4\u4e92\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u5bfc\u81f4\u8fc7\u6ee4\u6c14\u6ce1\u6548\u5e94\uff0c\u6392\u9664\u4e86\u6f5c\u5728\u76f8\u5173\u4f46\u672a\u88ab\u66dd\u5149\u7684\u5185\u5bb9\uff0c\u4f7f\u6a21\u578b\u504f\u5411\u4fdd\u5b88\u68c0\u7d22\u3002", "method": "\u63d0\u51faCroPS\u6846\u67b6\uff0c\u4ece\u4e09\u4e2a\u89c6\u89d2\u83b7\u53d6\u6b63\u6837\u672c\uff1a\u7528\u6237\u67e5\u8be2\u91cd\u6784\u884c\u4e3a\uff08\u67e5\u8be2\u7ea7\uff09\u3001\u63a8\u8350\u6d41\u4e2d\u7684\u4e92\u52a8\u6570\u636e\uff08\u7cfb\u7edf\u7ea7\uff09\u3001LLM\u5408\u6210\u7684\u4e16\u754c\u77e5\u8bc6\uff08\u77e5\u8bc6\u7ea7\uff09\uff0c\u5e76\u91c7\u7528\u5206\u5c42\u6807\u7b7e\u5206\u914d\u7b56\u7565\u548cH-InfoNCE\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728\u5feb\u624b\u641c\u7d22\u5e73\u53f0\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCroPS\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u79bb\u7ebf\u8bc4\u4f30\u548c\u5728\u7ebfA/B\u6d4b\u8bd5\u4e2d\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u68c0\u7d22\u6027\u80fd\uff0c\u964d\u4f4e\u4e86\u67e5\u8be2\u91cd\u6784\u7387\u3002", "conclusion": "CroPS\u6210\u529f\u7f13\u89e3\u4e86\u8fc7\u6ee4\u6c14\u6ce1\u95ee\u9898\uff0c\u5df2\u5728\u5feb\u624b\u641c\u7d22\u4e2d\u5168\u9762\u90e8\u7f72\uff0c\u4e3a\u6570\u4ebf\u7528\u6237\u63d0\u4f9b\u65e5\u5e38\u670d\u52a1\u3002"}}
