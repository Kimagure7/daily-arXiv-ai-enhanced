<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 12]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [GreyShot: Zeroshot and Privacy-preserving Recommender System by GM(1,1) Model](https://arxiv.org/abs/2511.05493)
*Hao Wang*

Main category: cs.IR

TL;DR: 提出了一种名为GreyShot的零样本隐私保护推荐系统算法，使用GM(1,1)灰系统模型解决冷启动问题，无需输入数据即可生成准确公平的推荐结果。


<details>
  <summary>Details</summary>
Motivation: 推荐系统面临冷启动问题，虽然已有转移学习和元学习等方法，但ZeroMat等新方法仍无法完全解决该问题，需要更有效的零样本解决方案。

Method: 利用在线评分数据的Poisson-Pareto特性，采用GM(1,1)灰系统模型构建零样本隐私保护推荐算法GreyShot，不依赖任何输入数据。

Result: 该方法能够有效生成准确且公平的推荐结果，成功解决了推荐系统的零样本问题。

Conclusion: 灰系统方法如GM(1,1)可以有效解决推荐系统的零样本问题。

Abstract: Every recommendation engineer needs to face the cold start problem when building his system. During the past decades, most scientists adopted transfer learning and meta learning to solve the problem. Although notable exceptions such as ZeroMat etc. have been invented in recent years, cold-start problem remains a challenging problem for many researchers. In this paper, we build a zeroshot and privacy-preserving recommender system algorithm GreyShot using GM(1,1) model by taking advantage of the Poisson-Pareto property of the online rating data. Our approach relies on no input data and is effective in generating both accurate and fair results. In conclusion, zeroshot problem of recommender systems could be effectively solved by grey system methods such as GM(1,1).

</details>


### [2] [IMDMR: An Intelligent Multi-Dimensional Memory Retrieval System for Enhanced Conversational AI](https://arxiv.org/abs/2511.05495)
*Tejas Pawar,Sarika Patil,Om Tilekar,Rushikesh Janwade,Vaibhav Helambe*

Main category: cs.IR

TL;DR: IMDMR是一个智能多维记忆检索系统，通过六维搜索架构显著提升对话AI的记忆能力，在整体性能上比最佳基线系统提升3.8倍。


<details>
  <summary>Details</summary>
Motivation: 现有对话AI系统在维持跨对话的连贯上下文记忆方面存在困难，限制了提供个性化和上下文相关响应的能力。

Method: 采用六维记忆检索架构（语义、实体、类别、意图、上下文和时间维度），结合智能查询处理、动态策略选择、跨记忆实体解析和高级记忆集成技术。

Result: 相比五个基线系统，IMDMR在整体性能上达到0.792（最佳基线为0.207），提升3.8倍。在偏好/兴趣查询和目标/愿望查询上表现尤其出色（0.630）。

Conclusion: IMDMR代表了对话AI记忆系统的重大进步，为增强用户交互和个性化体验提供了坚实基础。

Abstract: Conversational AI systems often struggle with maintaining coherent, contextual memory across extended interactions, limiting their ability to provide personalized and contextually relevant responses. This paper presents IMDMR (Intelligent Multi-Dimensional Memory Retrieval), a novel system that addresses these limitations through a multi-dimensional search architecture. Unlike existing memory systems that rely on single-dimensional approaches, IMDMR leverages six distinct memory dimensions-semantic, entity, category, intent, context, and temporal-to provide comprehensive memory retrieval capabilities. Our system incorporates intelligent query processing with dynamic strategy selection, cross-memory entity resolution, and advanced memory integration techniques. Through comprehensive evaluation against five baseline systems including LangChain RAG, LlamaIndex, MemGPT, and spaCy + RAG, IMDMR achieves a 3.8x improvement in overall performance (0.792 vs 0.207 for the best baseline). We present both simulated (0.314) and production (0.792) implementations, demonstrating the importance of real technology integration while maintaining superiority over all baseline systems. Ablation studies demonstrate the effectiveness of multi-dimensional search, with the full system outperforming individual dimension approaches by 23.3%. Query-type analysis reveals superior performance across all categories, particularly for preferences/interests (0.630) and goals/aspirations (0.630) queries. Comprehensive visualizations and statistical analysis confirm the significance of these improvements with p < 0.001 across all metrics. The results establish IMDMR as a significant advancement in conversational AI memory systems, providing a robust foundation for enhanced user interactions and personalized experiences.

</details>


### [3] [DOCUEVAL: An LLM-based AI Engineering Tool for Building Customisable Document Evaluation Workflows](https://arxiv.org/abs/2511.05496)
*Hao Zhang,Qinghua Lu,Liming Zhu*

Main category: cs.IR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Foundation models, such as large language models (LLMs), have the potential to streamline evaluation workflows and improve their performance. However, practical adoption faces challenges, such as customisability, accuracy, and scalability. In this paper, we present DOCUEVAL, an AI engineering tool for building customisable DOCUment EVALuation workflows. DOCUEVAL supports advanced document processing and customisable workflow design which allow users to define theory-grounded reviewer roles, specify evaluation criteria, experiment with different reasoning strategies and choose the assessment style. To ensure traceability, DOCUEVAL provides comprehensive logging of every run, along with source attribution and configuration management, allowing systematic comparison of results across alternative setups. By integrating these capabilities, DOCUEVAL directly addresses core software engineering challenges, including how to determine whether evaluators are "good enough" for deployment and how to empirically compare different evaluation strategies. We demonstrate the usefulness of DOCUEVAL through a real-world academic peer review case, showing how DOCUEVAL enables both the engineering of evaluators and scalable, reliable document evaluation.

</details>


### [4] [Biomedical Hypothesis Explainability with Graph-Based Context Retrieval](https://arxiv.org/abs/2511.05498)
*Ilya Tyagin,Saeideh Valipour,Aliaksandra Sikirzhytskaya,Michael Shtutman,Ilya Safro*

Main category: cs.IR

TL;DR: 提出了一种基于假设生成上下文检索框架的生物医学假设生成系统可解释性方法，结合语义图检索和数据限制训练，通过检索增强生成与LLMs集成，并引入反馈循环机制迭代改进解释质量。


<details>
  <summary>Details</summary>
Motivation: 为生物医学假设生成系统提供可解释性，模拟真实世界发现约束，通过上下文证据解释假设，并解决LLM生成解释中的缺陷问题。

Method: 基于假设生成上下文检索框架，结合语义图检索和数据限制训练，通过检索增强生成与LLMs集成，引入反馈循环机制迭代识别和修正解释缺陷。

Result: 在多个大语言模型上验证了方法性能，通过专家评估和大规模自动分析评估了解释和上下文检索质量。

Conclusion: 该方法为生物医学假设生成提供了有效的可解释性解决方案，能够生成基于科学文献证据的高质量解释。

Abstract: We introduce an explainability method for biomedical hypothesis generation systems, built on top of the novel Hypothesis Generation Context Retriever framework. Our approach combines semantic graph-based retrieval and relevant data-restrictive training to simulate real-world discovery constraints. Integrated with large language models (LLMs) via retrieval-augmented generation, the system explains hypotheses with contextual evidence using published scientific literature. We also propose a novel feedback loop approach, which iteratively identifies and corrects flawed parts of LLM-generated explanations, refining both the evidence paths and supporting context. We demonstrate the performance of our method with multiple large language models and evaluate the explanation and context retrieval quality through both expert-curated assessment and large-scale automated analysis. Our code is available at: https://github.com/IlyaTyagin/HGCR.

</details>


### [5] [Predicting Oscar-Nominated Screenplays with Sentence Embeddings](https://arxiv.org/abs/2511.05500)
*Francis Gross*

Main category: cs.IR

TL;DR: 使用现代语言模型预测奥斯卡剧本提名，通过结合剧本、摘要和标题特征，逻辑回归模型达到0.66的宏F1分数。


<details>
  <summary>Details</summary>
Motivation: 奥斯卡提名对电影行业的可见性和商业成功有重要影响，探索是否能用现代语言模型预测剧本提名。

Method: 创建Movie-O-Label数据集，将剧本分成重叠文本块并用E5句子嵌入模型编码，使用逻辑回归模型进行分类。

Result: 最佳模型在结合剧本、摘要和标题特征时达到宏F1分数0.66，精确率-召回率AP为0.445，ROC-AUC为0.79。

Conclusion: 基于现代文本嵌入的简单模型展现出良好的预测性能，为未来研究提供了起点。

Abstract: Oscar nominations are an important factor in the movie industry because they can boost both the visibility and the commercial success. This work explores whether it is possible to predict Oscar nominations for screenplays using modern language models. Since no suitable dataset was available, a new one called Movie-O-Label was created by combining the MovieSum collection of movie scripts with curated Oscar records. Each screenplay was represented by its title, Wikipedia summary, and full script. Long scripts were split into overlapping text chunks and encoded with the E5 sentence em bedding model. Then, the screenplay embed dings were classified using a logistic regression model. The best results were achieved when three feature inputs related to screenplays (script, summary, and title) were combined. The best-performing model reached a macro F1 score of 0.66, a precision recall AP of 0.445 with baseline 0.19 and a ROC-AUC of 0.79. The results suggest that even simple models based on modern text embeddings demonstrate good prediction performance and might be a starting point for future research.

</details>


### [6] [Ontology Learning and Knowledge Graph Construction: A Comparison of Approaches and Their Impact on RAG Performance](https://arxiv.org/abs/2511.05991)
*Tiago da Cruz,Bernardo Tavares,Francisco Belo*

Main category: cs.IR

TL;DR: 本研究比较了不同知识图谱构建策略对检索增强生成(RAG)系统性能的影响，发现基于关系数据库构建的本体引导知识图谱在性能上可与基于文本构建的方法竞争，同时具有成本更低、复杂度更低的优势。


<details>
  <summary>Details</summary>
Motivation: RAG系统的性能高度依赖于外部知识的表示方式，本研究旨在探索不同知识图谱构建策略如何影响RAG性能，特别是比较基于关系数据库和基于文本的本体构建方法。

Method: 比较了多种方法：标准向量检索RAG、GraphRAG，以及从关系数据库或文本语料库构建的本体引导知识图谱检索。重点分析了包含分块信息的本体引导知识图谱。

Result: 本体引导知识图谱（包含分块信息）实现了与最先进框架相竞争的性能，显著优于向量检索基线。基于关系数据库构建的本体引导知识图谱与基于文本构建的方法性能相当。

Conclusion: 基于关系数据库构建的本体引导知识图谱具有双重优势：只需一次性本体学习过程，大幅降低LLM使用成本；避免了基于文本方法中本体合并的复杂性。

Abstract: Retrieval-Augmented Generation (RAG) systems combine Large Language Models (LLMs) with external knowledge, and their performance depends heavily on how that knowledge is represented. This study investigates how different Knowledge Graph (KG) construction strategies influence RAG performance. We compare a variety of approaches: standard vector-based RAG, GraphRAG, and retrieval over KGs built from ontologies derived either from relational databases or textual corpora. Results show that ontology-guided KGs incorporating chunk information achieve competitive performance with state-of-the-art frameworks, substantially outperforming vector retrieval baselines. Moreover, the findings reveal that ontology-guided KGs built from relational databases perform competitively to ones built with ontologies extracted from text, with the benefit of offering a dual advantage: they require a one-time-only ontology learning process, substantially reducing LLM usage costs; and avoid the complexity of ontology merging inherent to text-based approaches.

</details>


### [7] [When Evidence Contradicts: Toward Safer Retrieval-Augmented Generation in Healthcare](https://arxiv.org/abs/2511.06668)
*Saeedeh Javadi,Sara Mirabi,Manan Gangar,Bahadorreza Ofoghi*

Main category: cs.IR

TL;DR: 研究评估了5个LLM在医疗问答中使用RAG时的表现，发现当检索到的文档包含过时或矛盾信息时，模型回答的准确性和一致性会下降，表明仅靠检索相似性不足以保证可靠的医疗RAG系统。


<details>
  <summary>Details</summary>
Motivation: 在医疗等高风险领域，LLM可能产生幻觉或错误信息，RAG被提出作为缓解策略。但当源文档包含过时或矛盾信息时，这种方法会引入错误。

Method: 创建基于澳大利亚TGA药品信息文档的基准数据集，将标题重新用作自然语言问题；检索PubMed摘要并按出版年份分层，以控制过时证据的时间评估；比较分析过时或矛盾内容对模型生成回答的频率和影响。

Result: 高度相似的摘要之间的确会降低性能，导致模型回答不一致和事实准确性降低。

Conclusion: 仅靠检索相似性不足以保证可靠的医疗RAG，需要矛盾感知的过滤策略来确保高风险领域中的可信回答。

Abstract: In high-stakes information domains such as healthcare, where large language models (LLMs) can produce hallucinations or misinformation, retrieval-augmented generation (RAG) has been proposed as a mitigation strategy, grounding model outputs in external, domain-specific documents. Yet, this approach can introduce errors when source documents contain outdated or contradictory information. This work investigates the performance of five LLMs in generating RAG-based responses to medicine-related queries. Our contributions are three-fold: i) the creation of a benchmark dataset using consumer medicine information documents from the Australian Therapeutic Goods Administration (TGA), where headings are repurposed as natural language questions, ii) the retrieval of PubMed abstracts using TGA headings, stratified across multiple publication years, to enable controlled temporal evaluation of outdated evidence, and iii) a comparative analysis of the frequency and impact of outdated or contradictory content on model-generated responses, assessing how LLMs integrate and reconcile temporally inconsistent information. Our findings show that contradictions between highly similar abstracts do, in fact, degrade performance, leading to inconsistencies and reduced factual accuracy in model answers. These results highlight that retrieval similarity alone is insufficient for reliable medical RAG and underscore the need for contradiction-aware filtering strategies to ensure trustworthy responses in high-stakes domains.

</details>


### [8] [Learning to Fast Unrank in Collaborative Filtering Recommendation](https://arxiv.org/abs/2511.06803)
*Junpeng Zhao,Lin Li,Ming Li,Amran Bhuiyan,Jimmy Huang*

Main category: cs.IR

TL;DR: L2UnRank是一种高效的推荐系统遗忘方法，通过降低目标项目的排名位置来实现隐私保护，相比现有方法提速50倍且保持推荐质量。


<details>
  <summary>Details</summary>
Motivation: 现有推荐遗忘方法存在效率低下和性能下降问题，无法满足实时遗忘需求，需要开发更高效的隐私保护方案。

Method: 采用三阶段方法：基于交互的p跳传播识别影响范围，计算结构性和语义性影响，基于影响信息进行高效的排名感知参数更新。

Result: 在多个数据集和骨干模型上验证，L2UnRank实现最先进的遗忘效果，推荐质量与重新训练相当，速度比现有方法快50倍。

Conclusion: L2UnRank提供了一种模型无关的高效推荐遗忘解决方案，在保护用户隐私的同时维持系统性能。

Abstract: Modern data-driven recommendation systems risk memorizing sensitive user behavioral patterns, raising privacy concerns. Existing recommendation unlearning methods, while capable of removing target data influence, suffer from inefficient unlearning speed and degraded performance, failing to meet real-time unlearning demands. Considering the ranking-oriented nature of recommendation systems, we present unranking, the process of reducing the ranking positions of target items while ensuring the formal guarantees of recommendation unlearning. To achieve efficient unranking, we propose Learning to Fast Unrank in Collaborative Filtering Recommendation (L2UnRank), which operates through three key stages: (a) identifying the influenced scope via interaction-based p-hop propagation, (b) computing structural and semantic influences for entities within this scope, and (c) performing efficient, ranking-aware parameter updates guided by influence information. Extensive experiments across multiple datasets and backbone models demonstrate L2UnRank's model-agnostic nature, achieving state-of-the-art unranking effectiveness and maintaining recommendation quality comparable to retraining, while also delivering a 50x speedup over existing methods. Codes are available at https://github.com/Juniper42/L2UnRank.

</details>


### [9] [Have We Really Understood Collaborative Information? An Empirical Investigation](https://arxiv.org/abs/2511.06905)
*Xiaokun Zhang,Zhaochun Ren,Bowei He,Ziqiang Cui,Chen Ma*

Main category: cs.IR

TL;DR: 本文系统研究了推荐系统中的协同信息，提出了量化定义，分析了其分布特征，评估了对推荐算法性能的影响，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前对推荐系统中协同信息的理解有限，缺乏量化定义、不清楚其在用户-物品交互中的表现方式、对其对推荐性能的影响了解不足。

Method: 通过澄清协同信息的物品共现模式特征，提出量化定义；从多个方面估计协同信息的分布；评估不同推荐算法对协同信息的捕获能力。

Result: 建立了实证分析框架，揭示了协同信息的结构化特征，发现了许多有洞察力的观察结果。

Conclusion: 这项工作推进了对协同信息的理解，为开发更有效的推荐系统提供了有价值的指导方针。

Abstract: Collaborative information serves as the cornerstone of recommender systems which typically focus on capturing it from user-item interactions to deliver personalized services. However, current understanding of this crucial resource remains limited. Specifically, a quantitative definition of collaborative information is missing, its manifestation within user-item interactions remains unclear, and its impact on recommendation performance is largely unknown. To bridge this gap, this work conducts a systematic investigation of collaborative information. We begin by clarifying collaborative information in terms of item co-occurrence patterns, identifying its main characteristics, and presenting a quantitative definition. We then estimate the distribution of collaborative information from several aspects, shedding light on how collaborative information is structured in practice. Furthermore, we evaluate the impact of collaborative information on the performance of various recommendation algorithms. Finally, we highlight challenges in effectively capturing collaborative information and outlook promising directions for future research. By establishing an empirical framework, we uncover many insightful observations that advance our understanding of collaborative information and offer valuable guidelines for developing more effective recommender systems.

</details>


### [10] [Fine-Tuning Diffusion-Based Recommender Systems via Reinforcement Learning with Reward Function Optimization](https://arxiv.org/abs/2511.06937)
*Yu Hou,Hua Li,Ha Young Kim,Won-Yong Shin*

Main category: cs.IR

TL;DR: ReFiT框架将强化学习微调集成到基于扩散的推荐系统中，通过马尔可夫决策过程建模去噪轨迹，使用协同信号感知奖励函数直接反映推荐质量，实现高效的后训练微调。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在推荐系统中表现出色，但从头训练计算成本高且收敛后收益递减。需要一种高效的微调方法来提升性能。

Method: 将去噪轨迹建模为马尔可夫决策过程，设计协同信号感知奖励函数，使用策略梯度优化最大化观察交互的对数似然。

Result: 在多个真实数据集上，ReFiT相比强基线获得显著性能提升（序列推荐上达36.3%），具有线性复杂度，在多种扩散推荐场景中泛化良好。

Conclusion: ReFiT框架通过强化学习微调有效提升了扩散推荐系统的性能，同时保持了计算效率，具有良好的通用性。

Abstract: Diffusion models recently emerged as a powerful paradigm for recommender systems, offering state-of-the-art performance by modeling the generative process of user-item interactions. However, training such models from scratch is both computationally expensive and yields diminishing returns once convergence is reached. To remedy these challenges, we propose ReFiT, a new framework that integrates Reinforcement learning (RL)-based Fine-Tuning into diffusion-based recommender systems. In contrast to prior RL approaches for diffusion models depending on external reward models, ReFiT adopts a task-aligned design: it formulates the denoising trajectory as a Markov decision process (MDP) and incorporates a collaborative signal-aware reward function that directly reflects recommendation quality. By tightly coupling the MDP structure with this reward signal, ReFiT empowers the RL agent to exploit high-order connectivity for fine-grained optimization, while avoiding the noisy or uninformative feedback common in naive reward designs. Leveraging policy gradient optimization, ReFiT maximizes exact log-likelihood of observed interactions, thereby enabling effective post hoc fine-tuning of diffusion recommenders. Comprehensive experiments on wide-ranging real-world datasets demonstrate that the proposed ReFiT framework (a) exhibits substantial performance gains over strong competitors (up to 36.3% on sequential recommendation), (b) demonstrates strong efficiency with linear complexity in the number of users or items, and (c) generalizes well across multiple diffusion-based recommendation scenarios. The source code and datasets are publicly available at https://anonymous.4open.science/r/ReFiT-4C60.

</details>


### [11] [Wavelet Enhanced Adaptive Frequency Filter for Sequential Recommendation](https://arxiv.org/abs/2511.07028)
*Huayang Xu,Huanhuan Yuan,Guanfeng Liu,Junhua Fang,Lei Zhao,Pengpeng Zhao*

Main category: cs.IR

TL;DR: 提出了一种基于小波增强自适应频率滤波的序列推荐方法，通过动态频率域滤波和小波特征增强来解决现有频率域方法在个性化过滤和非平稳信号处理方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有频率域推荐方法存在两个关键问题：使用静态过滤器忽略行为模式的个性化特性，以及全局傅里叶变换会模糊非平稳信号和短期波动。

Method: 包含两个核心模块：动态频率域滤波（根据行为序列动态调整滤波操作）和小波特征增强（通过小波变换重建序列，增强非平稳信号和短期波动）。

Result: 在四个广泛使用的基准数据集上的大量实验证明了该方法的优越性。

Conclusion: 该方法在长序列推荐场景中实现了全面的性能和效率优化。

Abstract: Sequential recommendation has garnered significant attention for its ability to capture dynamic preferences by mining users' historical interaction data. Given that users' complex and intertwined periodic preferences are difficult to disentangle in the time domain, recent research is exploring frequency domain analysis to identify these hidden patterns. However, current frequency-domain-based methods suffer from two key limitations: (i) They primarily employ static filters with fixed characteristics, overlooking the personalized nature of behavioral patterns; (ii) While the global discrete Fourier transform excels at modeling long-range dependencies, it can blur non-stationary signals and short-term fluctuations. To overcome these limitations, we propose a novel method called Wavelet Enhanced Adaptive Frequency Filter for Sequential Recommendation. Specifically, it consists of two vital modules: dynamic frequency-domain filtering and wavelet feature enhancement. The former is used to dynamically adjust filtering operations based on behavioral sequences to extract personalized global information, and the latter integrates wavelet transform to reconstruct sequences, enhancing blurred non-stationary signals and short-term fluctuations. Finally, these two modules work to achieve comprehensive performance and efficiency optimization in long sequential recommendation scenarios. Extensive experiments on four widely-used benchmark datasets demonstrate the superiority of our work.

</details>


### [12] [Hard vs. Noise: Resolving Hard-Noisy Sample Confusion in Recommender Systems via Large Language Models](https://arxiv.org/abs/2511.07295)
*Tianrui Song,Wen-Shuo Chao,Hao Liu*

Main category: cs.IR

TL;DR: LLMHNI框架利用大语言模型生成的两个辅助用户-物品相关性信号来区分困难样本和噪声样本，解决推荐系统中隐式反馈的噪声问题。


<details>
  <summary>Details</summary>
Motivation: 隐式反馈在训练推荐系统时不可避免地面临噪声问题，而传统方法难以区分噪声样本和困难样本，后者对建模用户偏好至关重要。

Method: 使用LLM获取用户-物品语义相关性进行负采样，提出目标对齐策略将LLM嵌入投影到用户-物品相关性建模空间，利用LLM推断的逻辑相关性识别样本，并通过图对比学习抑制不可靠边。

Result: 实证结果表明LLMHNI显著提高了去噪和推荐性能。

Conclusion: LLMHNI通过利用LLM生成的辅助信号有效解决了困难-噪声混淆问题，提升了推荐系统的性能。

Abstract: Implicit feedback, employed in training recommender systems, unavoidably confronts noise due to factors such as misclicks and position bias. Previous studies have attempted to identify noisy samples through their diverged data patterns, such as higher loss values, and mitigate their influence through sample dropping or reweighting. However, we observed that noisy samples and hard samples display similar patterns, leading to hard-noisy confusion issue. Such confusion is problematic as hard samples are vital for modeling user preferences. To solve this problem, we propose LLMHNI framework, leveraging two auxiliary user-item relevance signals generated by Large Language Models (LLMs) to differentiate hard and noisy samples. LLMHNI obtains user-item semantic relevance from LLM-encoded embeddings, which is used in negative sampling to select hard negatives while filtering out noisy false negatives. An objective alignment strategy is proposed to project LLM-encoded embeddings, originally for general language tasks, into a representation space optimized for user-item relevance modeling. LLMHNI also exploits LLM-inferred logical relevance within user-item interactions to identify hard and noisy samples. These LLM-inferred interactions are integrated into the interaction graph and guide denoising with cross-graph contrastive alignment. To eliminate the impact of unreliable interactions induced by LLM hallucination, we propose a graph contrastive learning strategy that aligns representations from randomly edge-dropped views to suppress unreliable edges. Empirical results demonstrate that LLMHNI significantly improves denoising and recommendation performance.

</details>
