<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 5]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [From AutoRecSys to AutoRecLab: A Call to Build, Evaluate, and Govern Autonomous Recommender-Systems Research Labs](https://arxiv.org/abs/2510.18104)
*Joeran Beel,Bela Gipp,Tobias Vente,Moritz Baumgart,Philipp Meister*

Main category: cs.IR

TL;DR: 提出从狭隘的AutoRecSys工具转向自主推荐系统研究实验室(AutoRecLab)，实现端到端自动化研究流程，包括问题构思、文献分析、实验设计执行、结果解释、论文草稿和溯源记录。


<details>
  <summary>Details</summary>
Motivation: 推荐系统研究在模型和评估方面取得进展，但忽视了研究过程本身的自动化。现有工具主要关注算法选择和超参数调优，缺乏端到端的自动化研究能力。

Method: 借鉴自动化科学的最新进展，提出AutoRecLab框架，结合LLM驱动的构思和报告与自动化实验，建立基准测试和竞赛来评估代理在最小人工输入下产生可复现发现的能力。

Result: 提出了推进该议程的五项具体行动：构建开放AutoRecLab原型、建立基准和竞赛、创建AI生成投稿的评审渠道、定义归因和可复现性标准、促进跨学科伦理对话。

Conclusion: 推进这一议程可以提高研究吞吐量、发现非显而易见的见解，并让推荐系统为新兴的人工研究智能做出贡献。呼吁组织社区研讨会来协调后续步骤并共同制定负责任集成自动化研究系统的指南。

Abstract: Recommender-systems research has accelerated model and evaluation advances,
yet largely neglects automating the research process itself. We argue for a
shift from narrow AutoRecSys tools -- focused on algorithm selection and
hyper-parameter tuning -- to an Autonomous Recommender-Systems Research Lab
(AutoRecLab) that integrates end-to-end automation: problem ideation,
literature analysis, experimental design and execution, result interpretation,
manuscript drafting, and provenance logging. Drawing on recent progress in
automated science (e.g., multi-agent AI Scientist and AI Co-Scientist systems),
we outline an agenda for the RecSys community: (1) build open AutoRecLab
prototypes that combine LLM-driven ideation and reporting with automated
experimentation; (2) establish benchmarks and competitions that evaluate agents
on producing reproducible RecSys findings with minimal human input; (3) create
review venues for transparently AI-generated submissions; (4) define standards
for attribution and reproducibility via detailed research logs and metadata;
and (5) foster interdisciplinary dialogue on ethics, governance, privacy, and
fairness in autonomous research. Advancing this agenda can increase research
throughput, surface non-obvious insights, and position RecSys to contribute to
emerging Artificial Research Intelligence. We conclude with a call to organise
a community retreat to coordinate next steps and co-author guidance for the
responsible integration of automated research systems.

</details>


### [2] [LIME: Link-based user-item Interaction Modeling with decoupled xor attention for Efficient test time scaling](https://arxiv.org/abs/2510.18239)
*Yunjiang Jiang,Ayush Agarwal,Yang Liu,Bi Xue*

Main category: cs.IR

TL;DR: LIME是一种新颖的推荐系统架构，通过低秩链接嵌入和线性注意力机制，显著降低了计算复杂度，实现了与最先进transformer模型相近的性能，但推理速度提高了10倍。


<details>
  <summary>Details</summary>
Motivation: 解决transformer在推荐系统中计算成本随用户序列长度和候选集大小线性或二次增长的问题，使扩展候选集或增加序列长度在推理时变得可行。

Method: 1. 低秩"链接嵌入"：解耦用户和候选交互，预计算注意力权重，使推理成本几乎与候选集大小无关；2. 线性注意力机制LIME-XOR：将用户序列长度的复杂度从二次降低到线性。

Result: 在公共和工业数据集上的实验表明，LIME实现了与最先进transformer相近的性能，但在大候选集或长序列长度下推理速度提高了10倍。在主要推荐平台测试中，LIME提高了用户参与度，同时保持了对候选集大小和用户历史长度的最小推理成本。

Conclusion: LIME为高效且表达能力强的推荐系统建立了新范式，解决了transformer在推荐系统中的计算瓶颈问题。

Abstract: Scaling large recommendation systems requires advancing three major
frontiers: processing longer user histories, expanding candidate sets, and
increasing model capacity. While promising, transformers' computational cost
scales quadratically with the user sequence length and linearly with the number
of candidates. This trade-off makes it prohibitively expensive to expand
candidate sets or increase sequence length at inference, despite the
significant performance improvements.
  We introduce \textbf{LIME}, a novel architecture that resolves this
trade-off. Through two key innovations, LIME fundamentally reduces
computational complexity. First, low-rank ``link embeddings" enable
pre-computation of attention weights by decoupling user and candidate
interactions, making the inference cost nearly independent of candidate set
size. Second, a linear attention mechanism, \textbf{LIME-XOR}, reduces the
complexity with respect to user sequence length from quadratic ($O(N^2)$) to
linear ($O(N)$).
  Experiments on public and industrial datasets show LIME achieves near-parity
with state-of-the-art transformers but with a 10$\times$ inference speedup on
large candidate sets or long sequence lengths. When tested on a major
recommendation platform, LIME improved user engagement while maintaining
minimal inference costs with respect to candidate set size and user history
length, establishing a new paradigm for efficient and expressive recommendation
systems.

</details>


### [3] [Enhancing Hotel Recommendations with AI: LLM-Based Review Summarization and Query-Driven Insights](https://arxiv.org/abs/2510.18277)
*Nikolaos Belibasakis,Anastasios Giannaros,Ioanna Giannoukou,Spyros Sioutas*

Main category: cs.IR

TL;DR: 开发了一个名为instaGuide的Web应用，利用大型语言模型(LLMs)自动分析Booking.com平台的用户评论，生成评论摘要并支持用户查询特定房产信息，显著提升短租公寓推荐效率。


<details>
  <summary>Details</summary>
Motivation: 随着Booking.com和AirBnB等预订平台数据量激增，用户难以高效浏览住宿选项和分析评论。虽然平台提供了星级、设施、价格等筛选功能，但最有价值的洞察来自非结构化的文本评论，而逐条阅读这些评论耗时且效率低下。

Method: 开发instaGuide Web应用，自动化提取Booking.com平台上的文本评论，利用多种LLM模型进行评论摘要和关键信息挖掘，支持用户针对特定房产方面进行查询。在开发过程中评估了多个LLM模型在准确性、成本和响应质量方面的表现。

Result: LLM驱动的摘要技术显著减少了用户寻找合适短租公寓所需的时间，改善了整体决策过程。

Conclusion: 基于LLM的评论摘要和挖掘技术能够有效提升短租公寓推荐系统的效率，为用户提供更有价值的决策支持。

Abstract: The increasing number of data a booking platform such as Booking.com and
AirBnB offers make it challenging for interested parties to browse through the
available accommodations and analyze reviews in an efficient way. Efforts have
been made from the booking platform providers to utilize recommender systems in
an effort to enable the user to filter the results by factors such as stars,
amenities, cost but most valuable insights can be provided by the unstructured
text-based reviews. Going through these reviews one-by-one requires a
substantial amount of time to be devoted while a respectable percentage of the
reviews won't provide to the user what they are actually looking for.
  This research publication explores how Large Language Models (LLMs) can
enhance short rental apartments recommendations by summarizing and mining key
insights from user reviews. The web application presented in this paper, named
"instaGuide", automates the procedure of isolating the text-based user reviews
from a property on the Booking.com platform, synthesizing the summary of the
reviews, and enabling the user to query specific aspects of the property in an
effort to gain feedback on their personal questions/criteria.
  During the development of the instaGuide tool, numerous LLM models were
evaluated based on accuracy, cost, and response quality. The results suggest
that the LLM-powered summarization reduces significantly the amount of time the
users need to devote on their search for the right short rental apartment,
improving the overall decision-making procedure.

</details>


### [4] [Evaluating LLM-Based Mobile App Recommendations: An Empirical Study](https://arxiv.org/abs/2510.18364)
*Quim Motger,Xavier Franch,Vincenzo Gervasi,Jordi Marco*

Main category: cs.IR

TL;DR: 本文对大型语言模型在移动应用推荐中的表现进行了实证分析，揭示了其推荐标准与传统应用商店优化指标之间的差异，以及推荐一致性和对明确排名指令的响应性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs越来越多地通过自然语言提示推荐移动应用，但其推荐逻辑不透明，引发了关于推荐一致性、可解释性以及与标准ASO指标对齐的疑问。

Method: 构建了16个通用排名标准的分类法，开发了系统评估框架来分析推荐一致性和对明确排名指令的响应性，并提供了可复现的研究包。

Result: 发现LLMs依赖广泛但碎片化的排名标准，仅部分与标准ASO指标对齐。排名靠前的应用在不同运行中较为一致，但随着排名深度和搜索特异性增加，变异性增大。LLMs对明确排名指令的敏感性各不相同。

Conclusion: 研究结果为终端用户、应用开发者和推荐系统研究人员在对话式应用发现这一新兴领域中提供了指导，揭示了LLMs在对话式应用发现中的复杂推理动态。

Abstract: Large Language Models (LLMs) are increasingly used to recommend mobile
applications through natural language prompts, offering a flexible alternative
to keyword-based app store search. Yet, the reasoning behind these
recommendations remains opaque, raising questions about their consistency,
explainability, and alignment with traditional App Store Optimization (ASO)
metrics. In this paper, we present an empirical analysis of how widely-used
general purpose LLMs generate, justify, and rank mobile app recommendations.
Our contributions are: (i) a taxonomy of 16 generalizable ranking criteria
elicited from LLM outputs; (ii) a systematic evaluation framework to analyse
recommendation consistency and responsiveness to explicit ranking instructions;
and (iii) a replication package to support reproducibility and future research
on AI-based recommendation systems. Our findings reveal that LLMs rely on a
broad yet fragmented set of ranking criteria, only partially aligned with
standard ASO metrics. While top-ranked apps tend to be consistent across runs,
variability increases with ranking depth and search specificity. LLMs exhibit
varying sensitivity to explicit ranking instructions - ranging from substantial
adaptations to near-identical outputs - highlighting their complex reasoning
dynamics in conversational app discovery. Our results aim to support end-users,
app developers, and recommender-systems researchers in navigating the emerging
landscape of conversational app discovery.

</details>


### [5] [LLMs as Sparse Retrievers:A Framework for First-Stage Product Search](https://arxiv.org/abs/2510.18527)
*Hongru Song,Yu-an Liu,Ruqing Zhang,Jiafeng Guo,Maarten de Rijke,Sen Li,Wenjun Peng,Fuyu Lv,Xueqi Cheng*

Main category: cs.IR

TL;DR: PROSPER是一个利用大语言模型作为稀疏检索器的产品搜索框架，通过字面残差网络缓解幻觉问题，使用词汇聚焦窗口实现从粗到细的稀疏化策略，显著提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 产品搜索中稀疏检索方法存在严重的词汇不匹配问题，导致性能不佳。大语言模型具有语义分析潜力，但直接应用会面临幻觉和训练初始化困难两大挑战。

Method: 提出PROSPER框架，包含：(1)字面残差网络，通过残差补偿机制强化被低估的字面词项；(2)词汇聚焦窗口，采用从粗到细的稀疏化策略实现有效训练初始化。

Result: 离线和在线实验表明，PROSPER显著优于稀疏基线方法，召回性能与先进稠密检索器相当，并在线上实现了收入增长。

Conclusion: PROSPER成功解决了LLM在稀疏检索中的幻觉和训练挑战，为产品搜索提供了高效且高性能的解决方案。

Abstract: Product search is a crucial component of modern e-commerce platforms, with
billions of user queries every day. In product search systems, first-stage
retrieval should achieve high recall while ensuring efficient online
deployment. Sparse retrieval is particularly attractive in this context due to
its interpretability and storage efficiency. However, sparse retrieval methods
suffer from severe vocabulary mismatch issues, leading to suboptimal
performance in product search scenarios.With their potential for semantic
analysis, large language models (LLMs) offer a promising avenue for mitigating
vocabulary mismatch issues and thereby improving retrieval quality. Directly
applying LLMs to sparse retrieval in product search exposes two key
challenges:(1)Queries and product titles are typically short and highly
susceptible to LLM-induced hallucinations, such as generating irrelevant
expansion terms or underweighting critical literal terms like brand names and
model numbers;(2)The large vocabulary space of LLMs leads to difficulty in
initializing training effectively, making it challenging to learn meaningful
sparse representations in such ultra-high-dimensional spaces.To address these
challenges, we propose PROSPER, a framework for PROduct search leveraging LLMs
as SParsE Retrievers. PROSPER incorporates: (1)A literal residual network that
alleviates hallucination in lexical expansion by reinforcing underweighted
literal terms through a residual compensation mechanism; and (2)A lexical
focusing window that facilitates effective training initialization via a
coarse-to-fine sparsification strategy.Extensive offline and online experiments
show that PROSPER significantly outperforms sparse baselines and achieves
recall performance comparable to advanced dense retrievers, while also
achieving revenue increments online.

</details>
