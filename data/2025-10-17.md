<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 12]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [FinAI Data Assistant: LLM-based Financial Database Query Processing with the OpenAI Function Calling API](https://arxiv.org/abs/2510.14162)
*Juhyeong Kim,Yejin Kim,Youngbin Lee,Hyunwoo Byun*

Main category: cs.IR

TL;DR: FinAI Data Assistant使用LLM结合OpenAI函数调用API，通过参数化查询而非完整SQL生成来实现金融数据库的自然语言查询，在可靠性、延迟和成本方面优于text-to-SQL方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统text-to-SQL方法在金融数据库查询中存在的可靠性、延迟和成本问题，探索LLM在金融数据查询中的实际应用效果。

Method: 结合LLM和OpenAI函数调用API，使用经过验证的参数化查询库而非生成完整SQL，通过三个研究问题评估LLM在金融数据查询中的表现。

Result: LLM单独预测存在不可忽视的误差和时间偏差；股票代码映射准确率接近完美；FinAI Data Assistant在延迟、成本和可靠性方面优于text-to-SQL基准。

Conclusion: 参数化查询方法在金融数据库自然语言查询中比text-to-SQL更具优势，但需权衡设计取舍和部署限制。

Abstract: We present FinAI Data Assistant, a practical approach for natural-language
querying over financial databases that combines large language models (LLMs)
with the OpenAI Function Calling API. Rather than synthesizing complete SQL via
text-to-SQL, our system routes user requests to a small library of vetted,
parameterized queries, trading generative flexibility for reliability, low
latency, and cost efficiency. We empirically study three questions: (RQ1)
whether LLMs alone can reliably recall or extrapolate time-dependent financial
data without external retrieval; (RQ2) how well LLMs map company names to stock
ticker symbols; and (RQ3) whether function calling outperforms text-to-SQL for
end-to-end database query processing. Across controlled experiments on prices
and fundamentals, LLM-only predictions exhibit non-negligible error and show
look-ahead bias primarily for stock prices relative to model knowledge cutoffs.
Ticker-mapping accuracy is near-perfect for NASDAQ-100 constituents and high
for S\&P~500 firms. Finally, FinAI Data Assistant achieves lower latency and
cost and higher reliability than a text-to-SQL baseline on our task suite. We
discuss design trade-offs, limitations, and avenues for deployment.

</details>


### [2] [Large Scale Retrieval for the LinkedIn Feed using Causal Language Models](https://arxiv.org/abs/2510.14223)
*Sudarshan Srinivasa Ramanujam,Antonio Alonso,Saurabh Kataria,Siddharth Dangi,Akhilesh Gupta,Birjodh Singh Tiwana,Manas Somaiya,Luke Simon,David Byrne,Sojeong Ha,Sen Zhou,Andrei Akterskii,Zhanglong Liu,Samira Sriram,Crescent Xiong,Zhoutao Pei,Angela Shao,Alex Li,Annie Xiao,Caitlin Kolb,Thomas Kistler,Zach Moore,Hamed Firooz*

Main category: cs.IR

TL;DR: LinkedIn提出了一种基于LLaMA 3大语言模型的检索方法，通过双编码器生成用户和内容的文本嵌入，用于大规模推荐系统的候选检索阶段。


<details>
  <summary>Details</summary>
Motivation: 在LinkedIn Feed等大规模推荐系统中，检索阶段需要从数亿候选内容中快速筛选出少量高质量内容，现有方法在效率和效果上存在挑战。

Method: 使用Meta的LLaMA 3大语言模型作为双编码器，仅基于文本输入生成用户和内容的嵌入表示，设计了完整的端到端流水线，包括提示设计、大规模微调技术和低延迟在线服务基础设施。

Result: 离线指标和在线A/B测试显示成员参与度显著提升，特别是对新用户效果更明显，表明高质量的建议内容有助于用户留存。

Conclusion: 这项工作证明了生成式语言模型可以有效地应用于工业级实时高吞吐量检索场景。

Abstract: In large scale recommendation systems like the LinkedIn Feed, the retrieval
stage is critical for narrowing hundreds of millions of potential candidates to
a manageable subset for ranking. LinkedIn's Feed serves suggested content from
outside of the member's network (based on the member's topical interests),
where 2000 candidates are retrieved from a pool of hundreds of millions
candidate with a latency budget of a few milliseconds and inbound QPS of
several thousand per second. This paper presents a novel retrieval approach
that fine-tunes a large causal language model (Meta's LLaMA 3) as a dual
encoder to generate high quality embeddings for both users (members) and
content (items), using only textual input. We describe the end to end pipeline,
including prompt design for embedding generation, techniques for fine-tuning at
LinkedIn's scale, and infrastructure for low latency, cost effective online
serving. We share our findings on how quantizing numerical features in the
prompt enables the information to get properly encoded in the embedding,
facilitating greater alignment between the retrieval and ranking layer. The
system was evaluated using offline metrics and an online A/B test, which showed
substantial improvements in member engagement. We observed significant gains
among newer members, who often lack strong network connections, indicating that
high-quality suggested content aids retention. This work demonstrates how
generative language models can be effectively adapted for real time, high
throughput retrieval in industrial applications.

</details>


### [3] [Synergistic Integration and Discrepancy Resolution of Contextualized Knowledge for Personalized Recommendation](https://arxiv.org/abs/2510.14257)
*Lingyu Mu,Hao Deng,Haibo Xing,Kaican Lin,Zhitong Zhu,Yu Zhang,Xiaoyi Zeng,Zhengxiao Liu,Zheng Lin,Jinxin Hu*

Main category: cs.IR

TL;DR: CoCo框架通过动态构建用户特定上下文知识嵌入，解决了LLM推荐系统中静态模板忽略用户偏好多样性和语义-行为特征浅层对齐的问题，在多个基准测试中实现最高8.58%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推荐系统采用静态模式提示机制，存在两个主要局限：忽视用户偏好多样性的通用模板结构，以及语义知识与行为特征空间的浅层对齐而非深度潜在空间整合。

Method: CoCo端到端框架采用双机制方法动态构建用户特定上下文知识嵌入，通过自适应知识融合和矛盾解决模块实现语义和行为潜在维度的深度整合。

Result: 在多样化基准数据集和企业级电商平台上的实验评估显示，CoCo相比七种前沿方法在推荐准确率上最高提升8.58%，在生产广告系统部署中实现1.91%的销售增长。

Conclusion: CoCo通过模块化设计和模型无关架构，为需要知识增强推理和个性化适应的下一代推荐系统提供了通用解决方案。

Abstract: The integration of large language models (LLMs) into recommendation systems
has revealed promising potential through their capacity to extract world
knowledge for enhanced reasoning capabilities. However, current methodologies
that adopt static schema-based prompting mechanisms encounter significant
limitations: (1) they employ universal template structures that neglect the
multi-faceted nature of user preference diversity; (2) they implement
superficial alignment between semantic knowledge representations and behavioral
feature spaces without achieving comprehensive latent space integration. To
address these challenges, we introduce CoCo, an end-to-end framework that
dynamically constructs user-specific contextual knowledge embeddings through a
dual-mechanism approach. Our method realizes profound integration of semantic
and behavioral latent dimensions via adaptive knowledge fusion and
contradiction resolution modules. Experimental evaluations across diverse
benchmark datasets and an enterprise-level e-commerce platform demonstrate
CoCo's superiority, achieving a maximum 8.58% improvement over seven
cutting-edge methods in recommendation accuracy. The framework's deployment on
a production advertising system resulted in a 1.91% sales growth, validating
its practical effectiveness. With its modular design and model-agnostic
architecture, CoCo provides a versatile solution for next-generation
recommendation systems requiring both knowledge-enhanced reasoning and
personalized adaptation.

</details>


### [4] [Large Reasoning Embedding Models: Towards Next-Generation Dense Retrieval Paradigm](https://arxiv.org/abs/2510.14321)
*Jianting Tang,Dongshuai Li,Tao Wen,Fuyu Lv,Dan Ou,Linli Xu*

Main category: cs.IR

TL;DR: 提出LREM模型，通过将推理过程融入表示学习来解决密集检索中的语义鸿沟问题，特别针对词汇差异大的困难查询，显著提升检索准确性。


<details>
  <summary>Details</summary>
Motivation: 现有密集检索模型虽然采用LLM生成嵌入，但仍采用直接嵌入方法，语义准确性不足，且对比学习导致模型偏向浅层词汇和语义匹配，对词汇差异大的困难查询性能显著下降。

Method: 采用两阶段训练：第一阶段通过SFT和InfoNCE损失在精心构建的Query-CoT-Item三元组上优化LLM，建立初步推理和嵌入能力；第二阶段通过强化学习进一步优化推理轨迹。

Result: 离线和在线实验验证了LREM的有效性，已于2025年8月在中国最大的电商平台部署。

Conclusion: LREM通过集成推理过程到表示学习中，有效弥合原始查询与目标商品之间的语义鸿沟，显著提升检索准确性。

Abstract: In modern e-commerce search systems, dense retrieval has become an
indispensable component. By computing similarities between query and item
(product) embeddings, it efficiently selects candidate products from
large-scale repositories. With the breakthroughs in large language models
(LLMs), mainstream embedding models have gradually shifted from BERT to LLMs
for more accurate text modeling. However, these models still adopt
direct-embedding methods, and the semantic accuracy of embeddings remains
inadequate. Therefore, contrastive learning is heavily employed to achieve
tight semantic alignment between positive pairs. Consequently, such models tend
to capture statistical co-occurrence patterns in the training data, biasing
them toward shallow lexical and semantic matches. For difficult queries
exhibiting notable lexical disparity from target items, the performance
degrades significantly. In this work, we propose the Large Reasoning Embedding
Model (LREM), which novelly integrates reasoning processes into representation
learning. For difficult queries, LREM first conducts reasoning to achieve a
deep understanding of the original query, and then produces a
reasoning-augmented query embedding for retrieval. This reasoning process
effectively bridges the semantic gap between original queries and target items,
significantly improving retrieval accuracy. Specifically, we adopt a two-stage
training process: the first stage optimizes the LLM on carefully curated
Query-CoT-Item triplets with SFT and InfoNCE losses to establish preliminary
reasoning and embedding capabilities, and the second stage further refines the
reasoning trajectories via reinforcement learning (RL). Extensive offline and
online experiments validate the effectiveness of LREM, leading to its
deployment on China's largest e-commerce platform since August 2025.

</details>


### [5] [Ensembling Multiple Hallucination Detectors Trained on VLLM Internal Representations](https://arxiv.org/abs/2510.14330)
*Yuto Nakamizo,Ryuhei Miyazato,Hikaru Tanabe,Ryuta Yamakura,Kiori Hatanaka*

Main category: cs.IR

TL;DR: 本文提出了一个基于逻辑回归的幻觉检测方法，通过使用VLM的隐藏状态和特定注意力头输出来减少视觉问答中的幻觉，在KDD Cup 2025的Meta CRAG-MM挑战中获得第五名。


<details>
  <summary>Details</summary>
Motivation: 由于错误答案会导致负分，需要减少VLM内部表示中的幻觉，以提高VQA准确性。

Method: 使用隐藏状态和特定注意力头输出训练逻辑回归幻觉检测模型，并采用集成方法。

Result: 虽然牺牲了一些正确答案，但显著减少了幻觉，在最终排行榜上获得前五名。

Conclusion: 基于逻辑回归的幻觉检测集成方法能有效减少VLM的幻觉问题，提升VQA性能。

Abstract: This paper presents the 5th place solution by our team, y3h2, for the Meta
CRAG-MM Challenge at KDD Cup 2025. The CRAG-MM benchmark is a visual question
answering (VQA) dataset focused on factual questions about images, including
egocentric images. The competition was contested based on VQA accuracy, as
judged by an LLM-based automatic evaluator. Since incorrect answers result in
negative scores, our strategy focused on reducing hallucinations from the
internal representations of the VLM. Specifically, we trained logistic
regression-based hallucination detection models using both the hidden_state and
the outputs of specific attention heads. We then employed an ensemble of these
models. As a result, while our method sacrificed some correct answers, it
significantly reduced hallucinations and allowed us to place among the top
entries on the final leaderboard. For implementation details and code, please
refer to
https://gitlab.aicrowd.com/htanabe/meta-comprehensive-rag-benchmark-starter-kit.

</details>


### [6] [GemiRec: Interest Quantization and Generation for Multi-Interest Recommendation](https://arxiv.org/abs/2510.14626)
*Zhibo Wu,Yunfan Wu,Quan Liu,Lin Jiang,Ping Yang,Yao Hu*

Main category: cs.IR

TL;DR: 提出GemiRec框架解决多兴趣推荐中的兴趣坍缩和兴趣演化建模不足问题，通过兴趣量化和兴趣生成机制改进多兴趣表示学习


<details>
  <summary>Details</summary>
Motivation: 传统多兴趣推荐存在两个核心问题：兴趣坍缩（多个用户表示同质化）和兴趣演化建模不足（难以捕捉用户历史行为中未出现的潜在兴趣）

Method: 提出GemiRec框架，包含三个模块：兴趣字典维护模块（共享量化兴趣字典）、多兴趣后验分布模块（生成模型捕捉用户未来兴趣分布）、多兴趣检索模块（使用多兴趣表示检索物品）

Result: 理论和实证分析均证明该方法的优势和有效性，已在2025年3月部署到生产环境，显示其工业应用价值

Conclusion: GemiRec通过框架级改进有效解决了多兴趣推荐中的关键问题，在工业检索阶段具有实用价值

Abstract: Multi-interest recommendation has gained attention, especially in industrial
retrieval stage. Unlike classical dual-tower methods, it generates multiple
user representations instead of a single one to model comprehensive user
interests. However, prior studies have identified two underlying limitations:
the first is interest collapse, where multiple representations homogenize. The
second is insufficient modeling of interest evolution, as they struggle to
capture latent interests absent from a user's historical behavior. We begin
with a thorough review of existing works in tackling these limitations. Then,
we attempt to tackle these limitations from a new perspective. Specifically, we
propose a framework-level refinement for multi-interest recommendation, named
GemiRec. The proposed framework leverages interest quantization to enforce a
structural interest separation and interest generation to learn the evolving
dynamics of user interests explicitly. It comprises three modules: (a) Interest
Dictionary Maintenance Module (IDMM) maintains a shared quantized interest
dictionary. (b) Multi-Interest Posterior Distribution Module (MIPDM) employs a
generative model to capture the distribution of user future interests. (c)
Multi-Interest Retrieval Module (MIRM) retrieves items using multiple
user-interest representations. Both theoretical and empirical analyses, as well
as extensive experiments, demonstrate its advantages and effectiveness.
Moreover, it has been deployed in production since March 2025, showing its
practical value in industrial applications.

</details>


### [7] [MR.Rec: Synergizing Memory and Reasoning for Personalized Recommendation Assistant with LLMs](https://arxiv.org/abs/2510.14629)
*Jiani Huang,Xingchen Zou,Lianghao Xia,Qing Li*

Main category: cs.IR

TL;DR: 提出MR.Rec框架，结合记忆和推理能力来提升LLM在推荐系统中的性能，通过RAG系统增强个性化，并设计强化学习框架优化记忆利用和推理策略。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在推荐系统中面临上下文窗口有限和单轮推理的约束，难以捕捉动态用户偏好和进行主动推理，需要解决深度个性化和智能推理的挑战。

Method: 开发综合RAG系统进行高效索引和检索外部记忆，集成推理增强的记忆检索，并设计强化学习框架训练LLM自主学习记忆利用和推理优化的策略。

Result: 在多个指标上显著优于现有最先进基线方法，验证了其在提供智能和个性化推荐方面的有效性。

Conclusion: MR.Rec通过动态记忆检索与自适应推理的结合，确保了更准确、上下文感知和高度个性化的推荐，解决了LLM在推荐系统中的关键挑战。

Abstract: The application of Large Language Models (LLMs) in recommender systems faces
key challenges in delivering deep personalization and intelligent reasoning,
especially for interactive scenarios. Current methods are often constrained by
limited context windows and single-turn reasoning, hindering their ability to
capture dynamic user preferences and proactively reason over recommendation
contexts. To address these limitations, we propose MR.Rec, a novel framework
that synergizes memory and reasoning for LLM-based recommendations. To achieve
personalization, we develop a comprehensive Retrieval-Augmented Generation
(RAG) system that efficiently indexes and retrieves relevant external memory to
enhance LLM personalization capabilities. Furthermore, to enable the synergy
between memory and reasoning, our RAG system goes beyond conventional
query-based retrieval by integrating reasoning enhanced memory retrieval.
Finally, we design a reinforcement learning framework that trains the LLM to
autonomously learn effective strategies for both memory utilization and
reasoning refinement. By combining dynamic memory retrieval with adaptive
reasoning, this approach ensures more accurate, context-aware, and highly
personalized recommendations. Extensive experiments demonstrate that MR.Rec
significantly outperforms state-of-the-art baselines across multiple metrics,
validating its efficacy in delivering intelligent and personalized
recommendations. We will release code and data upon paper notification.

</details>


### [8] [Causality Enhancement for Cross-Domain Recommendation](https://arxiv.org/abs/2510.14641)
*Zhibo Wu,Yunfan Wu,Lin Jiang,Ping Yang,Yao Hu*

Main category: cs.IR

TL;DR: 提出CE-CDR框架，通过因果图建模和部分标签因果损失来增强跨域推荐，解决源域任务不一致和因果关系忽略的问题


<details>
  <summary>Details</summary>
Motivation: 传统跨域推荐方法存在两个问题：源域任务不一致可能导致负迁移，忽略因果关系会限制源域特征的贡献。直接在有因果标签的数据上训练跨域表示是理想方案，但真实因果标签难以获取

Method: 1) 将跨域推荐建模为因果图；2) 启发式构建因果感知数据集；3) 推导理论上无偏的部分标签因果损失，生成增强的跨域表示；4) 将表示输入目标模型增强推荐

Result: 理论和实证分析证明了CE-CDR的合理性和有效性，可作为模型无关插件通用应用，已于2025年4月部署到生产环境

Conclusion: CE-CDR是首个探索因果增强跨域推荐的方法，通过因果建模和部分标签损失有效解决了跨域推荐中的关键挑战，具有实际应用价值

Abstract: Cross-domain recommendation forms a crucial component in recommendation
systems. It leverages auxiliary information through source domain tasks or
features to enhance target domain recommendations. However, incorporating
inconsistent source domain tasks may result in insufficient cross-domain
modeling or negative transfer. While incorporating source domain features
without considering the underlying causal relationships may limit their
contribution to final predictions. Thus, a natural idea is to directly train a
cross-domain representation on a causality-labeled dataset from the source to
target domain. Yet this direction has been rarely explored, as identifying
unbiased real causal labels is highly challenging in real-world scenarios. In
this work, we attempt to take a first step in this direction by proposing a
causality-enhanced framework, named CE-CDR. Specifically, we first reformulate
the cross-domain recommendation as a causal graph for principled guidance. We
then construct a causality-aware dataset heuristically. Subsequently, we derive
a theoretically unbiased Partial Label Causal Loss to generalize beyond the
biased causality-aware dataset to unseen cross-domain patterns, yielding an
enriched cross-domain representation, which is then fed into the target model
to enhance target-domain recommendations. Theoretical and empirical analyses,
as well as extensive experiments, demonstrate the rationality and effectiveness
of CE-CDR and its general applicability as a model-agnostic plugin. Moreover,
it has been deployed in production since April 2025, showing its practical
value in real-world applications.

</details>


### [9] [Dataset Pruning in RecSys and ML: Best Practice or Mal-Practice?](https://arxiv.org/abs/2510.14704)
*Leonie Winter*

Main category: cs.IR

TL;DR: 该论文研究了推荐系统数据修剪（移除交互次数少的用户）对数据集特性和算法性能的影响，发现传统算法在修剪数据上表现更好，但在未修剪测试集上优势消失，且性能随修剪程度增加而下降。


<details>
  <summary>Details</summary>
Motivation: 推荐系统研究严重依赖数据集，许多数据集（如MovieLens）都经过修剪处理，但修剪对数据集特性和算法性能的影响尚未充分研究。

Method: 分析五个基准数据集在未修剪和五个修剪级别（5、10、20、50、100次交互）下的结构分布特性，训练并测试11种代表性算法，并评估在修剪训练集上训练但在未修剪测试集上测试的模型。

Result: 常见的数据修剪可能高度选择性，某些数据集仅保留2%的原始用户；传统算法在修剪数据上训练和测试时获得更高nDCG@10分数，但在未修剪测试集上优势基本消失；所有算法在未修剪测试集上的性能随修剪程度增加而下降。

Conclusion: 数据修剪对推荐算法性能有显著影响，在修剪数据上观察到的性能提升可能无法泛化到真实场景，强调了数据集选择对评估结果的重要性。

Abstract: Offline evaluations in recommender system research depend heavily on
datasets, many of which are pruned, such as the widely used MovieLens
collections. This thesis examines the impact of data pruning - specifically,
removing users with fewer than a specified number of interactions - on both
dataset characteristics and algorithm performance. Five benchmark datasets were
analysed in both their unpruned form and at five successive pruning levels (5,
10, 20, 50, 100). For each coreset, we examined structural and distributional
characteristics and trained and tested eleven representative algorithms. To
further assess if pruned datasets lead to artificially inflated performance
results, we also evaluated models trained on the pruned train sets but tested
on unpruned data. Results show that commonly applied core pruning can be highly
selective, leaving as little as 2% of the original users in some datasets.
Traditional algorithms achieved higher nDCG@10 scores when both training and
testing on pruned data; however, this advantage largely disappeared when
evaluated on unpruned test sets. Across all algorithms, performance declined
with increasing pruning levels when tested on unpruned data, highlighting the
impact of dataset reduction on the performance of recommender algorithms.

</details>


### [10] [Cross-Scenario Unified Modeling of User Interests at Billion Scale](https://arxiv.org/abs/2510.14788)
*Manjie Xu,Cheng Chen,Xin Jia,Jingyi Zhou,Yongji Wu,Zejian Wang,Chi Zhang,Kai Zuo,Yibo Chen,Xu Tang,Yao Hu,Yixin Zhu*

Main category: cs.IR

TL;DR: RED-Rec是一个LLM增强的分层推荐引擎，用于统一跨场景用户兴趣建模，在工业级内容推荐系统中实现全面用户表示和多场景行为信号融合。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统在孤立场景中优化业务指标，忽略跨场景行为信号，难以在大规模部署中整合LLM等先进技术，限制了捕捉平台触点间整体用户兴趣的能力。

Method: 采用两塔LLM框架实现细粒度多面表示，通过场景感知的密集混合和查询策略有效融合多样化行为信号，捕捉跨场景用户意图模式并在服务时表达细粒度上下文特定意图。

Result: 在数亿用户的在线A/B测试中，在内容推荐和广告定向任务上均显示出显著性能提升，并引入了百万级序列推荐数据集RED-MMU用于离线训练和评估。

Conclusion: 该工作推进了统一用户建模，在大规模UGC平台中解锁了更深层次的个性化，促进了更有意义的用户参与。

Abstract: User interests on content platforms are inherently diverse, manifesting
through complex behavioral patterns across heterogeneous scenarios such as
search, feed browsing, and content discovery. Traditional recommendation
systems typically prioritize business metric optimization within isolated
specific scenarios, neglecting cross-scenario behavioral signals and struggling
to integrate advanced techniques like LLMs at billion-scale deployments, which
finally limits their ability to capture holistic user interests across platform
touchpoints. We propose RED-Rec, an LLM-enhanced hierarchical Recommender
Engine for Diversified scenarios, tailored for industry-level content
recommendation systems. RED-Rec unifies user interest representations across
multiple behavioral contexts by aggregating and synthesizing actions from
varied scenarios, resulting in comprehensive item and user modeling. At its
core, a two-tower LLM-powered framework enables nuanced, multifaceted
representations with deployment efficiency, and a scenario-aware dense mixing
and querying policy effectively fuses diverse behavioral signals to capture
cross-scenario user intent patterns and express fine-grained, context-specific
intents during serving. We validate RED-Rec through online A/B testing on
hundreds of millions of users in RedNote through online A/B testing, showing
substantial performance gains in both content recommendation and advertisement
targeting tasks. We further introduce a million-scale sequential recommendation
dataset, RED-MMU, for comprehensive offline training and evaluation. Our work
advances unified user modeling, unlocking deeper personalization and fostering
more meaningful user engagement in large-scale UGC platforms.

</details>


### [11] [A Simulation Framework for Studying Systemic Effects of Feedback Loops in Recommender Systems](https://arxiv.org/abs/2510.14857)
*Gabriele Barlacchi,Margherita Lalli,Emanuele Ferragina,Fosca Giannotti,Luca Pappalardo*

Main category: cs.IR

TL;DR: 该论文研究了推荐系统反馈循环对在线零售环境的影响，发现反馈循环在增加个体多样性的同时会减少集体多样性，并导致需求集中在少数热门商品上。


<details>
  <summary>Details</summary>
Motivation: 推荐系统与用户的持续互动形成了反馈循环，这些循环既影响个体行为也影响集体市场动态，需要系统性地研究这些影响。

Method: 使用亚马逊电商数据集，构建模拟框架来建模推荐系统在周期性重训练下的反馈循环，分析不同推荐算法对多样性、购买集中度和用户同质化的影响。

Result: 结果显示反馈循环存在系统性权衡：增加个体多样性但减少集体多样性，需求集中在少数热门商品上，某些推荐系统还会随时间增加用户同质化。

Conclusion: 推荐系统设计需要在个性化与长期多样性之间取得平衡。

Abstract: Recommender systems continuously interact with users, creating feedback loops
that shape both individual behavior and collective market dynamics. This paper
introduces a simulation framework to model these loops in online retail
environments, where recommenders are periodically retrained on evolving
user-item interactions. Using the Amazon e-Commerce dataset, we analyze how
different recommendation algorithms influence diversity, purchase
concentration, and user homogenization over time. Results reveal a systematic
trade-off: while the feedback loop increases individual diversity, it
simultaneously reduces collective diversity and concentrates demand on a few
popular items. Moreover, for some recommender systems, the feedback loop
increases user homogenization over time, making user purchase profiles
increasingly similar. These findings underscore the need for recommender
designs that balance personalization with long-term diversity.

</details>


### [12] [Fantastic (small) Retrievers and How to Train Them: mxbai-edge-colbert-v0 Tech Report](https://arxiv.org/abs/2510.14880)
*Rikiya Takehi,Benjamin Clavié,Sean Lee,Aamir Shakir*

Main category: cs.IR

TL;DR: 提出了mxbai-edge-colbert-v0模型（17M和32M参数），旨在改进检索和延迟交互模型，并支持从云端到本地设备的全尺度检索。该模型在短文本基准上优于ColBERTv2，在长上下文任务中效率显著。


<details>
  <summary>Details</summary>
Motivation: 开发能够在各种设备上运行的检索模型，支持从云端大规模检索到本地设备的小型模型，为未来实验提供基础骨干。

Method: 通过多次消融研究开发mxbai-edge-colbert-v0模型，将其作为小型概念验证模型进行蒸馏，专注于改进检索和延迟交互模型。

Result: 在常见短文本基准（BEIR）上表现优于ColBERTv2，在长上下文任务中实现了前所未有的效率提升。

Conclusion: mxbai-edge-colbert-v0模型为未来实验提供了坚实的基础，在保持小规模的同时实现了优异的检索性能，特别是在长上下文任务中的效率表现突出。

Abstract: In this work, we introduce mxbai-edge-colbert-v0 models, at two different
parameter counts: 17M and 32M. As part of our research, we conduct numerous
experiments to improve retrieval and late-interaction models, which we intend
to distill into smaller models as proof-of-concepts. Our ultimate aim is to
support retrieval at all scales, from large-scale retrieval which lives in the
cloud to models that can run locally, on any device. mxbai-edge-colbert-v0 is a
model that we hope will serve as a solid foundation backbone for all future
experiments, representing the first version of a long series of small
proof-of-concepts. As part of the development of mxbai-edge-colbert-v0, we
conducted multiple ablation studies, of which we report the results. In terms
of downstream performance, mxbai-edge-colbert-v0 is a particularly capable
small model, outperforming ColBERTv2 on common short-text benchmarks (BEIR) and
representing a large step forward in long-context tasks, with unprecedented
efficiency.

</details>
