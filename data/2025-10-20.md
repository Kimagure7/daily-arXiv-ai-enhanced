<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 11]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [DMRetriever: A Family of Models for Improved Text Retrieval in Disaster Management](https://arxiv.org/abs/2510.15087)
*Kai Yin,Xiangjue Dong,Chengkai Liu,Allen Lin,Lingfeng Shi,Ali Mostafavi,James Caverlee*

Main category: cs.IR

TL;DR: 提出了DMRetriever，这是首个专门为灾害管理设计的密集检索模型系列（33M到7.6B），通过三阶段训练框架在六个搜索意图上实现SOTA性能，且参数效率极高。


<details>
  <summary>Details</summary>
Motivation: 现有通用检索模型无法有效处理灾害管理场景中多样化的搜索意图，导致性能不一致和不可靠，需要专门针对该领域的检索模型。

Method: 采用三阶段训练框架：双向注意力适应、无监督对比预训练和难度感知渐进式指令微调，使用高级数据精炼管道生成的高质量数据进行训练。

Result: DMRetriever在所有六个搜索意图的每个模型规模上都实现了最先进的性能，参数效率极高，596M模型性能超过基线模型13.3倍，33M模型仅用基线7.6%的参数就超越基线。

Conclusion: DMRetriever是首个专门为灾害管理设计的检索模型，在性能和参数效率方面都表现出色，为灾害管理信息检索提供了可靠解决方案。

Abstract: Effective and efficient access to relevant information is essential for
disaster management. However, no retrieval model is specialized for disaster
management, and existing general-domain models fail to handle the varied search
intents inherent to disaster management scenarios, resulting in inconsistent
and unreliable performance. To this end, we introduce DMRetriever, the first
series of dense retrieval models (33M to 7.6B) tailored for this domain. It is
trained through a novel three-stage framework of bidirectional attention
adaptation, unsupervised contrastive pre-training, and difficulty-aware
progressive instruction fine-tuning, using high-quality data generated through
an advanced data refinement pipeline. Comprehensive experiments demonstrate
that DMRetriever achieves state-of-the-art (SOTA) performance across all six
search intents at every model scale. Moreover, DMRetriever is highly
parameter-efficient, with 596M model outperforming baselines over 13.3 X larger
and 33M model exceeding baselines with only 7.6% of their parameters. All
codes, data, and checkpoints are available at
https://github.com/KaiYin97/DMRETRIEVER

</details>


### [2] [MTmixAtt: Integrating Mixture-of-Experts with Multi-Mix Attention for Large-Scale Recommendation](https://arxiv.org/abs/2510.15286)
*Xianyang Qi,Yuan Tian,Zhaoyu Hu,Zhirui Kuai,Chang Liu,Hongxiang Lin,Lei Wang*

Main category: cs.IR

TL;DR: MTmixAtt是一个统一的混合专家架构，通过自动特征聚类和多混合注意力机制，解决了传统推荐系统中手动特征工程和场景特定架构的限制，在大规模推荐任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统依赖手动特征工程和场景特定架构，阻碍跨场景迁移和大规模部署，需要统一的解决方案。

Method: 提出MTmixAtt架构，包含AutoToken模块自动聚类异构特征为语义连贯的token，以及MTmixAttBlock模块通过可学习的混合矩阵、共享密集专家和场景感知稀疏专家实现高效token交互。

Result: 在美团工业TRec数据集上，MTmixAtt在CTR和CTCVR指标上优于现有最佳基线模型；在Homepage场景的在线A/B测试中，支付PV增加3.62%，实际支付GTV增加2.54%。

Conclusion: MTmixAtt提供了一个统一且可扩展的解决方案，能够跨场景建模任意异构特征，显著改善用户体验和商业成果。

Abstract: Industrial recommender systems critically depend on high-quality ranking
models. However, traditional pipelines still rely on manual feature engineering
and scenario-specific architectures, which hinder cross-scenario transfer and
large-scale deployment. To address these challenges, we propose
\textbf{MTmixAtt}, a unified Mixture-of-Experts (MoE) architecture with
Multi-Mix Attention, designed for large-scale recommendation tasks. MTmixAtt
integrates two key components. The \textbf{AutoToken} module automatically
clusters heterogeneous features into semantically coherent tokens, removing the
need for human-defined feature groups. The \textbf{MTmixAttBlock} module
enables efficient token interaction via a learnable mixing matrix, shared dense
experts, and scenario-aware sparse experts, capturing both global patterns and
scenario-specific behaviors within a single framework. Extensive experiments on
the industrial TRec dataset from Meituan demonstrate that MTmixAtt consistently
outperforms state-of-the-art baselines including Transformer-based models,
WuKong, HiFormer, MLP-Mixer, and RankMixer. At comparable parameter scales,
MTmixAtt achieves superior CTR and CTCVR metrics; scaling to MTmixAtt-1B yields
further monotonic gains. Large-scale online A/B tests validate the real-world
impact: in the \textit{Homepage} scenario, MTmixAtt increases Payment PV by
\textbf{+3.62\%} and Actual Payment GTV by \textbf{+2.54\%}. Overall, MTmixAtt
provides a unified and scalable solution for modeling arbitrary heterogeneous
features across scenarios, significantly improving both user experience and
commercial outcomes.

</details>


### [3] [GRank: Towards Target-Aware and Streamlined Industrial Retrieval with a Generate-Rank Framework](https://arxiv.org/abs/2510.15299)
*Yijia Sun,Shanshan Huang,Zhiyuan Guan,Qiang Luo,Ruiming Tang,Kun Gai,Guorui Zhou*

Main category: cs.IR

TL;DR: GRank是一个无需结构化索引的检索新范式，将目标感知学习与用户中心检索无缝统一，在召回率和延迟方面显著优于现有方法，已在生产环境中部署。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统检索阶段存在两个问题：双塔架构表达能力有限，无法捕捉细粒度用户-物品交互；结构化索引方法难以融入动态用户偏好且维护成本高昂。

Method: 提出三个关键创新：(1)目标感知生成器通过GPU加速MIPS进行个性化候选生成；(2)轻量级排序器在小规模候选集上执行细粒度推理；(3)端到端多任务学习框架确保生成和排序目标语义一致性。

Result: 在两个公共基准和十亿级生产语料上的实验显示，GRank将Recall@500提升超过30%，P99 QPS达到基于树和图检索器的1.7倍。

Conclusion: GRank已在生产环境中成功部署，服务4亿活跃用户，在线A/B测试证实核心参与度指标显著提升，总应用使用时间在主应用和轻量版分别增加0.160%和0.165%。

Abstract: Industrial-scale recommender systems rely on a cascade pipeline in which the
retrieval stage must return a high-recall candidate set from billions of items
under tight latency. Existing solutions ei- ther (i) suffer from limited
expressiveness in capturing fine-grained user-item interactions, as seen in
decoupled dual-tower architectures that rely on separate encoders, or
generative models that lack precise target-aware matching capabilities, or (ii)
build structured indices (tree, graph, quantization) whose item-centric
topologies struggle to incorporate dynamic user preferences and incur
prohibitive construction and maintenance costs.
  We present GRank, a novel structured-index-free retrieval paradigm that
seamlessly unifies target-aware learning with user-centric retrieval. Our key
innovations include: (1) A target-aware Generator trained to perform
personalized candidate generation via GPU-accelerated MIPS, eliminating
semantic drift and maintenance costs of structured indexing; (2) A lightweight
but powerful Ranker that performs fine-grained, candidate-specific inference on
small subsets; (3) An end-to-end multi-task learning framework that ensures
semantic consistency between generation and ranking objectives.
  Extensive experiments on two public benchmarks and a billion-item production
corpus demonstrate that GRank improves Recall@500 by over 30% and 1.7$\times$
the P99 QPS of state-of-the-art tree- and graph-based retrievers.
  GRank has been fully deployed in production in our recommendation platform
since Q2 2025, serving 400 million active users with 99.95% service
availability. Online A/B tests confirm significant improvements in core
engagement metrics, with Total App Usage Time increasing by 0.160% in the main
app and 0.165% in the Lite version.

</details>


### [4] [Dimension Mask Layer: Optimizing Embedding Efficiency for Scalable ID-based Models](https://arxiv.org/abs/2510.15308)
*Srijan Saket,Ikuhiro Ihara,Vaibhav Sharma,Danish Kalim*

Main category: cs.IR

TL;DR: 提出一种自动确定ID特征最优嵌入尺寸的方法，通过维度掩码层修剪嵌入向量，可减少40-50%的嵌入维度，显著降低模型内存占用且保持性能。


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统和社交媒体平台中的大规模ID特征需要大量嵌入表，消耗显著内存，导致模型庞大难以部署和维护。

Method: 定义自定义Keras维度掩码层，位于嵌入查找之后，仅允许前N个维度通过，从而修剪嵌入向量。

Result: 在公共数据集和生产环境A/B测试中，维度掩码层可将有效嵌入维度减少40-50%，显著提高内存效率。

Conclusion: 该方法为处理大量ID特征的平台提供了可扩展解决方案，优化资源使用和模型性能。

Abstract: In modern recommendation systems and social media platforms like Meta,
TikTok, and Instagram, large-scale ID-based features often require embedding
tables that consume significant memory. Managing these embedding sizes can be
challenging, leading to bulky models that are harder to deploy and maintain. In
this paper, we introduce a method to automatically determine the optimal
embedding size for ID features, significantly reducing the model size while
maintaining performance.
  Our approach involves defining a custom Keras layer called the dimension mask
layer, which sits directly after the embedding lookup. This layer trims the
embedding vector by allowing only the first N dimensions to pass through. By
doing this, we can reduce the input feature dimension by more than half with
minimal or no loss in model performance metrics. This reduction helps cut down
the memory footprint of the model and lowers the risk of overfitting due to
multicollinearity.
  Through offline experiments on public datasets and an online A/B test on a
real production dataset, we demonstrate that using a dimension mask layer can
shrink the effective embedding dimension by 40-50\%, leading to substantial
improvements in memory efficiency. This method provides a scalable solution for
platforms dealing with a high volume of ID features, optimizing both resource
usage and model performance.

</details>


### [5] [Fault Cause Identification across Manufacturing Lines through Ontology-Guided and Process-Aware FMEA Graph Learning with LLMs](https://arxiv.org/abs/2510.15428)
*Sho Okazaki,Kohei Kaminishi,Takuma Fujiu,Yusheng Wang,Jun Ota*

Main category: cs.IR

TL;DR: 提出了一种结合制造领域概念化和图神经网络推理的过程感知框架，用于提高FMEA知识在异构制造产线间的可重用性，在故障原因识别任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动化制造产线中故障原因识别面临系统复杂性、频繁重构以及现有FMEA知识可重用性有限的挑战。虽然FMEA工作表包含有价值的专家见解，但由于自然语言变异性、术语不一致和流程差异，它们在异构产线间的重用受到阻碍。

Method: 1. 通过本体引导的大型语言模型提取，将多个制造产线的FMEA工作表转换为统一的知识图谱；2. 使用具有过程感知评分函数的关系图卷积网络学习既尊重语义关系又考虑顺序流程的嵌入；3. 采用链接预测来推断和排序与目标产线流程一致的候选故障原因。

Result: 在汽车压力传感器装配产线的案例研究中，该方法在故障原因识别任务中表现最佳（F1@20 = 0.523），优于最先进的检索增强生成基线（0.267）和RGCN方法（0.400）。消融研究证实了LLM驱动的领域概念化和过程感知学习的贡献。

Conclusion: 该框架显著提高了FMEA知识在异构产线间的可迁移性，从而支持操作员更可靠地诊断故障，并为智能制造中未来领域自适应LLM应用铺平了道路。

Abstract: Fault cause identification in automated manufacturing lines is challenging
due to the system's complexity, frequent reconfigurations, and the limited
reusability of existing Failure Mode and Effects Analysis (FMEA) knowledge.
Although FMEA worksheets contain valuable expert insights, their reuse across
heterogeneous lines is hindered by natural language variability, inconsistent
terminology, and process differences. To address these limitations, this study
proposes a process-aware framework that enhances FMEA reusability by combining
manufacturing-domain conceptualization with graph neural network (GNN)
reasoning. First, FMEA worksheets from multiple manufacturing lines are
transformed into a unified knowledge graph through ontology-guided large
language model (LLM) extraction, capturing domain concepts such as actions,
states, components, and parameters. Second, a Relational Graph Convolutional
Network (RGCN) with the process-aware scoring function learns embeddings that
respect both semantic relationships and sequential process flows. Finally, link
prediction is employed to infer and rank candidate fault causes consistent with
the target line's process flow.
  A case study on automotive pressure sensor assembly lines demonstrates that
the proposed method outperforms a state-of-the-art retrieval-augmented
generation (RAG) baseline (F1@20 = 0.267) and an RGCN approach (0.400),
achieving the best performance (0.523) in fault cause identification. Ablation
studies confirm the contributions of both LLM-driven domain conceptualization
and process-aware learning. These results indicate that the proposed framework
significantly improves the transferability of FMEA knowledge across
heterogeneous lines, thereby supporting operators in diagnosing failures more
reliably and paving the way for future domain-adaptive LLM applications in
smart manufacturing.

</details>


### [6] [Enhance Large Language Models as Recommendation Systems with Collaborative Filtering](https://arxiv.org/abs/2510.15647)
*Zhisheng Yang,Xiaofei Xu,Ke Deng,Li Li*

Main category: cs.IR

TL;DR: 提出Critic-LLM-RS方法，通过训练独立的协同过滤模型Critic为LLM提供反馈，在无需微调LLM的情况下提升推荐质量。


<details>
  <summary>Details</summary>
Motivation: 现有非微调策略的LLM推荐方法缺乏特定任务的企业知识，且未显式集成协同过滤这一成功推荐技术。

Method: 训练独立的协同过滤模型Critic，学习用户-物品交互，为LLM提供反馈来优化推荐结果。

Result: 在真实数据集上的广泛实验验证了Critic-LLM-RS的有效性。

Conclusion: Critic-LLM-RS成功填补了非微调LLM推荐方法中协同过滤集成的空白，显著提升了推荐质量。

Abstract: As powerful tools in Natural Language Processing (NLP), Large Language Models
(LLMs) have been leveraged for crafting recommendations to achieve precise
alignment with user preferences and elevate the quality of the recommendations.
The existing approaches implement both non-tuning and tuning strategies.
Compared to following the tuning strategy, the approaches following the
non-tuning strategy avoid the relatively costly, time-consuming, and
expertise-requiring process of further training pre-trained LLMs on
task-specific datasets, but they suffer the issue of not having the
task-specific business or local enterprise knowledge. To the best of our
knowledge, none of the existing approaches following the non-tuning strategy
explicitly integrates collaborative filtering, one of the most successful
recommendation techniques. This study aims to fill the gap by proposing
critique-based LLMs as recommendation systems (Critic-LLM-RS). For our purpose,
we train a separate machine-learning model called Critic that implements
collaborative filtering for recommendations by learning from the interactions
between many users and items. The Critic provides critiques to LLMs to
significantly refine the recommendations. Extensive experiments have verified
the effectiveness of Critic-LLM-RS on real datasets.

</details>


### [7] [SQuAI: Scientific Question-Answering with Multi-Agent Retrieval-Augmented Generation](https://arxiv.org/abs/2510.15682)
*Ines Besrour,Jingbo He,Tobias Schreieder,Michael Färber*

Main category: cs.IR

TL;DR: SQuAI是一个可扩展且可信的多智能体检索增强生成框架，用于科学问答，通过分解复杂问题、混合检索和自适应文档过滤来提高答案的准确性和可追溯性。


<details>
  <summary>Details</summary>
Motivation: 解决现有RAG系统在科学领域中处理复杂开放域问题时的局限性，需要准确答案、明确引用和跨数百万科学文档的检索。

Method: 使用四个协作智能体分解复杂问题为子问题，通过混合稀疏-稠密检索获取目标证据，并自适应过滤文档以提高上下文相关性。

Result: 相比强RAG基线，系统在忠实性、答案相关性和上下文相关性方面提升了高达+0.088（12%），并发布了包含1000个科学问答证据三元组的基准数据集。

Conclusion: SQuAI通过透明推理、可验证引用和领域范围的可扩展性，展示了多智能体RAG如何实现更可信的科学问答。

Abstract: We present SQuAI (https://squai.scads.ai/), a scalable and trustworthy
multi-agent retrieval-augmented generation (RAG) framework for scientific
question answering (QA) with large language models (LLMs). SQuAI addresses key
limitations of existing RAG systems in the scholarly domain, where complex,
open-domain questions demand accurate answers, explicit claims with citations,
and retrieval across millions of scientific documents. Built on over 2.3
million full-text papers from arXiv.org, SQuAI employs four collaborative
agents to decompose complex questions into sub-questions, retrieve targeted
evidence via hybrid sparse-dense retrieval, and adaptively filter documents to
improve contextual relevance. To ensure faithfulness and traceability, SQuAI
integrates in-line citations for each generated claim and provides supporting
sentences from the source documents. Our system improves faithfulness, answer
relevance, and contextual relevance by up to +0.088 (12%) over a strong RAG
baseline. We further release a benchmark of 1,000 scientific
question-answer-evidence triplets to support reproducibility. With transparent
reasoning, verifiable citations, and domain-wide scalability, SQuAI
demonstrates how multi-agent RAG enables more trustworthy scientific QA with
LLMs.

</details>


### [8] [Mixture of Experts Approaches in Dense Retrieval Tasks](https://arxiv.org/abs/2510.15683)
*Effrosyni Sokli,Pranav Kasela,Georgios Peikos,Gabriella Pasi*

Main category: cs.IR

TL;DR: 提出SB-MoE方法，在密集检索模型的最后一个Transformer层后添加单个MoE块，相比传统MoE方法更高效，能提升轻量级模型的检索性能。


<details>
  <summary>Details</summary>
Motivation: 解决密集检索模型在特定任务和领域训练后泛化能力不足的问题，同时避免传统MoE方法参数过多的问题。

Method: 在密集检索模型的最终Transformer层后引入单个MoE块，通过两种评估设置测试模型性能：领域内微调评估和零样本泛化评估。

Result: SB-MoE对轻量级模型（如TinyBERT、BERT-Small）效果显著，在多个基准测试中表现优于标准微调；对于参数较多的模型需要更多训练样本才能提升性能。

Conclusion: SB-MoE是一种高效的MoE设计，特别适合轻量级密集检索模型，能有效提升检索性能而不过度增加参数。

Abstract: Dense Retrieval Models (DRMs) are a prominent development in Information
Retrieval (IR). A key challenge with these neural Transformer-based models is
that they often struggle to generalize beyond the specific tasks and domains
they were trained on. To address this challenge, prior research in IR
incorporated the Mixture-of-Experts (MoE) framework within each Transformer
layer of a DRM, which, though effective, substantially increased the number of
additional parameters. In this paper, we propose a more efficient design, which
introduces a single MoE block (SB-MoE) after the final Transformer layer. To
assess the retrieval effectiveness of SB-MoE, we perform an empirical
evaluation across three IR tasks. Our experiments involve two evaluation
setups, aiming to assess both in-domain effectiveness and the model's zero-shot
generalizability. In the first setup, we fine-tune SB-MoE with four different
underlying DRMs on seven IR benchmarks and evaluate them on their respective
test sets. In the second setup, we fine-tune SB-MoE on MSMARCO and perform
zero-shot evaluation on thirteen BEIR datasets. Additionally, we perform
further experiments to analyze the model's dependency on its hyperparameters
(i.e., the number of employed and activated experts) and investigate how this
variation affects SB-MoE's performance. The obtained results show that SB-MoE
is particularly effective for DRMs with lightweight base models, such as
TinyBERT and BERT-Small, consistently exceeding standard model fine-tuning
across benchmarks. For DRMs with more parameters, such as BERT-Base and
Contriever, our model requires a larger number of training samples to achieve
improved retrieval performance. Our code is available online at:
https://github.com/FaySokli/SB-MoE.

</details>


### [9] [GraphMind: Interactive Novelty Assessment System for Accelerating Scientific Discovery](https://arxiv.org/abs/2510.15706)
*Italo Luis da Silva,Hanqi Yan,Lin Gui,Yulan He*

Main category: cs.IR

TL;DR: GraphMind是一个交互式网络工具，通过整合外部API和LLMs来帮助用户评估科学论文的新颖性，提供可验证的上下文洞察。


<details>
  <summary>Details</summary>
Motivation: 现有LLM辅助科学文献分析方法透明度有限，缺乏通过信息检索模块实现结果可追溯性的机制，需要解决这一差距。

Method: 开发GraphMind工具，整合arXiv和Semantic Scholar等外部API与LLMs，支持论文的注释、提取、检索和分类，让用户能够标注论文关键元素并通过各种关系探索相关论文。

Result: GraphMind提供了一个易于使用的交互式网络工具，使用户能够捕捉科学论文的主要结构，从多个角度探索相关想法，并通过提供可验证的上下文洞察来评估新颖性。

Conclusion: GraphMind通过结合外部API和LLMs，为用户提供了科学思想核心贡献及其与现有工作联系的丰富结构化视图，有效支持科学论文新颖性评估。

Abstract: Large Language Models (LLMs) show strong reasoning and text generation
capabilities, prompting their use in scientific literature analysis, including
novelty assessment. While evaluating novelty of scientific papers is crucial
for peer review, it requires extensive knowledge of related work, something not
all reviewers have. While recent work on LLM-assisted scientific literature
analysis supports literature comparison, existing approaches offer limited
transparency and lack mechanisms for result traceability via an information
retrieval module. To address this gap, we introduce $\textbf{GraphMind}$, an
easy-to-use interactive web tool designed to assist users in evaluating the
novelty of scientific papers or drafted ideas. Specially, $\textbf{GraphMind}$
enables users to capture the main structure of a scientific paper, explore
related ideas through various perspectives, and assess novelty via providing
verifiable contextual insights. $\textbf{GraphMind}$ enables users to annotate
key elements of a paper, explore related papers through various relationships,
and assess novelty with contextual insight. This tool integrates external APIs
such as arXiv and Semantic Scholar with LLMs to support annotation, extraction,
retrieval and classification of papers. This combination provides users with a
rich, structured view of a scientific idea's core contributions and its
connections to existing work. $\textbf{GraphMind}$ is available at
https://oyarsa.github.io/graphmind and a demonstration video at
https://youtu.be/wKbjQpSvwJg. The source code is available at
https://github.com/oyarsa/graphmind.

</details>


### [10] [The 3rd Place Solution of CCIR CUP 2025: A Framework for Retrieval-Augmented Generation in Multi-Turn Legal Conversation](https://arxiv.org/abs/2510.15722)
*Da Li,Zecheng Fang,Qiang Yan,Wei Huang,Xuanpu Luo*

Main category: cs.IR

TL;DR: 本文介绍了在CCIR CUP 2025中应用的基于检索增强生成的法律知识检索与生成方法，结合大语言模型和信息检索系统来回答用户法律问题。


<details>
  <summary>Details</summary>
Motivation: 尽管检索增强生成在自然语言处理领域取得了显著进展，但在法律领域的应用仍处于探索阶段，需要专门的方法来处理法律知识的检索和生成。

Method: 利用大语言模型和信息检索系统，从可靠的法律来源中检索相关信息，然后生成基于法律条文的上下文相关回答。

Result: 该方法在CCIR CUP 2025中展示了基于法律知识提供相关回答的能力。

Conclusion: 检索增强生成技术可以有效地应用于法律领域，通过结合信息检索和大语言模型来提供基于法律条文的准确回答。

Abstract: Retrieval-Augmented Generation has made significant progress in the field of
natural language processing. By combining the advantages of information
retrieval and large language models, RAG can generate relevant and contextually
appropriate responses based on items retrieved from reliable sources. This
technology has demonstrated outstanding performance across multiple domains,
but its application in the legal field remains in its exploratory phase. In
this paper, we introduce our approach for "Legal Knowledge Retrieval and
Generation" in CCIR CUP 2025, which leverages large language models and
information retrieval systems to provide responses based on laws in response to
user questions.

</details>


### [11] [FACE: A General Framework for Mapping Collaborative Filtering Embeddings into LLM Tokens](https://arxiv.org/abs/2510.15729)
*Chao Wang,Yixin Song,Jinhui Ye,Chuan Qin,Dazhong Shen,Lingfeng Liu,Xiang Wang,Yanyong Zhang*

Main category: cs.IR

TL;DR: FACE框架将协同过滤嵌入映射到预训练LLM令牌，实现语义对齐，提升推荐性能且无需微调LLM


<details>
  <summary>Details</summary>
Motivation: 解决LLM难以解释CF方法产生的潜在非语义嵌入的问题，这限制了推荐效果和进一步应用

Method: 提出FACE框架：使用解耦投影模块分解CF嵌入为概念特定向量，通过量化自编码器将连续嵌入转换为LLM令牌，并设计对比对齐目标确保令牌与文本信号对齐

Result: 在三个真实世界推荐数据集上的实证结果显示基准模型性能提升，可解释性研究证实了描述符的可解释性

Conclusion: FACE是一个模型无关的框架，无需微调LLM即可实现语义对齐，并通过利用其预训练能力增强推荐性能

Abstract: Recently, large language models (LLMs) have been explored for integration
with collaborative filtering (CF)-based recommendation systems, which are
crucial for personalizing user experiences. However, a key challenge is that
LLMs struggle to interpret the latent, non-semantic embeddings produced by CF
approaches, limiting recommendation effectiveness and further applications. To
address this, we propose FACE, a general interpretable framework that maps CF
embeddings into pre-trained LLM tokens. Specifically, we introduce a
disentangled projection module to decompose CF embeddings into concept-specific
vectors, followed by a quantized autoencoder to convert continuous embeddings
into LLM tokens (descriptors). Then, we design a contrastive alignment
objective to ensure that the tokens align with corresponding textual signals.
Hence, the model-agnostic FACE framework achieves semantic alignment without
fine-tuning LLMs and enhances recommendation performance by leveraging their
pre-trained capabilities. Empirical results on three real-world recommendation
datasets demonstrate performance improvements in benchmark models, with
interpretability studies confirming the interpretability of the descriptors.
Code is available in https://github.com/YixinRoll/FACE.

</details>
