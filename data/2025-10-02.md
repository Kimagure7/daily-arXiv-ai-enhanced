<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 8]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Optimizing What Matters: AUC-Driven Learning for Robust Neural Retrieval](https://arxiv.org/abs/2510.00137)
*Nima Sheikholeslami,Erfan Hosseini,Patrice Bechard,Srivatsava Daruru,Sai Rajeswar*

Main category: cs.IR

TL;DR: 本文提出MW损失函数，通过最大化Mann-Whitney U统计量（等同于AUC）来替代传统的对比损失，解决了双编码器检索器在分数分离质量和AUC优化方面的根本缺陷。


<details>
  <summary>Details</summary>
Motivation: 传统噪声对比估计（NCE）目标虽然广泛使用，但被证明对分数分离质量不敏感且与AUC无关，导致检索器校准不佳和在RAG等下游任务中表现次优。

Method: 引入MW损失函数，通过最小化分数差异的二元交叉熵来最大化Mann-Whitney U统计量，从而直接优化AUC。

Result: 实验表明，使用MW损失训练的检索器在AUC和标准检索指标上持续优于对比损失方法，产生更好校准和更具区分度的检索器。

Conclusion: MW损失是比对比损失更优的经验替代方案，特别适用于RAG等高风险应用，能产生更可靠和有效的检索器。

Abstract: Dual-encoder retrievers depend on the principle that relevant documents
should score higher than irrelevant ones for a given query. Yet the dominant
Noise Contrastive Estimation (NCE) objective, which underpins Contrastive Loss,
optimizes a softened ranking surrogate that we rigorously prove is
fundamentally oblivious to score separation quality and unrelated to AUC. This
mismatch leads to poor calibration and suboptimal performance in downstream
tasks like retrieval-augmented generation (RAG). To address this fundamental
limitation, we introduce the MW loss, a new training objective that maximizes
the Mann-Whitney U statistic, which is mathematically equivalent to the Area
under the ROC Curve (AUC). MW loss encourages each positive-negative pair to be
correctly ranked by minimizing binary cross entropy over score differences. We
provide theoretical guarantees that MW loss directly upper-bounds the AoC,
better aligning optimization with retrieval goals. We further promote ROC
curves and AUC as natural threshold free diagnostics for evaluating retriever
calibration and ranking quality. Empirically, retrievers trained with MW loss
consistently outperform contrastive counterparts in AUC and standard retrieval
metrics. Our experiments show that MW loss is an empirically superior
alternative to Contrastive Loss, yielding better-calibrated and more
discriminative retrievers for high-stakes applications like RAG.

</details>


### [2] [HLTCOE at TREC 2024 NeuCLIR Track](https://arxiv.org/abs/2510.00143)
*Eugene Yang,Dawn Lawrie,Orion Weller,James Mayfield*

Main category: cs.IR

TL;DR: HLTCOE团队在TREC 2024 NeuCLIR赛道中应用了PLAID mT5重排序器、GPT-4重排序器、分数融合和文档翻译技术，并针对报告生成任务进行了系统组合实验。


<details>
  <summary>Details</summary>
Motivation: 解决跨语言信息检索(CLIR)和多语言信息检索(MLIR)中的挑战，特别是在新闻和技术文档检索以及报告生成任务中提升性能。

Method: 使用多种PLAID模型训练技术：翻译蒸馏(TD)、生成蒸馏(GD)和多语言翻译蒸馏(MTD)；采用GPT-4o和Claude-3.5-Sonnet进行结果总结和验证；使用GPT4o和GPT3.5Turbo提取和分组相关事实。

Result: 团队向所有NeuCLIR任务提交了运行结果：CLIR和MLIR新闻任务、技术文档任务以及报告生成任务。

Conclusion: 通过结合多种重排序技术、文档翻译和系统组合方法，为跨语言和多语言信息检索任务提供了全面的解决方案。

Abstract: The HLTCOE team applied PLAID, an mT5 reranker, GPT-4 reranker, score fusion,
and document translation to the TREC 2024 NeuCLIR track. For PLAID we included
a variety of models and training techniques -- Translate Distill (TD), Generate
Distill (GD) and multi-lingual translate-distill (MTD). TD uses scores from the
mT5 model over English MS MARCO query-document pairs to learn how to score
query-document pairs where the documents are translated to match the CLIR
setting. GD follows TD but uses passages from the collection and queries
generated by an LLM for training examples. MTD uses MS MARCO translated into
multiple languages, allowing experiments on how to batch the data during
training. Finally, for report generation we experimented with system
combination over different runs. One family of systems used either GPT-4o or
Claude-3.5-Sonnet to summarize the retrieved results from a series of
decomposed sub-questions. Another system took the output from those two models
and verified/combined them with Claude-3.5-Sonnet. The other family used GPT4o
and GPT3.5Turbo to extract and group relevant facts from the retrieved
documents based on the decomposed queries. The resulting submissions directly
concatenate the grouped facts to form the report and their documents of origin
as the citations. The team submitted runs to all NeuCLIR tasks: CLIR and MLIR
news tasks as well as the technical documents task and the report generation
task.

</details>


### [3] [Privacy-Preserving Learning-Augmented Data Structures](https://arxiv.org/abs/2510.00165)
*Prabhav Goyal,Vinesh Sridhar,Wilson Zheng*

Main category: cs.IR

TL;DR: 本文提出了第一个具有强历史独立性、鲁棒性并支持动态更新的学习增强数据结构，通过阈值化和配对技术实现了隐私安全与效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 学习增强数据结构使用预测频率估计来加速数据检索，但其隐私安全影响尚未充分探索。在安全漏洞情况下，数据结构应仅揭示当前内容，特别是学习增强数据结构因其布局会适应数据而更需要历史独立性。

Method: 提出了两种技术：阈值化（使任何学习增强数据结构具有鲁棒性）和配对（在动态设置中提供强历史独立性的简单技术）。

Result: 实验结果显示在安全性和效率之间存在权衡，但性能仍可与最先进技术竞争。

Conclusion: 这是向学习增强数据结构提供隐私和安全保证的第一步，提出的方法在保持竞争力的同时实现了强历史独立性。

Abstract: Learning-augmented data structures use predicted frequency estimates to
retrieve frequently occurring database elements faster than standard data
structures. Recent work has developed data structures that optimally exploit
these frequency estimates while maintaining robustness to adversarial
prediction errors. However, the privacy and security implications of this
setting remain largely unexplored.
  In the event of a security breach, data structures should reveal minimal
information beyond their current contents. This is even more crucial for
learning-augmented data structures, whose layout adapts to the data. A data
structure is history independent if its memory representation reveals no
information about past operations except what is inferred from its current
contents. In this work, we take the first step towards privacy and security
guarantees in this setting by proposing the first learning-augmented data
structure that is strongly history independent, robust, and supports dynamic
updates.
  To achieve this, we introduce two techniques: thresholding, which
automatically makes any learning-augmented data structure robust, and pairing,
a simple technique that provides strong history independence in the dynamic
setting. Our experimental results demonstrate a tradeoff between security and
efficiency but are still competitive with the state of the art.

</details>


### [4] [Milco: Learned Sparse Retrieval Across Languages via a Multilingual Connector](https://arxiv.org/abs/2510.00671)
*Thong Nguyen,Yibin Lei,Jia-Huei Ju,Eugene Yang,Andrew Yates*

Main category: cs.IR

TL;DR: MILCO是一种多语言稀疏检索架构，通过多语言连接器将不同语言的查询和文档映射到共享的英语词汇空间，结合稀疏对齐预训练和对比训练，在保持表示透明性和有效性的同时缓解语义崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 现有学习稀疏检索方法难以扩展到英语以外的语言，且罕见实体在投影到英语时容易丢失。

Method: 提出MILCO架构，使用多语言连接器进行映射，采用两阶段训练（稀疏对齐预训练+对比训练），并引入LexEcho头通过[ECHO]令牌增强源语言视图。

Result: 在多语言和跨语言LSR任务上达到最先进性能，优于BGE-M3和Qwen3-Embed等基线模型，支持通过后剪枝实现动态效率。

Conclusion: MILCO在保持高效性的同时显著提升了多语言稀疏检索的性能，特别是在罕见实体处理方面表现出色。

Abstract: Learned Sparse Retrieval (LSR) combines the efficiency of bi-encoders with
the transparency of lexical matching, but existing approaches struggle to scale
beyond English. We introduce MILCO, an LSR architecture that maps queries and
documents from different languages into a shared English lexical space via a
multilingual connector. MILCO is trained with a specialized two-stage regime
that combines Sparse Alignment Pretraining with contrastive training to provide
representation transparency and effectiveness while mitigating semantic
collapse. Motivated by the observation that uncommon entities are often lost
when projected into English, we propose a new LexEcho head, which enhances
robustness by augmenting the English lexical representation with a
source-language view obtained through a special [ECHO] token. MILCO achieves
state-of-the-art multilingual and cross-lingual LSR performance, outperforming
leading dense, sparse, and multi-vector baselines such as BGE-M3 and
Qwen3-Embed on standard multilingual benchmarks, while supporting dynamic
efficiency through post-hoc pruning. Notably, when using mass-based pruning to
reduce document representations to only 30 active dimensions on average, MILCO
560M outperforms the similarly-sized Qwen3-Embed 0.6B with 1024 dimensions.

</details>


### [5] [On Listwise Reranking for Corpus Feedback](https://arxiv.org/abs/2510.00887)
*Soyoung Yoon,Jongho Kim,Daeyong Kwon,Avishek Anand,Seung-won Hwang*

Main category: cs.IR

TL;DR: L2G是一个新颖的框架，通过从列表式重排序器日志中隐式诱导文档图，实现可扩展的基于图的检索，无需显式图计算的开销。


<details>
  <summary>Details</summary>
Motivation: 图感知自适应检索需要预计算的文档相似图，但这样的图通常不可用或产生二次内存成本。图自由重排序器使用LLM调用来实现竞争性能，但成本较高。

Method: 通过将重排序器信号转换为图结构，L2G隐式地从列表式重排序器日志中诱导文档图，实现无需显式图计算的基于图检索。

Result: 在TREC-DL和BEIR子集上的结果显示，L2G与基于oracle的图方法具有相同的效果，同时不产生额外的LLM调用。

Conclusion: L2G框架能够在不增加LLM调用成本的情况下，实现与显式图方法相当的性能，为基于图的检索提供了更高效的解决方案。

Abstract: Reranker improves retrieval performance by capturing document interactions.
At one extreme, graph-aware adaptive retrieval (GAR) represents an
information-rich regime, requiring a pre-computed document similarity graph in
reranking. However, as such graphs are often unavailable, or incur quadratic
memory costs even when available, graph-free rerankers leverage large language
model (LLM) calls to achieve competitive performance. We introduce L2G, a novel
framework that implicitly induces document graphs from listwise reranker logs.
By converting reranker signals into a graph structure, L2G enables scalable
graph-based retrieval without the overhead of explicit graph computation.
Results on the TREC-DL and BEIR subset show that L2G matches the effectiveness
of oracle-based graph methods, while incurring zero additional LLM calls.

</details>


### [6] [Bridging Language Gaps: Advances in Cross-Lingual Information Retrieval with Multilingual LLMs](https://arxiv.org/abs/2510.00908)
*Roksana Goworek,Olivia Macmillan-Scott,Eda B. Özyiğit*

Main category: cs.IR

TL;DR: 该论文综述了跨语言信息检索(CLIR)的发展历程，从早期基于翻译的方法到最新的嵌入驱动和生成技术，涵盖了核心组件、评估实践和可用资源，并指出了数据不平衡和语言变异等持续挑战。


<details>
  <summary>Details</summary>
Motivation: 传统CLIR研究将任务视为通过翻译增强的单语检索，孤立处理检索方法和跨语言能力。随着跨语言嵌入和多语言大语言模型的出现，需要全面梳理从翻译方法到嵌入驱动和生成技术的最新进展。

Method: 提供结构化综述，涵盖CLIR核心组件、评估实践和可用资源，分析从早期翻译方法到最先进的嵌入驱动和生成技术的发展历程。

Result: 识别了数据不平衡和语言变异等持续挑战，同时提出了推进公平有效跨语言信息检索的有前景方向，为构建稳健、包容和适应性强的检索系统提供指导。

Conclusion: 通过将CLIR置于信息检索和多语言处理更广泛的背景下，不仅回顾了当前能力，还为构建稳健、包容和适应性强的检索系统指明了未来方向。

Abstract: Cross-lingual information retrieval (CLIR) addresses the challenge of
retrieving relevant documents written in languages different from that of the
original query. Research in this area has typically framed the task as
monolingual retrieval augmented by translation, treating retrieval methods and
cross-lingual capabilities in isolation. Both monolingual and cross-lingual
retrieval usually follow a pipeline of query expansion, ranking, re-ranking
and, increasingly, question answering. Recent advances, however, have shifted
from translation-based methods toward embedding-based approaches and leverage
multilingual large language models (LLMs), for which aligning representations
across languages remains a central challenge. The emergence of cross-lingual
embeddings and multilingual LLMs has introduced a new paradigm, offering
improved retrieval performance and enabling answer generation. This survey
provides a comprehensive overview of developments from early translation-based
methods to state-of-the-art embedding-driven and generative techniques. It
presents a structured account of core CLIR components, evaluation practices,
and available resources. Persistent challenges such as data imbalance and
linguistic variation are identified, while promising directions are suggested
for advancing equitable and effective cross-lingual information retrieval. By
situating CLIR within the broader landscape of information retrieval and
multilingual language processing, this work not only reviews current
capabilities but also outlines future directions for building retrieval systems
that are robust, inclusive, and adaptable.

</details>


### [7] [Deep Learning-Based Approach for Improving Relational Aggregated Search](https://arxiv.org/abs/2510.00966)
*Sara Saad Soliman,Ahmed Younes,Islam Elkabani,Ashraf Elsayed*

Main category: cs.IR

TL;DR: 本研究使用堆叠自编码器和AraBERT嵌入来改进阿拉伯语文本在聚合搜索环境中的聚类效果，通过K-means算法发现搜索结果中的特征和关系，显著提升了聚类准确性和相关性。


<details>
  <summary>Details</summary>
Motivation: 由于互联网信息爆炸，需要开发聚合搜索系统来提升各种格式内容的检索和管理。传统搜索引擎存在不精确、缺乏上下文相关性和个性化的问题，因此需要更丰富、上下文感知的搜索结果表征方法。

Method: 应用堆叠自编码器和AraBERT嵌入等先进自然语言处理技术，结合K-means聚类算法来发现搜索结果中的特征和关系，并在不同的阿拉伯语查询上进行评估。

Result: 模型表明堆叠自编码器在表示学习中适合聚类任务，能显著改进搜索结果的聚类效果，同时展示了搜索结果的准确性和相关性得到提升。

Conclusion: 使用堆叠自编码器和AraBERT嵌入的方法能有效改进阿拉伯语文本在聚合搜索环境中的聚类性能，为搜索结果提供更准确和相关的组织方式。

Abstract: Due to an information explosion on the internet, there is a need for the
development of aggregated search systems that can boost the retrieval and
management of content in various formats. To further improve the clustering of
Arabic text data in aggregated search environments, this research investigates
the application of advanced natural language processing techniques, namely
stacked autoencoders and AraBERT embeddings. By transcending the limitations of
traditional search engines, which are imprecise, not contextually relevant, and
not personalized, we offer more enriched, context-aware characterizations of
search results, so we used a K-means clustering algorithm to discover
distinctive features and relationships in these results, we then used our
approach on different Arabic queries to evaluate its effectiveness. Our model
illustrates that using stacked autoencoders in representation learning suits
clustering tasks and can significantly improve clustering search results. It
also demonstrates improved accuracy and relevance of search results.

</details>


### [8] [ModernVBERT: Towards Smaller Visual Document Retrievers](https://arxiv.org/abs/2510.01149)
*Paul Teiletche,Quentin Macé,Max Conti,Antonio Loison,Gautier Viaud,Pierre Colombo,Manuel Faysse*

Main category: cs.IR

TL;DR: 本文提出了一种改进视觉文档检索模型的方法，通过系统实验揭示了注意力掩码、图像分辨率、模态对齐数据策略和对比目标等关键因素对性能的影响，并发布了ModernVBERT模型，该模型在文档检索任务上优于大10倍的模型。


<details>
  <summary>Details</summary>
Motivation: 当前多模态嵌入模型通常通过微调大型视觉语言解码器构建，虽然成本效益高，但这种重新利用方法往往会限制检索性能。

Method: 通过受控实验建立改进视觉文档检索模型的原理性方法，特别关注注意力掩码、图像分辨率、模态对齐数据策略和以晚期交互为中心的对比目标。

Result: 基于这些洞察发布了ModernVBERT，这是一个紧凑的2.5亿参数视觉语言编码器，在文档检索任务上微调后性能优于大10倍的模型。

Conclusion: 系统实验揭示了视觉文档检索模型的关键性能因素，提出的ModernVBERT证明了紧凑模型在适当设计下可以超越更大模型。

Abstract: Multimodal embedding models are gaining prevalence, notably for document
retrieval as efficient alternatives to text-only pipelines. These models are
typically built by finetuning large vision-language decoders (VLMs) with
contrastive losses on text-image pairs. In this work, we show that, while
cost-efficient, this repurposing approach often bottlenecks retrieval
performance. Through controlled experiments, we establish a principled recipe
for improving visual document retrieval models. We notably measure the impact
of attention masking, image resolution, modality alignment data regimes, and
late interaction centered contrastive objectives which emerge as central
performance factors. Building on these insights, we release ModernVBERT, a
compact 250M-parameter vision-language encoder that outperforms models up to 10
times larger when finetuned on document retrieval tasks. Models and code are
made available at https://huggingface.co/ModernVBERT.

</details>
