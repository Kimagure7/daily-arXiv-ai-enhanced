<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 21]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Enhancing Financial RAG with Agentic AI and Multi-HyDE: A Novel Approach to Knowledge Retrieval and Hallucination Reduction](https://arxiv.org/abs/2509.16369)
*Akshay Govind Srinivasan,Ryan Jacob George,Jayden Koshy Joe,Hrushikesh Kant,Harshith M R,Sachin Sundar,Sudharshan Suresh,Rahul Vimalkanth,Vijayavallabh*

Main category: cs.IR

TL;DR: 提出一个金融RAG框架，结合智能体AI和Multi-HyDE系统，通过生成多个非等价查询提升金融知识检索效果，在标准金融QA基准上准确率提升11.2%，幻觉减少15%。


<details>
  <summary>Details</summary>
Motivation: 金融问答需要精确可靠的知识检索，传统单一数据库和检索器无法处理复杂的金融监管文件、市场分析和多年报告等结构化金融语料库。

Method: 采用智能体AI和Multi-HyDE系统，生成多个非等价查询来增强检索覆盖面和效果，优化token效率和多步骤金融推理流程，结合关键词和基于表格的检索工具。

Result: 在标准金融QA基准测试中，准确率提高11.2%，幻觉减少15%，显著提升了答案的准确性和可靠性。

Conclusion: 该研究不仅提供了一个模块化、可适应的金融检索框架，还强调了结构化智能体工作流和多视角检索对于在高风险金融应用中可信部署AI的重要性。

Abstract: Accurate and reliable knowledge retrieval is vital for financial
question-answering, where continually updated data sources and complex,
high-stakes contexts demand precision. Traditional retrieval systems rely on a
single database and retriever, but financial applications require more
sophisticated approaches to handle intricate regulatory filings, market
analyses, and extensive multi-year reports. We introduce a framework for
financial Retrieval Augmented Generation (RAG) that leverages agentic AI and
the Multi-HyDE system, an approach that generates multiple, nonequivalent
queries to boost the effectiveness and coverage of retrieval from large,
structured financial corpora. Our pipeline is optimized for token efficiency
and multi-step financial reasoning, and we demonstrate that their combination
improves accuracy by 11.2% and reduces hallucinations by 15%. Our method is
evaluated on standard financial QA benchmarks, showing that integrating
domain-specific retrieval mechanisms such as Multi-HyDE with robust toolsets,
including keyword and table-based retrieval, significantly enhances both the
accuracy and reliability of answers. This research not only delivers a modular,
adaptable retrieval framework for finance but also highlights the importance of
structured agent workflows and multi-perspective retrieval for trustworthy
deployment of AI in high-stakes financial applications.

</details>


### [2] [Hierarchical Retrieval: The Geometry and a Pretrain-Finetune Recipe](https://arxiv.org/abs/2509.16411)
*Chong You,Rajesh Jayaram,Ananda Theertha Suresh,Robin Nittka,Felix Yu,Sanjiv Kumar*

Main category: cs.IR

TL;DR: 该论文研究双编码器模型在层次检索中的局限性，提出预训练-微调方法解决长距离检索性能下降问题，显著提升检索效果。


<details>
  <summary>Details</summary>
Motivation: 双编码器模型在信息检索中广泛应用，但其欧几里得几何嵌入空间限制了表达能力，特别是在层次检索场景中，匹配文档是查询的所有祖先节点时，存在长距离检索性能下降的问题。

Method: 首先证明双编码器在层次检索中的可行性，然后提出预训练-微调方法：先预训练模型理解层次结构，再微调优化检索性能，特别针对长距离文档对。

Result: 在WordNet层次结构上的实验显示，预训练-微调方法将长距离文档对的召回率从19%提升到76%，同时在购物查询数据集上也改善了相关产品检索效果。

Conclusion: 预训练-微调方法有效解决了双编码器在层次检索中的长距离性能下降问题，显著提升了检索质量，为层次结构数据检索提供了实用解决方案。

Abstract: Dual encoder (DE) models, where a pair of matching query and document are
embedded into similar vector representations, are widely used in information
retrieval due to their simplicity and scalability. However, the Euclidean
geometry of the embedding space limits the expressive power of DEs, which may
compromise their quality. This paper investigates such limitations in the
context of hierarchical retrieval (HR), where the document set has a
hierarchical structure and the matching documents for a query are all of its
ancestors. We first prove that DEs are feasible for HR as long as the embedding
dimension is linear in the depth of the hierarchy and logarithmic in the number
of documents. Then we study the problem of learning such embeddings in a
standard retrieval setup where DEs are trained on samples of matching query and
document pairs. Our experiments reveal a lost-in-the-long-distance phenomenon,
where retrieval accuracy degrades for documents further away in the hierarchy.
To address this, we introduce a pretrain-finetune recipe that significantly
improves long-distance retrieval without sacrificing performance on closer
documents. We experiment on a realistic hierarchy from WordNet for retrieving
documents at various levels of abstraction, and show that pretrain-finetune
boosts the recall on long-distance pairs from 19% to 76%. Finally, we
demonstrate that our method improves retrieval of relevant products on a
shopping queries dataset.

</details>


### [3] [Evaluating the Effectiveness and Scalability of LLM-Based Data Augmentation for Retrieval](https://arxiv.org/abs/2509.16442)
*Pranjal A. Chitale,Bishal Santra,Yashoteja Prabhu,Amit Sharma*

Main category: cs.IR

TL;DR: 本研究对LLM数据增强在检索任务中的有效性进行了系统性分析，发现增强效果存在收益递减现象，小模型增强可达到与大模型相当的效果，且增强对预训练不足的模型帮助最大。


<details>
  <summary>Details</summary>
Motivation: 解决紧凑双编码器模型因知识有限而性能不如LLM检索模型的问题，探索LLM数据增强在实际检索问题中的有效性和可扩展性。

Method: 通过超过100种不同的实验设置，系统研究检索模型、增强模型和增强策略等关键因素，包括增强规模、模型大小和多样性等维度。

Result: 数据增强能提升检索性能但存在收益递减；小LLM增强效果可与大模型竞争；增强对预训练不足的模型帮助最大。

Conclusion: 研究结果为制定更明智和高效的增强策略提供了指导，可在保证检索性能的同时实现成本效益最大化。

Abstract: Compact dual-encoder models are widely used for retrieval owing to their
efficiency and scalability. However, such models often underperform compared to
their Large Language Model (LLM)-based retrieval counterparts, likely due to
their limited world knowledge. While LLM-based data augmentation has been
proposed as a strategy to bridge this performance gap, there is insufficient
understanding of its effectiveness and scalability to real-world retrieval
problems. Existing research does not systematically explore key factors such as
the optimal augmentation scale, the necessity of using large augmentation
models, and whether diverse augmentations improve generalization, particularly
in out-of-distribution (OOD) settings. This work presents a comprehensive study
of the effectiveness of LLM augmentation for retrieval, comprising over 100
distinct experimental settings of retrieval models, augmentation models and
augmentation strategies. We find that, while augmentation enhances retrieval
performance, its benefits diminish beyond a certain augmentation scale, even
with diverse augmentation strategies. Surprisingly, we observe that
augmentation with smaller LLMs can achieve performance competitive with larger
augmentation models. Moreover, we examine how augmentation effectiveness varies
with retrieval model pre-training, revealing that augmentation provides the
most benefit to models which are not well pre-trained. Our insights pave the
way for more judicious and efficient augmentation strategies, thus enabling
informed decisions and maximizing retrieval performance while being more
cost-effective. Code and augmented datasets accompanying this work are publicly
available at https://aka.ms/DAGR.

</details>


### [4] [Purely Semantic Indexing for LLM-based Generative Recommendation and Retrieval](https://arxiv.org/abs/2509.16446)
*Ruohan Zhang,Jiacheng Li,Julian McAuley,Yupeng Hou*

Main category: cs.IR

TL;DR: 本文提出了一种纯语义索引方法，通过放松严格的最近质心选择来生成唯一且保持语义的ID，避免了现有方法中语义ID冲突的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的语义ID方法存在语义冲突问题，即语义相似的文档被分配相同的ID。常见的避免冲突策略是添加非语义标记，但这会引入随机性并扩大搜索空间，从而损害性能。

Method: 提出了两种模型无关的算法：穷举候选匹配（ECM）和递归残差搜索（RRS），通过放松严格的最近质心选择来实现唯一ID分配。

Result: 在顺序推荐、产品搜索和文档检索任务上的大量实验表明，该方法提高了整体性能和冷启动性能。

Conclusion: 确保ID唯一性的有效性得到了验证，纯语义索引方法在避免语义冲突的同时保持了语义信息。

Abstract: Semantic identifiers (IDs) have proven effective in adapting large language
models for generative recommendation and retrieval. However, existing methods
often suffer from semantic ID conflicts, where semantically similar documents
(or items) are assigned identical IDs. A common strategy to avoid conflicts is
to append a non-semantic token to distinguish them, which introduces randomness
and expands the search space, therefore hurting performance. In this paper, we
propose purely semantic indexing to generate unique, semantic-preserving IDs
without appending non-semantic tokens. We enable unique ID assignment by
relaxing the strict nearest-centroid selection and introduce two model-agnostic
algorithms: exhaustive candidate matching (ECM) and recursive residual
searching (RRS). Extensive experiments on sequential recommendation, product
search, and document retrieval tasks demonstrate that our methods improve both
overall and cold-start performance, highlighting the effectiveness of ensuring
ID uniqueness.

</details>


### [5] [Long document summarization using page specific target text alignment and distilling page importance](https://arxiv.org/abs/2509.16539)
*Pushpa Devi,Ayush Agrawal,Ashutosh Dubey,C. Ravindranath Chowdary*

Main category: cs.IR

TL;DR: 提出了PTS和PTSPI模型来解决长文档摘要问题，通过分页对齐和动态权重机制显著提升了摘要质量，在ROUGE指标上超越了现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 随着文本数据在新闻、法律、医疗等领域的快速增长，用户需要高效访问和理解大量内容。长文档摘要相比短文档摘要资源消耗大且研究较少，现有BART模型受限于上下文窗口长度。

Method: PTS模型将源文档分页，每页与目标摘要的相关部分对齐生成部分摘要。PTSPI在PTS基础上增加页面重要性层，提供动态权重和显式监督，重点关注信息量最大的页面。

Result: 在基准数据集上的实验显示，PTSPI在ROUGE-1和ROUGE-2分数上分别比现有最佳方法提升了6.32%和8.08%。

Conclusion: 提出的分页对齐方法和动态权重机制有效解决了长文档摘要的挑战，PTSPI模型在摘要质量上取得了显著提升。

Abstract: The rapid growth of textual data across news, legal, medical, and scientific
domains is becoming a challenge for efficiently accessing and understanding
large volumes of content. It is increasingly complex for users to consume and
extract meaningful information efficiently. Thus, raising the need for
summarization. Unlike short document summarization, long document abstractive
summarization is resource-intensive, and very little literature is present in
this direction. BART is a widely used efficient sequence-to-sequence
(seq-to-seq) model. However, when it comes to summarizing long documents, the
length of the context window limits its capabilities. We proposed a model
called PTS (Page-specific Target-text alignment Summarization) that extends the
seq-to-seq method for abstractive summarization by dividing the source document
into several pages. PTS aligns each page with the relevant part of the target
summary for better supervision. Partial summaries are generated for each page
of the document. We proposed another model called PTSPI (Page-specific
Target-text alignment Summarization with Page Importance), an extension to PTS
where an additional layer is placed before merging the partial summaries into
the final summary. This layer provides dynamic page weightage and explicit
supervision to focus on the most informative pages. We performed experiments on
the benchmark dataset and found that PTSPI outperformed the SOTA by 6.32\% in
ROUGE-1 and 8.08\% in ROUGE-2 scores.

</details>


### [6] [The Role of Vocabularies in Learning Sparse Representations for Ranking](https://arxiv.org/abs/2509.16621)
*Hiun Kim,Tae Kwan Lee,Taeryun Won*

Main category: cs.IR

TL;DR: 本研究探讨了SPLADE模型中词汇表大小和预训练权重对检索效率和效果的影响，通过构建100K词汇表的BERT模型并应用剪枝技术，发现扩大词汇表可在保持效率的同时提升检索效果。


<details>
  <summary>Details</summary>
Motivation: 目前对SPLADE模型中词汇表作用及其与检索效率效果关系的研究较少，需要深入探讨词汇表配置对检索性能的影响。

Method: 构建了两个100K词汇表的BERT模型（一个使用ESPLADE预训练，一个随机初始化），在真实搜索点击日志上微调，并应用基于logit分数的查询和文档剪枝技术。

Result: 实验结果显示，在应用剪枝后，两个100K词汇表模型在计算预算下比32K词汇表的普通SPLADE模型更有效，且ESPLADE模型比随机词汇表模型效果更好，同时保持相似的检索成本。

Conclusion: 词汇表大小和预训练权重在检索引擎中配置了查询、文档及其交互的表征规范，超越了其在NLP中的原始意义，为LSR的改进提供了新的方向。

Abstract: Learned Sparse Retrieval (LSR) such as SPLADE has growing interest for
effective semantic 1st stage matching while enjoying the efficiency of inverted
indices. A recent work on learning SPLADE models with expanded vocabularies
(ESPLADE) was proposed to represent queries and documents into a sparse space
of custom vocabulary which have different levels of vocabularic granularity.
Within this effort, however, there have not been many studies on the role of
vocabulary in SPLADE models and their relationship to retrieval efficiency and
effectiveness.
  To study this, we construct BERT models with 100K-sized output vocabularies,
one initialized with the ESPLADE pretraining method and one initialized
randomly. After finetune on real-world search click logs, we applied logit
score-based queries and documents pruning to max size for further balancing
efficiency. The experimental result in our evaluation set shows that, when
pruning is applied, the two models are effective compared to the 32K-sized
normal SPLADE model in the computational budget under the BM25. And the ESPLADE
models are more effective than the random vocab model, while having a similar
retrieval cost.
  The result indicates that the size and pretrained weight of output
vocabularies play the role of configuring the representational specification
for queries, documents, and their interactions in the retrieval engine, beyond
their original meaning and purposes in NLP. These findings can provide a new
room for improvement for LSR by identifying the importance of representational
specification from vocabulary configuration for efficient and effective
retrieval.

</details>


### [7] [Comparing RAG and GraphRAG for Page-Level Retrieval Question Answering on Math Textbook](https://arxiv.org/abs/2509.16780)
*Eason Chen,Chuangji Li,Shizhuo Li,Conrad Borchers,Zimo Xiao,Chloe Qianhui Zhao,Jionghao Lin,Kenneth R. Koedinger*

Main category: cs.IR

TL;DR: 该研究比较了基于嵌入的RAG和GraphRAG在数学教科书页面级问答中的表现，发现标准RAG在检索准确性和答案质量方面优于GraphRAG，后者由于基于实体的结构容易检索过多无关内容。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然能有效回答一般性问题，但缺乏与特定课程材料（如教科书和幻灯片）的领域知识对齐。研究旨在探索如何改进检索方法，为教育环境构建可靠的AI辅导解决方案。

Method: 使用477个问题-答案对的数据集，每个问题对应教科书的特定页面。比较基于嵌入的RAG方法和GraphRAG方法，评估检索准确性和生成答案质量（使用F1分数）。还探索了用LLM重新排序检索页面的方法。

Result: 基于嵌入的RAG在检索准确性和F1分数方面表现更好。GraphRAG由于基于实体的结构容易检索过多无关内容。使用LLM重新排序检索页面时效果不一，在处理较大上下文窗口时会出现性能下降和幻觉问题。

Conclusion: 研究强调了页面级检索系统在教育环境中的前景和挑战，需要更精细的检索方法来构建可靠的AI辅导解决方案，特别是在提供参考页码方面。

Abstract: Technology-enhanced learning environments often help students retrieve
relevant learning content for questions arising during self-paced study. Large
language models (LLMs) have emerged as novel aids for information retrieval
during learning. While LLMs are effective for general-purpose
question-answering, they typically lack alignment with the domain knowledge of
specific course materials such as textbooks and slides. We investigate
Retrieval-Augmented Generation (RAG) and GraphRAG, a knowledge graph-enhanced
RAG approach, for page-level question answering in an undergraduate mathematics
textbook. While RAG has been effective for retrieving discrete, contextually
relevant passages, GraphRAG may excel in modeling interconnected concepts and
hierarchical knowledge structures. We curate a dataset of 477 question-answer
pairs, each tied to a distinct textbook page. We then compare the standard
embedding-based RAG methods to GraphRAG for evaluating both retrieval
accuracy-whether the correct page is retrieved-and generated answer quality via
F1 scores. Our findings show that embedding-based RAG achieves higher retrieval
accuracy and better F1 scores compared to GraphRAG, which tends to retrieve
excessive and sometimes irrelevant content due to its entity-based structure.
We also explored re-ranking the retrieved pages with LLM and observed mixed
results, including performance drop and hallucinations when dealing with larger
context windows. Overall, this study highlights both the promises and
challenges of page-level retrieval systems in educational contexts, emphasizing
the need for more refined retrieval methods to build reliable AI tutoring
solutions in providing reference page numbers.

</details>


### [8] [Temporal-Aware User Behaviour Simulation with Large Language Models for Recommender Systems](https://arxiv.org/abs/2509.16895)
*Xinye Wanyan,Danula Hettiachchi,Chenglong Ma,Ziqi Xu,Jeffrey Chan*

Main category: cs.IR

TL;DR: DyTA4Rec是一个基于LLM的动态时序感知代理模拟器，用于推荐系统，通过建模用户行为的动态演化来改进模拟反馈与真实用户行为的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的推荐系统代理方法主要依赖静态用户画像，忽视了用户兴趣的时序性和动态性，限制了代理捕捉序列模式的能力。

Method: DyTA4Rec包含动态更新器用于实时画像优化、时序增强提示用于序列上下文建模、自适应聚合用于一致性反馈生成。

Result: 实验结果表明，DyTA4Rec在群体和个体层面显著提高了模拟行为与真实用户行为的一致性，通过建模动态特性和增强时序感知能力。

Conclusion: DyTA4Rec通过引入动态时序建模机制，有效解决了LLM代理在推荐系统中对用户行为动态性建模不足的问题，提升了模拟效果。

Abstract: Large Language Models (LLMs) demonstrate human-like capabilities in language
understanding, reasoning, and generation, driving interest in using LLM-based
agents to simulate human feedback in recommender systems. However, most
existing approaches rely on static user profiling, neglecting the temporal and
dynamic nature of user interests. This limitation stems from a disconnect
between language modelling and behaviour modelling, which constrains the
capacity of agents to represent sequential patterns. To address this challenge,
we propose a Dynamic Temporal-aware Agent-based simulator for Recommender
Systems, DyTA4Rec, which enables agents to model and utilise evolving user
behaviour based on historical interactions. DyTA4Rec features a dynamic updater
for real-time profile refinement, temporal-enhanced prompting for sequential
context, and self-adaptive aggregation for coherent feedback. Experimental
results at group and individual levels show that DyTA4Rec significantly
improves the alignment between simulated and actual user behaviour by modelling
dynamic characteristics and enhancing temporal awareness in LLM-based agents.

</details>


### [9] [Equip Pre-ranking with Target Attention by Residual Quantization](https://arxiv.org/abs/2509.16931)
*Yutong Li,Yu Zhu,Yichen Qiao,Ziyu Guan,Lv Shao,Tong Liu,Bo Zheng*

Main category: cs.IR

TL;DR: TARQ是一个新的预排序框架，通过残差量化技术将目标注意力模型的强大建模能力引入到延迟敏感的预排序阶段，解决了效率与效果之间的冲突。


<details>
  <summary>Details</summary>
Motivation: 工业推荐系统中预排序阶段面临效率与效果的冲突，强大的目标注意力模型计算成本过高无法用于预排序，而简单的向量积模型性能有限，这成为整个系统的性能瓶颈。

Method: TARQ框架借鉴生成模型思想，通过残差量化技术构建近似目标注意力的架构，将TA的建模能力首次引入到延迟关键的预排序阶段。

Result: 在淘宝进行的大规模离线和在线A/B测试表明，TARQ在排序性能上有显著提升，已在生产环境中全面部署，服务数千万日活用户并带来显著业务改进。

Conclusion: TARQ在准确性和效率之间建立了新的最先进权衡，成功解决了预排序阶段的性能瓶颈问题。

Abstract: The pre-ranking stage in industrial recommendation systems faces a
fundamental conflict between efficiency and effectiveness. While powerful
models like Target Attention (TA) excel at capturing complex feature
interactions in the ranking stage, their high computational cost makes them
infeasible for pre-ranking, which often relies on simplistic vector-product
models. This disparity creates a significant performance bottleneck for the
entire system. To bridge this gap, we propose TARQ, a novel pre-ranking
framework. Inspired by generative models, TARQ's key innovation is to equip
pre-ranking with an architecture approximate to TA by Residual Quantization.
This allows us to bring the modeling power of TA into the latency-critical
pre-ranking stage for the first time, establishing a new state-of-the-art
trade-off between accuracy and efficiency. Extensive offline experiments and
large-scale online A/B tests at Taobao demonstrate TARQ's significant
improvements in ranking performance. Consequently, our model has been fully
deployed in production, serving tens of millions of daily active users and
yielding substantial business improvements.

</details>


### [10] [Identifying and Upweighting Power-Niche Users to Mitigate Popularity Bias in Recommendations](https://arxiv.org/abs/2509.17265)
*David Liu,Erik Weis,Moritz Laber,Tina Eliassi-Rad,Brennan Klein*

Main category: cs.IR

TL;DR: 该论文研究了推荐系统中的流行度偏差问题，发现偏好小众物品的用户具有长尾活动分布，提出了同时考虑用户活跃度和物品流行度的BPR损失重加权框架，通过实验证明该方法能有效减少流行度偏差并提升整体性能。


<details>
  <summary>Details</summary>
Motivation: 推荐系统存在流行度偏差，过度推荐热门物品而忽视相关的小众物品。作者希望通过理解基准推荐数据集中用户与小众物品的交互，来缓解这种偏差。

Method: 提出基于用户活跃度和物品流行度的BPR损失重加权框架，引入两个可解释参数分别控制用户活跃度和物品流行度的重要性。将用户按活跃度（高活跃vs低活跃）和物品偏好（主流vs小众）进行划分。

Result: 实验表明，在多个基准数据集上，高活跃-小众偏好的用户数量显著多于预期。通过提升这类用户的权重，能够减少流行度偏差并提高整体推荐性能。

Conclusion: 与以往单独考虑用户活跃度或物品流行度的方法相比，同时考虑两者的交互作用能够获得帕累托优势的性能表现。

Abstract: Recommender systems have been shown to exhibit popularity bias by
over-recommending popular items and under-recommending relevant niche items. We
seek to understand interactions with niche items in benchmark recommendation
datasets as a step toward mitigating popularity bias. We find that, compared to
mainstream users, niche-preferring users exhibit a longer-tailed activity-level
distribution, indicating the existence of users who both prefer niche items and
exhibit high activity levels. We partition users along two axes: (1) activity
level ("power" vs. "light") and (2) item-popularity preference ("mainstream"
vs. "niche"), and show that in several benchmark datasets, the number of
power-niche users (high activity and niche preference) is statistically
significantly larger than expected under a null configuration model. Motivated
by this observation, we propose a framework for reweighting the Bayesian
Personalized Ranking (BPR) loss that simultaneously reweights based on user
activity level and item popularity. Our method introduces two interpretable
parameters: one controlling the significance of user activity level, and the
other of item popularity. Experiments on benchmark datasets show that
upweighting power-niche users reduces popularity bias and can increase overall
performance. In contrast to previous work that only considers user activity
level or item popularity in isolation, our results suggest that considering
their interaction leads to Pareto-dominant performance.

</details>


### [11] [MLLM-Driven Semantic Identifier Generation for Generative Cross-Modal Retrieval](https://arxiv.org/abs/2509.17359)
*Tianyuan Li,Lei Wang,Ahtamjan Ahmat,Yating Yang,Bo Ma,Rui Dong,Bangju Han*

Main category: cs.IR

TL;DR: 提出了一种词汇高效标识符生成框架，通过结构化语义标识符和理性引导监督策略改进生成式跨模态检索


<details>
  <summary>Details</summary>
Motivation: 现有生成式跨模态检索方法依赖手工字符串ID、聚类标签或需要词汇扩展的原子标识符，在语义对齐和可扩展性方面存在挑战

Method: 使用结构化语义标识符（包含对象、动作等概念级标记）和理性引导监督策略（生成一句话解释作为辅助监督信号）

Result: 该方法自然对齐模型的生成空间，无需修改分词器，提高了语义基础并减少了训练中的幻觉

Conclusion: 提出的框架有效解决了生成式跨模态检索中的语义对齐和可扩展性问题

Abstract: Generative cross-modal retrieval, which treats retrieval as a generation
task, has emerged as a promising direction with the rise of Multimodal Large
Language Models (MLLMs). In this setting, the model responds to a text query by
generating an identifier corresponding to the target image. However, existing
methods typically rely on manually crafted string IDs, clustering-based labels,
or atomic identifiers requiring vocabulary expansion, all of which face
challenges in semantic alignment or scalability.To address these limitations,
we propose a vocabulary-efficient identifier generation framework that prompts
MLLMs to generate Structured Semantic Identifiers from image-caption pairs.
These identifiers are composed of concept-level tokens such as objects and
actions, naturally aligning with the model's generation space without modifying
the tokenizer. Additionally, we introduce a Rationale-Guided Supervision
Strategy, prompting the model to produce a one-sentence explanation alongside
each identifier serves as an auxiliary supervision signal that improves
semantic grounding and reduces hallucinations during training.

</details>


### [12] [SeqUDA-Rec: Sequential User Behavior Enhanced Recommendation via Global Unsupervised Data Augmentation for Personalized Content Marketing](https://arxiv.org/abs/2509.17361)
*Ruihan Luo,Xuanjing Chen,Ziyang Ding*

Main category: cs.IR

TL;DR: SeqUDA-Rec是一个新颖的深度学习框架，通过整合用户行为序列和全局无监督数据增强来解决传统推荐系统的局限性，显著提升了推荐准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统存在两个主要问题：(1)依赖有限的显式用户反馈监督信号，(2)容易受到噪声或无意交互的影响。需要开发更鲁棒的个性化推荐方法。

Method: 构建全局用户-物品交互图(GUIG)，应用图对比学习生成鲁棒嵌入，使用基于Transformer的顺序编码器建模用户偏好演化，并采用GAN增强策略生成可信交互模式补充训练数据。

Result: 在两个真实营销数据集(Amazon Ads和TikTok Ad Clicks)上，SeqUDA-Rec显著优于SASRec、BERT4Rec和GCL4SR等最先进基线模型，NDCG@10提升6.7%，HR@10提升11.3%。

Conclusion: SeqUDA-Rec在个性化广告和智能内容推荐方面表现出色，证明了其有效性，为解决推荐系统中的监督信号稀疏性和噪声交互问题提供了有效解决方案。

Abstract: Personalized content marketing has become a crucial strategy for digital
platforms, aiming to deliver tailored advertisements and recommendations that
match user preferences. Traditional recommendation systems often suffer from
two limitations: (1) reliance on limited supervised signals derived from
explicit user feedback, and (2) vulnerability to noisy or unintentional
interactions. To address these challenges, we propose SeqUDA-Rec, a novel deep
learning framework that integrates user behavior sequences with global
unsupervised data augmentation to enhance recommendation accuracy and
robustness. Our approach first constructs a Global User-Item Interaction Graph
(GUIG) from all user behavior sequences, capturing both local and global item
associations. Then, a graph contrastive learning module is applied to generate
robust embeddings, while a sequential Transformer-based encoder models users'
evolving preferences. To further enhance diversity and counteract sparse
supervised labels, we employ a GAN-based augmentation strategy, generating
plausible interaction patterns and supplementing training data. Extensive
experiments on two real-world marketing datasets (Amazon Ads and TikTok Ad
Clicks) demonstrate that SeqUDA-Rec significantly outperforms state-of-the-art
baselines such as SASRec, BERT4Rec, and GCL4SR. Our model achieves a 6.7%
improvement in NDCG@10 and 11.3% improvement in HR@10, proving its
effectiveness in personalized advertising and intelligent content
recommendation.

</details>


### [13] [Simplified Longitudinal Retrieval Experiments: A Case Study on Query Expansion and Document Boosting](https://arxiv.org/abs/2509.17440)
*Jüri Keller,Maik Fröbe,Gijs Hendriksen,Daria Alexander,Martin Potthast,Philipp Schaer*

Main category: cs.IR

TL;DR: 提出了ir_datasets的自定义扩展，用于声明式地描述纵向检索实验的时间维度，以降低代码复杂性并提高可重复性。


<details>
  <summary>Details</summary>
Motivation: 传统的Cranfield式检索评估只包含静态的查询和文档集，缺乏时间维度，而纵向评估需要自定义逻辑，增加了研究软件的复杂性，降低了实验的可重复性和可扩展性。

Method: 基于在LongEval 2024的提交，开发了ir_datasets的自定义扩展，允许声明式地描述纵向检索实验的重要方面，如查询、文档和相关性反馈在时间上的可用性。

Result: 重新实现了LongEval 2024的提交，发现声明式访问可以降低代码的复杂性。

Conclusion: 提出的ir_datasets扩展能够有效简化纵向检索实验的实现，提高研究的可重复性和可扩展性。

Abstract: The longitudinal evaluation of retrieval systems aims to capture how
information needs and documents evolve over time. However, classical
Cranfield-style retrieval evaluations only consist of a static set of queries
and documents and thereby miss time as an evaluation dimension. Therefore,
longitudinal evaluations need to complement retrieval toolkits with custom
logic. This custom logic increases the complexity of research software, which
might reduce the reproducibility and extensibility of experiments. Based on our
submissions to the 2024 edition of LongEval, we propose a custom extension of
ir_datasets for longitudinal retrieval experiments. This extension allows for
declaratively, instead of imperatively, describing important aspects of
longitudinal retrieval experiments, e.g., which queries, documents, and/or
relevance feedback are available at which point in time. We reimplement our
submissions to LongEval 2024 against our new ir_datasets extension, and find
that the declarative access can reduce the complexity of the code.

</details>


### [14] [WildClaims: Information Access Conversations in the Wild(Chat)](https://arxiv.org/abs/2509.17442)
*Hideaki Joko,Shakiba Amirshahi,Charles L. A. Clarke,Faegheh Hasibi*

Main category: cs.IR

TL;DR: 论文通过分析WildChat数据集发现，现实对话中信息检索往往以系统做出的值得验证的事实断言形式隐式发生，而非传统显式信息访问。作者发布了WildClaims数据集来系统研究这一现象。


<details>
  <summary>Details</summary>
Motivation: 探索现实世界对话中信息检索的本质和必要性，因为现有研究主要关注传统的显式信息访问对话，而真实对话中的信息访问模式尚未被充分研究。

Method: 在WildChat数据集上进行观察性研究，构建并发布WildClaims数据集（包含121,905个从3,000个对话中提取的事实断言，每个都标注了检查价值）。

Result: 保守估计18%-51%的对话包含值得验证的断言，非保守估计可达76%，表明隐式信息访问在现实对话中普遍存在。

Conclusion: 需要超越传统的显式信息访问理解，关注现实用户-系统对话中出现的隐式信息访问问题。

Abstract: The rapid advancement of Large Language Models (LLMs) has transformed
conversational systems into practical tools used by millions. However, the
nature and necessity of information retrieval in real-world conversations
remain largely unexplored, as research has focused predominantly on
traditional, explicit information access conversations. The central question
is: What do real-world information access conversations look like? To this end,
we first conduct an observational study on the WildChat dataset, large-scale
user-ChatGPT conversations, finding that users' access to information occurs
implicitly as check-worthy factual assertions made by the system, even when the
conversation's primary intent is non-informational, such as creative writing.
To enable the systematic study of this phenomenon, we release the WildClaims
dataset, a novel resource consisting of 121,905 extracted factual claims from
7,587 utterances in 3,000 WildChat conversations, each annotated for
check-worthiness. Our preliminary analysis of this resource reveals that
conservatively 18% to 51% of conversations contain check-worthy assertions,
depending on the methods employed, and less conservatively, as many as 76% may
contain such assertions. This high prevalence underscores the importance of
moving beyond the traditional understanding of explicit information access, to
address the implicit information access that arises in real-world user-system
conversations.

</details>


### [15] [LongEval at CLEF 2025: Longitudinal Evaluation of IR Systems on Web and Scientific Data](https://arxiv.org/abs/2509.17469)
*Matteo Cancellieri,Alaa El-Ebshihy,Tobias Fink,Maik Fröbe,Petra Galuščáková,Gabriela Gonzalez-Saez,Lorraine Goeuriot,David Iommi,Jüri Keller,Petr Knoth,Philippe Mulhem,Florina Piroi,David Pride,Philipp Schaer*

Main category: cs.IR

TL;DR: LongEval实验室专注于评估信息检索系统随时间变化的性能，提供两个数据集来捕捉文档、查询和相关性评估不断变化的搜索场景。在第三届中，包含网页检索和科学文献检索两个任务，19个团队提交了方法，使用nDCG和多种衡量检索效果随时间变化的指标进行评估。


<details>
  <summary>Details</summary>
Motivation: 评估信息检索系统在数据随时间变化情况下的性能表现，解决现实世界中搜索场景动态演变的挑战。

Method: 提供两个包含动态变化文档、查询和相关性评估的数据集，设计网页检索和科学文献检索两个任务，使用nDCG和专门的时间变化评估指标来量化系统性能。

Result: 19个团队参与了评估，提交了各自的检索方法，通过多种时间感知的评估指标对系统性能进行了全面分析。

Conclusion: LongEval实验室成功建立了评估信息检索系统时间动态性能的框架，为研究检索系统在数据演变环境下的鲁棒性提供了重要基准。

Abstract: The LongEval lab focuses on the evaluation of information retrieval systems
over time. Two datasets are provided that capture evolving search scenarios
with changing documents, queries, and relevance assessments. Systems are
assessed from a temporal perspective-that is, evaluating retrieval
effectiveness as the data they operate on changes. In its third edition,
LongEval featured two retrieval tasks: one in the area of ad-hoc web retrieval,
and another focusing on scientific article retrieval. We present an overview of
this year's tasks and datasets, as well as the participating systems. A total
of 19 teams submitted their approaches, which we evaluated using nDCG and a
variety of measures that quantify changes in retrieval effectiveness over time.

</details>


### [16] [Human vs. Agent in Task-Oriented Conversations](https://arxiv.org/abs/2509.17619)
*Zhefan Wang,Ning Geng,Zhiqiang Guo,Weizhi Ma,Min Zhang*

Main category: cs.IR

TL;DR: 本文首次系统比较了LLM模拟用户与真实用户在个性化任务导向对话中的行为差异，提出了包含10个维度的分析框架，发现两者在多个关键行为维度上存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 任务导向对话系统需要大量高质量对话数据，但获取成本高昂。虽然LLM有生成合成对话的潜力，但LLM模拟用户能否有效替代真实人类对话尚不明确。

Method: 提出了包含对话策略、交互风格和对话评估三个关键方面共10个维度的综合分析框架，在四个代表性场景下收集了人类用户和LLM代理用户在相同条件下的平行对话数据集。

Result: 分析发现两类用户在问题解决方式、问题广度、用户参与度、上下文依赖、反馈极性、语言风格和幻觉意识等方面存在显著行为差异，但在深度优先/广度优先维度和有用性维度上表现一致。

Conclusion: 研究结果为推进基于LLM的用户模拟提供了关键见解，构建的多维度分类法为分析用户行为模式提供了可推广的框架，为未来如何在对话系统中使用用户模拟提供了新的思考视角。

Abstract: Task-oriented conversational systems are essential for efficiently addressing
diverse user needs, yet their development requires substantial amounts of
high-quality conversational data that is challenging and costly to obtain.
While large language models (LLMs) have demonstrated potential in generating
synthetic conversations, the extent to which these agent-generated interactions
can effectively substitute real human conversations remains unclear. This work
presents the first systematic comparison between LLM-simulated users and human
users in personalized task-oriented conversations. We propose a comprehensive
analytical framework encompassing three key aspects (conversation strategy,
interaction style, and conversation evaluation) and ten distinct dimensions for
evaluating user behaviors, and collect parallel conversational datasets from
both human users and LLM agent users across four representative scenarios under
identical conditions. Our analysis reveals significant behavioral differences
between the two user types in problem-solving approaches, question broadness,
user engagement, context dependency, feedback polarity and promise, language
style, and hallucination awareness. We found consistency in the agent users and
human users across the depth-first or breadth-first dimensions, as well as the
usefulness dimensions. These findings provide critical insights for advancing
LLM-based user simulation. Our multi-dimensional taxonomy constructed a
generalizable framework for analyzing user behavior patterns, offering insights
from LLM agent users and human users. By this work, we provide perspectives on
rethinking how to use user simulation in conversational systems in the future.

</details>


### [17] [A Generative Framework for Personalized Sticker Retrieval](https://arxiv.org/abs/2509.17749)
*Changjiang Zhou,Ruqing Zhang,Jiafeng Guo,Yu-An Liu,Fan Zhang,Ganyuan Luo,Xueqi Cheng*

Main category: cs.IR

TL;DR: PEARL是一个用于个性化贴纸检索的生成式框架，通过用户表示学习和意图感知学习目标来解决现有生成式检索方法缺乏个性化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于相关性的生成式检索方法通常缺乏个性化，导致检索结果与多样化的用户期望不匹配，特别是在个性化贴纸检索领域尚未得到充分探索。

Method: 设计了表示学习模型来学习区分性用户表示（基于个人信息和点击历史的三个预测任务），并提出意图感知学习目标来生成与用户查询意图一致的贴纸。

Result: 离线和在线评估结果表明，PEARL显著优于最先进的方法。

Conclusion: PEARL框架成功解决了个性化贴纸检索的挑战，通过用户偏好编码和意图对齐实现了更好的检索性能。

Abstract: Formulating information retrieval as a variant of generative modeling,
specifically using autoregressive models to generate relevant identifiers for a
given query, has recently attracted considerable attention. However, its
application to personalized sticker retrieval remains largely unexplored and
presents unique challenges: existing relevance-based generative retrieval
methods typically lack personalization, leading to a mismatch between diverse
user expectations and the retrieved results. To address this gap, we propose
PEARL, a novel generative framework for personalized sticker retrieval, and
make two key contributions: (i) To encode user-specific sticker preferences, we
design a representation learning model to learn discriminative user
representations. It is trained on three prediction tasks that leverage personal
information and click history; and (ii) To generate stickers aligned with a
user's query intent, we propose a novel intent-aware learning objective that
prioritizes stickers associated with higher-ranked intents. Empirical results
from both offline evaluations and online tests demonstrate that PEARL
significantly outperforms state-of-the-art methods.

</details>


### [18] [Shilling Recommender Systems by Generating Side-feature-aware Fake User Profiles](https://arxiv.org/abs/2509.17918)
*Yuanrong Wang,Yingpeng Du*

Main category: cs.IR

TL;DR: 本文扩展了Leg-UP框架，通过增强生成器架构来整合侧特征，生成能够感知侧特征的虚假用户配置文件，以应对推荐系统中的恶意攻击。


<details>
  <summary>Details</summary>
Motivation: 现有的攻击方法在仅包含评分矩阵的训练数据下能生成有效且隐蔽的虚假配置文件，但在侧特征存在并被推荐系统利用的场景下缺乏全面解决方案。

Method: 扩展Leg-UP框架，增强生成器架构以整合侧特征，生成侧特征感知的虚假用户配置文件。

Result: 在基准测试上的实验表明，该方法在保持隐蔽性的同时实现了强大的攻击性能。

Conclusion: 通过整合侧特征，该方法能够有效应对推荐系统中的恶意攻击，提升攻击效果并保持隐蔽性。

Abstract: Recommender systems (RS) greatly influence users' consumption decisions,
making them attractive targets for malicious shilling attacks that inject fake
user profiles to manipulate recommendations. Existing shilling methods can
generate effective and stealthy fake profiles when training data only contain
rating matrix, but they lack comprehensive solutions for scenarios where side
features are present and utilized by the recommender. To address this gap, we
extend the Leg-UP framework by enhancing the generator architecture to
incorporate side features, enabling the generation of side-feature-aware fake
user profiles. Experiments on benchmarks show that our method achieves strong
attack performance while maintaining stealthiness.

</details>


### [19] [A Knowledge Graph-based Retrieval-Augmented Generation Framework for Algorithm Selection in the Facility Layout Problem](https://arxiv.org/abs/2509.18054)
*Nikhil N S,Amol Dilip Joshi,Bilal Muhammed,Soban Babu*

Main category: cs.IR

TL;DR: 提出基于知识图谱检索增强生成(KG-RAG)的新方法，用于设施布局问题(FLP)的算法推荐，相比商业LLM聊天机器人显著提升了推荐准确性和推理能力。


<details>
  <summary>Details</summary>
Motivation: 设施布局问题是一个NP难的多目标优化问题，算法选择需要深厚专家知识且性能依赖具体问题特征，需要数据驱动的推荐方法来指导自动化设计系统中的算法选择。

Method: 构建领域特定知识图谱，采用多维度检索机制（图搜索、向量搜索、聚类搜索）从知识图谱中收集相关证据，利用大语言模型生成基于数据的算法推荐。

Result: 在多样化真实FLP测试案例中，KG-RAG方法在推荐准确性和推理能力上显著优于商业LLM聊天机器人。

Conclusion: KG-RAG框架能够有效利用领域知识为复杂优化问题提供数据驱动的算法推荐，具有实际应用价值。

Abstract: Selecting a solution algorithm for the Facility Layout Problem (FLP), an
NP-hard optimization problem with a multiobjective trade-off, is a complex task
that requires deep expert knowledge. The performance of a given algorithm
depends on specific problem characteristics such as its scale, objectives, and
constraints. This creates a need for a data-driven recommendation method to
guide algorithm selection in automated design systems. This paper introduces a
new recommendation method to make such expertise accessible, based on a
Knowledge Graph-based Retrieval-Augmented Generation (KG RAG) framework. To
address this, a domain-specific knowledge graph is constructed from published
literature. The method then employs a multi-faceted retrieval mechanism to
gather relevant evidence from this knowledge graph using three distinct
approaches, which include a precise graph-based search, flexible vector-based
search, and high-level cluster-based search. The retrieved evidence is utilized
by a Large Language Model (LLM) to generate algorithm recommendations with
data-driven reasoning. The proposed KG-RAG method is compared against a
commercial LLM chatbot with access to the knowledge base as a table, across a
series of diverse, real-world FLP test cases. Based on recommendation accuracy
and reasoning capability, the proposed method performed significantly better
than the commercial LLM chatbot.

</details>


### [20] [OnePiece: Bringing Context Engineering and Reasoning to Industrial Cascade Ranking System](https://arxiv.org/abs/2509.18091)
*Sunhao Dai,Jiakai Tang,Jiahua Wu,Kun Wang,Yuxuan Zhu,Bingjun Chen,Bangyang Hong,Yu Zhao,Cong Fu,Kangle Wu,Yabo Ni,Anxiang Zeng,Wenjie Wang,Xu Chen,Jun Xu,See-Kiong Ng*

Main category: cs.IR

TL;DR: OnePiece是一个统一框架，将LLM风格的上下文工程和推理机制集成到工业级检索和排序模型中，通过结构化上下文工程、块级隐式推理和渐进式多任务训练实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有工业推荐系统主要局限于移植Transformer架构，而LLM的成功不仅来自架构，还来自上下文工程和多步推理机制。这些机制在工业排序系统中尚未充分探索。

Method: 基于纯Transformer架构，提出三个创新：结构化上下文工程（增强交互历史）、块级隐式推理（多步表示细化）、渐进式多任务训练（利用用户反馈链监督推理步骤）。

Result: 在Shopee主要个性化搜索场景部署，实现一致在线收益：GMV/UU提升超过2%，广告收入增加2.90%。

Conclusion: OnePiece框架成功将LLM的上下文工程和推理机制引入工业推荐系统，证明了这些机制对提升排序性能的重要价值。

Abstract: Despite the growing interest in replicating the scaled success of large
language models (LLMs) in industrial search and recommender systems, most
existing industrial efforts remain limited to transplanting Transformer
architectures, which bring only incremental improvements over strong Deep
Learning Recommendation Models (DLRMs). From a first principle perspective, the
breakthroughs of LLMs stem not only from their architectures but also from two
complementary mechanisms: context engineering, which enriches raw input queries
with contextual cues to better elicit model capabilities, and multi-step
reasoning, which iteratively refines model outputs through intermediate
reasoning paths. However, these two mechanisms and their potential to unlock
substantial improvements remain largely underexplored in industrial ranking
systems.
  In this paper, we propose OnePiece, a unified framework that seamlessly
integrates LLM-style context engineering and reasoning into both retrieval and
ranking models of industrial cascaded pipelines. OnePiece is built on a pure
Transformer backbone and further introduces three key innovations: (1)
structured context engineering, which augments interaction history with
preference and scenario signals and unifies them into a structured tokenized
input sequence for both retrieval and ranking; (2) block-wise latent reasoning,
which equips the model with multi-step refinement of representations and scales
reasoning bandwidth via block size; (3) progressive multi-task training, which
leverages user feedback chains to effectively supervise reasoning steps during
training. OnePiece has been deployed in the main personalized search scenario
of Shopee and achieves consistent online gains across different key business
metrics, including over $+2\%$ GMV/UU and a $+2.90\%$ increase in advertising
revenue.

</details>


### [21] [MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction](https://arxiv.org/abs/2509.18095)
*Zilin Xiao,Qi Ma,Mengting Gu,Chun-cheng Jason Chen,Xintao Chen,Vicente Ordonez,Vijai Mohan*

Main category: cs.IR

TL;DR: MetaEmbed是一个新的多模态检索框架，通过引入可学习的Meta Tokens来生成紧凑但表达力强的多向量嵌入，支持测试时根据效率需求调整检索质量


<details>
  <summary>Details</summary>
Motivation: 现有的多模态嵌入方法要么将查询和候选对象压缩为单个向量（限制细粒度信息表达），要么生成过多向量（多向量检索成本过高），需要一种平衡表达力和效率的解决方案

Method: 在训练时向输入序列添加固定数量的可学习Meta Tokens，测试时使用其最后一层的上下文表示作为多向量嵌入。通过Matryoshka多向量检索训练，学习按粒度组织信息

Result: 在MMEB和ViDoRe基准测试中达到最先进的检索性能，能够稳健扩展到320亿参数模型

Conclusion: MetaEmbed框架实现了测试时多模态检索的可扩展性，用户可以通过选择用于索引和检索交互的token数量来平衡检索质量和效率需求

Abstract: Universal multimodal embedding models have achieved great success in
capturing semantic relevance between queries and candidates. However, current
methods either condense queries and candidates into a single vector,
potentially limiting the expressiveness for fine-grained information, or
produce too many vectors that are prohibitively expensive for multi-vector
retrieval. In this work, we introduce MetaEmbed, a new framework for multimodal
retrieval that rethinks how multimodal embeddings are constructed and
interacted with at scale. During training, a fixed number of learnable Meta
Tokens are appended to the input sequence. At test-time, their last-layer
contextualized representations serve as compact yet expressive multi-vector
embeddings. Through the proposed Matryoshka Multi-Vector Retrieval training,
MetaEmbed learns to organize information by granularity across multiple
vectors. As a result, we enable test-time scaling in multimodal retrieval,
where users can balance retrieval quality against efficiency demands by
selecting the number of tokens used for indexing and retrieval interactions.
Extensive evaluations on the Massive Multimodal Embedding Benchmark (MMEB) and
the Visual Document Retrieval Benchmark (ViDoRe) confirm that MetaEmbed
achieves state-of-the-art retrieval performance while scaling robustly to
models with 32B parameters.

</details>
