{"id": "2512.18117", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.18117", "abs": "https://arxiv.org/abs/2512.18117", "authors": ["Xiwen Chen", "Yen-Chieh Lien", "Susan Liu", "Mar\u00eda Casta\u00f1os", "Abolfazl Razi", "Xiaoting Zhao", "Congzhe Su"], "title": "Factorized Transport Alignment for Multimodal and Multiview E-commerce Representation Learning", "comment": "Accepted by WSDM'26", "summary": "The rapid growth of e-commerce requires robust multimodal representations that capture diverse signals from user-generated listings. Existing vision-language models (VLMs) typically align titles with primary images, i.e., single-view, but overlook non-primary images and auxiliary textual views that provide critical semantics in open marketplaces such as Etsy or Poshmark. To this end, we propose a framework that unifies multimodal and multi-view learning through Factorized Transport, a lightweight approximation of optimal transport, designed for scalability and deployment efficiency. During training, the method emphasizes primary views while stochastically sampling auxiliary ones, reducing training cost from quadratic in the number of views to constant per item. At inference, all views are fused into a single cached embedding, preserving the efficiency of two-tower retrieval with no additional online overhead. On an industrial dataset of 1M product listings and 0.3M interactions, our approach delivers consistent improvements in cross-view and query-to-item retrieval, achieving up to +7.9% Recall@500 over strong multimodal baselines. Overall, our framework bridges scalability with optimal transport-based learning, making multi-view pretraining practical for large-scale e-commerce search.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u56e0\u5b50\u5316\u4f20\u8f93\u7684\u591a\u89c6\u56fe\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u7535\u5546\u5e73\u53f0\u4e2d\u975e\u4e3b\u56fe\u548c\u591a\u6587\u672c\u89c6\u56fe\u7684\u8868\u793a\u5b66\u4e60\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u68c0\u7d22\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u53ea\u5bf9\u9f50\u6807\u9898\u548c\u4e3b\u56fe\uff08\u5355\u89c6\u56fe\uff09\uff0c\u5ffd\u7565\u4e86\u7535\u5546\u5e73\u53f0\u4e2d\u975e\u4e3b\u56fe\u50cf\u548c\u8f85\u52a9\u6587\u672c\u89c6\u56fe\u63d0\u4f9b\u7684\u4e30\u5bcc\u8bed\u4e49\u4fe1\u606f\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u591a\u89c6\u56fe\u8868\u793a\u5b66\u4e60\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u591a\u6a21\u6001\u548c\u591a\u89c6\u56fe\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u56e0\u5b50\u5316\u4f20\u8f93\uff08\u6700\u4f18\u4f20\u8f93\u7684\u8f7b\u91cf\u7ea7\u8fd1\u4f3c\uff09\u8fdb\u884c\u8bad\u7ec3\uff0c\u5f3a\u8c03\u4e3b\u89c6\u56fe\u7684\u540c\u65f6\u968f\u673a\u91c7\u6837\u8f85\u52a9\u89c6\u56fe\uff0c\u5c06\u8bad\u7ec3\u6210\u672c\u4ece\u89c6\u56fe\u6570\u91cf\u7684\u4e8c\u6b21\u65b9\u964d\u4f4e\u4e3a\u5e38\u6570\u3002", "result": "\u5728\u5305\u542b100\u4e07\u5546\u54c1\u5217\u8868\u548c30\u4e07\u4ea4\u4e92\u7684\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u8de8\u89c6\u56fe\u548c\u67e5\u8be2\u5230\u5546\u54c1\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u76f8\u6bd4\u5f3a\u5927\u591a\u6a21\u6001\u57fa\u7ebfRecall@500\u63d0\u5347\u9ad8\u8fbe7.9%\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u56e0\u5b50\u5316\u4f20\u8f93\u5c06\u53ef\u6269\u5c55\u6027\u4e0e\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u4f7f\u591a\u89c6\u56fe\u9884\u8bad\u7ec3\u5728\u5927\u89c4\u6a21\u7535\u5546\u641c\u7d22\u4e2d\u53d8\u5f97\u5b9e\u7528\uff0c\u5728\u4fdd\u6301\u4e24\u5854\u68c0\u7d22\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2512.18283", "categories": ["cs.IR", "cs.DL"], "pdf": "https://arxiv.org/pdf/2512.18283", "abs": "https://arxiv.org/abs/2512.18283", "authors": ["Tianji Jiang", "Wenqi Li", "Jiqun Liu"], "title": "Improving Data Reusability in Interactive Information Retrieval: Insights from the Community", "comment": "Accepted by CHIIR 2025", "summary": "In this study, we conducted semi-structured interviews with 21 IIR researchers to investigate their data reuse practices. This study aims to expand upon current findings by exploring IIR researchers' information-obtaining behaviors regarding data reuse. We identified the information about shared data characteristics that IIR researchers need when evaluating data reusability, as well as the sources they typically consult to obtain this information. We consider this work to be an initial step toward revealing IIR researchers' data reuse practices and identifying what the community needs to do to promote data reuse. We hope that this study, as well as future research, will inspire more individuals to contribute to ongoing efforts aimed at designing standards, infrastructures, and policies, as well as fostering a sustainable culture of data sharing and reuse in this field.", "AI": {"tldr": "\u901a\u8fc7\u8bbf\u8c0821\u540dIIR\u7814\u7a76\u4eba\u5458\uff0c\u8c03\u67e5\u5176\u6570\u636e\u91cd\u7528\u5b9e\u8df5\uff0c\u63a2\u7d22\u6570\u636e\u91cd\u7528\u4e2d\u7684\u4fe1\u606f\u83b7\u53d6\u884c\u4e3a\uff0c\u8bc6\u522b\u8bc4\u4f30\u6570\u636e\u53ef\u91cd\u7528\u6027\u6240\u9700\u7684\u4fe1\u606f\u7279\u5f81\u53ca\u83b7\u53d6\u6765\u6e90\u3002", "motivation": "\u6269\u5c55\u5f53\u524d\u7814\u7a76\u53d1\u73b0\uff0c\u63a2\u7d22IIR\u7814\u7a76\u4eba\u5458\u5728\u6570\u636e\u91cd\u7528\u4e2d\u7684\u4fe1\u606f\u83b7\u53d6\u884c\u4e3a\uff0c\u63ed\u793a\u6570\u636e\u91cd\u7528\u5b9e\u8df5\uff0c\u4e3a\u4fc3\u8fdb\u6570\u636e\u91cd\u7528\u63d0\u4f9b\u57fa\u7840\u3002", "method": "\u91c7\u7528\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u65b9\u6cd5\uff0c\u5bf921\u540dIIR\u7814\u7a76\u4eba\u5458\u8fdb\u884c\u8bbf\u8c08\uff0c\u5206\u6790\u5176\u6570\u636e\u91cd\u7528\u5b9e\u8df5\u548c\u4fe1\u606f\u83b7\u53d6\u884c\u4e3a\u3002", "result": "\u8bc6\u522b\u4e86IIR\u7814\u7a76\u4eba\u5458\u8bc4\u4f30\u6570\u636e\u53ef\u91cd\u7528\u6027\u65f6\u6240\u9700\u7684\u6570\u636e\u7279\u5f81\u4fe1\u606f\uff0c\u4ee5\u53ca\u4ed6\u4eec\u901a\u5e38\u83b7\u53d6\u8fd9\u4e9b\u4fe1\u606f\u7684\u6765\u6e90\u3002", "conclusion": "\u8fd9\u662f\u63ed\u793aIIR\u7814\u7a76\u4eba\u5458\u6570\u636e\u91cd\u7528\u5b9e\u8df5\u7684\u7b2c\u4e00\u6b65\uff0c\u4e3a\u8bbe\u8ba1\u6807\u51c6\u3001\u57fa\u7840\u8bbe\u65bd\u548c\u653f\u7b56\uff0c\u57f9\u517b\u53ef\u6301\u7eed\u7684\u6570\u636e\u5171\u4eab\u4e0e\u91cd\u7528\u6587\u5316\u63d0\u4f9b\u57fa\u7840\u3002"}}
{"id": "2512.18384", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18384", "abs": "https://arxiv.org/abs/2512.18384", "authors": ["Boris Genin", "Alexander Gorbunov", "Dmitry Zolkin", "Igor Nekrasov"], "title": "Datasets for machine learning and for assessing the intelligence level of automatic patent search systems", "comment": "14 pages, 3 figures, 2 tables", "summary": "The key to success in automating prior art search in patent research using artificial intelligence lies in developing large datasets for machine learning and ensuring their availability. This work is dedicated to providing a comprehensive solution to the problem of creating infrastructure for research in this field, including datasets and tools for calculating search quality criteria. The paper discusses the concept of semantic clusters of patent documents that determine the state of the art in a given subject, as proposed by the authors. A definition of such semantic clusters is also provided. Prior art search is presented as the task of identifying elements within a semantic cluster of patent documents in the subject area specified by the document under consideration. A generator of user-configurable datasets for machine learning, based on collections of U.S. and Russian patent documents, is described. The dataset generator creates a database of links to documents in semantic clusters. Then, based on user-defined parameters, it forms a dataset of semantic clusters in JSON format for machine learning. To evaluate machine learning outcomes, it is proposed to calculate search quality scores that account for semantic clusters of the documents being searched. To automate the evaluation process, the paper describes a utility developed by the authors for assessing the quality of prior art document search.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4e13\u5229\u73b0\u6709\u6280\u672f\u641c\u7d22\u81ea\u52a8\u5316\u7684\u57fa\u7840\u8bbe\u65bd\u89e3\u51b3\u65b9\u6848\uff0c\u5305\u62ec\u8bed\u4e49\u805a\u7c7b\u5b9a\u4e49\u3001\u53ef\u914d\u7f6e\u6570\u636e\u96c6\u751f\u6210\u5668\u548c\u641c\u7d22\u8d28\u91cf\u8bc4\u4f30\u5de5\u5177\u3002", "motivation": "\u4e13\u5229\u7814\u7a76\u4e2d\u7684\u73b0\u6709\u6280\u672f\u641c\u7d22\u81ea\u52a8\u5316\u9700\u8981\u5927\u89c4\u6a21\u673a\u5668\u5b66\u4e60\u6570\u636e\u96c6\uff0c\u4f46\u7f3a\u4e4f\u76f8\u5173\u57fa\u7840\u8bbe\u65bd\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u521b\u5efa\u8be5\u9886\u57df\u7814\u7a76\u57fa\u7840\u8bbe\u65bd\u7684\u95ee\u9898\uff0c\u5305\u62ec\u6570\u636e\u96c6\u548c\u641c\u7d22\u8d28\u91cf\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u805a\u7c7b\u6982\u5ff5\u6765\u5b9a\u4e49\u6280\u672f\u9886\u57df\u72b6\u6001\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8e\u7f8e\u4fc4\u4e13\u5229\u6587\u6863\u7684\u53ef\u914d\u7f6e\u6570\u636e\u96c6\u751f\u6210\u5668\uff0c\u521b\u5efa\u8bed\u4e49\u805a\u7c7b\u94fe\u63a5\u6570\u636e\u5e93\uff0c\u5e76\u5f00\u53d1\u4e86\u8003\u8651\u8bed\u4e49\u805a\u7c7b\u7684\u641c\u7d22\u8d28\u91cf\u8bc4\u4f30\u5de5\u5177\u3002", "result": "\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u73b0\u6709\u6280\u672f\u641c\u7d22\u7814\u7a76\u57fa\u7840\u8bbe\u65bd\uff1a\u8bed\u4e49\u805a\u7c7b\u5b9a\u4e49\u3001\u53ef\u914d\u7f6eJSON\u683c\u5f0f\u6570\u636e\u96c6\u751f\u6210\u5668\u3001\u4ee5\u53ca\u81ea\u52a8\u5316\u7684\u641c\u7d22\u8d28\u91cf\u8bc4\u4f30\u5de5\u5177\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u4e13\u5229\u73b0\u6709\u6280\u672f\u641c\u7d22\u7684\u673a\u5668\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u4e86\u5fc5\u8981\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u5305\u62ec\u6570\u636e\u96c6\u751f\u6210\u548c\u8d28\u91cf\u8bc4\u4f30\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u81ea\u52a8\u5316\u53d1\u5c55\u3002"}}
{"id": "2512.18434", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.18434", "abs": "https://arxiv.org/abs/2512.18434", "authors": ["Federica Valeau", "Odysseas Boufalis", "Polytimi Gkotsi", "Joshua Rosenthal", "David Vos"], "title": "Efficient Optimization of Hierarchical Identifiers for Generative Recommendation", "comment": "Accepted at ECIR 2026 Reproducibility Track (to appear)", "summary": "SEATER is a generative retrieval model that improves recommendation inference efficiency and retrieval quality by utilizing balanced tree-structured item identifiers and contrastive training objectives. We reproduce and validate SEATER's reported improvements in retrieval quality over strong baselines across all datasets from the original work, and extend the evaluation to Yambda, a large-scale music recommendation dataset. Our experiments verify SEATER's strong performance, but show that its tree construction step during training becomes a major bottleneck as the number of items grows. To address this, we implement and evaluate two alternative construction algorithms: a greedy method optimized for minimal build time, and a hybrid method that combines greedy clustering at high levels with more precise grouping at lower levels. The greedy method reduces tree construction time to less than 2% of the original with only a minor drop in quality on the dataset with the largest item collection. The hybrid method achieves retrieval quality on par with the original, and even improves on the largest dataset, while cutting construction time to just 5-8%. All data and code are publicly available for full reproducibility at https://github.com/joshrosie/re-seater.", "AI": {"tldr": "SEATER\u662f\u4e00\u79cd\u751f\u6210\u5f0f\u68c0\u7d22\u6a21\u578b\uff0c\u901a\u8fc7\u5e73\u8861\u6811\u7ed3\u6784\u6807\u8bc6\u7b26\u548c\u5bf9\u6bd4\u8bad\u7ec3\u76ee\u6807\u63d0\u5347\u63a8\u8350\u6548\u7387\u548c\u8d28\u91cf\u3002\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u4f18\u52bf\uff0c\u4f46\u53d1\u73b0\u6811\u6784\u5efa\u6b65\u9aa4\u5728\u5927\u89c4\u6a21\u9879\u76ee\u96c6\u4e0a\u6210\u4e3a\u74f6\u9888\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86\u4e24\u79cd\u4f18\u5316\u6784\u5efa\u7b97\u6cd5\uff1a\u8d2a\u5fc3\u6cd5\u548c\u6df7\u5408\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6784\u5efa\u65f6\u95f4\u3002", "motivation": "SEATER\u6a21\u578b\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u6811\u6784\u5efa\u6b65\u9aa4\u968f\u7740\u9879\u76ee\u6570\u91cf\u589e\u52a0\u6210\u4e3a\u4e3b\u8981\u74f6\u9888\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6784\u5efa\u7b97\u6cd5\u6765\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "method": "1. \u590d\u73b0\u5e76\u9a8c\u8bc1SEATER\u5728\u539f\u59cb\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\uff1b2. \u6269\u5c55\u5230Yambda\u5927\u89c4\u6a21\u97f3\u4e50\u63a8\u8350\u6570\u636e\u96c6\uff1b3. \u5b9e\u73b0\u4e24\u79cd\u66ff\u4ee3\u6784\u5efa\u7b97\u6cd5\uff1a\u8d2a\u5fc3\u6cd5\uff08\u4f18\u5316\u6784\u5efa\u65f6\u95f4\uff09\u548c\u6df7\u5408\u6cd5\uff08\u7ed3\u5408\u9ad8\u5c42\u8d2a\u5fc3\u805a\u7c7b\u548c\u4f4e\u5c42\u7cbe\u786e\u5206\u7ec4\uff09\u3002", "result": "\u8d2a\u5fc3\u6cd5\u5c06\u6811\u6784\u5efa\u65f6\u95f4\u51cf\u5c11\u5230\u539f\u59cb\u76842%\u4ee5\u4e0b\uff0c\u5728\u6700\u5927\u9879\u76ee\u96c6\u4e0a\u4ec5\u6709\u8f7b\u5fae\u8d28\u91cf\u4e0b\u964d\uff1b\u6df7\u5408\u6cd5\u5728\u4fdd\u6301\u4e0e\u539f\u59cb\u65b9\u6cd5\u76f8\u5f53\u68c0\u7d22\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u5728\u6700\u5927\u6570\u636e\u96c6\u4e0a\u751a\u81f3\u6709\u6240\u63d0\u5347\uff0c\u6784\u5efa\u65f6\u95f4\u51cf\u5c11\u52305-8%\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e24\u79cd\u6811\u6784\u5efa\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86SEATER\u7684\u8bad\u7ec3\u6548\u7387\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u9879\u76ee\u96c6\u4e0a\u7684\u74f6\u9888\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u68c0\u7d22\u8d28\u91cf\uff0c\u4e3a\u751f\u6210\u5f0f\u68c0\u7d22\u6a21\u578b\u7684\u5b9e\u7528\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.18683", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.18683", "abs": "https://arxiv.org/abs/2512.18683", "authors": ["Sebastian Sun"], "title": "CIRR: Causal-Invariant Retrieval-Augmented Recommendation with Faithful Explanations under Distribution Shift", "comment": null, "summary": "Recent advances in retrieval-augmented generation (RAG) have shown promise in enhancing recommendation systems with external knowledge. However, existing RAG-based recommenders face two critical challenges: (1) vulnerability to distribution shifts across different environments (e.g., time periods, user segments), leading to performance degradation in out-of-distribution (OOD) scenarios, and (2) lack of faithful explanations that can be verified against retrieved evidence. In this paper, we propose CIRR, a Causal-Invariant Retrieval-Augmented Recommendation framework that addresses both challenges simultaneously. CIRR learns environment-invariant user preference representations through causal inference, which guide a debiased retrieval process to select relevant evidence from multiple sources. Furthermore, we introduce consistency constraints that enforce faithfulness between retrieved evidence, generated explanations, and recommendation outputs. Extensive experiments on two real-world datasets demonstrate that CIRR achieves robust performance under distribution shifts, reducing performance degradation from 15.4% (baseline) to only 5.6% in OOD scenarios, while providing more faithful and interpretable explanations (26% improvement in faithfulness score) compared to state-of-the-art baselines.", "AI": {"tldr": "CIRR\u662f\u4e00\u4e2a\u56e0\u679c\u4e0d\u53d8\u6027\u68c0\u7d22\u589e\u5f3a\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7\u56e0\u679c\u63a8\u65ad\u5b66\u4e60\u73af\u5883\u4e0d\u53d8\u7684\u7528\u6237\u504f\u597d\u8868\u793a\uff0c\u5e76\u5f15\u5165\u4e00\u81f4\u6027\u7ea6\u675f\u786e\u4fdd\u89e3\u91ca\u7684\u5fe0\u5b9e\u6027\uff0c\u5728\u5206\u5e03\u504f\u79fb\u573a\u666f\u4e0b\u5b9e\u73b0\u9c81\u68d2\u63a8\u8350\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eRAG\u7684\u63a8\u8350\u7cfb\u7edf\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a1) \u5728\u4e0d\u540c\u73af\u5883\uff08\u5982\u65f6\u95f4\u5468\u671f\u3001\u7528\u6237\u7fa4\u4f53\uff09\u7684\u5206\u5e03\u504f\u79fb\u4e0b\u8106\u5f31\uff0c\u5bfc\u81f4OOD\u573a\u666f\u6027\u80fd\u4e0b\u964d\uff1b2) \u7f3a\u4e4f\u53ef\u57fa\u4e8e\u68c0\u7d22\u8bc1\u636e\u9a8c\u8bc1\u7684\u5fe0\u5b9e\u89e3\u91ca\u3002", "method": "\u63d0\u51faCIRR\u6846\u67b6\uff1a1) \u901a\u8fc7\u56e0\u679c\u63a8\u65ad\u5b66\u4e60\u73af\u5883\u4e0d\u53d8\u7684\u7528\u6237\u504f\u597d\u8868\u793a\uff0c\u6307\u5bfc\u53bb\u504f\u7684\u68c0\u7d22\u8fc7\u7a0b\u4ece\u591a\u6e90\u9009\u62e9\u76f8\u5173\u8bc1\u636e\uff1b2) \u5f15\u5165\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u5f3a\u5236\u68c0\u7d22\u8bc1\u636e\u3001\u751f\u6210\u89e3\u91ca\u548c\u63a8\u8350\u8f93\u51fa\u4e4b\u95f4\u7684\u5fe0\u5b9e\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff1aCIRR\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u5b9e\u73b0\u9c81\u68d2\u6027\u80fd\uff0c\u5c06OOD\u573a\u666f\u7684\u6027\u80fd\u4e0b\u964d\u4ece\u57fa\u7ebf15.4%\u964d\u81f3\u4ec55.6%\uff0c\u540c\u65f6\u63d0\u4f9b\u66f4\u5fe0\u5b9e\u53ef\u89e3\u91ca\u7684\u89e3\u91ca\uff08\u5fe0\u5b9e\u5ea6\u5206\u6570\u63d0\u534726%\uff09\u3002", "conclusion": "CIRR\u901a\u8fc7\u56e0\u679c\u4e0d\u53d8\u6027\u5b66\u4e60\u548c\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u540c\u65f6\u89e3\u51b3\u4e86RAG\u63a8\u8350\u7cfb\u7edf\u7684\u5206\u5e03\u504f\u79fb\u8106\u5f31\u6027\u548c\u89e3\u91ca\u5fe0\u5b9e\u6027\u95ee\u9898\uff0c\u4e3a\u6784\u5efa\u9c81\u68d2\u53ef\u4fe1\u7684\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2512.18996", "categories": ["cs.IR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.18996", "abs": "https://arxiv.org/abs/2512.18996", "authors": ["Chong Liu", "Ming Zhang", "Fei Li", "Hao Zhou", "Xiaoshuang Chen", "Ye Yuan"], "title": "Modular Layout Synthesis (MLS): Front-end Code via Structure Normalization and Constrained Generation", "comment": null, "summary": "Automated front-end engineering drastically reduces development cycles and minimizes manual coding overhead. While Generative AI has shown promise in translating designs to code, current solutions often produce monolithic scripts, failing to natively support modern ecosystems like React, Vue, or Angular. Furthermore, the generated code frequently suffers from poor modularity, making it difficult to maintain. To bridge this gap, we introduce Modular Layout Synthesis (MLS), a hierarchical framework that merges visual understanding with structural normalization. Initially, a visual-semantic encoder maps the screen capture into a serialized tree topology, capturing the essential layout hierarchy. Instead of simple parsing, we apply heuristic deduplication and pattern recognition to isolate reusable blocks, creating a framework-agnostic schema. Finally, a constraint-based generation protocol guides the LLM to synthesize production-ready code with strict typing and component props. Evaluations show that MLS significantly outperforms existing baselines, ensuring superior code reusability and structural integrity across multiple frameworks", "AI": {"tldr": "MLS\u662f\u4e00\u4e2a\u5206\u5c42\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u4e49\u7f16\u7801\u3001\u542f\u53d1\u5f0f\u53bb\u91cd\u548c\u7ea6\u675f\u751f\u6210\uff0c\u5c06\u8bbe\u8ba1\u56fe\u8f6c\u6362\u4e3a\u6a21\u5757\u5316\u3001\u53ef\u7ef4\u62a4\u7684\u524d\u7aef\u4ee3\u7801\uff0c\u652f\u6301React\u3001Vue\u3001Angular\u7b49\u73b0\u4ee3\u6846\u67b6\u3002", "motivation": "\u5f53\u524d\u751f\u6210\u5f0fAI\u5c06\u8bbe\u8ba1\u8f6c\u6362\u4e3a\u4ee3\u7801\u7684\u65b9\u6848\u901a\u5e38\u4ea7\u751f\u5355\u4e00\u811a\u672c\uff0c\u4e0d\u652f\u6301React\u3001Vue\u3001Angular\u7b49\u73b0\u4ee3\u6846\u67b6\u751f\u6001\u7cfb\u7edf\uff0c\u4e14\u751f\u6210\u7684\u4ee3\u7801\u6a21\u5757\u5316\u5dee\u3001\u96be\u4ee5\u7ef4\u62a4\u3002", "method": "1. \u89c6\u89c9\u8bed\u4e49\u7f16\u7801\u5668\u5c06\u5c4f\u5e55\u622a\u56fe\u6620\u5c04\u4e3a\u5e8f\u5217\u5316\u6811\u62d3\u6251\u7ed3\u6784\uff1b2. \u5e94\u7528\u542f\u53d1\u5f0f\u53bb\u91cd\u548c\u6a21\u5f0f\u8bc6\u522b\u9694\u79bb\u53ef\u91cd\u7528\u5757\uff0c\u521b\u5efa\u6846\u67b6\u65e0\u5173\u6a21\u5f0f\uff1b3. \u57fa\u4e8e\u7ea6\u675f\u7684\u751f\u6210\u534f\u8bae\u6307\u5bfcLLM\u5408\u6210\u751f\u4ea7\u5c31\u7eea\u4ee3\u7801\u3002", "result": "MLS\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u786e\u4fdd\u8de8\u591a\u4e2a\u6846\u67b6\u7684\u4ee3\u7801\u53ef\u91cd\u7528\u6027\u548c\u7ed3\u6784\u5b8c\u6574\u6027\u3002", "conclusion": "MLS\u901a\u8fc7\u5206\u5c42\u6846\u67b6\u89e3\u51b3\u4e86\u5f53\u524d\u8bbe\u8ba1\u5230\u4ee3\u7801\u8f6c\u6362\u4e2d\u7684\u6a21\u5757\u5316\u548c\u6846\u67b6\u652f\u6301\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u524d\u7aef\u5de5\u7a0b\u7684\u81ea\u52a8\u5316\u6c34\u5e73\u3002"}}
{"id": "2512.19360", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.19360", "abs": "https://arxiv.org/abs/2512.19360", "authors": ["Markus Ekvall", "Ludvig Bergenstr\u00e5hle", "Patrick Truong", "Ben Murrell", "Joakim Lundeberg"], "title": "Generative vector search to improve pathology foundation models across multimodal vision-language tasks", "comment": "13 pages main (54 total), 2 main figures (9 total)", "summary": "Retrieval-augmented generation improves large language models by grounding outputs in external knowledge sources, reducing hallucinations and addressing knowledge cutoffs. However, standard embedding-based retrieval fails to capture the complexity of multi-concept queries, particularly in domains like biomedicine, where biological data are inherently high-dimensional. For example,omics datasets, and clinical reports simultaneously exhibit numerous molecular, cellular, and physiological features. We present Stochastic Latent Matching (STHLM), a generative vector search method that samples query-conditioned embeddings from text or image inputs to enhance retrieval performance. Analogous to how Chain-of-Thought reasoning enables language models to \"think longer\" on complex problems, STHLM allows retrieval systems to \"search wider\" through iterative sampling. STHLM demonstrates critical improvements over classical vector retrieval across diverse benchmarks, including scientific literature, clinical notes, and tissue images, boosting retrieval performance by 10-30% through test-time compute (trading latency for accuracy), while enabling up to a 10-fold compression of embedding dimensions.", "AI": {"tldr": "STHLM\u662f\u4e00\u79cd\u751f\u6210\u5f0f\u5411\u91cf\u641c\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u91c7\u6837\u67e5\u8be2\u6761\u4ef6\u5d4c\u5165\u6765\u63d0\u5347\u591a\u6982\u5ff5\u67e5\u8be2\u7684\u68c0\u7d22\u6027\u80fd\uff0c\u5728\u751f\u7269\u533b\u5b66\u7b49\u9886\u57df\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5411\u91cf\u68c0\u7d22\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5d4c\u5165\u7684\u68c0\u7d22\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u591a\u6982\u5ff5\u590d\u6742\u67e5\u8be2\uff0c\u7279\u522b\u662f\u5728\u751f\u7269\u533b\u5b66\u7b49\u9ad8\u7ef4\u6570\u636e\u9886\u57df\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u68c0\u7d22\u65b9\u6cd5\u6765\u6355\u6349\u6570\u636e\u590d\u6742\u6027\u3002", "method": "\u63d0\u51fa\u968f\u673a\u6f5c\u5728\u5339\u914d\uff08STHLM\uff09\u65b9\u6cd5\uff0c\u4ece\u6587\u672c\u6216\u56fe\u50cf\u8f93\u5165\u4e2d\u91c7\u6837\u67e5\u8be2\u6761\u4ef6\u5d4c\u5165\uff0c\u901a\u8fc7\u8fed\u4ee3\u91c7\u6837\u5b9e\u73b0\u66f4\u5e7f\u6cdb\u7684\u641c\u7d22\uff0c\u7c7b\u4f3c\u4e8e\u601d\u7ef4\u94fe\u63a8\u7406\u8ba9\u8bed\u8a00\u6a21\u578b\"\u601d\u8003\u66f4\u4e45\"\u3002", "result": "STHLM\u5728\u79d1\u5b66\u6587\u732e\u3001\u4e34\u5e8a\u7b14\u8bb0\u548c\u7ec4\u7ec7\u56fe\u50cf\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u4f20\u7edf\u5411\u91cf\u68c0\u7d22\u63d0\u534710-30%\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5b9e\u73b0\u9ad8\u8fbe10\u500d\u7684\u5d4c\u5165\u7ef4\u5ea6\u538b\u7f29\u3002", "conclusion": "STHLM\u901a\u8fc7\u6d4b\u8bd5\u65f6\u8ba1\u7b97\uff08\u4ee5\u5ef6\u8fdf\u6362\u53d6\u51c6\u786e\u6027\uff09\u663e\u8457\u63d0\u5347\u68c0\u7d22\u6027\u80fd\uff0c\u4e3a\u5904\u7406\u9ad8\u7ef4\u590d\u6742\u67e5\u8be2\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
