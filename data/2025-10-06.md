<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 5]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Revisiting Query Variants: The Advantage of Retrieval Over Generation of Query Variants for Effective QPP](https://arxiv.org/abs/2510.02512)
*Fangzheng Tian,Debasis Ganguly,Craig Macdonald*

Main category: cs.IR

TL;DR: 提出了一种通过从训练集中检索查询变体来改进查询性能预测的方法，使用两跳检索策略扩展查询变体，显著提升了预测效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于查询变体的QPP方法使用查询扩展或非上下文嵌入生成查询变体，可能导致主题漂移和幻觉问题，需要更准确的查询变体获取方式。

Method: 从训练集中检索目标查询的查询变体，通过直接检索（1跳）和使用相关文档的二次检索（2跳）来扩展查询变体集合，提高信息需求相似查询的召回率。

Result: 在TREC DL'19和DL'20上的实验表明，使用该方法检索的查询变体的QPP方法比现有最佳生成式查询变体方法提升了约20%的性能。

Conclusion: 从训练集中检索查询变体比生成查询变体更有效，两跳检索策略能显著提升查询性能预测的准确性。

Abstract: Leveraging query variants (QVs), i.e., queries with potentially similar
information needs to the target query, has been shown to improve the
effectiveness of query performance prediction (QPP) approaches. Existing
QV-based QPP methods generate QVs facilitated by either query expansion or
non-contextual embeddings, which may introduce topical drifts and
hallucinations. In this paper, we propose a method that retrieves QVs from a
training set (e.g., MS MARCO) for a given target query of QPP. To achieve a
high recall in retrieving queries with the most similar information needs as
the target query from a training set, we extend the directly retrieved QVs
(1-hop QVs) by a second retrieval using their denoted relevant documents (which
yields 2-hop QVs). Our experiments, conducted on TREC DL'19 and DL'20, show
that the QPP methods with QVs retrieved by our method outperform the
best-performing existing generated-QV-based QPP approaches by as much as around
20\%, on neural ranking models like MonoT5.

</details>


### [2] [A Simple but Effective Elaborative Query Reformulation Approach for Natural Language Recommendation](https://arxiv.org/abs/2510.02656)
*Qianfeng Wen,Yifan Liu,Justin Cui,Joshua Zhang,Anton Korikov,George-Kirollos Saad,Scott Sanner*

Main category: cs.IR

TL;DR: 提出EQR方法，结合广度和深度生成查询子主题的丰富阐述，显著提升自然语言推荐系统处理宽泛和间接查询的性能


<details>
  <summary>Details</summary>
Motivation: 现有密集检索方法难以处理表达宽泛或间接用户意图的挑战性查询，而现有查询重构方法要么只关注查询子主题的广度扩展，要么只关注查询含义的深度阐述，未能同时兼顾两者

Method: 提出EQR方法，基于大语言模型生成具有信息丰富阐述的潜在查询子主题，结合广度和深度进行查询重构

Result: 在旅行、酒店和餐厅三个新自然语言推荐基准上的实验表明，EQR在各种评估指标上显著优于最先进的查询重构方法

Conclusion: 简单而有效的EQR方法能显著改善自然语言推荐系统处理具有宽泛和间接用户意图的查询

Abstract: Natural Language (NL) recommender systems aim to retrieve relevant items from
free-form user queries and item descriptions. Existing systems often rely on
dense retrieval (DR), which struggles to interpret challenging queries that
express broad (e.g., "cities for youth friendly activities") or indirect (e.g.,
"cities for a high school graduation trip") user intents. While query
reformulation (QR) has been widely adopted to improve such systems, existing QR
methods tend to focus only on expanding the range of query subtopics (breadth)
or elaborating on the potential meaning of a query (depth), but not both. In
this paper, we propose EQR (Elaborative Subtopic Query Reformulation), a large
language model-based QR method that combines both breadth and depth by
generating potential query subtopics with information-rich elaborations. We
also introduce three new natural language recommendation benchmarks in travel,
hotel, and restaurant domains to establish evaluation of NL recommendation with
challenging queries. Experiments show EQR substantially outperforms
state-of-the-art QR methods in various evaluation metrics, highlighting that a
simple yet effective QR approach can significantly improve NL recommender
systems for queries with broad and indirect user intents.

</details>


### [3] [Less LLM, More Documents: Searching for Improved RAG](https://arxiv.org/abs/2510.02657)
*Jingjie Ning,Yibo Kong,Yunfan Long,Jamie Callan*

Main category: cs.IR

TL;DR: 通过扩大检索器的语料库可以减少对大型语言模型的依赖，语料库扩展可以替代模型规模增长，中小型生成器搭配大语料库能达到与大型模型相似的效果。


<details>
  <summary>Details</summary>
Motivation: 虽然扩展生成器规模能提高RAG准确性，但会增加成本并限制部署能力，因此探索通过扩大检索器语料库来减少对大型LLM的依赖。

Method: 通过实验研究语料库扩展对RAG性能的影响，分析不同规模生成器与不同大小语料库的搭配效果。

Result: 语料库扩展能持续增强RAG性能，中等规模模型从语料扩展中获益最大，改进主要来自答案相关段落覆盖率的增加。

Conclusion: 投资更大的语料库是增强RAG的有效途径，通常可与扩大LLM本身相媲美，建立了语料库-生成器的权衡原则。

Abstract: Retrieval-Augmented Generation (RAG) couples document retrieval with large
language models (LLMs). While scaling generators improves accuracy, it also
raises cost and limits deployability. We explore an orthogonal axis: enlarging
the retriever's corpus to reduce reliance on large LLMs. Experimental results
show that corpus scaling consistently strengthens RAG and can often serve as a
substitute for increasing model size, though with diminishing returns at larger
scales. Small- and mid-sized generators paired with larger corpora often rival
much larger models with smaller corpora; mid-sized models tend to gain the
most, while tiny and large models benefit less. Our analysis shows that
improvements arise primarily from increased coverage of answer-bearing
passages, while utilization efficiency remains largely unchanged. These
findings establish a principled corpus-generator trade-off: investing in larger
corpora offers an effective path to stronger RAG, often comparable to enlarging
the LLM itself.

</details>


### [4] [AgenticRAG: Tool-Augmented Foundation Models for Zero-Shot Explainable Recommender Systems](https://arxiv.org/abs/2510.02668)
*Bo Ma,Hang Li,ZeHua Hu,XiaoFan Gui,LuYao Liu,Simon Liu*

Main category: cs.IR

TL;DR: AgenticRAG是一个结合工具增强基础模型和检索增强生成的新框架，用于零样本可解释推荐，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 基础模型在人工智能领域取得了革命性进展，但在推荐系统中的应用受到推理不透明和知识限制的约束。

Method: 整合外部工具调用、知识检索和思维链推理，创建无需任务特定训练即可进行透明决策的自主推荐代理。

Result: 在三个真实数据集上的实验结果显示，AgenticRAG在Amazon Electronics、MovieLens-1M和Yelp数据集上的NDCG@10分别提升了0.4%、0.8%和1.6%。

Conclusion: 该框架展现出优越的可解释性，同时保持了与传统方法相当的计算效率。

Abstract: Foundation models have revolutionized artificial intelligence, yet their
application in recommender systems remains limited by reasoning opacity and
knowledge constraints. This paper introduces AgenticRAG, a novel framework that
combines tool-augmented foundation models with retrieval-augmented generation
for zero-shot explainable recommendations. Our approach integrates external
tool invocation, knowledge retrieval, and chain-of-thought reasoning to create
autonomous recommendation agents capable of transparent decision-making without
task-specific training. Experimental results on three real-world datasets
demonstrate that AgenticRAG achieves consistent improvements over
state-of-the-art baselines, with NDCG@10 improvements of 0.4\% on Amazon
Electronics, 0.8\% on MovieLens-1M, and 1.6\% on Yelp datasets. The framework
exhibits superior explainability while maintaining computational efficiency
comparable to traditional methods.

</details>


### [5] [OpenZL: A Graph-Based Model for Compression](https://arxiv.org/abs/2510.03203)
*Yann Collet,Nick Terrell,W. Felix Handte,Danielle Rozenblit,Victor Zhang,Kevin Zhang,Yaelle Goldschlag,Jennifer Lee,Daniel Riegel,Stan Angelov,Nadav Rotem*

Main category: cs.IR

TL;DR: OpenZL提出了一种新的压缩模型——图模型，将压缩表示为模块化编解码器的有向无环图，实现了高性能、易部署的压缩方案。


<details>
  <summary>Details</summary>
Motivation: 通用压缩器在压缩比和性能之间存在权衡，而应用特定压缩器虽然性能优越但部署维护困难。需要一种既能提供高性能压缩，又易于部署和维护的新方法。

Method: 提出了压缩的图模型理论框架，将压缩表示为模块化编解码器的有向无环图。开发了OpenZL实现，使用自描述线格式，任何配置都可以通过通用解码器解压。

Result: 实验结果显示OpenZL在各种真实数据集上实现了优于最先进通用压缩器的压缩比和速度。在Meta内部部署中，开发时间从数月缩短到数天，并在大小和/或速度方面持续改进。

Conclusion: OpenZL代表了现代数据密集型应用中实用、可扩展和可维护数据压缩的进步，通过模块化设计和通用解码器解决了应用特定压缩器的部署和维护难题。

Abstract: Research in general-purpose lossless compression over the last decade has
largely found improvements in compression ratio that come at great cost to
resource utilization and processing throughput. However, most production
workloads require high throughput and low resource utilization, so most
research systems have seen little adoption. Instead, real world improvements in
compression are increasingly often realized by building application-specific
compressors which can exploit knowledge about the structure and semantics of
the data being compressed. These systems easily outperform even the best
generic compressors, but application-specific compression schemes are not
without drawbacks. They are inherently limited in applicability and are
difficult to maintain and deploy.
  We show that these challenges can be overcome with a new way of thinking
about compression. We propose the ``graph model'' of compression, a new
theoretical framework for representing compression as a directed acyclic graph
of modular codecs. This motivates OpenZL, an implementation of this model that
compresses data into a self-describing wire format, any configuration of which
can be decompressed by a universal decoder. OpenZL's design enables rapid
development of tailored compressors with minimal code, its universal decoder
eliminates deployment lag, and its investment in a well-vetted standard
component library minimizes security risks. Experimental results demonstrate
that OpenZL achieves superior compression ratios and speeds compared to
state-of-the-art general-purpose compressors on a variety of real-world
datasets. Internal deployments at Meta have also shown consistent improvements
in size and/or speed, with development timelines reduced from months to days.
OpenZL thus represents an advance in practical, scalable, and maintainable data
compression for modern data-intensive applications.

</details>
