<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 17]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [LiveNewsBench: Evaluating LLM Web Search Capabilities with Freshly Curated News](https://arxiv.org/abs/2602.13543)
*Yunfan Zhang,Kathleen McKeown,Smaranda Muresan*

Main category: cs.IR

TL;DR: LiveNewsBench是一个用于评估LLM代理式网络搜索能力的基准测试，通过从最新新闻自动生成问题-答案对，确保问题需要超出模型训练数据的信息，并支持构建大规模训练数据集。


<details>
  <summary>Details</summary>
Motivation: 当前评估具有网络搜索能力的LLM代理系统面临挑战，缺乏能够清晰区分模型内部知识和搜索能力的基准测试，且相关训练数据稀缺。

Method: 构建自动化的数据收集和问题生成流程，从最新新闻文章中生成需要多跳搜索、页面访问和推理的困难问题，确保问题需要超出模型训练数据的信息，并在测试集中包含人工验证样本。

Result: 创建了LiveNewsBench基准测试，评估了包括商业和开源LLM以及基于LLM的网络搜索API在内的广泛系统，提供了公开的排行榜、数据集和代码。

Conclusion: LiveNewsBench为评估LLM代理式网络搜索能力提供了严谨且定期更新的基准测试，解决了该领域的数据稀缺问题，并支持构建大规模训练数据集。

Abstract: Large Language Models (LLMs) with agentic web search capabilities show strong potential for tasks requiring real-time information access and complex fact retrieval, yet evaluating such systems remains challenging. We introduce \bench, a rigorous and regularly updated benchmark designed to assess the agentic web search abilities of LLMs. \bench automatically generates fresh question-answer pairs from recent news articles, ensuring that questions require information beyond an LLM's training data and enabling clear separation between internal knowledge and search capability. The benchmark features intentionally difficult questions requiring multi-hop search queries, page visits, and reasoning, making it well-suited for evaluating agentic search behavior. Our automated data curation and question generation pipeline enables frequent benchmark updates and supports construction of a large-scale training dataset for agentic web search models, addressing the scarcity of such data in the research community. To ensure reliable evaluation, we include a subset of human-verified samples in the test set. We evaluate a broad range of systems using \bench, including commercial and open-weight LLMs as well as LLM-based web search APIs. The leaderboard, datasets, and code are publicly available at livenewsbench.com.

</details>


### [2] [Unleash the Potential of Long Semantic IDs for Generative Recommendation](https://arxiv.org/abs/2602.13573)
*Ming Xia,Zhiqin Zhou,Guoxin Ma,Dongmin Huang*

Main category: cs.IR

TL;DR: ACERec提出了一种新的语义ID生成推荐框架，通过解耦细粒度标记化和高效序列建模之间的粒度差距，解决了语义表达性与计算效率的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有的语义ID生成推荐方法面临表达性与效率的权衡：基于残差量化(RQ)的方法限制语义ID长度以保证序列建模效率，而基于优化乘积量化(OPQ)的方法通过简单聚合压缩长语义ID，但会丢失细粒度语义信息。需要一种方法既能保持语义表达的丰富性，又能实现高效计算。

Method: ACERec框架包含三个核心组件：1) 注意力标记合并器(Attentive Token Merger)，将长的表达性语义标记提炼为紧凑的潜在表示；2) 专门的意图标记(Intent Token)，作为动态预测锚点；3) 双粒度学习目标，协调细粒度标记预测与全局项目级语义对齐，以捕捉连贯的用户意图。

Result: 在六个真实世界基准测试上的广泛实验表明，ACERec始终优于最先进的基线方法，在NDCG@10指标上平均提升了14.40%，有效调和了语义表达性与计算效率。

Conclusion: ACERec成功解决了语义ID生成推荐中长期存在的表达性与效率权衡问题，通过解耦粒度差距、引入注意力标记合并和意图标记机制，实现了既保持丰富语义信息又具备高效计算性能的推荐系统。

Abstract: Semantic ID-based generative recommendation represents items as sequences of discrete tokens, but it inherently faces a trade-off between representational expressiveness and computational efficiency. Residual Quantization (RQ)-based approaches restrict semantic IDs to be short to enable tractable sequential modeling, while Optimized Product Quantization (OPQ)-based methods compress long semantic IDs through naive rigid aggregation, inevitably discarding fine-grained semantic information. To resolve this dilemma, we propose ACERec, a novel framework that decouples the granularity gap between fine-grained tokenization and efficient sequential modeling. It employs an Attentive Token Merger to distill long expressive semantic tokens into compact latents and introduces a dedicated Intent Token serving as a dynamic prediction anchor. To capture cohesive user intents, we guide the learning process via a dual-granularity objective, harmonizing fine-grained token prediction with global item-level semantic alignment. Extensive experiments on six real-world benchmarks demonstrate that ACERec consistently outperforms state-of-the-art baselines, achieving an average improvement of 14.40\% in NDCG@10, effectively reconciling semantic expressiveness and computational efficiency.

</details>


### [3] [Climber-Pilot: A Non-Myopic Generative Recommendation Model Towards Better Instruction-Following](https://arxiv.org/abs/2602.13581)
*Da Guo,Shijia Wang,Qiang Xiao,Yintao Ren,Weisheng Li,Songpei Xu,Ming Yue,Bin Huang,Guanlin Wu,Chuanjiang Luo*

Main category: cs.IR

TL;DR: Climber-Pilot是一个统一的生成式检索框架，通过时间感知多项目预测缓解生成式检索的短视问题，并通过条件引导稀疏注意力实现指令跟随检索，在工业推荐系统中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 生成式检索在推荐系统中展现出优于传统双塔架构的序列建模能力，但在大规模工业场景中存在两个关键问题：1）由于单步推理和严格延迟约束，模型容易陷入局部最优预测，无法捕捉长期和多项目消费模式；2）实际检索系统需要遵循明确的检索指令（如类别控制、策略约束），现有条件或后处理过滤方法往往损害相关性或效率。

Method: 提出Climber-Pilot统一框架：1）时间感知多项目预测（TAMIP）：通过时间感知掩码将长期多项目预见性蒸馏到模型参数中，缓解局部最优预测同时保持高效单步推理；2）条件引导稀疏注意力（CGSA）：通过稀疏注意力将业务约束直接融入生成过程，无需额外推理步骤。

Result: 在网易云音乐（最大的音乐流媒体平台之一）进行的离线实验和在线A/B测试表明，Climber-Pilot显著优于最先进的基线方法，核心业务指标提升4.24%。

Conclusion: Climber-Pilot成功解决了生成式检索在工业场景中的短视问题和指令跟随挑战，通过统一的训练和推理框架实现了性能显著提升，为大规模推荐系统的生成式检索提供了有效解决方案。

Abstract: Generative retrieval has emerged as a promising paradigm in recommender systems, offering superior sequence modeling capabilities over traditional dual-tower architectures. However, in large-scale industrial scenarios, such models often suffer from inherent myopia: due to single-step inference and strict latency constraints, they tend to collapse diverse user intents into locally optimal predictions, failing to capture long-horizon and multi-item consumption patterns. Moreover, real-world retrieval systems must follow explicit retrieval instructions, such as category-level control and policy constraints. Incorporating such instruction-following behavior into generative retrieval remains challenging, as existing conditioning or post-hoc filtering approaches often compromise relevance or efficiency. In this work, we present Climber-Pilot, a unified generative retrieval framework to address both limitations. First, we introduce Time-Aware Multi-Item Prediction (TAMIP), a novel training paradigm designed to mitigate inherent myopia in generative retrieval. By distilling long-horizon, multi-item foresight into model parameters through time-aware masking, TAMIP alleviates locally optimal predictions while preserving efficient single-step inference. Second, to support flexible instruction-following retrieval, we propose Condition-Guided Sparse Attention (CGSA), which incorporates business constraints directly into the generative process via sparse attention, without introducing additional inference steps. Extensive offline experiments and online A/B testing at NetEase Cloud Music, one of the largest music streaming platforms, demonstrate that Climber-Pilot significantly outperforms state-of-the-art baselines, achieving a 4.24\% lift of the core business metric.

</details>


### [4] [GEMs: Breaking the Long-Sequence Barrier in Generative Recommendation with a Multi-Stream Decoder](https://arxiv.org/abs/2602.13631)
*Yu Zhou,Chengcheng Guo,Kuo Cai,Ji Liu,Qiang Luo,Ruiming Tang,Han Li,Kun Gai,Guorui Zhou*

Main category: cs.IR

TL;DR: GEMs提出多流解码器框架解决生成式推荐中长序列处理难题，通过划分近期、中期、生命周期三个时间流，采用定制化推理方案，实现用户终身兴趣建模，在工业环境中成功处理超过10万次交互序列。


<details>
  <summary>Details</summary>
Motivation: 生成式推荐在处理极长用户行为序列时面临两大挑战：1）高计算成本迫使实际序列长度受限，无法捕捉用户终身兴趣；2）注意力机制的"近期偏好"偏差削弱了长期历史学习。需要突破长序列处理瓶颈。

Method: 提出GEMs框架，将用户行为划分为三个时间流：近期、中期、生命周期。为每个流设计定制化推理方案：近期流使用单阶段实时提取器；中期流采用轻量级索引器进行跨注意力平衡精度与成本；生命周期流使用两阶段离线-在线压缩模块。通过参数无关的融合策略整合多流表示。

Result: 在大规模工业数据集上的实验表明，GEMs在推荐准确性上显著优于最先进方法。GEMs是首个成功部署在高并发工业环境中的终身生成式推荐框架，在处理超过10万次用户交互序列的同时实现了优越的推理效率。

Conclusion: GEMs通过多流解码器框架有效解决了生成式推荐中的长序列处理瓶颈，实现了对用户终身兴趣的全面建模，在保持高推理效率的同时显著提升了推荐性能，具有重要的工业应用价值。

Abstract: While generative recommendations (GR) possess strong sequential reasoning capabilities, they face significant challenges when processing extremely long user behavior sequences: the high computational cost forces practical sequence lengths to be limited, preventing models from capturing users' lifelong interests; meanwhile, the inherent "recency bias" of attention mechanisms further weakens learning from long-term history. To overcome this bottleneck, we propose GEMs (Generative rEcommendation with a Multi-stream decoder), a novel and unified framework designed to break the long-sequence barrier by capturing users' lifelong interaction sequences through a multi-stream perspective. Specifically, GEMs partitions user behaviors into three temporal streams$\unicode{x2014}$Recent, Mid-term, and Lifecycle$\unicode{x2014}$and employs tailored inference schemes for each: a one-stage real-time extractor for immediate dynamics, a lightweight indexer for cross attention to balance accuracy and cost for mid-term sequences, and a two-stage offline-online compression module for lifelong modeling. These streams are integrated via a parameter-free fusion strategy to enable holistic interest representation. Extensive experiments on large-scale industrial datasets demonstrate that GEMs significantly outperforms state-of-the-art methods in recommendation accuracy. Notably, GEMs is the first lifelong GR framework successfully deployed in a high-concurrency industrial environment, achieving superior inference efficiency while processing user sequences of over 100,000 interactions.

</details>


### [5] [PT-RAG: Structure-Fidelity Retrieval-Augmented Generation for Academic Papers](https://arxiv.org/abs/2602.13647)
*Rui Yu,Tianyi Wang,Ruixia Liu,Yinglong Wang*

Main category: cs.IR

TL;DR: PT-RAG：基于论文原生层次结构的RAG框架，通过构建结构保真索引和路径引导检索，在固定token预算下提供紧凑、连贯的低熵检索上下文，提升学术问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法将学术论文扁平化为非结构化块，破坏了原生层次结构，导致检索空间混乱、产生碎片化上下文、在有限token预算下将token分配给非证据区域，增加下游语言模型的推理负担。

Method: 1. 继承论文原生层次结构构建结构保真的PaperTree索引；2. 设计路径引导检索机制，将查询语义对齐到相关章节，在固定token预算下选择高相关性的根到叶路径；3. 引入基于熵的结构诊断指标量化检索碎片化和证据分配准确性。

Result: 在三个学术问答基准测试中，PT-RAG相比强基线实现了更低的章节熵和证据对齐交叉熵，表明减少了上下文碎片化并更精确地将token分配给证据区域，这些结构优势直接转化为更高的答案质量。

Conclusion: PT-RAG通过将论文原生层次结构作为低熵检索先验，避免了破坏性预处理导致的熵增加，为后续检索提供了原生低熵结构基础，在固定token预算下能提供更紧凑、连贯的检索上下文，显著提升学术问答性能。

Abstract: Retrieval-augmented generation (RAG) is increasingly applied to question-answering over long academic papers, where accurate evidence allocation under a fixed token budget is critical. Existing approaches typically flatten academic papers into unstructured chunks during preprocessing, which destroys the native hierarchical structure. This loss forces retrieval to operate in a disordered space, thereby producing fragmented contexts, misallocating tokens to non-evidential regions under finite token budgets, and increasing the reasoning burden for downstream language models. To address these issues, we propose PT-RAG, an RAG framework that treats the native hierarchical structure of academic papers as a low-entropy retrieval prior. PT-RAG first inherits the native hierarchy to construct a structure-fidelity PaperTree index, which prevents entropy increase at the source. It then designs a path-guided retrieval mechanism that aligns query semantics to relevant sections and selects high relevance root-to-leaf paths under a fixed token budget, yielding compact, coherent, and low-entropy retrieval contexts. In contrast to existing RAG approaches, PT-RAG avoids entropy increase caused by destructive preprocessing and provides a native low-entropy structural basis for subsequent retrieval. To assess this design, we introduce entropy-based structural diagnostics that quantify retrieval fragmentation and evidence allocation accuracy. On three academic question-answering benchmarks, PT-RAG achieves consistently lower section entropy and evidence alignment cross entropy than strong baselines, indicating reduced context fragmentation and more precise allocation to evidential regions. These structural advantages directly translate into higher answer quality.

</details>


### [6] [Pailitao-VL: Unified Embedding and Reranker for Real-Time Multi-Modal Industrial Search](https://arxiv.org/abs/2602.13704)
*Lei Chen,Chen Ju,Xu Chen,Zhicheng Wang,Yuheng Jiao,Hongfeng Zhan,Zhaoyang Li,Shihao Xu,Zhixiang Zhao,Tong Jia,Jinsong Lan,Xiaoyong Zhu,Bo Zheng*

Main category: cs.IR

TL;DR: Pailitao-VL是一个多模态检索系统，通过从对比学习转向绝对ID识别任务，以及从点式评估转向列表式比较校准策略，解决了现有方案在检索粒度、环境噪声和效率性能差距方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决当前最先进多模态检索系统中的三个关键挑战：检索粒度不足、对环境噪声的脆弱性、以及效率与性能之间的巨大差距。

Method: 1. 将嵌入范式从传统对比学习转变为绝对ID识别任务，通过将实例锚定到由数十亿语义原型定义的全局一致潜在空间。2. 将生成式重排序从孤立点式评估演变为比较校准列表式策略，结合基于分块的比较推理和校准的绝对相关性评分。

Result: 在阿里巴巴电商平台的离线基准测试和在线A/B测试中，Pailitao-VL实现了最先进的性能，并带来了显著的商业影响。

Conclusion: 这项工作展示了一条在要求苛刻的大规模生产环境中部署基于MLLM的高级检索架构的稳健且可扩展的路径。

Abstract: In this work, we presented Pailitao-VL, a comprehensive multi-modal retrieval system engineered for high-precision, real-time industrial search. We here address three critical challenges in the current SOTA solution: insufficient retrieval granularity, vulnerability to environmental noise, and prohibitive efficiency-performance gap. Our primary contribution lies in two fundamental paradigm shifts. First, we transitioned the embedding paradigm from traditional contrastive learning to an absolute ID-recognition task. Through anchoring instances to a globally consistent latent space defined by billions of semantic prototypes, we successfully overcome the stochasticity and granularity bottlenecks inherent in existing embedding solutions. Second, we evolved the generative reranker from isolated pointwise evaluation to the compare-and-calibrate listwise policy. By synergizing chunk-based comparative reasoning with calibrated absolute relevance scoring, the system achieves nuanced discriminative resolution while circumventing the prohibitive latency typically associated with conventional reranking methods. Extensive offline benchmarks and online A/B tests on Alibaba e-commerce platform confirm that Pailitao-VL achieves state-of-the-art performance and delivers substantial business impact. This work demonstrates a robust and scalable path for deploying advanced MLLM-based retrieval architectures in demanding, large-scale production environments.

</details>


### [7] [DMESR: Dual-view MLLM-based Enhancing Framework for Multimodal Sequential Recommendation](https://arxiv.org/abs/2602.13715)
*Mingyao Huang,Qidong Liu,Wenxuan Yang,Moranxin Wang,Yuqi Sun,Haiping Zhu,Feng Tian,Yan Chen*

Main category: cs.IR

TL;DR: 提出DMESR框架，通过双视角MLLM增强多模态序列推荐，解决模态对齐不足和细粒度语义丢失问题


<details>
  <summary>Details</summary>
Motivation: 现有MLLM增强的推荐方法存在两个关键局限：1) 难以有效对齐多模态表示，导致跨模态语义信息利用不足；2) 过度依赖MLLM生成内容，忽略了物品原始文本数据中的细粒度语义线索

Method: 提出DMESR框架：1) 使用对比学习机制对齐MLLM生成的跨模态语义表示；2) 引入交叉注意力融合模块，将MLLM获得的粗粒度语义知识与细粒度原始文本语义相结合；3) 将融合后的表示无缝集成到下游序列推荐模型中

Result: 在三个真实世界数据集和三种流行序列推荐架构上的广泛实验表明，所提方法具有优越的有效性和泛化能力

Conclusion: DMESR框架成功解决了MLLM增强推荐中的模态对齐和细粒度语义保留问题，为多模态序列推荐提供了有效的增强方案

Abstract: Sequential Recommender Systems (SRS) aim to predict users' next interaction based on their historical behaviors, while still facing the challenge of data sparsity. With the rapid advancement of Multimodal Large Language Models (MLLMs), leveraging their multimodal understanding capabilities to enrich item semantic representation has emerged as an effective enhancement strategy for SRS. However, existing MLLM-enhanced recommendation methods still suffer from two key limitations. First, they struggle to effectively align multimodal representations, leading to suboptimal utilization of semantic information across modalities. Second, they often overly rely on MLLM-generated content while overlooking the fine-grained semantic cues contained in the original textual data of items. To address these issues, we propose a Dual-view MLLM-based Enhancing framework for multimodal Sequential Recommendation (DMESR). For the misalignment issue, we employ a contrastive learning mechanism to align the cross-modal semantic representations generated by MLLMs. For the loss of fine-grained semantics, we introduce a cross-attention fusion module that integrates the coarse-grained semantic knowledge obtained from MLLMs with the fine-grained original textual semantics. Finally, these two fused representations can be seamlessly integrated into the downstream sequential recommendation models. Extensive experiments conducted on three real-world datasets and three popular sequential recommendation architectures demonstrate the superior effectiveness and generalizability of our proposed approach.

</details>


### [8] [A Tale of Two Graphs: Separating Knowledge Exploration from Outline Structure for Open-Ended Deep Research](https://arxiv.org/abs/2602.13830)
*Zhuofan Shi,Ming Ma,Zekun Yao,Fangkai Yang,Jue Zhang,Dongge Han,Victor Rühle,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.IR

TL;DR: DualGraph记忆架构通过分离知识存储与写作结构，使用知识图谱和提纲图谱协同演化，解决了开放深度研究中知识积累与规划的问题，显著提升了报告质量和事实准确性。


<details>
  <summary>Details</summary>
Motivation: 现有开放深度研究(OEDR)代理主要采用线性"搜索-生成"积累或提纲中心规划方法。前者在证据增多时会出现"迷失中间"问题，后者仅依赖提纲隐式推断知识缺口，对识别缺失关系和触发定向探索的监督较弱。

Method: 提出DualGraph记忆架构，分离代理的知识与写作过程。维护两个协同演化的图谱：提纲图谱(OG)和知识图谱(KG)。KG存储细粒度知识单元（核心实体、概念及其关系），通过分析KG拓扑结构和OG的结构信号，生成定向搜索查询，实现高效、全面的迭代知识驱动探索和精炼。

Result: 在DeepResearch Bench、DeepResearchGym和DeepConsult三个基准测试中，DualGraph在报告深度、广度和事实基础方面均优于最先进的基线方法。例如，在DeepResearch Bench上使用GPT-5达到了53.08的RACE分数。消融研究证实了双图设计的核心作用。

Conclusion: DualGraph通过分离知识表示与写作结构，提供更有效的知识缺口识别和定向探索机制，显著提升了开放深度研究代理的性能，为长时程研究工作流程提供了更强大的架构支持。

Abstract: Open-Ended Deep Research (OEDR) pushes LLM agents beyond short-form QA toward long-horizon workflows that iteratively search, connect, and synthesize evidence into structured reports. However, existing OEDR agents largely follow either linear ``search-then-generate'' accumulation or outline-centric planning. The former suffers from lost-in-the-middle failures as evidence grows, while the latter relies on the LLM to implicitly infer knowledge gaps from the outline alone, providing weak supervision for identifying missing relations and triggering targeted exploration. We present DualGraph memory, an architecture that separates what the agent knows from how it writes. DualGraph maintains two co-evolving graphs: an Outline Graph (OG), and a Knowledge Graph (KG), a semantic memory that stores fine-grained knowledge units, including core entities, concepts, and their relations. By analyzing the KG topology together with structural signals from the OG, DualGraph generates targeted search queries, enabling more efficient and comprehensive iterative knowledge-driven exploration and refinement. Across DeepResearch Bench, DeepResearchGym, and DeepConsult, DualGraph consistently outperforms state-of-the-art baselines in report depth, breadth, and factual grounding; for example, it reaches a 53.08 RACE score on DeepResearch Bench with GPT-5. Moreover, ablation studies confirm the central role of the dual-graph design.

</details>


### [9] [DAIAN: Deep Adaptive Intent-Aware Network for CTR Prediction in Trigger-Induced Recommendation](https://arxiv.org/abs/2602.13971)
*Zhihao Lv,Longtao Zhang,Ailong He,Shuzhi Cao,Shuguang Han,Jufeng Chen*

Main category: cs.IR

TL;DR: DAIAN是一个深度自适应意图感知网络，通过分析用户点击与触发物品的关联来提取个性化意图表示，解决推荐系统中的意图短视问题，并在公开和工业数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 现有触发诱导推荐方法存在意图短视问题，过度强调触发物品的作用，只推荐与触发物品高度相关的商品。同时，基于ID的交互稀疏性限制了捕捉用户偏好的效果。

Method: 提出深度自适应意图感知网络(DAIAN)：1)通过分析用户点击与触发物品的关联提取个性化意图表示；2)检索相关历史行为挖掘用户多样化意图；3)使用ID和语义信息的混合增强器强化相似性；4)基于不同意图进行自适应选择。

Result: 在公开数据集和工业电商数据集上的实验结果表明DAIAN的有效性，能够更好地解决意图短视问题并提升推荐性能。

Conclusion: DAIAN通过动态适应用户意图偏好，有效解决了触发诱导推荐中的意图短视问题，并通过混合增强器缓解了交互稀疏性限制，为电商推荐系统提供了更精准的实时推荐方案。

Abstract: Recommendation systems are essential for personalizing e-commerce shopping experiences. Among these, Trigger-Induced Recommendation (TIR) has emerged as a key scenario, which utilizes a trigger item (explicitly represents a user's instantaneous interest), enabling precise, real-time recommendations. Although several trigger-based techniques have been proposed, most of them struggle to address the intent myopia issue, that is, a recommendation system overemphasizes the role of trigger items and narrowly focuses on suggesting commodities that are highly relevant to trigger items. Meanwhile, existing methods rely on collaborative behavior patterns between trigger and recommended items to identify the user's preferences, yet the sparsity of ID-based interaction restricts their effectiveness. To this end, we propose the Deep Adaptive Intent-Aware Network (DAIAN) that dynamically adapts to users' intent preferences. In general, we first extract the users' personalized intent representations by analyzing the correlation between a user's click and the trigger item, and accordingly retrieve the user's related historical behaviors to mine the user's diverse intent. Besides, sparse collaborative behaviors constrain the performance in capturing items associated with user intent. Hence, we reinforce similarity by leveraging a hybrid enhancer with ID and semantic information, followed by adaptive selection based on varying intents. Experimental results on public datasets and our industrial e-commerce datasets demonstrate the effectiveness of DAIAN.

</details>


### [10] [MixFormer: Co-Scaling Up Dense and Sequence in Industrial Recommenders](https://arxiv.org/abs/2602.14110)
*Xu Huang,Hao Zhang,Zhifang Fan,Yunwen Huang,Zhuoxing Wei,Zheng Chai,Jinan Ni,Yuchao Zheng,Qiwei Chen*

Main category: cs.IR

TL;DR: MixFormer：为推荐系统设计的统一Transformer架构，将序列建模和特征交互集成到单一主干中，解决现有分离设计的协同扩展问题，并在工业推荐系统中展现优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的推荐模型存在结构碎片化问题，序列建模和特征交互作为独立模块实现，导致在有限计算预算下，模型容量必须在密集特征交互和序列建模之间进行次优分配，存在协同扩展挑战。

Method: 提出MixFormer统一Transformer架构，在单一主干中联合建模序列行为和特征交互；采用统一参数化实现密集容量和序列长度的有效协同扩展；引入用户-物品解耦策略进行效率优化，减少冗余计算和推理延迟。

Result: 在大规模工业数据集上，MixFormer持续展现出优越的准确性和效率；在抖音和抖音轻量版两个生产推荐系统的大规模在线A/B测试中，用户参与度指标（包括活跃天数和应用内使用时长）均获得一致提升。

Conclusion: MixFormer通过统一架构解决了推荐系统中序列建模和特征交互的协同扩展问题，实现了更好的表达能力与工业实用性平衡，为大规模推荐系统的Transformer架构设计提供了有效解决方案。

Abstract: As industrial recommender systems enter a scaling-driven regime, Transformer architectures have become increasingly attractive for scaling models towards larger capacity and longer sequence. However, existing Transformer-based recommendation models remain structurally fragmented, where sequence modeling and feature interaction are implemented as separate modules with independent parameterization. Such designs introduce a fundamental co-scaling challenge, as model capacity must be suboptimally allocated between dense feature interaction and sequence modeling under a limited computational budget. In this work, we propose MixFormer, a unified Transformer-style architecture tailored for recommender systems, which jointly models sequential behaviors and feature interactions within a single backbone. Through a unified parameterization, MixFormer enables effective co-scaling across both dense capacity and sequence length, mitigating the trade-off observed in decoupled designs. Moreover, the integrated architecture facilitates deep interaction between sequential and non-sequential representations, allowing high-order feature semantics to directly inform sequence aggregation and enhancing overall expressiveness. To ensure industrial practicality, we further introduce a user-item decoupling strategy for efficiency optimizations that significantly reduce redundant computation and inference latency. Extensive experiments on large-scale industrial datasets demonstrate that MixFormer consistently exhibits superior accuracy and efficiency. Furthermore, large-scale online A/B tests on two production recommender systems, Douyin and Douyin Lite, show consistent improvements in user engagement metrics, including active days and in-app usage duration.

</details>


### [11] [High Precision Audience Expansion via Extreme Classification in a Two-Sided Marketplace](https://arxiv.org/abs/2602.14358)
*Dillon Davis,Huiji Gao,Thomas Legrand,Juan Manuel Caicedo Carvajal,Malay Haldar,Kedar Bellare,Moutupsi Paul,Soumyadip Banerjee,Liwei He,Stephanie Moyerman,Sanjeev Katariya*

Main category: cs.IR

TL;DR: Airbnb重新设计了搜索系统，使用2500万个统一网格单元进行高精度位置检索，替代了原有的贝叶斯多臂老虎机矩形边界预测方法


<details>
  <summary>Details</summary>
Motivation: Airbnb搜索面临全球房源多样性与用户需求差异大的挑战，需要高效的检索阶段来筛选潜在可预订房源。现有基于深度贝叶斯多臂老虎机的矩形边界预测方法存在效率问题，需要更精确的位置检索方案

Method: 将全球划分为2500万个统一网格单元，重新架构搜索系统，从最可能被预订的高精度矩形地图单元子集中进行检索

Result: 论文展示了新方法的方法论、挑战和实施效果，提高了检索效率和预订转化率

Conclusion: 基于统一网格单元的高精度位置检索系统比原有的矩形边界预测方法更有效，能够更好地平衡全球房源多样性与用户需求差异

Abstract: Airbnb search must balance a worldwide, highly varied supply of homes with guests whose location, amenity, style, and price expectations differ widely. Meeting those expectations hinges on an efficient retrieval stage that surfaces only the listings a guest might realistically book, before resource intensive ranking models are applied to determine the best results. Unlike many recommendation engines, our system faces a distinctive challenge, location retrieval, that sits upstream of ranking and determines which geographic areas are queried in order to filter inventory to a candidate set. The preexisting approach employs a deep bayesian bandit based system to predict a rectangular retrieval bounds area that can be used for filtering. The purpose of this paper is to demonstrate the methodology, challenges, and impact of rearchitecting search to retrieve from the subset of most bookable high precision rectangular map cells defined by dividing the world into 25M uniform cells.

</details>


### [12] [Behavioral Feature Boosting via Substitute Relationships for E-commerce Search](https://arxiv.org/abs/2602.14502)
*Chaosheng Dong,Michinari Momma,Yijia Wang,Yan Gao,Yi Sun*

Main category: cs.IR

TL;DR: 提出BFS方法，通过聚合替代产品的行为特征来缓解电商新品冷启动问题，提升搜索相关性和产品发现


<details>
  <summary>Details</summary>
Motivation: 电商平台新产品面临冷启动问题：交互数据有限导致搜索可见性降低和相关性排序受损，影响新品竞争力

Method: BFS（行为特征增强）方法：识别满足相似用户需求的替代产品，聚合其点击、加购、购买、评分等行为信号，为新品提供"热启动"数据

Result: 在大型电商平台上的离线和在线实验表明，BFS显著提升了冷启动产品的搜索相关性和产品发现能力，已上线生产环境服务用户

Conclusion: BFS是一种简单有效、可扩展且实用的方法，能改善用户体验，增加新品曝光，缓解电商搜索中的冷启动问题

Abstract: On E-commerce platforms, new products often suffer from the cold-start problem: limited interaction data reduces their search visibility and hurts relevance ranking. To address this, we propose a simple yet effective behavior feature boosting method that leverages substitute relationships among products (BFS). BFS identifies substitutes-products that satisfy similar user needs-and aggregates their behavioral signals (e.g., clicks, add-to-carts, purchases, and ratings) to provide a warm start for new items. Incorporating these enriched signals into ranking models mitigates cold-start effects and improves relevance and competitiveness. Experiments on a large E-commerce platform, both offline and online, show that BFS significantly improves search relevance and product discovery for cold-start products. BFS is scalable and practical, improving user experience while increasing exposure for newly launched items in E-commerce search. The BFS-enhanced ranking model has been launched in production and has served customers since 2025.

</details>


### [13] [Adaptive Autoguidance for Item-Side Fairness in Diffusion Recommender Systems](https://arxiv.org/abs/2602.14706)
*Zihan Li,Gustavo Escobedo,Marta Moscati,Oleg Lesota,Markus Schedl*

Main category: cs.IR

TL;DR: A2G-DiffRec：一种通过自适应自引导机制缓解扩散推荐系统中流行度偏差的推荐模型，在保持推荐准确性的同时提升项目侧公平性。


<details>
  <summary>Details</summary>
Motivation: 扩散推荐系统虽然推荐准确率高，但通常存在流行度偏差问题，导致不同流行度的项目曝光不均。现有方法未能有效平衡推荐准确性与项目公平性。

Method: 提出A2G-DiffRec模型，采用自适应自引导机制：主模型由一个训练较弱的自身版本引导。不同于固定引导权重，该模型在训练中通过流行度正则化监督，自适应地权衡主模型和弱模型的输出，促进不同流行度项目的均衡曝光。

Result: 在MovieLens-1M、Foursquare-Tokyo和Music4All-Onion数据集上的实验表明，A2G-DiffRec能有效提升项目侧公平性，相比现有引导扩散推荐器和其他非扩散基线，仅以微小的准确性下降为代价。

Conclusion: A2G-DiffRec通过自适应自引导机制成功缓解了扩散推荐系统的流行度偏差问题，在保持推荐性能的同时显著提升了项目曝光公平性，为推荐系统的公平性研究提供了新思路。

Abstract: Diffusion recommender systems achieve strong recommendation accuracy but often suffer from popularity bias, resulting in unequal item exposure. To address this shortcoming, we introduce A2G-DiffRec, a diffusion recommender that incorporates adaptive autoguidance, where the main model is guided by a less-trained version of itself. Instead of using a fixed guidance weight, A2G-DiffRec learns to adaptively weigh the outputs of the main and weak models during training, supervised by a popularity regularization that promotes balanced exposure across items with different popularity levels. Experimental results on the MovieLens-1M, Foursquare-Tokyo, and Music4All-Onion datasets show that A2G-DiffRec is effective in enhancing item-side fairness at a marginal cost of accuracy reduction compared to existing guided diffusion recommenders and other non-diffusion baselines.

</details>


### [14] [Orcheo: A Modular Full-Stack Platform for Conversational Search](https://arxiv.org/abs/2602.14710)
*Shaojie Jiang,Svitlana Vakulenko,Maarten de Rijke*

Main category: cs.IR

TL;DR: Orcheo是一个开源对话搜索平台，提供模块化架构、生产就绪基础设施和50+现成组件，旨在降低对话搜索研究的工程门槛。


<details>
  <summary>Details</summary>
Motivation: 对话搜索研究面临两大障碍：缺乏统一的框架来高效共享研究成果，以及难以部署用于用户评估的端到端原型系统。

Method: 设计Orcheo平台，采用模块化架构（单文件节点模块）、生产就绪基础设施（双执行模式、安全凭证管理、执行遥测）和包含50+现成组件的入门套件。

Result: 通过案例研究验证了Orcheo的实用性和易用性，平台已作为开源项目在GitHub上发布（MIT许可证）。

Conclusion: Orcheo成功填补了对话搜索研究中的框架空白，通过降低工程复杂度促进了研究成果的共享和可复现性。

Abstract: Conversational search (CS) requires a complex software engineering pipeline that integrates query reformulation, ranking, and response generation. CS researchers currently face two barriers: the lack of a unified framework for efficiently sharing contributions with the community, and the difficulty of deploying end-to-end prototypes needed for user evaluation. We introduce Orcheo, an open-source platform designed to bridge this gap. Orcheo offers three key advantages: (i) A modular architecture promotes component reuse through single-file node modules, facilitating sharing and reproducibility in CS research; (ii) Production-ready infrastructure bridges the prototype-to-system gap via dual execution modes, secure credential management, and execution telemetry, with built-in AI coding support that lowers the learning curve; (iii) Starter-kit assets include 50+ off-the-shelf components for query understanding, ranking, and response generation, enabling the rapid bootstrapping of complete CS pipelines. We describe the framework architecture and validate Orcheo's utility through case studies that highlight modularity and ease of use. Orcheo is released as open source under the MIT License at https://github.com/ShaojieJiang/orcheo.

</details>


### [15] [Intent-Driven Dynamic Chunking: Segmenting Documents to Reflect Predicted Information Needs](https://arxiv.org/abs/2602.14784)
*Christos Koutsiaris*

Main category: cs.IR

TL;DR: IDC使用预测的用户查询意图来动态分割文档，通过LLM生成意图并用动态规划找到最优分块边界，在多个QA数据集上显著提升检索准确率并减少分块数量。


<details>
  <summary>Details</summary>
Motivation: 传统文档分割方法（如固定长度或基于连贯性的分割）忽略了用户意图，导致分块可能分割答案或包含无关噪声，影响信息检索系统的性能。

Method: 提出意图驱动的动态分块（IDC）：1）使用大语言模型预测文档可能的用户查询意图；2）采用动态规划算法找到全局最优的分块边界，避免贪婪算法的缺陷。

Result: 在六个不同的问答数据集上评估，IDC在五个数据集上优于传统分块策略，top-1检索准确率提升5%-67%，在第六个数据集上与最佳基线持平。同时IDC产生的分块数量比基线方法少40-60%，答案覆盖率达到93-100%。

Conclusion: 将文档结构与预期的信息需求对齐能显著提升检索性能，特别是对于长文档和异构文档。IDC展示了意图感知分割的有效性，为信息检索系统提供了更好的文档分割方法。

Abstract: Breaking long documents into smaller segments is a fundamental challenge in information retrieval. Whether for search engines, question-answering systems, or retrieval-augmented generation (RAG), effective segmentation determines how well systems can locate and return relevant information. However, traditional methods, such as fixed-length or coherence-based segmentation, ignore user intent, leading to chunks that split answers or contain irrelevant noise. We introduce Intent-Driven Dynamic Chunking (IDC), a novel approach that uses predicted user queries to guide document segmentation. IDC leverages a Large Language Model to generate likely user intents for a document and then employs a dynamic programming algorithm to find the globally optimal chunk boundaries. This represents a novel application of DP to intent-aware segmentation that avoids greedy pitfalls. We evaluated IDC on six diverse question-answering datasets, including news articles, Wikipedia, academic papers, and technical documentation. IDC outperformed traditional chunking strategies on five datasets, improving top-1 retrieval accuracy by 5% to 67%, and matched the best baseline on the sixth. Additionally, IDC produced 40-60% fewer chunks than baseline methods while achieving 93-100% answer coverage. These results demonstrate that aligning document structure with anticipated information needs significantly boosts retrieval performance, particularly for long and heterogeneous documents.

</details>


### [16] [Beyond Retractions: Forensic Scientometrics Techniques to Identify Research Misconduct, Citation Leakage, and Funding Anomalies](https://arxiv.org/abs/2602.14793)
*Leslie D. McIntosh,Alexandra Sinclair,Simon Linacre*

Main category: cs.IR

TL;DR: 对虚构的Pharmakon神经科学研究网络进行法证科学计量学案例研究，该网络在2019-2022年间通过合法学术出版渠道运作


<details>
  <summary>Details</summary>
Motivation: 研究虚构研究网络如何通过合法学术出版渠道运作，揭示学术出版系统中的漏洞和潜在风险

Method: 采用法证科学计量学方法，对虚构的Pharmakon神经科学研究网络进行案例研究，分析其在2019-2022年间的运作模式

Result: 揭示了虚构研究网络如何成功嵌入合法学术出版渠道，暴露了学术出版系统的脆弱性和监管漏洞

Conclusion: 学术出版系统需要加强验证机制，法证科学计量学是检测和防范此类学术欺诈的有效工具

Abstract: This paper presents a forensic scientometric case study of the Pharmakon Neuroscience Research Network, a fabricated research collective that operated primarily between 2019 and 2022 while embedding itself within legitimate scholarly publishing channels.

</details>


### [17] [DRAMA: Domain Retrieval using Adaptive Module Allocation](https://arxiv.org/abs/2602.14960)
*Pranav Kasela,Marco Braga,Ophir Frieder,Nazli Goharian,Gabriella Pasi,Raffaele Perego*

Main category: cs.IR

TL;DR: DRAMA是一个参数和能耗高效的神经检索框架，通过动态门控机制选择领域特定适配器，在保持检索效果的同时大幅减少计算资源和参数使用。


<details>
  <summary>Details</summary>
Motivation: 神经检索模型虽然效果优秀，但计算和能耗成本高，在多领域场景中扩展性受限。训练和维护领域特定模型效率低下，而统一模型的跨领域泛化能力又难以保证。

Method: 提出DRAMA框架，集成领域特定适配器模块和动态门控机制，为每个查询选择最相关的领域知识。新领域可通过轻量级适配器训练高效添加，无需完整模型重训练。

Result: 在多个Web检索基准测试中，DRAMA达到与领域特定模型相当的效果，同时仅使用其一小部分参数和计算资源。

Conclusion: 能量感知的模型设计能显著提升神经IR的可扩展性和可持续性，DRAMA框架在保持检索效果的同时大幅降低了环境足迹。

Abstract: Neural models are increasingly used in Web-scale Information Retrieval (IR). However, relying on these models introduces substantial computational and energy requirements, leading to increasing attention toward their environmental cost and the sustainability of large-scale deployments. While neural IR models deliver high retrieval effectiveness, their scalability is constrained in multi-domain scenarios, where training and maintaining domain-specific models is inefficient and achieving robust cross-domain generalisation within a unified model remains difficult. This paper introduces DRAMA (Domain Retrieval using Adaptive Module Allocation), an energy- and parameter-efficient framework designed to reduce the environmental footprint of neural retrieval. DRAMA integrates domain-specific adapter modules with a dynamic gating mechanism that selects the most relevant domain knowledge for each query. New domains can be added efficiently through lightweight adapter training, avoiding full model retraining. We evaluate DRAMA on multiple Web retrieval benchmarks covering different domains. Our extensive evaluation shows that DRAMA achieves comparable effectiveness to domain-specific models while using only a fraction of their parameters and computational resources. These findings show that energy-aware model design can significantly improve scalability and sustainability in neural IR.

</details>
