{"id": "2512.06334", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.06334", "abs": "https://arxiv.org/abs/2512.06334", "authors": ["Van-Thinh Vo", "Minh-Khoi Nguyen", "Minh-Huy Tran", "Anh-Quan Nguyen-Tran", "Duy-Tan Nguyen", "Khanh-Loi Nguyen", "Anh-Minh Phan"], "title": "Enhanced Multimodal Video Retrieval System: Integrating Query Expansion and Cross-modal Temporal Event Retrieval", "comment": "11 pages, 6 figures, SOICT 2025", "summary": "Multimedia information retrieval from videos remains a challenging problem. While recent systems have advanced multimodal search through semantic, object, and OCR queries - and can retrieve temporally consecutive scenes - they often rely on a single query modality for an entire sequence, limiting robustness in complex temporal contexts. To overcome this, we propose a cross-modal temporal event retrieval framework that enables different query modalities to describe distinct scenes within a sequence. To determine decision thresholds for scene transition and slide change adaptively, we build Kernel Density Gaussian Mixture Thresholding (KDE-GMM) algorithm, ensuring optimal keyframe selection. These extracted keyframes act as compact, high-quality visual exemplars that retain each segment's semantic essence, improving retrieval precision and efficiency. Additionally, the system incorporates a large language model (LLM) to refine and expand user queries, enhancing overall retrieval performance. The proposed system's effectiveness and robustness were demonstrated through its strong results in the Ho Chi Minh AI Challenge 2025.", "AI": {"tldr": "\u63d0\u51fa\u8de8\u6a21\u6001\u65f6\u5e8f\u4e8b\u4ef6\u68c0\u7d22\u6846\u67b6\uff0c\u5141\u8bb8\u4e0d\u540c\u67e5\u8be2\u6a21\u6001\u63cf\u8ff0\u5e8f\u5217\u4e2d\u7684\u4e0d\u540c\u573a\u666f\uff0c\u901a\u8fc7KDE-GMM\u7b97\u6cd5\u81ea\u9002\u5e94\u786e\u5b9a\u573a\u666f\u8f6c\u6362\u9608\u503c\uff0c\u63d0\u53d6\u5173\u952e\u5e27\u4f5c\u4e3a\u9ad8\u8d28\u91cf\u89c6\u89c9\u793a\u4f8b\uff0c\u7ed3\u5408LLM\u4f18\u5316\u67e5\u8be2\uff0c\u5728\u80e1\u5fd7\u660eAI\u6311\u6218\u8d5b2025\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u591a\u6a21\u6001\u68c0\u7d22\u7cfb\u7edf\u901a\u5e38\u4f7f\u7528\u5355\u4e00\u67e5\u8be2\u6a21\u6001\u63cf\u8ff0\u6574\u4e2a\u5e8f\u5217\uff0c\u5728\u590d\u6742\u65f6\u5e8f\u573a\u666f\u4e2d\u9c81\u68d2\u6027\u4e0d\u8db3\u3002\u9700\u8981\u80fd\u591f\u652f\u6301\u4e0d\u540c\u67e5\u8be2\u6a21\u6001\u63cf\u8ff0\u5e8f\u5217\u4e2d\u4e0d\u540c\u573a\u666f\u7684\u68c0\u7d22\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u8de8\u6a21\u6001\u65f6\u5e8f\u4e8b\u4ef6\u68c0\u7d22\u6846\u67b6\uff1a1) \u4f7f\u7528KDE-GMM\u7b97\u6cd5\u81ea\u9002\u5e94\u786e\u5b9a\u573a\u666f\u8f6c\u6362\u548c\u5e7b\u706f\u7247\u53d8\u5316\u7684\u51b3\u7b56\u9608\u503c\uff0c\u5b9e\u73b0\u6700\u4f18\u5173\u952e\u5e27\u9009\u62e9\uff1b2) \u63d0\u53d6\u7684\u5173\u952e\u5e27\u4f5c\u4e3a\u7d27\u51d1\u9ad8\u8d28\u91cf\u89c6\u89c9\u793a\u4f8b\uff0c\u4fdd\u7559\u6bcf\u4e2a\u7247\u6bb5\u7684\u8bed\u4e49\u672c\u8d28\uff1b3) \u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u4f18\u5316\u548c\u6269\u5c55\u7528\u6237\u67e5\u8be2\u3002", "result": "\u5728\u80e1\u5fd7\u660eAI\u6311\u6218\u8d5b2025\u4e2d\u5c55\u793a\u4e86\u7cfb\u7edf\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u68c0\u7d22\u7ed3\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684\u8de8\u6a21\u6001\u65f6\u5e8f\u4e8b\u4ef6\u68c0\u7d22\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u65f6\u5e8f\u573a\u666f\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9608\u503c\u786e\u5b9a\u548cLLM\u67e5\u8be2\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u591a\u6a21\u6001\u68c0\u7d22\u7684\u7cbe\u5ea6\u548c\u6548\u7387\u3002"}}
{"id": "2512.06381", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.06381", "abs": "https://arxiv.org/abs/2512.06381", "authors": ["Tao Wang", "Xun Luo", "Jinlong Guo", "Yuliang Yan", "Jian Wu", "Yuning Jiang", "Bo Zheng"], "title": "Beyond Existing Retrievals: Cross-Scenario Incremental Sample Learning Framework", "comment": null, "summary": "The parallelized multi-retrieval architecture has been widely adopted in large-scale recommender systems for its computational efficiency and comprehensive coverage of user interests. Many retrieval methods typically integrate additional cross-scenario samples to enhance the overall performance ceiling. However, those model designs neglect the fact that a part of the cross-scenario samples have already been retrieved by existing models within a system, leading to diminishing marginal utility in delivering incremental performance gains. In this paper, we propose a novel retrieval framework IncRec, specifically for cross-scenario incremental sample learning. The innovations of IncRec can be highlighted as two aspects. Firstly, we construct extreme cross-scenario incremental samples that are not retrieved by any existing model. And we design an incremental sample learning framework which focuses on capturing incremental representation to improve the overall retrieval performance. Secondly, we introduce a consistency-aware alignment module to further make the model prefer incremental samples with high exposure probability. Extensive offline and online A/B tests validate the superiority of our framework over state-of-the-art retrieval methods. In particular, we deploy IncRec in the Taobao homepage recommendation, achieving a 1% increase in online transaction count, demonstrating its practical applicability.", "AI": {"tldr": "\u63d0\u51faIncRec\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u8de8\u573a\u666f\u589e\u91cf\u6837\u672c\u5b66\u4e60\uff0c\u901a\u8fc7\u6784\u5efa\u672a\u88ab\u73b0\u6709\u6a21\u578b\u68c0\u7d22\u7684\u589e\u91cf\u6837\u672c\u548c\u4e00\u81f4\u6027\u5bf9\u9f50\u6a21\u5757\uff0c\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5e76\u884c\u591a\u68c0\u7d22\u67b6\u6784\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u8bb8\u591a\u65b9\u6cd5\u5728\u6574\u5408\u8de8\u573a\u666f\u6837\u672c\u65f6\u5ffd\u7565\u4e86\u90e8\u5206\u6837\u672c\u5df2\u88ab\u73b0\u6709\u6a21\u578b\u68c0\u7d22\u7684\u4e8b\u5b9e\uff0c\u5bfc\u81f4\u8fb9\u9645\u6548\u7528\u9012\u51cf\uff0c\u65e0\u6cd5\u5b9e\u73b0\u771f\u6b63\u7684\u6027\u80fd\u63d0\u5347\u3002", "method": "1. \u6784\u5efa\u6781\u7aef\u8de8\u573a\u666f\u589e\u91cf\u6837\u672c\uff08\u672a\u88ab\u4efb\u4f55\u73b0\u6709\u6a21\u578b\u68c0\u7d22\u7684\u6837\u672c\uff09\uff1b2. \u8bbe\u8ba1\u589e\u91cf\u6837\u672c\u5b66\u4e60\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u6355\u6349\u589e\u91cf\u8868\u793a\uff1b3. \u5f15\u5165\u4e00\u81f4\u6027\u611f\u77e5\u5bf9\u9f50\u6a21\u5757\uff0c\u4f7f\u6a21\u578b\u504f\u597d\u5177\u6709\u9ad8\u66dd\u5149\u6982\u7387\u7684\u589e\u91cf\u6837\u672c\u3002", "result": "\u901a\u8fc7\u79bb\u7ebf\u548c\u5728\u7ebfA/B\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u4f18\u8d8a\u6027\uff0c\u5728\u6dd8\u5b9d\u9996\u9875\u63a8\u8350\u4e2d\u90e8\u7f72IncRec\uff0c\u5b9e\u73b0\u4e86\u5728\u7ebf\u4ea4\u6613\u91cf1%\u7684\u589e\u957f\uff0c\u8bc1\u660e\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "IncRec\u6846\u67b6\u901a\u8fc7\u4e13\u6ce8\u4e8e\u8de8\u573a\u666f\u589e\u91cf\u6837\u672c\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u68c0\u7d22\u65b9\u6cd5\u4e2d\u6837\u672c\u91cd\u53e0\u5bfc\u81f4\u7684\u8fb9\u9645\u6548\u7528\u9012\u51cf\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u7cfb\u7edf\u7684\u68c0\u7d22\u6027\u80fd\u3002"}}
{"id": "2512.06449", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.06449", "abs": "https://arxiv.org/abs/2512.06449", "authors": ["Jaewon Ahn", "Woosung Jang", "Beakcheol Jang"], "title": "Enhancing Medical Cross-Modal Hashing Retrieval using Dropout-Voting Mixture-of-Experts Fusion", "comment": "5 pages, 1 figure, workshop paper (MMGenSR 2025)", "summary": "In recent years, cross-modal retrieval using images and text has become an active area of research, especially in the medical domain. The abundance of data in various modalities in this field has led to a growing importance of cross-modal retrieval for efficient image interpretation, data-driven diagnostic support, and medical education. In the context of the increasing integration of distributed medical data across healthcare facilities with the objective of enhancing interoperability, it is imperative to optimize the performance of retrieval systems in terms of the speed, memory efficiency, and accuracy of the retrieved data. This necessity arises in response to the substantial surge in data volume that characterizes contemporary medical practices. In this study, we propose a novel framework that incorporates dropout voting and mixture-of-experts (MoE) based contrastive fusion modules into a CLIP-based cross-modal hashing retrieval structure. We also propose the application of hybrid loss. So we now call our model MCMFH which is a medical cross-modal fusion hashing retrieval. Our method enables the simultaneous achievement of high accuracy and fast retrieval speed in low-memory environments. The model is demonstrated through experiments on radiological and non-radiological medical datasets.", "AI": {"tldr": "\u63d0\u51faMCMFH\u6a21\u578b\uff0c\u7d50\u5408dropout voting\u548cMoE\u5c0d\u6bd4\u878d\u5408\u6a21\u584a\u7684CLIP\u8de8\u6a21\u614b\u54c8\u5e0c\u6aa2\u7d22\u6846\u67b6\uff0c\u5be6\u73fe\u9ad8\u7cbe\u5ea6\u3001\u5feb\u901f\u6aa2\u7d22\u548c\u4f4e\u5167\u5b58\u6d88\u8017", "motivation": "\u91ab\u7642\u9818\u57df\u591a\u6a21\u614b\u6578\u64da\u8c50\u5bcc\uff0c\u8de8\u6a21\u614b\u6aa2\u7d22\u5c0d\u5716\u50cf\u89e3\u91cb\u3001\u8a3a\u65b7\u652f\u6301\u548c\u91ab\u5b78\u6559\u80b2\u65e5\u76ca\u91cd\u8981\u3002\u96a8\u8457\u91ab\u7642\u6578\u64da\u5206\u4f48\u5f0f\u6574\u5408\u548c\u6578\u64da\u91cf\u6fc0\u589e\uff0c\u9700\u8981\u512a\u5316\u6aa2\u7d22\u7cfb\u7d71\u7684\u901f\u5ea6\u3001\u5167\u5b58\u6548\u7387\u548c\u6e96\u78ba\u6027", "method": "\u63d0\u51faMCMFH\u6a21\u578b\uff1a1) \u57fa\u65bcCLIP\u7684\u8de8\u6a21\u614b\u54c8\u5e0c\u6aa2\u7d22\u7d50\u69cb\uff1b2) \u5f15\u5165dropout voting\u6a5f\u5236\uff1b3) \u6df7\u5408\u5c08\u5bb6(MoE)\u5c0d\u6bd4\u878d\u5408\u6a21\u584a\uff1b4) \u61c9\u7528\u6df7\u5408\u640d\u5931\u51fd\u6578", "result": "\u5728\u653e\u5c04\u5b78\u548c\u975e\u653e\u5c04\u5b78\u91ab\u7642\u6578\u64da\u96c6\u4e0a\u5be6\u9a57\u8b49\u660e\uff0c\u6a21\u578b\u80fd\u5728\u4f4e\u5167\u5b58\u74b0\u5883\u4e0b\u540c\u6642\u5be6\u73fe\u9ad8\u6e96\u78ba\u6027\u548c\u5feb\u901f\u6aa2\u7d22\u901f\u5ea6", "conclusion": "\u63d0\u51fa\u7684MCMFH\u6846\u67b6\u6709\u6548\u89e3\u6c7a\u4e86\u91ab\u7642\u8de8\u6a21\u614b\u6aa2\u7d22\u4e2d\u901f\u5ea6\u3001\u5167\u5b58\u548c\u6e96\u78ba\u6027\u7684\u5e73\u8861\u554f\u984c\uff0c\u70ba\u91ab\u7642\u6578\u64da\u6aa2\u7d22\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u6c7a\u65b9\u6848"}}
{"id": "2512.06590", "categories": ["cs.IR", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.06590", "abs": "https://arxiv.org/abs/2512.06590", "authors": ["Tendai Mukande", "Esraa Ali", "Annalina Caputo", "Ruihai Dong", "Noel OConnor"], "title": "Towards Efficient Hypergraph and Multi-LLM Agent Recommender Systems", "comment": "8 Pages", "summary": "Recommender Systems (RSs) have become the cornerstone of various applications such as e-commerce and social media platforms. The evolution of RSs is paramount in the digital era, in which personalised user experience is tailored to the user's preferences. Large Language Models (LLMs) have sparked a new paradigm - generative retrieval and recommendation. Despite their potential, generative RS methods face issues such as hallucination, which degrades the recommendation performance, and high computational cost in practical scenarios. To address these issues, we introduce HGLMRec, a novel Multi-LLM agent-based RS that incorporates a hypergraph encoder designed to capture complex, multi-behaviour relationships between users and items. The HGLMRec model retrieves only the relevant tokens during inference, reducing computational overhead while enriching the retrieval context. Experimental results show performance improvement by HGLMRec against state-of-the-art baselines at lower computational cost.", "AI": {"tldr": "HGLMRec\uff1a\u4e00\u79cd\u57fa\u4e8e\u591aLLM\u4ee3\u7406\u7684\u63a8\u8350\u7cfb\u7edf\uff0c\u4f7f\u7528\u8d85\u56fe\u7f16\u7801\u5668\u6355\u6349\u7528\u6237\u4e0e\u7269\u54c1\u95f4\u7684\u590d\u6742\u591a\u884c\u4e3a\u5173\u7cfb\uff0c\u901a\u8fc7\u4ec5\u68c0\u7d22\u76f8\u5173token\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u5347\u6027\u80fd", "motivation": "\u751f\u6210\u5f0f\u63a8\u8350\u7cfb\u7edf\u9762\u4e34\u5e7b\u89c9\u95ee\u9898\uff08\u964d\u4f4e\u63a8\u8350\u6027\u80fd\uff09\u548c\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u89e3\u51b3\u8fd9\u4e24\u4e2a\u6311\u6218\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51faHGLMRec\u6a21\u578b\uff0c\u91c7\u7528\u591aLLM\u4ee3\u7406\u67b6\u6784\uff0c\u96c6\u6210\u8d85\u56fe\u7f16\u7801\u5668\u6355\u6349\u7528\u6237\u4e0e\u7269\u54c1\u95f4\u7684\u590d\u6742\u591a\u884c\u4e3a\u5173\u7cfb\uff0c\u5728\u63a8\u7406\u65f6\u4ec5\u68c0\u7d22\u76f8\u5173token\u4ee5\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aHGLMRec\u5728\u8f83\u4f4e\u8ba1\u7b97\u6210\u672c\u4e0b\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "HGLMRec\u901a\u8fc7\u8d85\u56fe\u7f16\u7801\u548c\u591aLLM\u4ee3\u7406\u67b6\u6784\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u751f\u6210\u5f0f\u63a8\u8350\u7cfb\u7edf\u7684\u5e7b\u89c9\u548c\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u4e0e\u6548\u7387\u4f18\u5316\u7684\u5e73\u8861"}}
{"id": "2512.06641", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06641", "abs": "https://arxiv.org/abs/2512.06641", "authors": ["Yihan Chen", "Benfeng Xu", "Xiaorui Wang", "Zhendong Mao"], "title": "An Index-based Approach for Efficient and Effective Web Content Extraction", "comment": null, "summary": "As web agents (e.g., Deep Research) routinely consume massive volumes of web pages to gather and analyze information, LLM context management -- under large token budgets and low signal density -- emerges as a foundational, high-importance, and technically challenging problem for agentic and RAG pipelines. Existing solutions for extracting relevant content are inadequate: generative extraction models suffer from high latency, rule-based heuristics lack adaptability, and chunk-and-rerank methods are blind to webpage structure. To overcome these issues, we introduce Index-based Web Content Extraction to reframe the extraction process from slow, token-by-token generation into a highly efficient, discriminative task of index prediction, achieving both effectiveness and efficiency. We partition HTML into structure-aware, addressable segments, and extract only the positional indices of content relevant to a given query. This method decouples extraction latency from content length, enabling rapid, query-relevant extraction. We first evaluate our method as a post-retrieval processing component within an RAG QA system and find that it improves QA accuracy. Then we directly measure its match rate with the target content in two scenarios: main content extraction (ME) and query-relevant extraction (QE). Experimental results show that our method outperforms existing works in both accuracy and speed, effectively bridging the gap between LLMs and the vast webpages.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7d22\u5f15\u7684\u7f51\u9875\u5185\u5bb9\u63d0\u53d6\u65b9\u6cd5\uff0c\u5c06\u63d0\u53d6\u4efb\u52a1\u4ece\u7f13\u6162\u7684\u751f\u6210\u5f0f\u8fc7\u7a0b\u8f6c\u53d8\u4e3a\u9ad8\u6548\u7684\u5224\u522b\u5f0f\u7d22\u5f15\u9884\u6d4b\uff0c\u89e3\u51b3\u4e86LLM\u4e0a\u4e0b\u6587\u7ba1\u7406\u4e2d\u5927\u89c4\u6a21\u7f51\u9875\u5904\u7406\u7684\u95ee\u9898\u3002", "motivation": "\u7f51\u7edc\u4ee3\u7406\uff08\u5982Deep Research\uff09\u9700\u8981\u5904\u7406\u5927\u91cf\u7f51\u9875\uff0c\u4f46\u73b0\u6709\u5185\u5bb9\u63d0\u53d6\u65b9\u6cd5\u5b58\u5728\u4e0d\u8db3\uff1a\u751f\u6210\u5f0f\u6a21\u578b\u5ef6\u8fdf\u9ad8\uff0c\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u7f3a\u4e4f\u9002\u5e94\u6027\uff0c\u5206\u5757\u91cd\u6392\u5e8f\u65b9\u6cd5\u5ffd\u7565\u7f51\u9875\u7ed3\u6784\u3002\u9700\u8981\u4e00\u79cd\u65e2\u9ad8\u6548\u53c8\u51c6\u786e\u7684\u5185\u5bb9\u63d0\u53d6\u65b9\u6848\u3002", "method": "\u5c06HTML\u5206\u5272\u4e3a\u7ed3\u6784\u611f\u77e5\u3001\u53ef\u5bfb\u5740\u7684\u7247\u6bb5\uff0c\u7136\u540e\u4ec5\u63d0\u53d6\u4e0e\u67e5\u8be2\u76f8\u5173\u7684\u5185\u5bb9\u4f4d\u7f6e\u7d22\u5f15\u3002\u8fd9\u79cd\u65b9\u6cd5\u5c06\u63d0\u53d6\u5ef6\u8fdf\u4e0e\u5185\u5bb9\u957f\u5ea6\u89e3\u8026\uff0c\u5b9e\u73b0\u5feb\u901f\u3001\u67e5\u8be2\u76f8\u5173\u7684\u63d0\u53d6\u3002", "result": "\u5728RAG QA\u7cfb\u7edf\u4e2d\u4f5c\u4e3a\u540e\u68c0\u7d22\u5904\u7406\u7ec4\u4ef6\u4f7f\u7528\u65f6\u63d0\u9ad8\u4e86QA\u51c6\u786e\u6027\u3002\u5728\u4e24\u79cd\u573a\u666f\uff08\u4e3b\u8981\u5185\u5bb9\u63d0\u53d6ME\u548c\u67e5\u8be2\u76f8\u5173\u63d0\u53d6QE\uff09\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u901f\u5ea6\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u5de5\u4f5c\u3002", "conclusion": "\u57fa\u4e8e\u7d22\u5f15\u7684\u7f51\u9875\u5185\u5bb9\u63d0\u53d6\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86LLM\u4e0e\u6d77\u91cf\u7f51\u9875\u4e4b\u95f4\u7684\u5dee\u8ddd\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u5185\u5bb9\u63d0\u53d6\uff0c\u4e3a\u667a\u80fd\u4ee3\u7406\u548cRAG\u7ba1\u9053\u63d0\u4f9b\u4e86\u57fa\u7840\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2512.06700", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.06700", "abs": "https://arxiv.org/abs/2512.06700", "authors": ["Jiangxia Cao", "Ruochen Yang", "Xiang Chen", "Changxin Lao", "Yueyang Liu", "Yusheng Huang", "Yuanhao Tian", "Xiangyu Wu", "Shuang Yang", "Zhaojie Liu", "Guorui Zhou"], "title": "Foresight Prediction Enhanced Live-Streaming Recommendation", "comment": "Accepted by WSDM 2026", "summary": "Live-streaming, as an emerging media enabling real-time interaction between authors and users, has attracted significant attention. Unlike the stable playback time of traditional TV live or the fixed content of short video, live-streaming, due to the dynamics of content and time, poses higher requirements for the recommendation algorithm of the platform - understanding the ever-changing content in real time and push it to users at the appropriate moment. Through analysis, we find that users have a better experience and express more positive behaviors during highlight moments of the live-streaming. Furthermore, since the model lacks access to future content during recommendation, yet user engagement depends on how well subsequent content aligns with their interests, an intuitive solution is to predict future live-streaming content. Therefore, we perform semantic quantization on live-streaming segments to obtain Semantic ids (Sid), encode the historical Sid sequence to capture the author's characteristics, and model Sid evolution trend to enable foresight prediction of future content. This foresight enhances the ranking model through refined features. Extensive offline and online experiments demonstrate the effectiveness of our method.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u76f4\u64ad\u63a8\u8350\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u91cf\u5316\u76f4\u64ad\u7247\u6bb5\u83b7\u53d6\u8bed\u4e49ID\uff0c\u7f16\u7801\u5386\u53f2\u5e8f\u5217\u6355\u6349\u4f5c\u8005\u7279\u5f81\uff0c\u5efa\u6a21\u8bed\u4e49ID\u6f14\u5316\u8d8b\u52bf\u6765\u9884\u6d4b\u672a\u6765\u5185\u5bb9\uff0c\u4ece\u800c\u63d0\u5347\u63a8\u8350\u6548\u679c\u3002", "motivation": "\u76f4\u64ad\u4f5c\u4e3a\u4e00\u79cd\u65b0\u5174\u5a92\u4f53\uff0c\u5185\u5bb9\u52a8\u6001\u53d8\u5316\u4e14\u65f6\u95f4\u654f\u611f\uff0c\u5bf9\u63a8\u8350\u7b97\u6cd5\u63d0\u51fa\u4e86\u66f4\u9ad8\u8981\u6c42\u3002\u7814\u7a76\u53d1\u73b0\u7528\u6237\u5728\u76f4\u64ad\u9ad8\u5149\u65f6\u523b\u4f53\u9a8c\u66f4\u597d\u4e14\u884c\u4e3a\u66f4\u79ef\u6781\uff0c\u4f46\u7531\u4e8e\u63a8\u8350\u65f6\u6a21\u578b\u65e0\u6cd5\u8bbf\u95ee\u672a\u6765\u5185\u5bb9\uff0c\u800c\u7528\u6237\u53c2\u4e0e\u5ea6\u53d6\u51b3\u4e8e\u540e\u7eed\u5185\u5bb9\u662f\u5426\u7b26\u5408\u5176\u5174\u8da3\uff0c\u56e0\u6b64\u9700\u8981\u9884\u6d4b\u672a\u6765\u76f4\u64ad\u5185\u5bb9\u3002", "method": "1. \u5bf9\u76f4\u64ad\u7247\u6bb5\u8fdb\u884c\u8bed\u4e49\u91cf\u5316\u83b7\u5f97\u8bed\u4e49ID\uff1b2. \u7f16\u7801\u5386\u53f2\u8bed\u4e49ID\u5e8f\u5217\u6355\u6349\u4f5c\u8005\u7279\u5f81\uff1b3. \u5efa\u6a21\u8bed\u4e49ID\u6f14\u5316\u8d8b\u52bf\u4ee5\u5b9e\u73b0\u5bf9\u672a\u6765\u5185\u5bb9\u7684\u9884\u89c1\u6027\u9884\u6d4b\uff1b4. \u5c06\u8fd9\u79cd\u9884\u89c1\u6027\u901a\u8fc7\u7cbe\u7ec6\u5316\u7279\u5f81\u589e\u5f3a\u6392\u5e8f\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u79bb\u7ebf\u548c\u5728\u7ebf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u9884\u6d4b\u76f4\u64ad\u672a\u6765\u5185\u5bb9\uff0c\u901a\u8fc7\u8bed\u4e49\u91cf\u5316\u548c\u6f14\u5316\u8d8b\u52bf\u5efa\u6a21\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u6027\u80fd\uff0c\u6539\u5584\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2512.06879", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06879", "abs": "https://arxiv.org/abs/2512.06879", "authors": ["Li Ju", "Jun Zhao", "Mingxu Chai", "Ziyu Shen", "Xiangyang Wang", "Yage Geng", "Chunchun Ma", "Hao Peng", "Guangbin Li", "Tao Li", "Chengyong Liao", "Fu Wang", "Xiaolong Wang", "Junshen Chen", "Rui Gong", "Shijia Liang", "Feiyan Li", "Ming Zhang", "Kexin Tan", "Jujie Ye", "Zhiheng Xi", "Shihan Dou", "Tao Gui", "Yuankai Ying", "Yang Shi", "Yue Zhang", "Qi Zhang"], "title": "WisPaper: Your AI Scholar Search Engine", "comment": "17 pages, 2 figures", "summary": "Researchers struggle to efficiently locate and manage relevant literature within the exponentially growing body of scientific publications. We present \\textsc{WisPaper}, an intelligent academic retrieval and literature management platform that addresses this challenge through three integrated capabilities: (1) \\textit{Scholar Search}, featuring both quick keyword-based and deep agentic search modes for efficient paper discovery; (2) \\textit{Library}, a customizable knowledge base for systematic literature organization; and (3) \\textit{AI Feeds}, an intelligent recommendation system that automatically delivers relevant new publications based on user interests. Unlike existing academic tools, \\textsc{WisPaper} provides a closed-loop workflow that seamlessly connects literature discovery, management, and continuous tracking of research frontiers. Our multilingual and multidisciplinary system significantly reduces the time researchers from diverse backgrounds spend on paper screening and management, enabling them to focus on their core research activities. The platform is publicly accessible and serves researchers across academia and industry.", "AI": {"tldr": "WisPaper\u662f\u4e00\u4e2a\u667a\u80fd\u5b66\u672f\u68c0\u7d22\u4e0e\u6587\u732e\u7ba1\u7406\u5e73\u53f0\uff0c\u96c6\u6210\u4e86\u5b66\u8005\u641c\u7d22\u3001\u6587\u732e\u5e93\u548cAI\u63a8\u8350\u4e09\u5927\u529f\u80fd\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4ece\u53d1\u73b0\u5230\u7ba1\u7406\u7684\u95ed\u73af\u5de5\u4f5c\u6d41\u3002", "motivation": "\u968f\u7740\u79d1\u5b66\u51fa\u7248\u7269\u5448\u6307\u6570\u7ea7\u589e\u957f\uff0c\u7814\u7a76\u4eba\u5458\u96be\u4ee5\u9ad8\u6548\u5b9a\u4f4d\u548c\u7ba1\u7406\u76f8\u5173\u6587\u732e\u3002\u73b0\u6709\u5b66\u672f\u5de5\u5177\u65e0\u6cd5\u63d0\u4f9b\u4ece\u6587\u732e\u53d1\u73b0\u5230\u6301\u7eed\u8ddf\u8e2a\u7684\u5b8c\u6574\u95ed\u73af\u5de5\u4f5c\u6d41\u3002", "method": "WisPaper\u901a\u8fc7\u4e09\u4e2a\u96c6\u6210\u529f\u80fd\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff1a1) Scholar Search\uff08\u5feb\u901f\u5173\u952e\u8bcd\u641c\u7d22\u548c\u6df1\u5ea6\u667a\u80fd\u641c\u7d22\u6a21\u5f0f\uff09\uff1b2) Library\uff08\u53ef\u5b9a\u5236\u7684\u77e5\u8bc6\u5e93\u7528\u4e8e\u7cfb\u7edf\u5316\u6587\u732e\u7ec4\u7ec7\uff09\uff1b3) AI Feeds\uff08\u57fa\u4e8e\u7528\u6237\u5174\u8da3\u81ea\u52a8\u63a8\u8350\u76f8\u5173\u65b0\u51fa\u7248\u7269\u7684\u667a\u80fd\u63a8\u8350\u7cfb\u7edf\uff09\u3002", "result": "\u8be5\u591a\u8bed\u8a00\u3001\u591a\u5b66\u79d1\u7cfb\u7edf\u663e\u8457\u51cf\u5c11\u4e86\u4e0d\u540c\u80cc\u666f\u7814\u7a76\u4eba\u5458\u5728\u6587\u732e\u7b5b\u9009\u548c\u7ba1\u7406\u4e0a\u82b1\u8d39\u7684\u65f6\u95f4\uff0c\u4f7f\u4ed6\u4eec\u80fd\u591f\u4e13\u6ce8\u4e8e\u6838\u5fc3\u7814\u7a76\u6d3b\u52a8\u3002\u5e73\u53f0\u5df2\u516c\u5f00\u53ef\u7528\uff0c\u670d\u52a1\u4e8e\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u7684\u7814\u7a76\u4eba\u5458\u3002", "conclusion": "WisPaper\u63d0\u4f9b\u4e86\u4e00\u4e2a\u72ec\u7279\u7684\u95ed\u73af\u5de5\u4f5c\u6d41\uff0c\u65e0\u7f1d\u8fde\u63a5\u6587\u732e\u53d1\u73b0\u3001\u7ba1\u7406\u548c\u7814\u7a76\u524d\u6cbf\u7684\u6301\u7eed\u8ddf\u8e2a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5b66\u672f\u5de5\u5177\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u7814\u7a76\u6548\u7387\u3002"}}
{"id": "2512.06883", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.06883", "abs": "https://arxiv.org/abs/2512.06883", "authors": ["Zhongtao Rao", "Peilin Zhou", "Dading Chong", "Zhiwei Chen", "Shoujin Wang", "Nan Tang"], "title": "Structural and Disentangled Adaptation of Large Vision Language Models for Multimodal Recommendation", "comment": null, "summary": "Multimodal recommendation enhances accuracy by leveraging visual and textual signals, and its success largely depends on learning high-quality cross-modal representations. Recent advances in Large Vision-Language Models (LVLMs) offer unified multimodal representation learning, making them a promising backbone. However, applying LVLMs to recommendation remains challenging due to (i) representation misalignment, where domain gaps between item data and general pre-training lead to unaligned embedding spaces, and (ii) gradient conflicts during fine-tuning, where shared adapters cause interference and a lack of discriminative power. To address this, we propose SDA, a lightweight framework for Structural and Disentangled Adaptation, which integrates two components: Cross-Modal Structural Alignment (CMSA) and Modality-Disentangled Adaptation. CMSA aligns embeddings using intra-modal structures as a soft teacher, while MoDA mitigates gradient conflicts via expertized, gated low-rank paths to disentangle gradient flows. Experiments on three public Amazon datasets show SDA integrates seamlessly with existing multimodal and sequential recommenders, yielding average gains of 6.15% in Hit@10 and 8.64% in NDCG@10. It also achieves up to 12.83% and 18.70% gains on long-tail items with minimal inference overhead. Our code and full experimental results are available at https://github.com/RaoZhongtao/SDA.", "AI": {"tldr": "SDA\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5bf9\u9f50\u548c\u6a21\u6001\u89e3\u8026\u9002\u914d\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u63a8\u8350\u4e2dLVLMs\u7684\u8868\u793a\u4e0d\u5bf9\u9f50\u548c\u68af\u5ea6\u51b2\u7a81\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u63a8\u8350\u6027\u80fd", "motivation": "\u591a\u6a21\u6001\u63a8\u8350\u4f9d\u8d56\u9ad8\u8d28\u91cf\u8de8\u6a21\u6001\u8868\u793a\uff0c\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(LVLMs)\u63d0\u4f9b\u4e86\u7edf\u4e00\u8868\u793a\u5b66\u4e60\u80fd\u529b\uff0c\u4f46\u5e94\u7528\u4e8e\u63a8\u8350\u65f6\u9762\u4e34\u4e24\u4e2a\u6311\u6218\uff1a1) \u8868\u793a\u4e0d\u5bf9\u9f50\uff08\u9886\u57df\u5dee\u8ddd\u5bfc\u81f4\u5d4c\u5165\u7a7a\u95f4\u4e0d\u5339\u914d\uff09\uff0c2) \u68af\u5ea6\u51b2\u7a81\uff08\u5171\u4eab\u9002\u914d\u5668\u5bfc\u81f4\u5e72\u6270\u548c\u5224\u522b\u80fd\u529b\u4e0d\u8db3\uff09", "method": "\u63d0\u51faSDA\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u7ec4\u4ef6\uff1a1) \u8de8\u6a21\u6001\u7ed3\u6784\u5bf9\u9f50(CMSA)\uff1a\u4f7f\u7528\u6a21\u6001\u5185\u7ed3\u6784\u4f5c\u4e3a\u8f6f\u6559\u5e08\u5bf9\u9f50\u5d4c\u5165\uff1b2) \u6a21\u6001\u89e3\u8026\u9002\u914d(MoDA)\uff1a\u901a\u8fc7\u4e13\u5bb6\u5316\u3001\u95e8\u63a7\u4f4e\u79e9\u8def\u5f84\u89e3\u8026\u68af\u5ea6\u6d41\uff0c\u7f13\u89e3\u68af\u5ea6\u51b2\u7a81", "result": "\u5728\u4e09\u4e2aAmazon\u6570\u636e\u96c6\u4e0a\uff0cSDA\u4e0e\u73b0\u6709\u591a\u6a21\u6001\u548c\u5e8f\u5217\u63a8\u8350\u5668\u65e0\u7f1d\u96c6\u6210\uff0c\u5e73\u5747\u63d0\u5347Hit@10 6.15%\uff0cNDCG@10 8.64%\uff0c\u957f\u5c3e\u7269\u54c1\u4e0a\u5206\u522b\u63d0\u534712.83%\u548c18.70%\uff0c\u63a8\u7406\u5f00\u9500\u6700\u5c0f", "conclusion": "SDA\u6709\u6548\u89e3\u51b3\u4e86LVLMs\u5728\u63a8\u8350\u4e2d\u7684\u8868\u793a\u4e0d\u5bf9\u9f50\u548c\u68af\u5ea6\u51b2\u7a81\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u63a8\u8350\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u957f\u5c3e\u7269\u54c1\u4e0a\u8868\u73b0\u7a81\u51fa"}}
{"id": "2512.07000", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07000", "abs": "https://arxiv.org/abs/2512.07000", "authors": ["Abderaouf Bahi", "Ibtissem Gasmi"], "title": "Benchmarking Deep Neural Networks for Modern Recommendation Systems", "comment": null, "summary": "This paper examines the deployment of seven different neural network architectures CNN, RNN, GNN, Autoencoder, Transformer, NCF, and Siamese Networks on three distinct datasets: Retail E-commerce, Amazon Products, and Netflix Prize. It evaluates their effectiveness through metrics such as accuracy, recall, F1-score, and diversity in recommendations. The results demonstrate that GNNs are particularly adept at managing complex item relationships in e-commerce environments, whereas RNNs are effective in capturing the temporal dynamics that are essential for platforms such as Netflix.. Siamese Networks are emphasized for their contribution to the diversification of recommendations, particularly in retail settings. Despite their benefits, issues like computational demands, reliance on extensive data, and the challenge of balancing accurate and diverse recommendations are addressed. The study seeks to inform the advancement of recommendation systems by suggesting hybrid methods that merge the strengths of various models to better satisfy user preferences and accommodate the evolving demands of contemporary digital platforms.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u4e03\u79cd\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u5728\u4e09\u4e2a\u63a8\u8350\u7cfb\u7edf\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0GNN\u64c5\u957f\u5904\u7406\u7535\u5546\u590d\u6742\u5173\u7cfb\uff0cRNN\u9002\u5408\u65f6\u5e8f\u52a8\u6001\uff0cSiamese\u7f51\u7edc\u80fd\u63d0\u5347\u63a8\u8350\u591a\u6837\u6027\uff0c\u5efa\u8bae\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u5e73\u8861\u51c6\u786e\u6027\u4e0e\u591a\u6837\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u6548\u679c\uff0c\u4e3a\u5f00\u53d1\u66f4\u6709\u6548\u7684\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u6307\u5bfc\uff0c\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5728\u51c6\u786e\u6027\u3001\u591a\u6837\u6027\u3001\u8ba1\u7b97\u6548\u7387\u7b49\u65b9\u9762\u7684\u5e73\u8861\u95ee\u9898\u3002", "method": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\uff08\u96f6\u552e\u7535\u5546\u3001\u4e9a\u9a6c\u900a\u4ea7\u54c1\u3001Netflix Prize\uff09\u4e0a\u90e8\u7f72\u4e03\u79cd\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff08CNN\u3001RNN\u3001GNN\u3001Autoencoder\u3001Transformer\u3001NCF\u3001Siamese Networks\uff09\uff0c\u4f7f\u7528\u51c6\u786e\u7387\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\u548c\u591a\u6837\u6027\u7b49\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "GNN\u5728\u7535\u5546\u73af\u5883\u4e2d\u5904\u7406\u590d\u6742\u7269\u54c1\u5173\u7cfb\u8868\u73b0\u6700\u4f73\uff1bRNN\u5728Netflix\u7b49\u5e73\u53f0\u7684\u65f6\u5e8f\u52a8\u6001\u6355\u6349\u4e2d\u6548\u679c\u663e\u8457\uff1bSiamese\u7f51\u7edc\u5728\u96f6\u552e\u573a\u666f\u4e2d\u80fd\u6709\u6548\u63d0\u5347\u63a8\u8350\u591a\u6837\u6027\u3002\u6240\u6709\u6a21\u578b\u90fd\u9762\u4e34\u8ba1\u7b97\u9700\u6c42\u9ad8\u3001\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u3001\u51c6\u786e\u6027\u4e0e\u591a\u6837\u6027\u96be\u4ee5\u5e73\u8861\u7b49\u95ee\u9898\u3002", "conclusion": "\u5efa\u8bae\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u7ed3\u5408\u4e0d\u540c\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u4ee5\u66f4\u597d\u5730\u6ee1\u8db3\u7528\u6237\u504f\u597d\u548c\u9002\u5e94\u73b0\u4ee3\u6570\u5b57\u5e73\u53f0\u7684\u6f14\u8fdb\u9700\u6c42\uff0c\u63a8\u52a8\u63a8\u8350\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.07216", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07216", "abs": "https://arxiv.org/abs/2512.07216", "authors": ["Bin Wu", "Feifan Yang", "Zhangming Chan", "Yu-Ran Gu", "Jiawei Feng", "Chao Yi", "Xiang-Rong Sheng", "Han Zhu", "Jian Xu", "Mang Ye", "Bo Zheng"], "title": "MUSE: A Simple Yet Effective Multimodal Search-Based Framework for Lifelong User Interest Modeling", "comment": null, "summary": "Lifelong user interest modeling is crucial for industrial recommender systems, yet existing approaches rely predominantly on ID-based features, suffering from poor generalization on long-tail items and limited semantic expressiveness. While recent work explores multimodal representations for behavior retrieval in the General Search Unit (GSU), they often neglect multimodal integration in the fine-grained modeling stage -- the Exact Search Unit (ESU). In this work, we present a systematic analysis of how to effectively leverage multimodal signals across both stages of the two-stage lifelong modeling framework. Our key insight is that simplicity suffices in the GSU: lightweight cosine similarity with high-quality multimodal embeddings outperforms complex retrieval mechanisms. In contrast, the ESU demands richer multimodal sequence modeling and effective ID-multimodal fusion to unlock its full potential. Guided by these principles, we propose MUSE, a simple yet effective multimodal search-based framework. MUSE has been deployed in Taobao display advertising system, enabling 100K-length user behavior sequence modeling and delivering significant gains in top-line metrics with negligible online latency overhead. To foster community research, we share industrial deployment practices and open-source the first large-scale dataset featuring ultra-long behavior sequences paired with high-quality multimodal embeddings. Our code and data is available at https://taobao-mm.github.io.", "AI": {"tldr": "MUSE\u662f\u4e00\u4e2a\u7528\u4e8e\u6dd8\u5b9d\u5c55\u793a\u5e7f\u544a\u7cfb\u7edf\u7684\u591a\u6a21\u6001\u641c\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u7b80\u5355\u9ad8\u6548\u7684\u68c0\u7d22\u673a\u5236\u5904\u7406\u8d85\u957f\u7528\u6237\u884c\u4e3a\u5e8f\u5217\uff0c\u5728\u5de5\u4e1a\u90e8\u7f72\u4e2d\u663e\u8457\u63d0\u5347\u63a8\u8350\u6548\u679c", "motivation": "\u73b0\u6709\u7ec8\u8eab\u7528\u6237\u5174\u8da3\u5efa\u6a21\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56ID\u7279\u5f81\uff0c\u5728\u957f\u5c3e\u9879\u76ee\u4e0a\u6cdb\u5316\u80fd\u529b\u5dee\u4e14\u8bed\u4e49\u8868\u8fbe\u80fd\u529b\u6709\u9650\u3002\u867d\u7136\u6700\u8fd1\u5de5\u4f5c\u63a2\u7d22\u4e86\u5728\u901a\u7528\u641c\u7d22\u5355\u5143(GSU)\u4e2d\u4f7f\u7528\u591a\u6a21\u6001\u8868\u793a\u8fdb\u884c\u884c\u4e3a\u68c0\u7d22\uff0c\u4f46\u5f80\u5f80\u5ffd\u7565\u4e86\u5728\u7cbe\u786e\u641c\u7d22\u5355\u5143(ESU)\u7684\u7ec6\u7c92\u5ea6\u5efa\u6a21\u9636\u6bb5\u7684\u591a\u6a21\u6001\u6574\u5408", "method": "\u63d0\u51faMUSE\u591a\u6a21\u6001\u641c\u7d22\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u5728GSU\u9636\u6bb5\u4f7f\u7528\u8f7b\u91cf\u7ea7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u4e0e\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u5d4c\u5165\u8fdb\u884c\u68c0\u7d22\uff1b\u5728ESU\u9636\u6bb5\u8fdb\u884c\u4e30\u5bcc\u7684\u591a\u6a21\u6001\u5e8f\u5217\u5efa\u6a21\u548c\u6709\u6548\u7684ID-\u591a\u6a21\u6001\u878d\u5408\u3002\u652f\u630110\u4e07\u957f\u5ea6\u7684\u7528\u6237\u884c\u4e3a\u5e8f\u5217\u5efa\u6a21", "result": "\u5df2\u5728\u6dd8\u5b9d\u5c55\u793a\u5e7f\u544a\u7cfb\u7edf\u90e8\u7f72\uff0c\u80fd\u591f\u5904\u740610\u4e07\u957f\u5ea6\u7684\u7528\u6237\u884c\u4e3a\u5e8f\u5217\uff0c\u5728\u5173\u952e\u6307\u6807\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u4e14\u5728\u7ebf\u5ef6\u8fdf\u5f00\u9500\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002\u540c\u65f6\u5f00\u6e90\u4e86\u9996\u4e2a\u5305\u542b\u8d85\u957f\u884c\u4e3a\u5e8f\u5217\u548c\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u5d4c\u5165\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u53d1\u73b0\uff0c\u5728GSU\u9636\u6bb5\u7b80\u5355\u6027\u8db3\u591f\uff0c\u800cESU\u9636\u6bb5\u9700\u8981\u4e30\u5bcc\u7684\u591a\u6a21\u6001\u5e8f\u5217\u5efa\u6a21\u548c\u878d\u5408\u3002MUSE\u6846\u67b6\u7b80\u5355\u6709\u6548\uff0c\u5df2\u5728\u5de5\u4e1a\u573a\u666f\u6210\u529f\u90e8\u7f72\uff0c\u4e3a\u793e\u533a\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u548c\u5b9e\u8df5\u7ecf\u9a8c"}}
{"id": "2512.07384", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.07384", "abs": "https://arxiv.org/abs/2512.07384", "authors": ["Daniele Malitesta", "Claudio Pomo", "Vito Walter Anelli", "Alberto Carlo Maria Mancino", "Alejandro Bellog\u00edn", "Tommaso Di Noia"], "title": "On the Impact of Graph Neural Networks in Recommender Systems: A Topological Perspective", "comment": null, "summary": "In recommender systems, user-item interactions can be modeled as a bipartite graph, where user and item nodes are connected by undirected edges. This graph-based view has motivated the rapid adoption of graph neural networks (GNNs), which often outperform collaborative filtering (CF) methods such as latent factor models, deep neural networks, and generative strategies. Yet, despite their empirical success, the reasons why GNNs offer systematic advantages over other CF approaches remain only partially understood. This monograph advances a topology-centered perspective on GNN-based recommendation. We argue that a comprehensive understanding of these models' performance should consider the structural properties of user-item graphs and their interaction with GNN architectural design. To support this view, we introduce a formal taxonomy that distills common modeling patterns across eleven representative GNN-based recommendation approaches and consolidates them into a unified conceptual pipeline. We further formalize thirteen classical and topological characteristics of recommendation datasets and reinterpret them through the lens of graph machine learning. Using these definitions, we analyze the considered GNN-based recommender architectures to assess how and to what extent they encode such properties. Building on this analysis, we derive an explanatory framework that links measurable dataset characteristics to model behavior and performance. Taken together, this monograph re-frames GNN-based recommendation through its topological underpinnings and outlines open theoretical, data-centric, and evaluation challenges for the next generation of topology-aware recommender systems.", "AI": {"tldr": "\u8be5\u4e13\u8457\u4ece\u62d3\u6251\u5b66\u89d2\u5ea6\u5206\u6790GNN\u63a8\u8350\u7cfb\u7edf\uff0c\u63d0\u51fa\u7edf\u4e00\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u5c06\u6570\u636e\u96c6\u7279\u5f81\u4e0e\u6a21\u578b\u6027\u80fd\u8054\u7cfb\u8d77\u6765\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u62d3\u6251\u611f\u77e5\u63a8\u8350\u7cfb\u7edf\u6307\u660e\u65b9\u5411\u3002", "motivation": "\u5c3d\u7ba1GNN\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u76f8\u5bf9\u4e8e\u4f20\u7edf\u534f\u540c\u8fc7\u6ee4\u65b9\u6cd5\u7684\u7cfb\u7edf\u4f18\u52bf\u539f\u56e0\u5c1a\u4e0d\u5b8c\u5168\u6e05\u695a\u3002\u4f5c\u8005\u8ba4\u4e3a\u9700\u8981\u4ece\u7528\u6237-\u7269\u54c1\u56fe\u7684\u7ed3\u6784\u7279\u6027\u53ca\u5176\u4e0eGNN\u67b6\u6784\u8bbe\u8ba1\u7684\u4ea4\u4e92\u89d2\u5ea6\u6765\u5168\u9762\u7406\u89e3\u8fd9\u4e9b\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "1) \u63d0\u51fa\u4ee5\u62d3\u6251\u4e3a\u4e2d\u5fc3\u7684\u5206\u6790\u89c6\u89d2\uff1b2) \u5efa\u7acb\u5f62\u5f0f\u5316\u5206\u7c7b\u6cd5\uff0c\u5c0611\u79cd\u4ee3\u8868\u6027GNN\u63a8\u8350\u65b9\u6cd5\u7edf\u4e00\u4e3a\u6982\u5ff5\u7ba1\u9053\uff1b3) \u5f62\u5f0f\u531613\u4e2a\u7ecf\u5178\u548c\u62d3\u6251\u6570\u636e\u96c6\u7279\u5f81\uff1b4) \u5206\u6790GNN\u63a8\u8350\u67b6\u6784\u5982\u4f55\u7f16\u7801\u8fd9\u4e9b\u7279\u6027\uff1b5) \u6784\u5efa\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u5c06\u53ef\u6d4b\u91cf\u7684\u6570\u636e\u96c6\u7279\u5f81\u4e0e\u6a21\u578b\u884c\u4e3a\u548c\u6027\u80fd\u8054\u7cfb\u8d77\u6765\u3002", "result": "\u5efa\u7acb\u4e86GNN\u63a8\u8350\u7cfb\u7edf\u7684\u62d3\u6251\u5b66\u5206\u6790\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u6570\u636e\u96c6\u7ed3\u6784\u7279\u6027\u4e0e\u6a21\u578b\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4e3a\u7406\u89e3GNN\u63a8\u8350\u4f18\u52bf\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u91ca\u3002", "conclusion": "\u8be5\u4e13\u8457\u901a\u8fc7\u62d3\u6251\u5b66\u57fa\u7840\u91cd\u65b0\u6784\u5efa\u4e86GNN\u63a8\u8350\u7cfb\u7edf\uff0c\u5e76\u6307\u51fa\u4e86\u4e0b\u4e00\u4ee3\u62d3\u6251\u611f\u77e5\u63a8\u8350\u7cfb\u7edf\u5728\u7406\u8bba\u3001\u6570\u636e\u4e2d\u5fc3\u548c\u8bc4\u4f30\u65b9\u9762\u7684\u5f00\u653e\u6311\u6218\u3002"}}
{"id": "2512.07424", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.07424", "abs": "https://arxiv.org/abs/2512.07424", "authors": ["Jiangxia Cao", "Shuo Yang", "Zijun Wang", "Qinghai Tan"], "title": "OnePiece: The Great Route to Generative Recommendation -- A Case Study from Tencent Algorithm Competition", "comment": "Work in progress", "summary": "In past years, the OpenAI's Scaling-Laws shows the amazing intelligence with the next-token prediction paradigm in neural language modeling, which pointing out a free-lunch way to enhance the model performance by scaling the model parameters. In RecSys, the retrieval stage is also follows a 'next-token prediction' paradigm, to recall the hunderds of items from the global item set, thus the generative recommendation usually refers specifically to the retrieval stage (without Tree-based methods). This raises a philosophical question: without a ground-truth next item, does the generative recommendation also holds a potential scaling law? In retrospect, the generative recommendation has two different technique paradigms: (1) ANN-based framework, utilizing the compressed user embedding to retrieve nearest other items in embedding space, e.g, Kuaiformer. (2) Auto-regressive-based framework, employing the beam search to decode the item from whole space, e.g, OneRec. In this paper, we devise a unified encoder-decoder framework to validate their scaling-laws at same time. Our empirical finding is that both of their losses strictly adhere to power-law Scaling Laws ($R^2$>0.9) within our unified architecture.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u751f\u6210\u5f0f\u63a8\u8350\u7cfb\u7edf\u662f\u5426\u9075\u5faa\u7f29\u653e\u5b9a\u5f8b\uff0c\u901a\u8fc7\u7edf\u4e00\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6846\u67b6\u9a8c\u8bc1ANN\u548c\u81ea\u56de\u5f52\u4e24\u79cd\u8303\u5f0f\u7684\u7f29\u653e\u89c4\u5f8b\uff0c\u53d1\u73b0\u4e24\u8005\u635f\u5931\u90fd\u4e25\u683c\u9075\u5faa\u5e42\u5f8b\u7f29\u653e\u5b9a\u5f8b\u3002", "motivation": "\u53d7\u5230OpenAI\u7f29\u653e\u5b9a\u5f8b\u5728\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u542f\u53d1\uff0c\u7814\u7a76\u8005\u60f3\u77e5\u9053\u751f\u6210\u5f0f\u63a8\u8350\u7cfb\u7edf\u662f\u5426\u4e5f\u5b58\u5728\u7c7b\u4f3c\u7684\u7f29\u653e\u5b9a\u5f8b\u3002\u751f\u6210\u5f0f\u63a8\u8350\u901a\u5e38\u6307\u68c0\u7d22\u9636\u6bb5\uff0c\u4f46\u6ca1\u6709\u771f\u5b9e\u7684\u4e0b\u4e00\u4e2a\u7269\u54c1\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\uff0c\u8fd9\u5f15\u53d1\u4e86\u4e00\u4e2a\u54f2\u5b66\u95ee\u9898\uff1a\u751f\u6210\u5f0f\u63a8\u8350\u662f\u5426\u4e5f\u5b58\u5728\u6f5c\u5728\u7684\u7f29\u653e\u5b9a\u5f8b\uff1f", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6846\u67b6\uff0c\u540c\u65f6\u9a8c\u8bc1\u4e24\u79cd\u751f\u6210\u5f0f\u63a8\u8350\u6280\u672f\u8303\u5f0f\uff1a1) ANN-based\u6846\u67b6\uff08\u4f7f\u7528\u538b\u7f29\u7528\u6237\u5d4c\u5165\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u68c0\u7d22\u6700\u8fd1\u90bb\u7269\u54c1\uff09\uff1b2) \u81ea\u56de\u5f52\u6846\u67b6\uff08\u4f7f\u7528beam search\u4ece\u6574\u4e2a\u7a7a\u95f4\u89e3\u7801\u7269\u54c1\uff09\u3002\u5728\u8be5\u7edf\u4e00\u67b6\u6784\u4e0b\u9a8c\u8bc1\u5b83\u4eec\u7684\u7f29\u653e\u5b9a\u5f8b\u3002", "result": "\u5b9e\u8bc1\u53d1\u73b0\u4e24\u79cd\u8303\u5f0f\u7684\u635f\u5931\u90fd\u4e25\u683c\u9075\u5faa\u5e42\u5f8b\u7f29\u653e\u5b9a\u5f8b\uff08R\u00b2>0.9\uff09\uff0c\u8868\u660e\u751f\u6210\u5f0f\u63a8\u8350\u7cfb\u7edf\u786e\u5b9e\u5b58\u5728\u7f29\u653e\u5b9a\u5f8b\u3002", "conclusion": "\u751f\u6210\u5f0f\u63a8\u8350\u7cfb\u7edf\u786e\u5b9e\u5b58\u5728\u7f29\u653e\u5b9a\u5f8b\uff0cANN\u548c\u81ea\u56de\u5f52\u4e24\u79cd\u8303\u5f0f\u5728\u7edf\u4e00\u67b6\u6784\u4e0b\u90fd\u8868\u73b0\u51fa\u4e25\u683c\u7684\u5e42\u5f8b\u7f29\u653e\u5173\u7cfb\uff0c\u8fd9\u4e3a\u63a8\u8350\u7cfb\u7edf\u7684\u89c4\u6a21\u5316\u53d1\u5c55\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2512.07452", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.07452", "abs": "https://arxiv.org/abs/2512.07452", "authors": ["Clarisse Bardiot", "Pierre-Carl Langlais", "Bernard Jacquemin", "Jacob Hart", "Antonios Lagarias", "Nicolas Foucault", "Aur\u00e9lie Lema\u00eetre-Legargeant", "Jeanne Fras"], "title": "From Show Programmes to Data: Designing a Workflow to Make Performing Arts Ephemera Accessible Through Language Models", "comment": "19 pages, 8 figures, 5 tables, 17 references", "summary": "Many heritage institutions hold extensive collections of theatre programmes, which remain largely underused due to their complex layouts and lack of structured metadata. In this paper, we present a workflow for transforming such documents into structured data using a combination of multimodal large language models (LLMs), an ontology-based reasoning model, and a custom extension of the Linked Art framework. We show how vision-language models can accurately parse and transcribe born-digital and digitised programmes, achieving over 98% of correct extraction. To overcome the challenges of semantic annotation, we train a reasoning model (POntAvignon) using reinforcement learning with both formal and semantic rewards. This approach enables automated RDF triple generation and supports alignment with existing knowledge graphs. Through a case study based on the Festival d'Avignon corpus, we demonstrate the potential for large-scale, ontology-driven analysis of performing arts data. Our results open new possibilities for interoperable, explainable, and sustainable computational theatre historiography.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3001\u672c\u4f53\u63a8\u7406\u6a21\u578b\u548cLinked Art\u6846\u67b6\u7684\u5de5\u4f5c\u6d41\uff0c\u5c06\u5267\u9662\u8282\u76ee\u5355\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u6570\u636e\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u3001\u53ef\u4e92\u64cd\u4f5c\u7684\u8868\u6f14\u827a\u672f\u6570\u636e\u5206\u6790\u3002", "motivation": "\u8bb8\u591a\u6587\u5316\u9057\u4ea7\u673a\u6784\u6536\u85cf\u4e86\u5927\u91cf\u5267\u9662\u8282\u76ee\u5355\uff0c\u4f46\u7531\u4e8e\u5176\u590d\u6742\u5e03\u5c40\u548c\u7f3a\u4e4f\u7ed3\u6784\u5316\u5143\u6570\u636e\uff0c\u8fd9\u4e9b\u8d44\u6e90\u5927\u591a\u672a\u88ab\u5145\u5206\u5229\u7528\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5c06\u8fd9\u4e9b\u6587\u6863\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u6570\u636e\uff0c\u4ee5\u652f\u6301\u5927\u89c4\u6a21\u8868\u6f14\u827a\u672f\u6570\u636e\u5206\u6790\u3002", "method": "1) \u4f7f\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u51c6\u786e\u89e3\u6790\u548c\u8f6c\u5f55\u6570\u5b57\u539f\u751f\u548c\u6570\u5b57\u5316\u8282\u76ee\u5355\uff1b2) \u8bad\u7ec3\u57fa\u4e8e\u672c\u4f53\u7684\u63a8\u7406\u6a21\u578b(POntAvignon)\uff0c\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7ed3\u5408\u5f62\u5f0f\u5316\u548c\u8bed\u4e49\u5316\u5956\u52b1\uff1b3) \u6269\u5c55Linked Art\u6846\u67b6\uff0c\u5b9e\u73b0\u81ea\u52a8RDF\u4e09\u5143\u7ec4\u751f\u6210\u5e76\u4e0e\u73b0\u6709\u77e5\u8bc6\u56fe\u8c31\u5bf9\u9f50\u3002", "result": "\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u4e86\u8d85\u8fc798%\u7684\u6b63\u786e\u63d0\u53d6\u7387\u3002\u901a\u8fc7\u963f\u7ef4\u5c3c\u7fc1\u827a\u672f\u8282\u8bed\u6599\u5e93\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u5927\u89c4\u6a21\u3001\u672c\u4f53\u9a71\u52a8\u7684\u8868\u6f14\u827a\u672f\u6570\u636e\u5206\u6790\u6f5c\u529b\u3002\u8be5\u65b9\u6cd5\u652f\u6301\u53ef\u4e92\u64cd\u4f5c\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u6301\u7eed\u7684\u8ba1\u7b97\u620f\u5267\u53f2\u5b66\u3002", "conclusion": "\u63d0\u51fa\u7684\u5de5\u4f5c\u6d41\u6210\u529f\u5c06\u590d\u6742\u5267\u9662\u8282\u76ee\u5355\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u6570\u636e\uff0c\u4e3a\u6587\u5316\u9057\u4ea7\u673a\u6784\u63d0\u4f9b\u4e86\u65b0\u7684\u5206\u6790\u53ef\u80fd\u6027\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u591a\u6a21\u6001LLM\u3001\u672c\u4f53\u63a8\u7406\u548cLinked Art\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u7684\u8868\u6f14\u827a\u672f\u6570\u636e\u8f6c\u6362\uff0c\u4e3a\u8ba1\u7b97\u620f\u5267\u53f2\u5b66\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2512.07650", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07650", "abs": "https://arxiv.org/abs/2512.07650", "authors": ["Fuyuan Lyu", "Zhentai Chen", "Jingyan Jiang", "Lingjie Li", "Xing Tang", "Xiuqiang He", "Xue Liu"], "title": "Exploring Test-time Scaling via Prediction Merging on Large-Scale Recommendation", "comment": null, "summary": "Inspired by the success of language models (LM), scaling up deep learning recommendation systems (DLRS) has become a recent trend in the community. All previous methods tend to scale up the model parameters during training time. However, how to efficiently utilize and scale up computational resources during test time remains underexplored, which can prove to be a scaling-efficient approach and bring orthogonal improvements in LM domains. The key point in applying test-time scaling to DLRS lies in effectively generating diverse yet meaningful outputs for the same instance. We propose two ways: One is to explore the heterogeneity of different model architectures. The other is to utilize the randomness of model initialization under a homogeneous architecture. The evaluation is conducted across eight models, including both classic and SOTA models, on three benchmarks. Sufficient evidence proves the effectiveness of both solutions. We further prove that under the same inference budget, test-time scaling can outperform parameter scaling. Our test-time scaling can also be seamlessly accelerated with the increase in parallel servers when deployed online, without affecting the inference time on the user side. Code is available.", "AI": {"tldr": "\u63d0\u51fa\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u578b\u67b6\u6784\u5f02\u8d28\u6027\u6216\u540c\u6784\u6a21\u578b\u968f\u673a\u521d\u59cb\u5316\u751f\u6210\u591a\u6837\u5316\u8f93\u51fa\uff0c\u5728\u76f8\u540c\u63a8\u7406\u9884\u7b97\u4e0b\u4f18\u4e8e\u53c2\u6570\u6269\u5c55\uff0c\u5e76\u53ef\u65e0\u7f1d\u52a0\u901f\u90e8\u7f72", "motivation": "\u867d\u7136\u8bed\u8a00\u6a21\u578b\u6210\u529f\u63a8\u52a8\u4e86\u63a8\u8350\u7cfb\u7edf\u7684\u6269\u5c55\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8bad\u7ec3\u65f6\u53c2\u6570\u6269\u5c55\uff0c\u6d4b\u8bd5\u65f6\u5982\u4f55\u9ad8\u6548\u5229\u7528\u8ba1\u7b97\u8d44\u6e90\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u6d4b\u8bd5\u65f6\u6269\u5c55\u53ef\u4f5c\u4e3a\u6b63\u4ea4\u6539\u8fdb\u65b9\u5411\uff0c\u4e3a\u63a8\u8350\u7cfb\u7edf\u5e26\u6765\u65b0\u7684\u6269\u5c55\u6548\u7387\u63d0\u5347", "method": "\u63d0\u51fa\u4e24\u79cd\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\uff1a1) \u5229\u7528\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u7684\u5f02\u8d28\u6027\u751f\u6210\u591a\u6837\u5316\u8f93\u51fa\uff1b2) \u5728\u540c\u6784\u67b6\u6784\u4e0b\u5229\u7528\u6a21\u578b\u521d\u59cb\u5316\u7684\u968f\u673a\u6027\u751f\u6210\u591a\u6837\u5316\u8f93\u51fa\u3002\u57288\u4e2a\u6a21\u578b\uff08\u5305\u62ec\u7ecf\u5178\u548cSOTA\u6a21\u578b\uff09\u548c3\u4e2a\u57fa\u51c6\u4e0a\u8fdb\u884c\u8bc4\u4f30", "result": "\u4e24\u79cd\u89e3\u51b3\u65b9\u6848\u5747\u88ab\u8bc1\u660e\u6709\u6548\u3002\u5728\u76f8\u540c\u63a8\u7406\u9884\u7b97\u4e0b\uff0c\u6d4b\u8bd5\u65f6\u6269\u5c55\u80fd\u591f\u8d85\u8d8a\u53c2\u6570\u6269\u5c55\u3002\u6d4b\u8bd5\u65f6\u6269\u5c55\u8fd8\u53ef\u968f\u7740\u5e76\u884c\u670d\u52a1\u5668\u589e\u52a0\u800c\u65e0\u7f1d\u52a0\u901f\uff0c\u4e14\u4e0d\u5f71\u54cd\u7528\u6237\u7aef\u63a8\u7406\u65f6\u95f4", "conclusion": "\u6d4b\u8bd5\u65f6\u6269\u5c55\u662f\u63a8\u8350\u7cfb\u7edf\u6269\u5c55\u7684\u6709\u6548\u6b63\u4ea4\u65b9\u6cd5\uff0c\u80fd\u591f\u9ad8\u6548\u5229\u7528\u8ba1\u7b97\u8d44\u6e90\uff0c\u5728\u76f8\u540c\u9884\u7b97\u4e0b\u4f18\u4e8e\u4f20\u7edf\u53c2\u6570\u6269\u5c55\uff0c\u5e76\u652f\u6301\u5728\u7ebf\u90e8\u7f72\u65f6\u7684\u65e0\u7f1d\u52a0\u901f"}}
