<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 4]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [δ-EMG: A Monotonic Graph Index for Approximate Nearest Neighbor Search](https://arxiv.org/abs/2511.16921)
*Liming Xiang,Jing Feng,Ziqi Yin,Zijian Li,Daihao Xue,Hongchao Qin,Ronghua Li,Guoren Wang*

Main category: cs.IR

TL;DR: 提出了一种误差有界的近似最近邻搜索方法，通过δ-EMG图结构确保返回结果是真实值的(1/δ)近似，在保持高召回率的同时提供理论保证。


<details>
  <summary>Details</summary>
Motivation: 现有近似最近邻搜索方法主要基于ε-召回率原则，只关注正确结果的召回率，无法保证错误结果的偏差范围。需要一种能提供近似误差保证的方法。

Method: 采用基于图的框架，提出δ-EMG（误差有界单调图），在构建图时强制执行δ-单调几何约束，确保贪心搜索收敛到(1/δ)近似邻居。进一步设计误差有界top-k搜索算法和可扩展的δ-EMQG变体。

Result: 在ANN-Benchmarks数据集上验证了方法的有效性。在召回率要求0.99时，SIFT1M数据集上达到19,000 QPS，比其他方法提升40%以上。

Conclusion: 提出的误差有界近似最近邻搜索方法不仅提供理论保证，在实际性能上也显著优于现有方法，为高维检索系统提供了更可靠的解决方案。

Abstract: Approximate nearest neighbor (ANN) search in high-dimensional spaces is a foundational component of many modern retrieval and recommendation systems. Currently, almost all algorithms follow an $ε$-Recall-Bounded principle when comparing performance: they require the ANN search results to achieve a recall of more than $1-ε$ and then compare query-per-second (QPS) performance. However, this approach only accounts for the recall of true positive results and does not provide guarantees on the deviation of incorrect results. To address this limitation, we focus on an Error-Bounded ANN method, which ensures that the returned results are a $(1/δ)$-approximation of the true values. Our approach adopts a graph-based framework. To enable Error-Bounded ANN search, we propose a $δ$-EMG (Error-bounded Monotonic Graph), which, for the first time, provides a provable approximation for arbitrary queries. By enforcing a $δ$-monotonic geometric constraint during graph construction, $δ$-EMG ensures that any greedy search converges to a $(1/δ)$-approximate neighbor without backtracking. Building on this foundation, we design an error-bounded top-$k$ ANN search algorithm that adaptively controls approximation accuracy during query time. To make the framework practical at scale, we introduce $δ$-EMQG (Error-bounded Monotonic Quantized Graph), a localized and degree-balanced variant with near-linear construction complexity. We further integrate vector quantization to accelerate distance computation while preserving theoretical guarantees. Extensive experiments on the ANN-Benchmarks dataset demonstrate the effectiveness of our approach. Under a recall requirement of 0.99, our algorithm achieves 19,000 QPS on the SIFT1M dataset, outperforming other methods by more than 40\%.

</details>


### [2] [RASTP: Representation-Aware Semantic Token Pruning for Generative Recommendation with Semantic Identifiers](https://arxiv.org/abs/2511.16943)
*Tianyu Zhan,Kairui Fu,Zheqi Lv,Shengyu Zhang*

Main category: cs.IR

TL;DR: RASTP是一种表示感知的语义令牌剪枝方法，通过动态剪枝输入序列中信息量较低的语义令牌，显著减少训练时间和内存消耗，同时保持推荐性能。


<details>
  <summary>Details</summary>
Motivation: 生成式推荐系统使用语义标识符(SIDs)表示物品，但多个SIDs会显著增加输入序列长度，导致计算复杂度和内存消耗增加。现有方法主要优化注意力计算和KV缓存，但直接剪枝输入序列中的低信息令牌更有效。

Method: 提出RASTP方法，通过结合表示幅度衡量的语义显著性和累积注意力权重得出的注意力中心性来评估令牌重要性，动态剪枝低信息或无关的语义令牌。

Result: 在三个真实世界的Amazon数据集上的实验表明，RASTP将训练时间减少了26.7%，同时保持或略微提高了推荐性能。

Conclusion: RASTP通过直接剪枝输入序列中的低信息语义令牌，有效解决了生成式推荐系统中序列长度过长的问题，在显著减少计算成本的同时保持了模型性能。

Abstract: Generative recommendation systems typically leverage Semantic Identifiers (SIDs), which represent each item as a sequence of tokens that encode semantic information. However, representing item ID with multiple SIDs significantly increases input sequence length, which is a major determinant of computational complexity and memory consumption. While existing efforts primarily focus on optimizing attention computation and KV cache, we propose RASTP (Representation-Aware Semantic Token Pruning), which directly prunes less informative tokens in the input sequence. Specifically, RASTP evaluates token importance by combining semantic saliency, measured via representation magnitude, and attention centrality, derived from cumulative attention weights. Since RASTP dynamically prunes low-information or irrelevant semantic tokens, experiments on three real-world Amazon datasets show that RASTP reduces training time by 26.7\%, while maintaining or slightly improving recommendation performance. The code has been open-sourced at https://github.com/Yuzt-zju/RASTP.

</details>


### [3] [CLLMRec: LLM-powered Cognitive-Aware Concept Recommendation via Semantic Alignment and Prerequisite Knowledge Distillation](https://arxiv.org/abs/2511.17041)
*Xiangrui Xiong,Yichuan Lu,Zifei Pan,Chang Sun*

Main category: cs.IR

TL;DR: 提出CLLMRec框架，利用大语言模型解决MOOC概念推荐问题，无需依赖结构化知识图谱，通过语义对齐和先验知识蒸馏实现个性化推荐。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖高质量结构化知识图谱，但在真实教育场景中这类图谱往往稀缺，限制了概念推荐的效果。

Method: 采用双技术支柱：语义对齐构建统一表示空间，先验知识蒸馏使用教师-学生架构从LLM提取概念先决关系；结合深度知识追踪建模学习者实时认知状态。

Result: 在两个真实MOOC数据集上的实验表明，CLLMRec在多个评估指标上显著优于现有基线方法。

Conclusion: CLLMRec能够在不依赖显式结构先验的情况下，生成真正认知感知和个性化的概念推荐。

Abstract: The growth of Massive Open Online Courses (MOOCs) presents significant challenges for personalized learning, where concept recommendation is crucial. Existing approaches typically rely on heterogeneous information networks or knowledge graphs to capture conceptual relationships, combined with knowledge tracing models to assess learners' cognitive states. However, these methods face significant limitations due to their dependence on high-quality structured knowledge graphs, which are often scarce in real-world educational scenarios. To address this fundamental challenge, this paper proposes CLLMRec, a novel framework that leverages Large Language Models through two synergistic technical pillars: Semantic Alignment and Prerequisite Knowledge Distillation. The Semantic Alignment component constructs a unified representation space by encoding unstructured textual descriptions of learners and concepts. The Prerequisite Knowledge Distillation paradigm employs a teacher-student architecture, where a large teacher LLM (implemented as the Prior Knowledge Aware Component) extracts conceptual prerequisite relationships from its internalized world knowledge and distills them into soft labels to train an efficient student ranker. Building upon these foundations, our framework incorporates a fine-ranking mechanism that explicitly models learners' real-time cognitive states through deep knowledge tracing, ensuring recommendations are both structurally sound and cognitively appropriate. Extensive experiments on two real-world MOOC datasets demonstrate that CLLMRec significantly outperforms existing baseline methods across multiple evaluation metrics, validating its effectiveness in generating truly cognitive-aware and personalized concept recommendations without relying on explicit structural priors.

</details>


### [4] [Parametric Retrieval-Augmented Generation using Latent Routing of LoRA Adapters](https://arxiv.org/abs/2511.17044)
*Zhan Su,Fengran Mo,Jian-yun Nie*

Main category: cs.IR

TL;DR: Poly-PRAG是一种新的参数化检索增强生成方法，通过潜在路由编码过程解决传统PRAG的一对一文档编码方案的数据稀缺和高推理开销问题，在多个知识密集型NLP任务上取得最先进结果。


<details>
  <summary>Details</summary>
Motivation: 传统PRAG方法采用一对一文档编码方案，每个文档使用专用LoRA适配器，导致训练数据稀缺和推理时高计算开销。

Method: 提出潜在路由编码过程，将文档集编码视为多任务学习过程，使用少量潜在LoRA适配器编码整个文档空间，在线推理时根据查询选择性激活潜在专家子集。

Result: 在多个知识密集型NLP任务上进行了全面评估，在四个不同数据集上取得了最先进的结果。

Conclusion: Poly-PRAG通过潜在路由编码有效解决了传统PRAG方法的局限性，显著提升了参数化检索增强生成的效率和性能。

Abstract: Parametric Retrieval-Augmented Generation (PRAG) is a novel RAG paradigm that integrates external knowledge directly into a Large Language Model (LLM) by parameterizing documents using LoRA adapters, demonstrating reduced inference costs compared to traditional RAG approaches. However, current PRAG approaches adopt a \textbf{one-to-one} document encoding scheme, using a dedicated LoRA adapter for each individual document. This scheme introduces two major limitations: First, it leads to data scarcity, as the training datasets for individual LoRA adapters are limited. Second, it incurs high overhead during inference, requiring the merging of LLM weights with a new LoRA adapter for every candidate passage, which is computationally inefficient. To overcome these challenges, we propose a novel paradigm for encoding passages in PRAG that utilizes a latent routing encoding process (Poly-PRAG). During offline encoding, we treat the encoding of a set of documents as a multi-task learning process, where each passage is assigned a unique task identifier. By employing a routing function, we use a small set of latent LoRA adapters to encode the entire passage space. During online inference, this routing function selectively activates a subset of latent experts based on the input query. We conduct comprehensive evaluations of Poly-PRAG across multiple knowledge-intensive NLP tasks. Our extensive experiments demonstrate the effectiveness of the proposed method, achieving state-of-the-art results on four distinct datasets.

</details>
