{"id": "2601.11560", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11560", "abs": "https://arxiv.org/abs/2601.11560", "authors": ["Zifeng Wang", "Zheng Chen", "Ziwei Yang", "Xuan Wang", "Qiao Jin", "Yifan Peng", "Zhiyong Lu", "Jimeng Sun"], "title": "DeepEvidence: Empowering Biomedical Discovery with Deep Knowledge Graph Research", "comment": null, "summary": "Biomedical knowledge graphs (KGs) encode vast, heterogeneous information spanning literature, genes, pathways, drugs, diseases, and clinical trials, but leveraging them collectively for scientific discovery remains difficult. Their structural differences, continual evolution, and limited cross-resource alignment require substantial manual integration, limiting the depth and scale of knowledge exploration. We introduce DeepEvidence, an AI-agent framework designed to perform Deep Research across various heterogeneous biomedical KGs. Unlike generic Deep Research systems that rely primarily on internet-scale text, DeepEvidence incorporates specialized knowledge-graph tooling and coordinated exploration strategies to systematically bridge heterogeneous resources. At its core is an orchestrator that directs two complementary agents: Breadth-First ReSearch (BFRS) for broad, multi-graph entity search, and Depth-First ReSearch (DFRS) for multi-hop, evidence-focused reasoning. An internal, incrementally built evidence graph provides a structured record of retrieved entities, relations, and supporting evidence. To operate at scale, DeepEvidence includes unified interfaces for querying diverse biomedical APIs and an execution sandbox that enables programmatic data retrieval, extraction, and analysis. Across established deep-reasoning benchmarks and four key stages of the biomedical discovery lifecycle: drug discovery, pre-clinical experimentation, clinical trial development, and evidence-based medicine, DeepEvidence demonstrates substantial gains in systematic exploration and evidence synthesis. These results highlight the potential of knowledge-graph-driven Deep Research to accelerate biomedical discovery.", "AI": {"tldr": "DeepEvidence\u662f\u4e00\u4e2aAI\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u8c03\u5e7f\u5ea6\u4f18\u5148\u548c\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\u7b56\u7565\uff0c\u5728\u5f02\u6784\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u4e0a\u8fdb\u884c\u6df1\u5ea6\u7814\u7a76\uff0c\u52a0\u901f\u751f\u7269\u533b\u5b66\u53d1\u73b0\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u5305\u542b\u5927\u91cf\u5f02\u6784\u4fe1\u606f\uff0c\u4f46\u7531\u4e8e\u7ed3\u6784\u5dee\u5f02\u3001\u6301\u7eed\u6f14\u5316\u548c\u6709\u9650\u7684\u8de8\u8d44\u6e90\u5bf9\u9f50\uff0c\u9700\u8981\u5927\u91cf\u4eba\u5de5\u6574\u5408\uff0c\u9650\u5236\u4e86\u77e5\u8bc6\u63a2\u7d22\u7684\u6df1\u5ea6\u548c\u89c4\u6a21\u3002", "method": "\u91c7\u7528AI\u4ee3\u7406\u6846\u67b6\uff0c\u5305\u542b\u534f\u8c03\u5668\u3001\u5e7f\u5ea6\u4f18\u5148\u641c\u7d22\u4ee3\u7406\uff08BFRS\uff09\u548c\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\u4ee3\u7406\uff08DFRS\uff09\uff0c\u6784\u5efa\u589e\u91cf\u8bc1\u636e\u56fe\uff0c\u63d0\u4f9b\u7edf\u4e00\u63a5\u53e3\u67e5\u8be2\u591a\u6837\u751f\u7269\u533b\u5b66API\u548c\u6267\u884c\u6c99\u7bb1\u3002", "result": "\u5728\u6df1\u5ea6\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u548c\u751f\u7269\u533b\u5b66\u53d1\u73b0\u751f\u547d\u5468\u671f\u7684\u56db\u4e2a\u5173\u952e\u9636\u6bb5\uff08\u836f\u7269\u53d1\u73b0\u3001\u4e34\u5e8a\u524d\u5b9e\u9a8c\u3001\u4e34\u5e8a\u8bd5\u9a8c\u5f00\u53d1\u3001\u5faa\u8bc1\u533b\u5b66\uff09\u4e2d\uff0cDeepEvidence\u5728\u7cfb\u7edf\u63a2\u7d22\u548c\u8bc1\u636e\u5408\u6210\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u77e5\u8bc6\u56fe\u8c31\u9a71\u52a8\u7684\u6df1\u5ea6\u7814\u7a76\u6709\u6f5c\u529b\u52a0\u901f\u751f\u7269\u533b\u5b66\u53d1\u73b0\uff0cDeepEvidence\u6846\u67b6\u5c55\u793a\u4e86\u5728\u5f02\u6784\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u4e0a\u8fdb\u884c\u7cfb\u7edf\u63a2\u7d22\u548c\u8bc1\u636e\u5408\u6210\u7684\u80fd\u529b\u3002"}}
{"id": "2601.11863", "categories": ["cs.IR", "cs.AI", "cs.CE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11863", "abs": "https://arxiv.org/abs/2601.11863", "authors": ["Raquib Bin Yousuf", "Shengzhe Xu", "Mandar Sharma", "Andrew Neeser", "Chris Latimer", "Naren Ramakrishnan"], "title": "Utilizing Metadata for Better Retrieval-Augmented Generation", "comment": "The 48th European Conference on Information Retrieval (ECIR 2026)", "summary": "Retrieval-Augmented Generation systems depend on retrieving semantically relevant document chunks to support accurate, grounded outputs from large language models. In structured and repetitive corpora such as regulatory filings, chunk similarity alone often fails to distinguish between documents with overlapping language. Practitioners often flatten metadata into input text as a heuristic, but the impact and trade-offs of this practice remain poorly understood. We present a systematic study of metadata-aware retrieval strategies, comparing plain-text baselines with approaches that embed metadata directly. Our evaluation spans metadata-as-text (prefix and suffix), a dual-encoder unified embedding that fuses metadata and content in a single index, dual-encoder late-fusion retrieval, and metadata-aware query reformulation. Across multiple retrieval metrics and question types, we find that prefixing and unified embeddings consistently outperform plain-text baselines, with the unified at times exceeding prefixing while being easier to maintain. Beyond empirical comparisons, we analyze embedding space, showing that metadata integration improves effectiveness by increasing intra-document cohesion, reducing inter-document confusion, and widening the separation between relevant and irrelevant chunks. Field-level ablations show that structural cues provide strong disambiguating signals. Our code, evaluation framework, and the RAGMATE-10K dataset are publicly hosted.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u5143\u6570\u636e\u611f\u77e5\u68c0\u7d22\u7b56\u7565\uff0c\u53d1\u73b0\u5728\u7ed3\u6784\u5316\u91cd\u590d\u8bed\u6599\u4e2d\uff0c\u5143\u6570\u636e\u96c6\u6210\u80fd\u663e\u8457\u63d0\u5347\u68c0\u7d22\u6548\u679c\uff0c\u524d\u7f00\u6cd5\u548c\u7edf\u4e00\u5d4c\u5165\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u5728\u7ed3\u6784\u5316\u91cd\u590d\u8bed\u6599\uff08\u5982\u76d1\u7ba1\u6587\u4ef6\uff09\u4e2d\uff0c\u4ec5\u4f9d\u8d56\u6587\u672c\u76f8\u4f3c\u6027\u7684\u68c0\u7d22\u65b9\u6cd5\u96be\u4ee5\u533a\u5206\u8bed\u8a00\u91cd\u53e0\u7684\u6587\u6863\u3002\u5b9e\u8df5\u4e2d\u5e38\u5c06\u5143\u6570\u636e\u4f5c\u4e3a\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u4f46\u5176\u5f71\u54cd\u548c\u6743\u8861\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u6bd4\u8f83\u4e86\u591a\u79cd\u5143\u6570\u636e\u611f\u77e5\u68c0\u7d22\u7b56\u7565\uff1a\u5143\u6570\u636e\u4f5c\u4e3a\u6587\u672c\uff08\u524d\u7f00\u548c\u540e\u7f00\uff09\u3001\u878d\u5408\u5143\u6570\u636e\u548c\u5185\u5bb9\u7684\u7edf\u4e00\u5d4c\u5165\u3001\u53cc\u7f16\u7801\u5668\u540e\u671f\u878d\u5408\u68c0\u7d22\u3001\u4ee5\u53ca\u5143\u6570\u636e\u611f\u77e5\u67e5\u8be2\u91cd\u6784\u3002", "result": "\u524d\u7f00\u6cd5\u548c\u7edf\u4e00\u5d4c\u5165\u4e00\u81f4\u4f18\u4e8e\u7eaf\u6587\u672c\u57fa\u7ebf\uff0c\u7edf\u4e00\u5d4c\u5165\u6709\u65f6\u8d85\u8d8a\u524d\u7f00\u6cd5\u4e14\u66f4\u6613\u7ef4\u62a4\u3002\u5143\u6570\u636e\u96c6\u6210\u901a\u8fc7\u589e\u5f3a\u6587\u6863\u5185\u805a\u6027\u3001\u51cf\u5c11\u6587\u6863\u95f4\u6df7\u6dc6\u3001\u6269\u5927\u76f8\u5173\u4e0e\u4e0d\u76f8\u5173\u5757\u95f4\u8ddd\u79bb\u6765\u63d0\u5347\u6548\u679c\u3002", "conclusion": "\u5143\u6570\u636e\u96c6\u6210\u80fd\u663e\u8457\u6539\u5584\u7ed3\u6784\u5316\u8bed\u6599\u4e2d\u7684\u68c0\u7d22\u6548\u679c\uff0c\u7ed3\u6784\u7ebf\u7d22\u63d0\u4f9b\u5f3a\u6d88\u6b67\u4fe1\u53f7\u3002\u7814\u7a76\u63d0\u4f9b\u4e86\u4ee3\u7801\u3001\u8bc4\u4f30\u6846\u67b6\u548cRAGMATE-10K\u6570\u636e\u96c6\u3002"}}
{"id": "2601.11874", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.11874", "abs": "https://arxiv.org/abs/2601.11874", "authors": ["Suchana Datta", "Dwaipayan Roy", "Derek Greene", "Gerardine Meaney", "Karen Wade", "Philipp Mayr"], "title": "Cultural Analytics for Good: Building Inclusive Evaluation Frameworks for Historical IR", "comment": null, "summary": "This work bridges the fields of information retrieval and cultural analytics to support equitable access to historical knowledge. Using the British Library BL19 digital collection (more than 35,000 works from 1700-1899), we construct a benchmark for studying changes in language, terminology and retrieval in the 19th-century fiction and non-fiction. Our approach combines expert-driven query design, paragraph-level relevance annotation, and Large Language Model (LLM) assistance to create a scalable evaluation framework grounded in human expertise. We focus on knowledge transfer from fiction to non-fiction, investigating how narrative understanding and semantic richness in fiction can improve retrieval for scholarly and factual materials. This interdisciplinary framework not only improves retrieval accuracy but also fosters interpretability, transparency, and cultural inclusivity in digital archives. Our work provides both practical evaluation resources and a methodological paradigm for developing retrieval systems that support richer, historically aware engagement with digital archives, ultimately working towards more emancipatory knowledge infrastructures.", "AI": {"tldr": "\u6784\u5efa\u8de8\u4fe1\u606f\u68c0\u7d22\u4e0e\u6587\u5316\u5206\u6790\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u5229\u7528\u82f1\u56fd\u56fe\u4e66\u9986BL19\u6570\u5b57\u9986\u85cf\uff081700-1899\u5e743.5\u4e07+\u4f5c\u54c1\uff09\uff0c\u7814\u7a7619\u4e16\u7eaa\u5c0f\u8bf4\u4e0e\u975e\u5c0f\u8bf4\u4e2d\u7684\u8bed\u8a00\u53d8\u5316\u4e0e\u68c0\u7d22\uff0c\u7ed3\u5408\u4e13\u5bb6\u67e5\u8be2\u8bbe\u8ba1\u3001\u6bb5\u843d\u7ea7\u76f8\u5173\u6027\u6807\u6ce8\u548cLLM\u8f85\u52a9\uff0c\u63a2\u7d22\u5c0f\u8bf4\u5230\u975e\u5c0f\u8bf4\u7684\u77e5\u8bc6\u8fc1\u79fb\uff0c\u63d0\u5347\u6570\u5b57\u6863\u6848\u7684\u68c0\u7d22\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6587\u5316\u5305\u5bb9\u6027\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u5f25\u5408\u4fe1\u606f\u68c0\u7d22\u4e0e\u6587\u5316\u5206\u6790\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u652f\u6301\u5386\u53f2\u77e5\u8bc6\u7684\u516c\u5e73\u83b7\u53d6\u3002\u901a\u8fc7\u6784\u5efa19\u4e16\u7eaa\u6587\u672c\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u89e3\u51b3\u6570\u5b57\u6863\u6848\u4e2d\u8bed\u8a00\u6f14\u53d8\u3001\u672f\u8bed\u53d8\u5316\u5bf9\u68c0\u7d22\u7cfb\u7edf\u7684\u5f71\u54cd\uff0c\u4fc3\u8fdb\u66f4\u5177\u5305\u5bb9\u6027\u548c\u89e3\u91ca\u6027\u7684\u77e5\u8bc6\u57fa\u7840\u8bbe\u65bd\u3002", "method": "\u4f7f\u7528\u82f1\u56fd\u56fe\u4e66\u9986BL19\u6570\u5b57\u9986\u85cf\uff081700-1899\u5e74\u8d85\u8fc735,000\u90e8\u4f5c\u54c1\uff09\u6784\u5efa\u57fa\u51c6\uff1b\u7ed3\u5408\u4e13\u5bb6\u9a71\u52a8\u7684\u67e5\u8be2\u8bbe\u8ba1\u3001\u6bb5\u843d\u7ea7\u76f8\u5173\u6027\u6807\u6ce8\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8f85\u52a9\uff0c\u521b\u5efa\u57fa\u4e8e\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u7684\u53ef\u6269\u5c55\u8bc4\u4f30\u6846\u67b6\uff1b\u91cd\u70b9\u7814\u7a76\u4ece\u5c0f\u8bf4\u5230\u975e\u5c0f\u8bf4\u7684\u77e5\u8bc6\u8fc1\u79fb\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u8bc4\u4f30\u8d44\u6e90\u548c\u65b9\u6cd5\u8bba\u8303\u5f0f\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u68c0\u7d22\u51c6\u786e\u6027\uff0c\u8fd8\u4fc3\u8fdb\u4e86\u6570\u5b57\u6863\u6848\u7684\u53ef\u89e3\u91ca\u6027\u3001\u900f\u660e\u5ea6\u548c\u6587\u5316\u5305\u5bb9\u6027\uff0c\u652f\u6301\u66f4\u4e30\u5bcc\u3001\u66f4\u5177\u5386\u53f2\u610f\u8bc6\u7684\u5bf9\u6570\u5b57\u6863\u6848\u7684\u53c2\u4e0e\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f00\u53d1\u652f\u6301\u66f4\u4e30\u5bcc\u3001\u66f4\u5177\u5386\u53f2\u610f\u8bc6\u53c2\u4e0e\u6570\u5b57\u6863\u6848\u7684\u68c0\u7d22\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u8df5\u8d44\u6e90\u548c\u65b9\u6cd5\u8bba\u6846\u67b6\uff0c\u6700\u7ec8\u671d\u7740\u66f4\u89e3\u653e\u7684\u77e5\u8bc6\u57fa\u7840\u8bbe\u65bd\u8fc8\u8fdb\uff0c\u4fc3\u8fdb\u4e86\u8de8\u5b66\u79d1\u5408\u4f5c\u548c\u6587\u5316\u5305\u5bb9\u6027\u3002"}}
{"id": "2601.11888", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.11888", "abs": "https://arxiv.org/abs/2601.11888", "authors": ["Wenhan Liu", "Xinyu Ma", "Yutao Zhu", "Yuchen Li", "Daiting Shi", "Dawei Yin", "Zhicheng Dou"], "title": "Agentic-R: Learning to Retrieve for Agentic Search", "comment": null, "summary": "Agentic search has recently emerged as a powerful paradigm, where an agent interleaves multi-step reasoning with on-demand retrieval to solve complex questions. Despite its success, how to design a retriever for agentic search remains largely underexplored. Existing search agents typically rely on similarity-based retrievers, while similar passages are not always useful for final answer generation. In this paper, we propose a novel retriever training framework tailored for agentic search. Unlike retrievers designed for single-turn retrieval-augmented generation (RAG) that only rely on local passage utility, we propose to use both local query-passage relevance and global answer correctness to measure passage utility in a multi-turn agentic search. We further introduce an iterative training strategy, where the search agent and the retriever are optimized bidirectionally and iteratively. Different from RAG retrievers that are only trained once with fixed questions, our retriever is continuously improved using evolving and higher-quality queries from the agent. Extensive experiments on seven single-hop and multi-hop QA benchmarks demonstrate that our retriever, termed \\ours{}, consistently outperforms strong baselines across different search agents. Our codes are available at: https://github.com/8421BCD/Agentic-R.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u95e8\u4e3a\u667a\u80fd\u4f53\u641c\u7d22\u8bbe\u8ba1\u7684\u68c0\u7d22\u5668\u8bad\u7ec3\u6846\u67b6\uff0c\u4f7f\u7528\u5c40\u90e8\u67e5\u8be2-\u6bb5\u843d\u76f8\u5173\u6027\u548c\u5168\u5c40\u7b54\u6848\u6b63\u786e\u6027\u6765\u8861\u91cf\u6bb5\u843d\u6548\u7528\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u8bad\u7ec3\u7b56\u7565\u4f18\u5316\u68c0\u7d22\u5668\u548c\u641c\u7d22\u667a\u80fd\u4f53\u3002", "motivation": "\u73b0\u6709\u641c\u7d22\u667a\u80fd\u4f53\u901a\u5e38\u4f9d\u8d56\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u68c0\u7d22\u5668\uff0c\u4f46\u76f8\u4f3c\u6bb5\u843d\u5e76\u4e0d\u603b\u662f\u5bf9\u6700\u7ec8\u7b54\u6848\u751f\u6210\u6709\u7528\u3002\u5982\u4f55\u4e3a\u667a\u80fd\u4f53\u641c\u7d22\u8bbe\u8ba1\u68c0\u7d22\u5668\u4ecd\u7136\u662f\u4e00\u4e2a\u672a\u5145\u5206\u63a2\u7d22\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u68c0\u7d22\u5668\u8bad\u7ec3\u6846\u67b6\uff1a1) \u4f7f\u7528\u5c40\u90e8\u67e5\u8be2-\u6bb5\u843d\u76f8\u5173\u6027\u548c\u5168\u5c40\u7b54\u6848\u6b63\u786e\u6027\u6765\u8861\u91cf\u591a\u8f6e\u667a\u80fd\u4f53\u641c\u7d22\u4e2d\u7684\u6bb5\u843d\u6548\u7528\uff1b2) \u5f15\u5165\u8fed\u4ee3\u8bad\u7ec3\u7b56\u7565\uff0c\u641c\u7d22\u667a\u80fd\u4f53\u548c\u68c0\u7d22\u5668\u53cc\u5411\u8fed\u4ee3\u4f18\u5316\uff1b3) \u68c0\u7d22\u5668\u4f7f\u7528\u667a\u80fd\u4f53\u751f\u6210\u7684\u6f14\u5316\u9ad8\u8d28\u91cf\u67e5\u8be2\u6301\u7eed\u6539\u8fdb\u3002", "result": "\u5728\u4e03\u4e2a\u5355\u8df3\u548c\u591a\u8df3\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u68c0\u7d22\u5668\uff08\u79f0\u4e3aAgentic-R\uff09\u5728\u4e0d\u540c\u641c\u7d22\u667a\u80fd\u4f53\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u667a\u80fd\u4f53\u641c\u7d22\u8bbe\u8ba1\u4e86\u4e13\u95e8\u7684\u68c0\u7d22\u5668\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5c40\u90e8\u548c\u5168\u5c40\u6548\u7528\u8bc4\u4f30\u4ee5\u53ca\u8fed\u4ee3\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u641c\u7d22\u7684\u6027\u80fd\u3002"}}
{"id": "2601.12301", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.12301", "abs": "https://arxiv.org/abs/2601.12301", "authors": ["Mingrui Liu", "Sixiao Zhang", "Cheng Long"], "title": "Facet-Aware Multi-Head Mixture-of-Experts Model with Text-Enhanced Pre-training for Sequential Recommendation", "comment": "Extended from WSDM paper. arXiv admin note: substantial text overlap with arXiv:2411.01457", "summary": "Sequential recommendation (SR) systems excel at capturing users' dynamic preferences by leveraging their interaction histories. Most existing SR systems assign a single embedding vector to each item to represent its features, adopting various models to combine these embeddings into a sequence representation that captures user intent. However, we argue that this representation alone is insufficient to capture an item's multi-faceted nature (e.g., movie genres, starring actors). Furthermore, users often exhibit complex and varied preferences within these facets (e.g., liking both action and musical films within the genre facet), which are challenging to fully represent with static identifiers. To address these issues, we propose a novel architecture titled Facet-Aware Multi-Head Mixture-of-Experts Model for Sequential Recommendation (FAME). We leverage sub-embeddings from each head in the final multi-head attention layer to predict the next item separately, effectively capturing distinct item facets. A gating mechanism then integrates these predictions by dynamically determining their importance. Additionally, we introduce a Mixture-of-Experts (MoE) network within each attention head to disentangle varied user preferences within each facet, utilizing a learnable router network to aggregate expert outputs based on context. Complementing this architecture, we design a Text-Enhanced Facet-Aware Pre-training module to overcome the limitations of randomly initialized embeddings. By utilizing a pre-trained text encoder and employing an alternating supervised contrastive learning objective, we explicitly disentangle facet-specific features from textual metadata (e.g., descriptions) before sequential training begins. This ensures that the item embeddings are semantically robust and aligned with the downstream multi-facet framework.", "AI": {"tldr": "\u63d0\u51faFAME\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u9762\u611f\u77e5\u7684\u591a\u5934\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u89e3\u51b3\u5e8f\u5217\u63a8\u8350\u4e2d\u7269\u54c1\u591a\u9762\u6027\u548c\u7528\u6237\u504f\u597d\u591a\u6837\u6027\u7684\u95ee\u9898\uff0c\u5e76\u7ed3\u5408\u6587\u672c\u589e\u5f3a\u7684\u9884\u8bad\u7ec3\u6a21\u5757\u63d0\u5347\u8bed\u4e49\u8868\u793a\u3002", "motivation": "\u73b0\u6709\u5e8f\u5217\u63a8\u8350\u7cfb\u7edf\u901a\u5e38\u4e3a\u6bcf\u4e2a\u7269\u54c1\u5206\u914d\u5355\u4e00\u5d4c\u5165\u5411\u91cf\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u7269\u54c1\u7684\u591a\u9762\u6027\uff08\u5982\u7535\u5f71\u7c7b\u578b\u3001\u6f14\u5458\u7b49\uff09\u548c\u7528\u6237\u5728\u5404\u4e2a\u9762\u4e0a\u7684\u590d\u6742\u504f\u597d\u53d8\u5316\u3002", "method": "1. FAME\u67b6\u6784\uff1a\u5229\u7528\u591a\u5934\u6ce8\u610f\u529b\u6700\u540e\u4e00\u5c42\u7684\u5b50\u5d4c\u5165\u5206\u522b\u9884\u6d4b\u4e0b\u4e00\u7269\u54c1\uff0c\u6355\u6349\u4e0d\u540c\u7269\u54c1\u9762\uff1b\u901a\u8fc7\u95e8\u63a7\u673a\u5236\u52a8\u6001\u6574\u5408\u9884\u6d4b\u7ed3\u679c\u30022. \u6bcf\u4e2a\u6ce8\u610f\u529b\u5934\u5185\u5f15\u5165\u6df7\u5408\u4e13\u5bb6\u7f51\u7edc\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u8def\u7531\u5668\u7f51\u7edc\u57fa\u4e8e\u4e0a\u4e0b\u6587\u805a\u5408\u4e13\u5bb6\u8f93\u51fa\uff0c\u89e3\u8026\u7528\u6237\u5728\u5404\u9762\u4e0a\u7684\u591a\u6837\u504f\u597d\u30023. \u6587\u672c\u589e\u5f3a\u7684\u591a\u9762\u611f\u77e5\u9884\u8bad\u7ec3\u6a21\u5757\uff1a\u4f7f\u7528\u9884\u8bad\u7ec3\u6587\u672c\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u4ea4\u66ff\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u4ece\u6587\u672c\u5143\u6570\u636e\u4e2d\u663e\u5f0f\u89e3\u8026\u9762\u7279\u5b9a\u7279\u5f81\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u66f4\u6709\u6548\u5730\u6355\u6349\u7269\u54c1\u7684\u591a\u9762\u6027\u548c\u7528\u6237\u7684\u590d\u6742\u504f\u597d\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u786e\u4fdd\u7269\u54c1\u5d4c\u5165\u5177\u6709\u8bed\u4e49\u9c81\u68d2\u6027\u5e76\u4e0e\u4e0b\u6e38\u591a\u9762\u6846\u67b6\u5bf9\u9f50\u3002", "conclusion": "FAME\u6a21\u578b\u901a\u8fc7\u89e3\u8026\u7269\u54c1\u591a\u9762\u6027\u548c\u7528\u6237\u504f\u597d\u591a\u6837\u6027\uff0c\u7ed3\u5408\u6587\u672c\u589e\u5f3a\u9884\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e8f\u5217\u63a8\u8350\u7684\u6027\u80fd\uff0c\u4e3a\u5904\u7406\u590d\u6742\u63a8\u8350\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12544", "categories": ["cs.IR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.12544", "abs": "https://arxiv.org/abs/2601.12544", "authors": ["Leif Azzopardi", "Adam Roegiest"], "title": "Information Farming: From Berry Picking to Berry Growing", "comment": "ACM CHIIR 2026", "summary": "The classic paradigms of Berry Picking and Information Foraging Theory have framed users as gatherers, opportunistically searching across distributed sources to satisfy evolving information needs. However, the rise of GenAI is driving a fundamental transformation in how people produce, structure, and reuse information - one that these paradigms no longer fully capture. This transformation is analogous to the Neolithic Revolution, when societies shifted from hunting and gathering to cultivation. Generative technologies empower users to \"farm\" information by planting seeds in the form of prompts, cultivating workflows over time, and harvesting richly structured, relevant yields within their own plots, rather than foraging across others people's patches. In this perspectives paper, we introduce the notion of Information Farming as a conceptual framework and argue that it represents a natural evolution in how people engage with information. Drawing on historical analogy and empirical evidence, we examine the benefits and opportunities of information farming, its implications for design and evaluation, and the accompanying risks posed by this transition. We hypothesize that as GenAI technologies proliferate, cultivating information will increasingly supplant transient, patch-based foraging as a dominant mode of engagement, marking a broader shift in human-information interaction and its study.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u4fe1\u606f\u8015\u4f5c\"\u65b0\u6846\u67b6\uff0c\u7c7b\u6bd4\u65b0\u77f3\u5668\u9769\u547d\uff0c\u8ba4\u4e3a\u751f\u6210\u5f0fAI\u6b63\u63a8\u52a8\u4eba\u7c7b\u4ece\u4fe1\u606f\"\u91c7\u96c6\"\u8f6c\u5411\"\u8015\u4f5c\"\u6a21\u5f0f", "motivation": "\u4f20\u7edf\u7684\u4fe1\u606f\u91c7\u96c6\u7406\u8bba\u548c\u4fe1\u606f\u89c5\u98df\u7406\u8bba\u5c06\u7528\u6237\u89c6\u4e3a\u4fe1\u606f\u6536\u96c6\u8005\uff0c\u4f46\u751f\u6210\u5f0fAI\u7684\u51fa\u73b0\u6b63\u5728\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u4eba\u4eec\u751f\u4ea7\u3001\u7ec4\u7ec7\u548c\u91cd\u7528\u4fe1\u606f\u7684\u65b9\u5f0f\uff0c\u8fd9\u4e9b\u4f20\u7edf\u8303\u5f0f\u5df2\u65e0\u6cd5\u5b8c\u5168\u6355\u6349\u8fd9\u4e00\u8f6c\u53d8", "method": "\u91c7\u7528\u5386\u53f2\u7c7b\u6bd4\u548c\u5b9e\u8bc1\u8bc1\u636e\uff0c\u5f15\u5165\"\u4fe1\u606f\u8015\u4f5c\"\u4f5c\u4e3a\u6982\u5ff5\u6846\u67b6\uff0c\u5206\u6790\u5176\u76ca\u5904\u3001\u673a\u4f1a\u3001\u8bbe\u8ba1\u5f71\u54cd\u548c\u4f34\u968f\u98ce\u9669", "result": "\u63d0\u51fa\u4fe1\u606f\u8015\u4f5c\u4ee3\u8868\u4eba\u7c7b\u4e0e\u4fe1\u606f\u4e92\u52a8\u65b9\u5f0f\u7684\u81ea\u7136\u6f14\u8fdb\uff0c\u968f\u7740\u751f\u6210\u5f0fAI\u6280\u672f\u666e\u53ca\uff0c\u4fe1\u606f\u8015\u4f5c\u5c06\u9010\u6e10\u53d6\u4ee3\u4e34\u65f6\u6027\u7684\u3001\u57fa\u4e8e\u788e\u7247\u7684\u4fe1\u606f\u89c5\u98df\u6210\u4e3a\u4e3b\u5bfc\u6a21\u5f0f", "conclusion": "\u4fe1\u606f\u8015\u4f5c\u6846\u67b6\u6807\u5fd7\u7740\u4eba\u673a\u4fe1\u606f\u4ea4\u4e92\u53ca\u5176\u7814\u7a76\u7684\u66f4\u5e7f\u6cdb\u8f6c\u53d8\uff0c\u9700\u8981\u65b0\u7684\u8bbe\u8ba1\u539f\u5219\u548c\u8bc4\u4f30\u65b9\u6cd5\u6765\u5e94\u5bf9\u8fd9\u4e00\u8f6c\u578b"}}
{"id": "2601.12681", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.12681", "abs": "https://arxiv.org/abs/2601.12681", "authors": ["Yunwen Huang", "Shiyong Hong", "Xijun Xiao", "Jinqiu Jin", "Xuanyuan Luo", "Zhe Wang", "Zheng Chai", "Shikang Wu", "Yuchao Zheng", "Jingjian Lin"], "title": "HyFormer: Revisiting the Roles of Sequence Modeling and Feature Interaction in CTR Prediction", "comment": null, "summary": "Industrial large-scale recommendation models (LRMs) face the challenge of jointly modeling long-range user behavior sequences and heterogeneous non-sequential features under strict efficiency constraints. However, most existing architectures employ a decoupled pipeline: long sequences are first compressed with a query-token based sequence compressor like LONGER, followed by fusion with dense features through token-mixing modules like RankMixer, which thereby limits both the representation capacity and the interaction flexibility. This paper presents HyFormer, a unified hybrid transformer architecture that tightly integrates long-sequence modeling and feature interaction into a single backbone. From the perspective of sequence modeling, we revisit and redesign query tokens in LRMs, and frame the LRM modeling task as an alternating optimization process that integrates two core components: Query Decoding which expands non-sequential features into Global Tokens and performs long sequence decoding over layer-wise key-value representations of long behavioral sequences; and Query Boosting which enhances cross-query and cross-sequence heterogeneous interactions via efficient token mixing. The two complementary mechanisms are performed iteratively to refine semantic representations across layers. Extensive experiments on billion-scale industrial datasets demonstrate that HyFormer consistently outperforms strong LONGER and RankMixer baselines under comparable parameter and FLOPs budgets, while exhibiting superior scaling behavior with increasing parameters and FLOPs. Large-scale online A/B tests in high-traffic production systems further validate its effectiveness, showing significant gains over deployed state-of-the-art models. These results highlight the practicality and scalability of HyFormer as a unified modeling framework for industrial LRMs.", "AI": {"tldr": "HyFormer\u63d0\u51fa\u7edf\u4e00\u6df7\u5408Transformer\u67b6\u6784\uff0c\u5c06\u957f\u5e8f\u5217\u5efa\u6a21\u548c\u7279\u5f81\u4ea4\u4e92\u96c6\u6210\u5230\u5355\u4e00\u9aa8\u5e72\u7f51\u7edc\u4e2d\uff0c\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\u67e5\u8be2\u89e3\u7801\u548c\u67e5\u8be2\u589e\u5f3a\u673a\u5236\uff0c\u5728\u5de5\u4e1a\u7ea7\u63a8\u8350\u7cfb\u7edf\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5de5\u4e1a\u5927\u89c4\u6a21\u63a8\u8350\u6a21\u578b\u9762\u4e34\u8054\u5408\u5efa\u6a21\u957f\u7528\u6237\u884c\u4e3a\u5e8f\u5217\u548c\u5f02\u6784\u975e\u5e8f\u5217\u7279\u5f81\u7684\u6311\u6218\uff0c\u73b0\u6709\u67b6\u6784\u91c7\u7528\u89e3\u8026\u6d41\u6c34\u7ebf\uff08\u5148\u538b\u7f29\u957f\u5e8f\u5217\u518d\u878d\u5408\u7279\u5f81\uff09\uff0c\u9650\u5236\u4e86\u8868\u793a\u80fd\u529b\u548c\u4ea4\u4e92\u7075\u6d3b\u6027\u3002", "method": "\u8bbe\u8ba1\u7edf\u4e00\u6df7\u5408Transformer\u67b6\u6784\uff0c\u5305\u542b\u4e24\u4e2a\u4ea4\u66ff\u4f18\u5316\u7684\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u67e5\u8be2\u89e3\u7801\uff1a\u5c06\u975e\u5e8f\u5217\u7279\u5f81\u6269\u5c55\u4e3a\u5168\u5c40\u4ee4\u724c\uff0c\u5728\u957f\u884c\u4e3a\u5e8f\u5217\u7684\u5c42\u95f4\u952e\u503c\u8868\u793a\u4e0a\u8fdb\u884c\u957f\u5e8f\u5217\u89e3\u7801\uff1b2) \u67e5\u8be2\u589e\u5f3a\uff1a\u901a\u8fc7\u9ad8\u6548\u4ee4\u724c\u6df7\u5408\u589e\u5f3a\u8de8\u67e5\u8be2\u548c\u8de8\u5e8f\u5217\u7684\u5f02\u6784\u4ea4\u4e92\u3002", "result": "\u5728\u5341\u4ebf\u7ea7\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\uff0cHyFormer\u5728\u53ef\u6bd4\u53c2\u6570\u548cFLOPs\u9884\u7b97\u4e0b\u6301\u7eed\u4f18\u4e8eLONGER\u548cRankMixer\u57fa\u7ebf\uff0c\u5e76\u5c55\u73b0\u51fa\u968f\u7740\u53c2\u6570\u548cFLOPs\u589e\u52a0\u800c\u66f4\u4f18\u7684\u6269\u5c55\u884c\u4e3a\u3002\u5927\u89c4\u6a21\u5728\u7ebfA/B\u6d4b\u8bd5\u5728\u9ad8\u6d41\u91cf\u751f\u4ea7\u7cfb\u7edf\u4e2d\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "HyFormer\u4f5c\u4e3a\u5de5\u4e1a\u5927\u89c4\u6a21\u63a8\u8350\u6a21\u578b\u7684\u7edf\u4e00\u5efa\u6a21\u6846\u67b6\uff0c\u5177\u6709\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u80fd\u591f\u6709\u6548\u6574\u5408\u957f\u5e8f\u5217\u5efa\u6a21\u548c\u7279\u5f81\u4ea4\u4e92\uff0c\u663e\u8457\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2601.12828", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.12828", "abs": "https://arxiv.org/abs/2601.12828", "authors": ["Masoud Mansoury", "Jin Huang", "Mykola Pechenizkiy", "Herke van Hoof", "Maarten de Rijke"], "title": "The Unfairness of Multifactorial Bias in Recommendation", "comment": null, "summary": "Popularity bias and positivity bias are two prominent sources of bias in recommender systems. Both arise from input data, propagate through recommendation models, and lead to unfair or suboptimal outcomes. Popularity bias occurs when a small subset of items receives most interactions, while positivity bias stems from the over-representation of high rating values. Although each bias has been studied independently, their combined effect, to which we refer to as multifactorial bias, remains underexplored. In this work, we examine how multifactorial bias influences item-side fairness, focusing on exposure bias, which reflects the unequal visibility of items in recommendation outputs. Through simulation studies, we find that positivity bias is disproportionately concentrated on popular items, further amplifying their over-exposure. Motivated by this insight, we adapt a percentile-based rating transformation as a pre-processing strategy to mitigate multifactorial bias. Experiments using six recommendation algorithms across four public datasets show that this approach improves exposure fairness with negligible accuracy loss. We also demonstrate that integrating this pre-processing step into post-processing fairness pipelines enhances their effectiveness and efficiency, enabling comparable or better fairness with reduced computational cost. These findings highlight the importance of addressing multifactorial bias and demonstrate the practical value of simple, data-driven pre-processing methods for improving fairness in recommender systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u591a\u56e0\u7d20\u504f\u89c1\uff08\u6d41\u884c\u5ea6\u504f\u89c1\u548c\u79ef\u6781\u6027\u504f\u89c1\u7684\u7ec4\u5408\u6548\u5e94\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u767e\u5206\u4f4d\u6570\u8bc4\u5206\u7684\u9884\u5904\u7406\u65b9\u6cd5\u6765\u7f13\u89e3\u8fd9\u79cd\u504f\u89c1\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6539\u5584\u66dd\u5149\u516c\u5e73\u6027\u4e14\u51e0\u4e4e\u4e0d\u5f71\u54cd\u63a8\u8350\u51c6\u786e\u6027\u3002", "motivation": "\u63a8\u8350\u7cfb\u7edf\u4e2d\u5b58\u5728\u6d41\u884c\u5ea6\u504f\u89c1\u548c\u79ef\u6781\u6027\u504f\u89c1\uff0c\u8fd9\u4e24\u79cd\u504f\u89c1\u90fd\u6e90\u4e8e\u8f93\u5165\u6570\u636e\u5e76\u901a\u8fc7\u63a8\u8350\u6a21\u578b\u4f20\u64ad\uff0c\u5bfc\u81f4\u4e0d\u516c\u5e73\u6216\u6b21\u4f18\u7ed3\u679c\u3002\u867d\u7136\u6bcf\u79cd\u504f\u89c1\u90fd\u88ab\u72ec\u7acb\u7814\u7a76\u8fc7\uff0c\u4f46\u5b83\u4eec\u7684\u7ec4\u5408\u6548\u5e94\uff08\u591a\u56e0\u7d20\u504f\u89c1\uff09\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u4f5c\u8005\u53d1\u73b0\u79ef\u6781\u6027\u504f\u89c1\u4e0d\u6210\u6bd4\u4f8b\u5730\u96c6\u4e2d\u5728\u6d41\u884c\u7269\u54c1\u4e0a\uff0c\u8fdb\u4e00\u6b65\u653e\u5927\u4e86\u5b83\u4eec\u7684\u8fc7\u5ea6\u66dd\u5149\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u6a21\u62df\u7814\u7a76\u5206\u6790\u591a\u56e0\u7d20\u504f\u89c1\u5bf9\u7269\u54c1\u4fa7\u516c\u5e73\u6027\u7684\u5f71\u54cd\uff0c\u91cd\u70b9\u5173\u6ce8\u66dd\u5149\u504f\u89c1\u3002\u7136\u540e\u91c7\u7528\u57fa\u4e8e\u767e\u5206\u4f4d\u6570\u7684\u8bc4\u5206\u8f6c\u6362\u4f5c\u4e3a\u9884\u5904\u7406\u7b56\u7565\u6765\u7f13\u89e3\u591a\u56e0\u7d20\u504f\u89c1\u3002\u8be5\u65b9\u6cd5\u5728\u56db\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u4f7f\u7528\u516d\u79cd\u63a8\u8350\u7b97\u6cd5\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u540e\u5904\u7406\u516c\u5e73\u6027\u6d41\u7a0b\u4e2d\u4ee5\u8bc4\u4f30\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u767e\u5206\u4f4d\u6570\u7684\u9884\u5904\u7406\u65b9\u6cd5\u80fd\u591f\u6539\u5584\u66dd\u5149\u516c\u5e73\u6027\uff0c\u540c\u65f6\u51c6\u786e\u7387\u635f\u5931\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002\u5c06\u8be5\u9884\u5904\u7406\u6b65\u9aa4\u96c6\u6210\u5230\u540e\u5904\u7406\u516c\u5e73\u6027\u6d41\u7a0b\u4e2d\uff0c\u53ef\u4ee5\u589e\u5f3a\u5176\u6548\u679c\u548c\u6548\u7387\uff0c\u4ee5\u66f4\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u5b9e\u73b0\u76f8\u5f53\u6216\u66f4\u597d\u7684\u516c\u5e73\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u89e3\u51b3\u591a\u56e0\u7d20\u504f\u89c1\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u7b80\u5355\u3001\u6570\u636e\u9a71\u52a8\u7684\u9884\u5904\u7406\u65b9\u6cd5\u5728\u6539\u5584\u63a8\u8350\u7cfb\u7edf\u516c\u5e73\u6027\u65b9\u9762\u7684\u5b9e\u7528\u4ef7\u503c\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u63a8\u8350\u7cfb\u7edf\u516c\u5e73\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u65b9\u6cd5\u3002"}}
{"id": "2601.12985", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.12985", "abs": "https://arxiv.org/abs/2601.12985", "authors": ["Melanie A. Kilian", "David Elsweiler"], "title": "Rules, Resources, and Restrictions: A Taxonomy of Task-Based Information Request Intents", "comment": "11 pages, 1 figure, to be published in: 2026 ACM SIGIR Conference on Human Information Interaction and Retrieval (CHIIR '26), March 22-26, 2026, Seattle, WA, USA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3786304.3787863", "summary": "Understanding and classifying query intents can improve retrieval effectiveness by helping align search results with the motivations behind user queries. However, existing intent taxonomies are typically derived from system log data and capture mostly isolated information needs, while the broader task context often remains unaddressed. This limitation becomes increasingly relevant as interactions with Large Language Models (LLMs) expand user expectations from simple query answering toward comprehensive task support, for example, with purchasing decisions or in travel planning. At the same time, current LLMs still struggle to fully interpret complex and multifaceted tasks. To address this gap, we argue for a stronger task-based perspective on query intent. Drawing on a grounded-theory-based interview study with airport information clerks, we present a taxonomy of task-based information request intents that bridges the gap between traditional query-focused approaches and the emerging demands of AI-driven task-oriented search.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4efb\u52a1\u7684\u4fe1\u606f\u8bf7\u6c42\u610f\u56fe\u5206\u7c7b\u6cd5\uff0c\u5f25\u8865\u4f20\u7edf\u67e5\u8be2\u610f\u56fe\u5206\u7c7b\u4e0eAI\u9a71\u52a8\u4efb\u52a1\u5bfc\u5411\u641c\u7d22\u4e4b\u95f4\u7684\u5dee\u8ddd", "motivation": "\u73b0\u6709\u610f\u56fe\u5206\u7c7b\u4e3b\u8981\u57fa\u4e8e\u7cfb\u7edf\u65e5\u5fd7\u6570\u636e\uff0c\u5173\u6ce8\u5b64\u7acb\u4fe1\u606f\u9700\u6c42\uff0c\u800c\u5ffd\u7565\u4e86\u66f4\u5e7f\u6cdb\u7684\u4efb\u52a1\u4e0a\u4e0b\u6587\u3002\u968f\u7740\u7528\u6237\u5bf9LLMs\u7684\u671f\u671b\u4ece\u7b80\u5355\u95ee\u7b54\u6269\u5c55\u5230\u5168\u9762\u4efb\u52a1\u652f\u6301\uff0c\u9700\u8981\u66f4\u5f3a\u7684\u4efb\u52a1\u5bfc\u5411\u89c6\u89d2\u6765\u7406\u89e3\u67e5\u8be2\u610f\u56fe\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u624e\u6839\u7406\u8bba\u7684\u8bbf\u8c08\u7814\u7a76\uff0c\u91c7\u8bbf\u673a\u573a\u4fe1\u606f\u53f0\u5de5\u4f5c\u4eba\u5458\uff0c\u5206\u6790\u771f\u5b9e\u4efb\u52a1\u573a\u666f\u4e2d\u7684\u4fe1\u606f\u8bf7\u6c42\u6a21\u5f0f\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4efb\u52a1\u578b\u4fe1\u606f\u8bf7\u6c42\u610f\u56fe\u5206\u7c7b\u6cd5\uff0c\u80fd\u591f\u66f4\u597d\u5730\u8fde\u63a5\u4f20\u7edf\u67e5\u8be2\u5bfc\u5411\u65b9\u6cd5\u548cAI\u9a71\u52a8\u4efb\u52a1\u5bfc\u5411\u641c\u7d22\u7684\u65b0\u9700\u6c42\u3002", "conclusion": "\u9700\u8981\u4ece\u4efb\u52a1\u89d2\u5ea6\u91cd\u65b0\u5ba1\u89c6\u67e5\u8be2\u610f\u56fe\u5206\u7c7b\uff0c\u63d0\u51fa\u7684\u5206\u7c7b\u6cd5\u6709\u52a9\u4e8e\u63d0\u5347\u68c0\u7d22\u6548\u679c\uff0c\u66f4\u597d\u5730\u652f\u6301\u590d\u6742\u591a\u65b9\u9762\u7684\u4efb\u52a1\u9700\u6c42\u3002"}}
{"id": "2601.13222", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13222", "abs": "https://arxiv.org/abs/2601.13222", "authors": ["Laura Dietz", "Bryan Li", "Gabrielle Liu", "Jia-Huei Ju", "Eugene Yang", "Dawn Lawrie", "William Walden", "James Mayfield"], "title": "Incorporating Q&A Nuggets into Retrieval-Augmented Generation", "comment": null, "summary": "RAGE systems integrate ideas from automatic evaluation (E) into Retrieval-augmented Generation (RAG). As one such example, we present Crucible, a Nugget-Augmented Generation System that preserves explicit citation provenance by constructing a bank of Q&A nuggets from retrieved documents and uses them to guide extraction, selection, and report generation. Reasoning on nuggets avoids repeated information through clear and interpretable Q&A semantics - instead of opaque cluster abstractions - while maintaining citation provenance throughout the entire generation process. Evaluated on the TREC NeuCLIR 2024 collection, our Crucible system substantially outperforms Ginger, a recent nugget-based RAG system, in nugget recall, density, and citation grounding.", "AI": {"tldr": "RAGE\u7cfb\u7edf\u5c06\u81ea\u52a8\u8bc4\u4f30\u878d\u5165\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff0c\u63d0\u51faCrucible\u7cfb\u7edf\uff0c\u901a\u8fc7\u6784\u5efa\u95ee\u7b54\u5757\u5e93\u6765\u4fdd\u6301\u663e\u5f0f\u5f15\u7528\u6eaf\u6e90\uff0c\u5728TREC NeuCLIR 2024\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709RAG\u7cfb\u7edf\u5728\u4fdd\u6301\u5f15\u7528\u6eaf\u6e90\u548c\u907f\u514d\u4fe1\u606f\u91cd\u590d\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u900f\u660e\u3001\u53ef\u89e3\u91ca\u7684\u8bed\u4e49\u8868\u793a\u6765\u6539\u8fdb\u751f\u6210\u8d28\u91cf\u3002", "method": "\u63d0\u51faCrucible\u7cfb\u7edf\uff1a1\uff09\u4ece\u68c0\u7d22\u6587\u6863\u6784\u5efa\u95ee\u7b54\u5757\u5e93\uff1b2\uff09\u4f7f\u7528\u95ee\u7b54\u5757\u6307\u5bfc\u63d0\u53d6\u3001\u9009\u62e9\u548c\u62a5\u544a\u751f\u6210\uff1b3\uff09\u57fa\u4e8e\u95ee\u7b54\u8bed\u4e49\u8fdb\u884c\u63a8\u7406\uff0c\u907f\u514d\u4fe1\u606f\u91cd\u590d\uff0c\u4fdd\u6301\u5b8c\u6574\u5f15\u7528\u6eaf\u6e90\u3002", "result": "\u5728TREC NeuCLIR 2024\u6570\u636e\u96c6\u4e0a\uff0cCrucible\u5728\u5757\u53ec\u56de\u7387\u3001\u5bc6\u5ea6\u548c\u5f15\u7528\u57fa\u7840\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u8fd1\u7684\u57fa\u4e8e\u5757\u7684RAG\u7cfb\u7edfGinger\u3002", "conclusion": "RAGE\u7cfb\u7edf\u901a\u8fc7\u96c6\u6210\u81ea\u52a8\u8bc4\u4f30\u548c\u57fa\u4e8e\u95ee\u7b54\u5757\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u4fdd\u6301\u5f15\u7528\u6eaf\u6e90\uff0c\u63d0\u9ad8\u751f\u6210\u8d28\u91cf\uff0c\u4e3aRAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2601.13227", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13227", "abs": "https://arxiv.org/abs/2601.13227", "authors": ["Laura Dietz", "Bryan Li", "Eugene Yang", "Dawn Lawrie", "William Walden", "James Mayfield"], "title": "Insider Knowledge: How Much Can RAG Systems Gain from Evaluation Secrets?", "comment": null, "summary": "RAG systems are increasingly evaluated and optimized using LLM judges, an approach that is rapidly becoming the dominant paradigm for system assessment. Nugget-based approaches in particular are now embedded not only in evaluation frameworks but also in the architectures of RAG systems themselves. While this integration can lead to genuine improvements, it also creates a risk of faulty measurements due to circularity. In this paper, we investigate this risk through comparative experiments with nugget-based RAG systems, including Ginger and Crucible, against strong baselines such as GPT-Researcher. By deliberately modifying Crucible to generate outputs optimized for an LLM judge, we show that near-perfect evaluation scores can be achieved when elements of the evaluation - such as prompt templates or gold nuggets - are leaked or can be predicted. Our results highlight the importance of blind evaluation settings and methodological diversity to guard against mistaking metric overfitting for genuine system progress.", "AI": {"tldr": "\u8bba\u6587\u8b66\u544aRAG\u7cfb\u7edf\u4e2d\u57fa\u4e8enugget\u7684LLM\u8bc4\u4f30\u5b58\u5728\u5faa\u73af\u98ce\u9669\uff0c\u5f53\u8bc4\u4f30\u5143\u7d20\u6cc4\u9732\u65f6\u53ef\u80fd\u5bfc\u81f4\u865a\u5047\u7684\u9ad8\u5206\uff0c\u5f3a\u8c03\u9700\u8981\u76f2\u8bc4\u4f30\u548c\u65b9\u6cd5\u591a\u6837\u6027\u3002", "motivation": "RAG\u7cfb\u7edf\u8d8a\u6765\u8d8a\u591a\u5730\u4f7f\u7528LLM\u8bc4\u4f30\u8fdb\u884c\u4f18\u5316\uff0c\u8fd9\u79cd\u57fa\u4e8enugget\u7684\u65b9\u6cd5\u5df2\u88ab\u6574\u5408\u5230\u7cfb\u7edf\u67b6\u6784\u4e2d\u3002\u867d\u7136\u80fd\u5e26\u6765\u6539\u8fdb\uff0c\u4f46\u4e5f\u5b58\u5728\u5faa\u73af\u6027\u5bfc\u81f4\u9519\u8bef\u6d4b\u91cf\u7684\u98ce\u9669\uff0c\u9700\u8981\u7814\u7a76\u8fd9\u79cd\u98ce\u9669\u7684\u5b9e\u9645\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u5b9e\u9a8c\u7814\u7a76\u57fa\u4e8enugget\u7684RAG\u7cfb\u7edf\uff08\u5982Ginger\u548cCrucible\uff09\u4e0e\u5f3a\u57fa\u7ebf\uff08\u5982GPT-Researcher\uff09\u3002\u6545\u610f\u4fee\u6539Crucible\u4ee5\u751f\u6210\u9488\u5bf9LLM\u8bc4\u4f30\u4f18\u5316\u7684\u8f93\u51fa\uff0c\u6d4b\u8bd5\u5f53\u8bc4\u4f30\u5143\u7d20\uff08\u5982\u63d0\u793a\u6a21\u677f\u6216\u9ec4\u91d1nuggets\uff09\u6cc4\u9732\u6216\u53ef\u9884\u6d4b\u65f6\u7684\u6548\u679c\u3002", "result": "\u5f53\u8bc4\u4f30\u5143\u7d20\u6cc4\u9732\u65f6\uff0c\u53ef\u4ee5\u8f7b\u677e\u83b7\u5f97\u63a5\u8fd1\u5b8c\u7f8e\u7684\u8bc4\u4f30\u5206\u6570\u3002\u8fd9\u8868\u660e\u5b58\u5728\u4e25\u91cd\u7684\u5ea6\u91cf\u8fc7\u62df\u5408\u98ce\u9669\uff0c\u53ef\u80fd\u5c06\u7cfb\u7edf\u5bf9\u8bc4\u4f30\u6307\u6807\u7684\u4f18\u5316\u8bef\u8ba4\u4e3a\u662f\u771f\u6b63\u7684\u6027\u80fd\u8fdb\u6b65\u3002", "conclusion": "\u5fc5\u987b\u91c7\u7528\u76f2\u8bc4\u4f30\u8bbe\u7f6e\u548c\u65b9\u6cd5\u591a\u6837\u6027\u6765\u9632\u6b62\u5c06\u5ea6\u91cf\u8fc7\u62df\u5408\u8bef\u8ba4\u4e3a\u7cfb\u7edf\u771f\u6b63\u8fdb\u6b65\u3002RAG\u7cfb\u7edf\u7684\u8bc4\u4f30\u9700\u8981\u66f4\u4e25\u8c28\u7684\u65b9\u6cd5\u8bba\u6765\u786e\u4fdd\u8bc4\u4f30\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.13353", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.13353", "abs": "https://arxiv.org/abs/2601.13353", "authors": ["Bahdja Boudoua", "Nadia Guiffant", "Mathieu Roche", "Maguelonne Teisseire", "Annelise Tran"], "title": "Guidelines for the Creation of an Annotated Corpus", "comment": "8 pages, 3 figures", "summary": "This document, based on feedback from UMR TETIS members and the scientific literature, provides a generic methodology for creating annotation guidelines and annotated textual datasets (corpora). It covers methodological aspects, as well as storage, sharing, and valorization of the data. It includes definitions and examples to clearly illustrate each step of the process, thus providing a comprehensive framework to support the creation and use of corpora in various research contexts.", "AI": {"tldr": "\u63d0\u4f9b\u521b\u5efa\u6587\u672c\u6807\u6ce8\u6307\u5357\u548c\u6807\u6ce8\u8bed\u6599\u5e93\u7684\u901a\u7528\u65b9\u6cd5\u8bba\u6846\u67b6\uff0c\u6db5\u76d6\u65b9\u6cd5\u3001\u5b58\u50a8\u3001\u5171\u4eab\u548c\u5229\u7528\u7b49\u65b9\u9762", "motivation": "\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u8bba\u6307\u5bfc\uff0c\u4ee5\u652f\u6301\u5728\u5404\u79cd\u7814\u7a76\u80cc\u666f\u4e0b\u521b\u5efa\u548c\u4f7f\u7528\u9ad8\u8d28\u91cf\u7684\u6807\u6ce8\u8bed\u6599\u5e93", "method": "\u57fa\u4e8eUMR TETIS\u6210\u5458\u53cd\u9988\u548c\u79d1\u5b66\u6587\u732e\uff0c\u63d0\u51fa\u5305\u542b\u5b9a\u4e49\u3001\u793a\u4f8b\u7684\u9010\u6b65\u6d41\u7a0b\uff0c\u6db5\u76d6\u65b9\u6cd5\u8bba\u3001\u5b58\u50a8\u3001\u5171\u4eab\u548c\u5229\u7528\u7b49\u65b9\u9762", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6e05\u6670\u7684\u5b9a\u4e49\u548c\u793a\u4f8b\u8bf4\u660e\u6bcf\u4e2a\u6b65\u9aa4\uff0c\u652f\u6301\u4e0d\u540c\u7814\u7a76\u573a\u666f\u4e0b\u7684\u8bed\u6599\u5e93\u521b\u5efa\u548c\u4f7f\u7528", "conclusion": "\u8be5\u6587\u6863\u4e3a\u521b\u5efa\u6807\u6ce8\u6307\u5357\u548c\u6807\u6ce8\u8bed\u6599\u5e93\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u901a\u7528\u65b9\u6cd5\u8bba\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u8bed\u6599\u5e93\u7684\u8d28\u91cf\u548c\u53ef\u7528\u6027"}}
{"id": "2601.13505", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.13505", "abs": "https://arxiv.org/abs/2601.13505", "authors": ["Wei Yuan", "Shutong Qiao", "Tong Chen", "Quoc Viet Hung Nguyen", "Zi Huang", "Hongzhi Yin"], "title": "Integrating Vision-Centric Text Understanding for Conversational Recommender Systems", "comment": null, "summary": "Conversational Recommender Systems (CRSs) have attracted growing attention for their ability to deliver personalized recommendations through natural language interactions. To more accurately infer user preferences from multi-turn conversations, recent works increasingly expand conversational context (e.g., by incorporating diverse entity information or retrieving related dialogues). While such context enrichment can assist preference modeling, it also introduces longer and more heterogeneous inputs, leading to practical issues such as input length constraints, text style inconsistency, and irrelevant textual noise, thereby raising the demand for stronger language understanding ability. In this paper, we propose STARCRS, a Screen-Text-AwaRe Conversational Recommender System that integrates two complementary text understanding modes: (1) a screen-reading pathway that encodes auxiliary textual information as visual tokens, mimicking skim reading on a screen, and (2) an LLM-based textual pathway that focuses on a limited set of critical content for fine-grained reasoning. We design a knowledge-anchored fusion framework that combines contrastive alignment, cross-attention interaction, and adaptive gating to integrate the two modes for improved preference modeling and response generation. Extensive experiments on two widely used benchmarks demonstrate that STARCRS consistently improves both recommendation accuracy and generated response quality.", "AI": {"tldr": "STARCRS\u662f\u4e00\u4e2a\u7ed3\u5408\u5c4f\u5e55\u9605\u8bfb\u548cLLM\u6587\u672c\u7406\u89e3\u7684\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\uff0c\u901a\u8fc7\u53cc\u6a21\u6001\u878d\u5408\u63d0\u5347\u63a8\u8350\u51c6\u786e\u6027\u548c\u54cd\u5e94\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u901a\u8fc7\u6269\u5c55\u5bf9\u8bdd\u4e0a\u4e0b\u6587\uff08\u5982\u5b9e\u4f53\u4fe1\u606f\u3001\u76f8\u5173\u5bf9\u8bdd\uff09\u6765\u63d0\u5347\u504f\u597d\u5efa\u6a21\uff0c\u4f46\u8fd9\u5bfc\u81f4\u8f93\u5165\u66f4\u957f\u3001\u66f4\u5f02\u6784\uff0c\u5f15\u53d1\u8f93\u5165\u957f\u5ea6\u9650\u5236\u3001\u6587\u672c\u98ce\u683c\u4e0d\u4e00\u81f4\u3001\u65e0\u5173\u566a\u58f0\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u5f3a\u7684\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u3002", "method": "\u63d0\u51faSTARCRS\u7cfb\u7edf\uff0c\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u7684\u6587\u672c\u7406\u89e3\u6a21\u5f0f\uff1a1) \u5c4f\u5e55\u9605\u8bfb\u8def\u5f84\uff0c\u5c06\u8f85\u52a9\u6587\u672c\u4fe1\u606f\u7f16\u7801\u4e3a\u89c6\u89c9\u6807\u8bb0\uff0c\u6a21\u62df\u5c4f\u5e55\u6d4f\u89c8\uff1b2) \u57fa\u4e8eLLM\u7684\u6587\u672c\u8def\u5f84\uff0c\u4e13\u6ce8\u4e8e\u6709\u9650\u5173\u952e\u5185\u5bb9\u8fdb\u884c\u7ec6\u7c92\u5ea6\u63a8\u7406\u3002\u8bbe\u8ba1\u77e5\u8bc6\u951a\u5b9a\u878d\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5bf9\u9f50\u3001\u4ea4\u53c9\u6ce8\u610f\u529b\u4ea4\u4e92\u548c\u81ea\u9002\u5e94\u95e8\u63a7\u6765\u6574\u5408\u4e24\u79cd\u6a21\u5f0f\u3002", "result": "\u5728\u4e24\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\uff0c\u8bc1\u660eSTARCRS\u5728\u63a8\u8350\u51c6\u786e\u6027\u548c\u751f\u6210\u54cd\u5e94\u8d28\u91cf\u65b9\u9762\u90fd\u6709\u6301\u7eed\u63d0\u5347\u3002", "conclusion": "STARCRS\u901a\u8fc7\u6574\u5408\u5c4f\u5e55\u9605\u8bfb\u548cLLM\u6587\u672c\u7406\u89e3\u4e24\u79cd\u4e92\u8865\u6a21\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u4e2d\u4e0a\u4e0b\u6587\u6269\u5c55\u5e26\u6765\u7684\u5b9e\u9645\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u504f\u597d\u5efa\u6a21\u548c\u54cd\u5e94\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2601.13525", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.13525", "abs": "https://arxiv.org/abs/2601.13525", "authors": ["Chunsheng Zuo", "Daniel Khashabi"], "title": "More Than Efficiency: Embedding Compression Improves Domain Adaptation in Dense Retrieval", "comment": null, "summary": "Dense retrievers powered by pretrained embeddings are widely used for document retrieval but struggle in specialized domains due to the mismatches between the training and target domain distributions. Domain adaptation typically requires costly annotation and retraining of query-document pairs. In this work, we revisit an overlooked alternative: applying PCA to domain embeddings to derive lower-dimensional representations that preserve domain-relevant features while discarding non-discriminative components. Though traditionally used for efficiency, we demonstrate that this simple embedding compression can effectively improve retrieval performance. Evaluated across 9 retrievers and 14 MTEB datasets, PCA applied solely to query embeddings improves NDCG@10 in 75.4% of model-dataset pairs, offering a simple and lightweight method for domain adaptation.", "AI": {"tldr": "PCA\u538b\u7f29\u67e5\u8be2\u5d4c\u5165\u53ef\u63d0\u5347\u4e13\u4e1a\u9886\u57df\u68c0\u7d22\u6027\u80fd\uff0c\u65e0\u9700\u6602\u8d35\u6807\u6ce8\u548c\u91cd\u65b0\u8bad\u7ec3", "motivation": "\u57fa\u4e8e\u9884\u8bad\u7ec3\u5d4c\u5165\u7684\u5bc6\u96c6\u68c0\u7d22\u5668\u5728\u4e13\u4e1a\u9886\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u8bad\u7ec3\u548c\u76ee\u6807\u9886\u57df\u5206\u5e03\u4e0d\u5339\u914d\u3002\u4f20\u7edf\u9886\u57df\u9002\u5e94\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u7684\u6807\u6ce8\u548c\u91cd\u65b0\u8bad\u7ec3\u67e5\u8be2-\u6587\u6863\u5bf9\u3002", "method": "\u91cd\u65b0\u5ba1\u89c6\u88ab\u5ffd\u89c6\u7684\u66ff\u4ee3\u65b9\u6848\uff1a\u5bf9\u9886\u57df\u5d4c\u5165\u5e94\u7528PCA\uff0c\u83b7\u5f97\u4f4e\u7ef4\u8868\u793a\uff0c\u4fdd\u7559\u9886\u57df\u76f8\u5173\u7279\u5f81\uff0c\u4e22\u5f03\u975e\u5224\u522b\u6027\u6210\u5206\u3002\u867d\u7136\u4f20\u7edf\u4e0a\u7528\u4e8e\u6548\u7387\uff0c\u4f46\u672c\u6587\u8bc1\u660e\u8fd9\u79cd\u7b80\u5355\u7684\u5d4c\u5165\u538b\u7f29\u80fd\u6709\u6548\u63d0\u5347\u68c0\u7d22\u6027\u80fd\u3002", "result": "\u57289\u4e2a\u68c0\u7d22\u5668\u548c14\u4e2aMTEB\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u4ec5\u5bf9\u67e5\u8be2\u5d4c\u5165\u5e94\u7528PCA\uff0c\u572875.4%\u7684\u6a21\u578b-\u6570\u636e\u96c6\u5bf9\u4e0a\u6539\u5584\u4e86NDCG@10\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u8f7b\u91cf\u7684\u9886\u57df\u9002\u5e94\u65b9\u6cd5\u3002", "conclusion": "PCA\u5d4c\u5165\u538b\u7f29\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u9886\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u65e0\u9700\u6602\u8d35\u6807\u6ce8\u548c\u91cd\u65b0\u8bad\u7ec3\uff0c\u80fd\u663e\u8457\u63d0\u5347\u4e13\u4e1a\u9886\u57df\u68c0\u7d22\u6027\u80fd\u3002"}}
{"id": "2601.13609", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.13609", "abs": "https://arxiv.org/abs/2601.13609", "authors": ["Yoji Tomita", "Tomohiko Yokoyama"], "title": "Balancing Fairness and High Match Rates in Reciprocal Recommender Systems: A Nash Social Welfare Approach", "comment": "arXiv admin note: text overlap with arXiv:2409.00720", "summary": "Matching platforms, such as online dating services and job recommendations, have become increasingly prevalent. For the success of these platforms, it is crucial to design reciprocal recommender systems (RRSs) that not only increase the total number of matches but also avoid creating unfairness among users. In this paper, we investigate the fairness of RRSs on matching platforms. From the perspective of fair division, we define the users' opportunities to be recommended and establish the fairness concept of envy-freeness in the allocation of these opportunities. We first introduce the Social Welfare (SW) method, which approximately maximizes the number of matches, and show that it leads to significant unfairness in recommendation opportunities, illustrating the trade-off between fairness and match rates. To address this challenge, we propose the Nash Social Welfare (NSW) method, which alternately optimizes two NSW functions and achieves nearly envy-free recommendations. We further generalize the SW and NSW method to the $\u03b1$-SW method, which balances the trade-off between fairness and high match rates. Additionally, we develop a computationally efficient approximation algorithm for the SW/NSW/$\u03b1$-SW methods based on the Sinkhorn algorithm. Through extensive experiments on both synthetic datasets and two real-world datasets, we demonstrate the practical effectiveness of our approach.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5339\u914d\u5e73\u53f0\u4e2d\u4e92\u60e0\u63a8\u8350\u7cfb\u7edf\u7684\u516c\u5e73\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u793e\u4f1a\u798f\u5229\u548c\u7eb3\u4ec0\u793e\u4f1a\u798f\u5229\u7684\u65b9\u6cd5\u6765\u5e73\u8861\u5339\u914d\u6548\u7387\u548c\u516c\u5e73\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u9ad8\u6548\u7684\u8fd1\u4f3c\u7b97\u6cd5\u3002", "motivation": "\u968f\u7740\u5728\u7ebf\u7ea6\u4f1a\u670d\u52a1\u548c\u804c\u4f4d\u63a8\u8350\u7b49\u5339\u914d\u5e73\u53f0\u7684\u666e\u53ca\uff0c\u8bbe\u8ba1\u65e2\u80fd\u589e\u52a0\u5339\u914d\u6570\u91cf\u53c8\u80fd\u907f\u514d\u7528\u6237\u95f4\u4e0d\u516c\u5e73\u7684\u4e92\u60e0\u63a8\u8350\u7cfb\u7edf\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u53ea\u5173\u6ce8\u6700\u5927\u5316\u5339\u914d\u6570\u91cf\uff0c\u800c\u5ffd\u89c6\u4e86\u63a8\u8350\u673a\u4f1a\u7684\u516c\u5e73\u5206\u914d\u95ee\u9898\u3002", "method": "\u8bba\u6587\u4ece\u516c\u5e73\u5206\u914d\u7684\u89d2\u5ea6\u5b9a\u4e49\u4e86\u7528\u6237\u7684\u63a8\u8350\u673a\u4f1a\uff0c\u5e76\u5efa\u7acb\u4e86\u5ac9\u5992\u81ea\u7531\uff08envy-freeness\uff09\u7684\u516c\u5e73\u6982\u5ff5\u3002\u9996\u5148\u5f15\u5165\u793e\u4f1a\u798f\u5229\uff08SW\uff09\u65b9\u6cd5\u8fd1\u4f3c\u6700\u5927\u5316\u5339\u914d\u6570\u91cf\uff0c\u7136\u540e\u63d0\u51fa\u7eb3\u4ec0\u793e\u4f1a\u798f\u5229\uff08NSW\uff09\u65b9\u6cd5\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\u4e24\u4e2aNSW\u51fd\u6570\u5b9e\u73b0\u63a5\u8fd1\u5ac9\u5992\u81ea\u7531\u7684\u63a8\u8350\u3002\u8fdb\u4e00\u6b65\u5c06SW\u548cNSW\u65b9\u6cd5\u63a8\u5e7f\u5230\u03b1-SW\u65b9\u6cd5\uff0c\u4ee5\u5e73\u8861\u516c\u5e73\u6027\u548c\u5339\u914d\u7387\u4e4b\u95f4\u7684\u6743\u8861\u3002\u57fa\u4e8eSinkhorn\u7b97\u6cd5\u5f00\u53d1\u4e86\u8ba1\u7b97\u9ad8\u6548\u7684\u8fd1\u4f3c\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSW\u65b9\u6cd5\u867d\u7136\u80fd\u8fd1\u4f3c\u6700\u5927\u5316\u5339\u914d\u6570\u91cf\uff0c\u4f46\u4f1a\u5bfc\u81f4\u663e\u8457\u7684\u63a8\u8350\u673a\u4f1a\u4e0d\u516c\u5e73\uff0c\u63ed\u793a\u4e86\u516c\u5e73\u6027\u548c\u5339\u914d\u7387\u4e4b\u95f4\u7684\u6743\u8861\u3002NSW\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u63a5\u8fd1\u5ac9\u5992\u81ea\u7531\u7684\u63a8\u8350\uff0c\u800c\u03b1-SW\u65b9\u6cd5\u53ef\u4ee5\u5728\u516c\u5e73\u6027\u548c\u9ad8\u5339\u914d\u7387\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u5b9e\u9645\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5339\u914d\u5e73\u53f0\u4e2d\u7684\u4e92\u60e0\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u516c\u5e73\u6027\u5206\u6790\u6846\u67b6\u548c\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u793e\u4f1a\u798f\u5229\u548c\u7eb3\u4ec0\u793e\u4f1a\u798f\u5229\u65b9\u6cd5\u6709\u6548\u5e73\u8861\u4e86\u5339\u914d\u6548\u7387\u548c\u516c\u5e73\u6027\uff0c\u63d0\u51fa\u7684\u03b1-SW\u65b9\u6cd5\u548c\u9ad8\u6548\u7b97\u6cd5\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.13856", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.13856", "abs": "https://arxiv.org/abs/2601.13856", "authors": ["Wei Ye", "Yixin Su", "Yueguo Chen", "Longxiang Gao", "Jianjun Li", "Ruixuan Li", "Rui Zhang"], "title": "Question-Focused Filtering for Knowledge-based VQA", "comment": null, "summary": "Knowledge-based Visual Question Answering (KB-VQA) aims to answer questions by integrating images with external knowledge. Effective knowledge filtering is crucial for improving accuracy. Typical filtering methods use similarity metrics to locate relevant article sections from one article, leading to information selection errors at the article and intra-article levels. Although recent explorations of Multimodal Large Language Model (MLLM)-based filtering methods demonstrate superior semantic understanding and cross-article filtering capabilities, their high computational cost limits practical application. To address these issues, this paper proposes a question-focused filtering method. This approach can perform question-focused, cross-article filtering, efficiently obtaining high-quality filtered knowledge while keeping computational costs comparable to typical methods. Specifically, we design a trainable Question-Focused Filter (QFF) and a Chunk-based Dynamic Multi-Article Selection (CDA) module, which collectively alleviate information selection errors at both the article and intra-article levels. Experiments show that our method outperforms current state-of-the-art models by 4.9% on E-VQA and 3.8% on InfoSeek, validating its effectiveness. The code is publicly available at: https://github.com/leaffeall/QKVQA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u95ee\u9898\u805a\u7126\u7684\u77e5\u8bc6\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u7528\u4e8e\u77e5\u8bc6\u5e93\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\uff0c\u901a\u8fc7\u53ef\u8bad\u7ec3\u7684\u95ee\u9898\u805a\u7126\u8fc7\u6ee4\u5668\u548c\u57fa\u4e8e\u5206\u5757\u7684\u52a8\u6001\u591a\u6587\u7ae0\u9009\u62e9\u6a21\u5757\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6210\u672c\u53ef\u63a7\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u77e5\u8bc6\u8fc7\u6ee4\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u4f20\u7edf\u57fa\u4e8e\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u7684\u65b9\u6cd5\u53ea\u80fd\u4ece\u5355\u7bc7\u6587\u7ae0\u4e2d\u5b9a\u4f4d\u76f8\u5173\u90e8\u5206\uff0c\u5bfc\u81f4\u6587\u7ae0\u7ea7\u548c\u6587\u7ae0\u5185\u90e8\u7684\u4fe1\u606f\u9009\u62e9\u9519\u8bef\uff1b2\uff09\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8fc7\u6ee4\u65b9\u6cd5\u867d\u7136\u5177\u6709\u66f4\u597d\u7684\u8bed\u4e49\u7406\u89e3\u548c\u8de8\u6587\u7ae0\u8fc7\u6ee4\u80fd\u529b\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u95ee\u9898\u805a\u7126\u7684\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1\uff09\u53ef\u8bad\u7ec3\u7684\u95ee\u9898\u805a\u7126\u8fc7\u6ee4\u5668\uff08QFF\uff09\uff0c\u7528\u4e8e\u8fdb\u884c\u95ee\u9898\u805a\u7126\u7684\u8de8\u6587\u7ae0\u8fc7\u6ee4\uff1b2\uff09\u57fa\u4e8e\u5206\u5757\u7684\u52a8\u6001\u591a\u6587\u7ae0\u9009\u62e9\uff08CDA\uff09\u6a21\u5757\uff0c\u5171\u540c\u7f13\u89e3\u6587\u7ae0\u7ea7\u548c\u6587\u7ae0\u5185\u90e8\u7684\u4fe1\u606f\u9009\u62e9\u9519\u8bef\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728E-VQA\u6570\u636e\u96c6\u4e0a\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u6a21\u578b\u9ad8\u51fa4.9%\uff0c\u5728InfoSeek\u6570\u636e\u96c6\u4e0a\u9ad8\u51fa3.8%\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\u7684\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u63d0\u51fa\u7684\u95ee\u9898\u805a\u7126\u8fc7\u6ee4\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u83b7\u53d6\u9ad8\u8d28\u91cf\u8fc7\u6ee4\u77e5\u8bc6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u77e5\u8bc6\u8fc7\u6ee4\u65b9\u6cd5\u5728\u4fe1\u606f\u9009\u62e9\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u7684\u95ee\u9898\uff0c\u4e3a\u77e5\u8bc6\u5e93\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13938", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13938", "abs": "https://arxiv.org/abs/2601.13938", "authors": ["Heyang Zhou", "JiaJia Chen", "Xiaolu Chen", "Jie Bao", "Zhen Chen", "Yong Liao"], "title": "IF-GEO: Conflict-Aware Instruction Fusion for Multi-Query Generative Engine Optimization", "comment": "9 pages, 3 figures. Submitted to ACL 2026. Corresponding author: Zhen Chen", "summary": "As Generative Engines revolutionize information retrieval by synthesizing direct answers from retrieved sources, ensuring source visibility becomes a significant challenge. Improving it through targeted content revisions is a practical strategy termed Generative Engine Optimization (GEO). However, optimizing a document for diverse queries presents a constrained optimization challenge where heterogeneous queries often impose conflicting and competing revision requirements under a limited content budget. To address this challenge, we propose IF-GEO, a \"diverge-then-converge\" framework comprising two phases: (i) mining distinct optimization preferences from representative latent queries; (ii) synthesizing a Global Revision Blueprint for guided editing by coordinating preferences via conflict-aware instruction fusion. To explicitly quantify IF-GEO's objective of cross-query stability, we introduce risk-aware stability metrics. Experiments on multi-query benchmarks demonstrate that IF-GEO achieves substantial performance gains while maintaining robustness across diverse retrieval scenarios.", "AI": {"tldr": "IF-GEO\u662f\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u5f15\u64ce\u4f18\u5316\u7684\"\u53d1\u6563-\u6536\u655b\"\u6846\u67b6\uff0c\u901a\u8fc7\u6316\u6398\u4e0d\u540c\u67e5\u8be2\u7684\u4f18\u5316\u504f\u597d\u5e76\u5408\u6210\u5168\u5c40\u4fee\u8ba2\u84dd\u56fe\uff0c\u89e3\u51b3\u591a\u67e5\u8be2\u4f18\u5316\u4e2d\u7684\u51b2\u7a81\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f15\u64ce\u901a\u8fc7\u68c0\u7d22\u6e90\u5408\u6210\u76f4\u63a5\u7b54\u6848\u6765\u9769\u65b0\u4fe1\u606f\u68c0\u7d22\uff0c\u786e\u4fdd\u6e90\u53ef\u89c1\u6027\u6210\u4e3a\u91cd\u8981\u6311\u6218\u3002\u9488\u5bf9\u4e0d\u540c\u67e5\u8be2\u4f18\u5316\u6587\u6863\u65f6\uff0c\u5f02\u8d28\u67e5\u8be2\u5728\u6709\u9650\u5185\u5bb9\u9884\u7b97\u4e0b\u5f80\u5f80\u4ea7\u751f\u51b2\u7a81\u548c\u7ade\u4e89\u7684\u4fee\u8ba2\u9700\u6c42\u3002", "method": "\u63d0\u51faIF-GEO\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a(1)\u4ece\u4ee3\u8868\u6027\u6f5c\u5728\u67e5\u8be2\u4e2d\u6316\u6398\u4e0d\u540c\u7684\u4f18\u5316\u504f\u597d\uff1b(2)\u901a\u8fc7\u51b2\u7a81\u611f\u77e5\u7684\u6307\u4ee4\u878d\u5408\u534f\u8c03\u504f\u597d\uff0c\u5408\u6210\u5168\u5c40\u4fee\u8ba2\u84dd\u56fe\u4ee5\u6307\u5bfc\u7f16\u8f91\u3002\u5f15\u5165\u98ce\u9669\u611f\u77e5\u7a33\u5b9a\u6027\u6307\u6807\u6765\u91cf\u5316\u8de8\u67e5\u8be2\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u591a\u67e5\u8be2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cIF-GEO\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u5728\u4e0d\u540c\u68c0\u7d22\u573a\u666f\u4e0b\u4fdd\u6301\u4e86\u9c81\u68d2\u6027\u3002", "conclusion": "IF-GEO\u901a\u8fc7\u534f\u8c03\u51b2\u7a81\u7684\u67e5\u8be2\u504f\u597d\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u751f\u6210\u5f15\u64ce\u4f18\u5316\u4e2d\u7684\u591a\u67e5\u8be2\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6587\u6863\u5728\u751f\u6210\u5f15\u64ce\u4e2d\u7684\u53ef\u89c1\u6027\u3002"}}
{"id": "2601.14001", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14001", "abs": "https://arxiv.org/abs/2601.14001", "authors": ["Niall McGuire", "Yashar Moshfeghi"], "title": "Auditory Brain Passage Retrieval: Cross-Sensory EEG Training for Neural Information Retrieval", "comment": "Accepted At ECIR 2026", "summary": "Query formulation from internal information needs remains fundamentally challenging across all Information Retrieval paradigms due to cognitive complexity and physical impairments. Brain Passage Retrieval (BPR) addresses this by directly mapping EEG signals to passage representations without intermediate text translation. However, existing BPR research exclusively uses visual stimuli, leaving critical questions unanswered: Can auditory EEG enable effective retrieval for voice-based interfaces and visually impaired users? Can training on combined EEG datasets from different sensory modalities improve performance despite severe data scarcity? We present the first systematic investigation of auditory EEG for BPR and evaluate cross-sensory training benefits. Using dual encoder architectures with four pooling strategies (CLS, mean, max, multi-vector), we conduct controlled experiments comparing auditory-only, visual-only, and combined training on the Alice (auditory) and Nieuwland (visual) datasets. Results demonstrate that auditory EEG consistently outperforms visual EEG, and cross-sensory training with CLS pooling achieves substantial improvements over individual training: 31% in MRR (0.474), 43% in Hit@1 (0.314), and 28% in Hit@10 (0.858). Critically, combined auditory EEG models surpass BM25 text baselines (MRR: 0.474 vs 0.428), establishing neural queries as competitive with traditional retrieval whilst enabling accessible interfaces. These findings validate auditory neural interfaces for IR tasks and demonstrate that cross-sensory training addresses data scarcity whilst outperforming single-modality approaches Code: https://github.com/NiallMcguire/Audio_BPR", "AI": {"tldr": "\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u542c\u89c9EEG\u7528\u4e8e\u8111\u90e8\u6bb5\u843d\u68c0\u7d22\uff0c\u53d1\u73b0\u542c\u89c9EEG\u4f18\u4e8e\u89c6\u89c9EEG\uff0c\u8de8\u611f\u5b98\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u751a\u81f3\u8d85\u8d8aBM25\u6587\u672c\u57fa\u7ebf", "motivation": "\u73b0\u6709\u8111\u90e8\u6bb5\u843d\u68c0\u7d22\u7814\u7a76\u4ec5\u4f7f\u7528\u89c6\u89c9\u523a\u6fc0\uff0c\u65e0\u6cd5\u56de\u7b54\u5173\u952e\u95ee\u9898\uff1a\u542c\u89c9EEG\u80fd\u5426\u652f\u6301\u8bed\u97f3\u754c\u9762\u548c\u89c6\u969c\u7528\u6237\uff1f\u8de8\u611f\u5b98\u8bad\u7ec3\u80fd\u5426\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u63d0\u5347\u6027\u80fd\uff1f", "method": "\u4f7f\u7528\u53cc\u7f16\u7801\u5668\u67b6\u6784\u548c\u56db\u79cd\u6c60\u5316\u7b56\u7565\uff08CLS\u3001\u5747\u503c\u3001\u6700\u5927\u3001\u591a\u5411\u91cf\uff09\uff0c\u5728Alice\uff08\u542c\u89c9\uff09\u548cNieuwland\uff08\u89c6\u89c9\uff09\u6570\u636e\u96c6\u4e0a\u5bf9\u6bd4\u7eaf\u542c\u89c9\u3001\u7eaf\u89c6\u89c9\u548c\u7ec4\u5408\u8bad\u7ec3", "result": "\u542c\u89c9EEG\u59cb\u7ec8\u4f18\u4e8e\u89c6\u89c9EEG\uff0c\u8de8\u611f\u5b98\u8bad\u7ec3\uff08CLS\u6c60\u5316\uff09\u76f8\u6bd4\u5355\u72ec\u8bad\u7ec3\u663e\u8457\u63d0\u5347\uff1aMRR\u63d0\u9ad831%\uff080.474\uff09\uff0cHit@1\u63d0\u9ad843%\uff080.314\uff09\uff0cHit@10\u63d0\u9ad828%\uff080.858\uff09\u3002\u7ec4\u5408\u542c\u89c9EEG\u6a21\u578b\u8d85\u8d8aBM25\u6587\u672c\u57fa\u7ebf\uff08MRR: 0.474 vs 0.428\uff09", "conclusion": "\u9a8c\u8bc1\u4e86\u542c\u89c9\u795e\u7ecf\u63a5\u53e3\u5728\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u8de8\u611f\u5b98\u8bad\u7ec3\u65e2\u80fd\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u53c8\u4f18\u4e8e\u5355\u6a21\u6001\u65b9\u6cd5\uff0c\u4f7f\u795e\u7ecf\u67e5\u8be2\u4e0e\u4f20\u7edf\u68c0\u7d22\u76f8\u7ade\u4e89\uff0c\u540c\u65f6\u652f\u6301\u65e0\u969c\u788d\u754c\u9762"}}
{"id": "2601.14224", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.14224", "abs": "https://arxiv.org/abs/2601.14224", "authors": ["Sahel Sharifymoghaddam", "Jimmy Lin"], "title": "Rerank Before You Reason: Analyzing Reranking Tradeoffs through Effective Token Cost in Deep Search Agents", "comment": "10 pages, 7 figures", "summary": "Deep research agents rely on iterative retrieval and reasoning to answer complex queries, but scaling test-time computation raises significant efficiency concerns. We study how to allocate reasoning budget in deep search pipelines, focusing on the role of listwise reranking. Using the BrowseComp-Plus benchmark, we analyze tradeoffs between model scale, reasoning effort, reranking depth, and total token cost via a novel effective token cost (ETC) metric. Our results show that reranking consistently improves retrieval and end-to-end accuracy, and that moderate reranking often yields larger gains than increasing search-time reasoning, achieving comparable accuracy at substantially lower cost. All our code is available at https://github.com/texttron/BrowseComp-Plus.git", "AI": {"tldr": "\u7814\u7a76\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u4e2d\u63a8\u7406\u9884\u7b97\u5206\u914d\u95ee\u9898\uff0c\u53d1\u73b0\u9002\u5ea6\u7684\u5217\u8868\u91cd\u6392\u5e8f\u6bd4\u589e\u52a0\u641c\u7d22\u65f6\u63a8\u7406\u66f4\u6709\u6548\uff0c\u80fd\u4ee5\u66f4\u4f4e\u6210\u672c\u83b7\u5f97\u53ef\u6bd4\u7cbe\u5ea6", "motivation": "\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u4f9d\u8d56\u8fed\u4ee3\u68c0\u7d22\u548c\u63a8\u7406\u56de\u7b54\u590d\u6742\u67e5\u8be2\uff0c\u4f46\u6269\u5c55\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u4f1a\u5e26\u6765\u663e\u8457\u7684\u6548\u7387\u95ee\u9898\u3002\u9700\u8981\u7814\u7a76\u5982\u4f55\u5728\u6df1\u5ea6\u641c\u7d22\u6d41\u7a0b\u4e2d\u5206\u914d\u63a8\u7406\u9884\u7b97\uff0c\u7279\u522b\u5173\u6ce8\u5217\u8868\u91cd\u6392\u5e8f\u7684\u4f5c\u7528\u3002", "method": "\u4f7f\u7528BrowseComp-Plus\u57fa\u51c6\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u6709\u6548\u4ee4\u724c\u6210\u672c\uff08ETC\uff09\u6307\u6807\uff0c\u5206\u6790\u6a21\u578b\u89c4\u6a21\u3001\u63a8\u7406\u52aa\u529b\u3001\u91cd\u6392\u5e8f\u6df1\u5ea6\u548c\u603b\u4ee4\u724c\u6210\u672c\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "result": "\u91cd\u6392\u5e8f\u6301\u7eed\u6539\u8fdb\u68c0\u7d22\u548c\u7aef\u5230\u7aef\u51c6\u786e\u6027\uff1b\u9002\u5ea6\u7684\u91cd\u6392\u5e8f\u901a\u5e38\u6bd4\u589e\u52a0\u641c\u7d22\u65f6\u63a8\u7406\u5e26\u6765\u66f4\u5927\u6536\u76ca\uff0c\u80fd\u4ee5\u663e\u8457\u66f4\u4f4e\u7684\u6210\u672c\u5b9e\u73b0\u53ef\u6bd4\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u5728\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u4e2d\uff0c\u9002\u5ea6\u7684\u5217\u8868\u91cd\u6392\u5e8f\u662f\u6bd4\u589e\u52a0\u641c\u7d22\u65f6\u63a8\u7406\u66f4\u6709\u6548\u7684\u9884\u7b97\u5206\u914d\u7b56\u7565\uff0c\u80fd\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u6210\u672c\u3002"}}
{"id": "2601.14245", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.14245", "abs": "https://arxiv.org/abs/2601.14245", "authors": ["Zhongyu Yang", "Wei Pang", "Yingfang Yuan"], "title": "XR: Cross-Modal Agents for Composed Image Retrieval", "comment": "Accepted by WWW 2026. Project: https://01yzzyu.github.io/xr.github.io/", "summary": "Retrieval is being redefined by agentic AI, demanding multimodal reasoning beyond conventional similarity-based paradigms. Composed Image Retrieval (CIR) exemplifies this shift as each query combines a reference image with textual modifications, requiring compositional understanding across modalities. While embedding-based CIR methods have achieved progress, they remain narrow in perspective, capturing limited cross-modal cues and lacking semantic reasoning. To address these limitations, we introduce XR, a training-free multi-agent framework that reframes retrieval as a progressively coordinated reasoning process. It orchestrates three specialized types of agents: imagination agents synthesize target representations through cross-modal generation, similarity agents perform coarse filtering via hybrid matching, and question agents verify factual consistency through targeted reasoning for fine filtering. Through progressive multi-agent coordination, XR iteratively refines retrieval to meet both semantic and visual query constraints, achieving up to a 38% gain over strong training-free and training-based baselines on FashionIQ, CIRR, and CIRCO, while ablations show each agent is essential. Code is available: https://01yzzyu.github.io/xr.github.io/.", "AI": {"tldr": "XR\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u68c0\u7d22\u91cd\u6784\u4e3a\u6e10\u8fdb\u5f0f\u534f\u8c03\u63a8\u7406\u8fc7\u7a0b\uff0c\u901a\u8fc7\u4e09\u7c7b\u667a\u80fd\u4f53\uff08\u60f3\u8c61\u3001\u76f8\u4f3c\u6027\u3001\u95ee\u9898\uff09\u534f\u540c\u5de5\u4f5c\uff0c\u5728\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\u4efb\u52a1\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5d4c\u5165\u7684\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\u65b9\u6cd5\u5b58\u5728\u89c6\u89d2\u72ed\u7a84\u3001\u8de8\u6a21\u6001\u7ebf\u7d22\u6355\u6349\u6709\u9650\u3001\u7f3a\u4e4f\u8bed\u4e49\u63a8\u7406\u7b49\u95ee\u9898\uff0c\u65e0\u6cd5\u6ee1\u8db3\u667a\u80fd\u4f53AI\u65f6\u4ee3\u5bf9\u591a\u6a21\u6001\u63a8\u7406\u7684\u9700\u6c42\u3002", "method": "XR\u6846\u67b6\u5305\u542b\u4e09\u7c7b\u667a\u80fd\u4f53\uff1a\u60f3\u8c61\u667a\u80fd\u4f53\u901a\u8fc7\u8de8\u6a21\u6001\u751f\u6210\u5408\u6210\u76ee\u6807\u8868\u793a\uff1b\u76f8\u4f3c\u6027\u667a\u80fd\u4f53\u901a\u8fc7\u6df7\u5408\u5339\u914d\u8fdb\u884c\u7c97\u7b5b\u9009\uff1b\u95ee\u9898\u667a\u80fd\u4f53\u901a\u8fc7\u9488\u5bf9\u6027\u63a8\u7406\u8fdb\u884c\u4e8b\u5b9e\u4e00\u81f4\u6027\u9a8c\u8bc1\u548c\u7ec6\u7b5b\u9009\u3002\u901a\u8fc7\u6e10\u8fdb\u5f0f\u591a\u667a\u80fd\u4f53\u534f\u8c03\u8fed\u4ee3\u4f18\u5316\u68c0\u7d22\u7ed3\u679c\u3002", "result": "\u5728FashionIQ\u3001CIRR\u548cCIRCO\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cXR\u6bd4\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u9ad8\u8fbe38%\uff0c\u6d88\u878d\u5b9e\u9a8c\u8bc1\u660e\u6bcf\u4e2a\u667a\u80fd\u4f53\u90fd\u662f\u5fc5\u8981\u7684\u3002", "conclusion": "XR\u901a\u8fc7\u5c06\u68c0\u7d22\u91cd\u6784\u4e3a\u6e10\u8fdb\u5f0f\u591a\u667a\u80fd\u4f53\u63a8\u7406\u8fc7\u7a0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\u4e2d\u7684\u8bed\u4e49\u548c\u89c6\u89c9\u7ea6\u675f\u95ee\u9898\uff0c\u4e3a\u667a\u80fd\u4f53AI\u65f6\u4ee3\u7684\u68c0\u7d22\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
