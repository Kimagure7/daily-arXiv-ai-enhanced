<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 7]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [FITMM: Adaptive Frequency-Aware Multimodal Recommendation via Information-Theoretic Representation Learning](https://arxiv.org/abs/2601.22498)
*Wei Yang,Rui Zhong,Yiqun Chen,Shixuan Li,Heng Ping,Chi Lu,Peng Jiang*

Main category: cs.IR

TL;DR: FITMM：基于频率感知信息理论的跨模态推荐框架，通过频谱分解和多模态频带对齐提升推荐性能


<details>
  <summary>Details</summary>
Motivation: 现有多模态推荐系统在空间域融合模态，掩盖了信号的频率结构，放大了模态间的错位和冗余问题

Method: 提出FITMM框架：构建图增强的物品表示，进行模态级频谱分解获得正交频带，构建轻量级频带内多模态组件，通过残差任务自适应门聚合频带，引入频率域信息瓶颈正则化和跨模态频谱一致性损失

Result: 在三个真实世界数据集上的广泛实验表明，FITMM持续且显著优于先进基线方法

Conclusion: 频率感知的信息理论框架为多模态推荐提供了更有效的模态融合方法，通过频谱分解和频带对齐解决了模态错位和冗余问题

Abstract: Multimodal recommendation aims to enhance user preference modeling by leveraging rich item content such as images and text. Yet dominant systems fuse modalities in the spatial domain, obscuring the frequency structure of signals and amplifying misalignment and redundancy. We adopt a spectral information-theoretic view and show that, under an orthogonal transform that approximately block-diagonalizes bandwise covariances, the Gaussian Information Bottleneck objective decouples across frequency bands, providing a principled basis for separate-then-fuse paradigm. Building on this foundation, we propose FITMM, a Frequency-aware Information-Theoretic framework for multimodal recommendation. FITMM constructs graph-enhanced item representations, performs modality-wise spectral decomposition to obtain orthogonal bands, and forms lightweight within-band multimodal components. A residual, task-adaptive gate aggregates bands into the final representation. To control redundancy and improve generalization, we regularize training with a frequency-domain IB term that allocates capacity across bands (Wiener-like shrinkage with shut-off of weak bands). We further introduce a cross-modal spectral consistency loss that aligns modalities within each band. The model is jointly optimized with the standard recommendation loss. Extensive experiments on three real-world datasets demonstrate that FITMM consistently and significantly outperforms advanced baselines.

</details>


### [2] [SCaLRec: Semantic Calibration for LLM-enabled Cloud-Device Sequential Recommendation](https://arxiv.org/abs/2601.22543)
*Ruiqi Zheng,Jinli Cao,Jiao Yin,Hongzhi Yin*

Main category: cs.IR

TL;DR: SCaLRec通过设备端语义校准模块，在无法实时调用云端LLM时，利用最新交互证据调整缓存的语义嵌入，解决云端语义陈旧导致的推荐性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 云端-设备协同推荐中，由于无法为每个请求实时调用云端LLM生成语义用户表示，导致缓存的语义嵌入变得陈旧，与用户最新交互不匹配，造成推荐性能下降。

Method: 提出SCaLRec框架：1) 评估缓存语义在用户最新交互下的可靠性；2) 在设备端设计语义校准模块，利用最新交互证据调整缓存的语义嵌入，无需每次请求都调用云端LLM。

Result: 在真实世界数据集上的实验表明，SCaLRec在云端语义陈旧情况下，相比强基线方法能持续提升推荐性能。

Conclusion: SCaLRec有效解决了云端语义陈旧问题，通过在设备端校准缓存语义嵌入，实现了在无法实时调用云端LLM时的推荐质量保持，为云端-设备协同推荐提供了实用解决方案。

Abstract: Cloud-device collaborative recommendation partitions computation across the cloud and user devices: the cloud provides semantic user modeling, while the device leverages recent interactions and cloud semantic signals for privacy-preserving, responsive reranking. With large language models (LLMs) on the cloud, semantic user representations can improve sequential recommendation by capturing high-level intent. However, regenerating such representations via cloud LLM inference for every request is often infeasible at real-world scale. As a result, on-device reranking commonly reuses a cached cloud semantic user embedding across requests. We empirically identify a cloud semantic staleness effect: reused embeddings become less aligned with the user's latest interactions, leading to measurable ranking degradation.
  Most existing LLM-enabled cloud-device recommenders are typically designed around on-demand cloud semantics, either by assuming low-latency cloud LLM access or by regenerating semantic embeddings per request. When per-request regeneration is infeasible and cached semantics must be reused, two technical challenges arise: (1) deciding when cached cloud semantics remain useful for on-device reranking, and (2) maintaining ranking quality when the cloud LLM cannot be invoked and only cached semantics are available. To address this gap, we introduce the Semantic Calibration for LLM-enabled Cloud-Device Recommendation (SCaLRec). First, it estimates the reliability of cached semantics under the user's latest interactions. Second, an on-device semantic calibration module is proposed to adjusts the cached semantic embedding on-device using up-to-date interaction evidence, without per-request cloud LLM involvement. Experiments on real-world datasets show that SCaLRec consistently improves recommendation performance over strong baselines under cloud semantic staleness.

</details>


### [3] [PersonaAct: Simulating Short-Video Users with Personalized Agents for Counterfactual Filter Bubble Auditing](https://arxiv.org/abs/2601.22547)
*Shilong Zhao,Qinggang Yang,Zhiyi Yin,Xiaoshi Wang,Zhenxing Chen,Du Su,Xueqi Cheng*

Main category: cs.IR

TL;DR: 提出PersonaAct框架，通过基于真实行为数据训练的多模态智能体模拟短视频用户，用于审计推荐系统中的信息茧房现象


<details>
  <summary>Details</summary>
Motivation: 短视频平台依赖个性化推荐，引发信息茧房担忧。现有审计方法成本高、隐私敏感，现有模拟器依赖文本信号且个性化弱，无法真实再现用户行为

Method: 提出PersonaAct框架：1) 通过自动访谈结合行为分析和结构化提问合成可解释的用户画像；2) 使用监督微调和强化学习在多模态观察上训练智能体；3) 部署智能体进行信息茧房审计，评估内容多样性和逃脱潜力

Result: 相比通用LLM基线有显著保真度提升，能真实再现用户行为。结果显示交互过程中内容显著窄化，但Bilibili平台表现出最强的逃脱潜力。发布了首个开源多模态短视频数据集和代码

Conclusion: PersonaAct框架能有效模拟短视频用户行为，支持可重复的推荐系统审计，揭示了信息茧房现象及其在不同平台上的差异

Abstract: Short-video platforms rely on personalized recommendation, raising concerns about filter bubbles that narrow content exposure. Auditing such phenomena at scale is challenging because real user studies are costly and privacy-sensitive, and existing simulators fail to reproduce realistic behaviors due to their reliance on textual signals and weak personalization. We propose PersonaAct, a framework for simulating short-video users with persona-conditioned multimodal agents trained on real behavioral traces for auditing filter bubbles in breadth and depth. PersonaAct synthesizes interpretable personas through automated interviews combining behavioral analysis with structured questioning, then trains agents on multimodal observations using supervised fine-tuning and reinforcement learning. We deploy trained agents for filter bubble auditing and evaluate bubble breadth via content diversity and bubble depth via escape potential. The evaluation demonstrates substantial improvements in fidelity over generic LLM baselines, enabling realistic behavior reproduction. Results reveal significant content narrowing over interaction. However, we find that Bilibili demonstrates the strongest escape potential. We release the first open multimodal short-video dataset and code to support reproducible auditing of recommender systems.

</details>


### [4] [Farewell to Item IDs: Unlocking the Scaling Potential of Large Ranking Models via Semantic Tokens](https://arxiv.org/abs/2601.22694)
*Zhen Zhao,Tong Zhang,Jie Xu,Qingliang Cai,Qile Zhang,Leyuan Yang,Daorui Xiao,Xiaojia Chang*

Main category: cs.IR

TL;DR: TRM框架使用语义token替代传统item ID，解决了大规模排序系统中物品快速变化导致的嵌入训练和维护问题，在减少存储的同时提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有大规模排序系统依赖item ID，将每个物品视为独立分类符号并映射到学习嵌入。随着物品快速出现和消失，这些嵌入变得难以训练和维护，这种不稳定性阻碍了神经网络参数的有效学习，限制了排序模型的可扩展性。

Method: 提出TRM框架，使用语义token替代item ID，改进token生成和应用流程。语义token具有更好的扩展潜力，能够更好地处理物品的动态变化。

Result: TRM实现了33%的稀疏存储减少和0.85%的AUC提升。实验表明当模型容量扩展时，TRM能持续超越最先进模型。在大规模个性化搜索引擎部署中，A/B测试显示用户活跃天数提升0.26%，查询变化率提升0.75%。

Conclusion: 语义token比item ID具有更大的扩展潜力，TRM框架通过改进token生成和应用流程，有效解决了大规模排序系统中物品动态变化带来的挑战，实现了存储效率和模型性能的双重提升。

Abstract: Recent studies on scaling up ranking models have achieved substantial improvement for recommendation systems and search engines. However, most large-scale ranking systems rely on item IDs, where each item is treated as an independent categorical symbol and mapped to a learned embedding. As items rapidly appear and disappear, these embeddings become difficult to train and maintain. This instability impedes effective learning of neural network parameters and limits the scalability of ranking models. In this paper, we show that semantic tokens possess greater scaling potential compared to item IDs. Our proposed framework TRM improves the token generation and application pipeline, leading to 33% reduction in sparse storage while achieving 0.85% AUC increase. Extensive experiments further show that TRM could consistently outperform state-of-the-art models when model capacity scales. Finally, TRM has been successfully deployed on large-scale personalized search engines, yielding 0.26% and 0.75% improvement on user active days and change query ratio respectively through A/B test.

</details>


### [5] [Compact Hypercube Embeddings for Fast Text-based Wildlife Observation Retrieval](https://arxiv.org/abs/2601.22783)
*Ilyass Moummad,Marius Miron,David Robinson,Kawtar Zaher,Hervé Goëau,Olivier Pietquin,Pierre Bonnet,Emmanuel Chemla,Matthieu Geist,Alexis Joly*

Main category: cs.IR

TL;DR: 提出紧凑超立方体嵌入框架，用于快速文本驱动的野生动物观测检索，通过二进制表示大幅降低大规模多模态数据库的存储和搜索成本。


<details>
  <summary>Details</summary>
Motivation: 大规模生物多样性监测平台依赖多模态野生动物观测数据，但现有基础模型的高维相似性搜索计算成本过高，难以从海量档案中高效检索相关观测记录。

Method: 基于跨视图代码对齐哈希框架，将轻量级哈希扩展到多模态设置，在共享汉明空间中对齐自然语言描述与视觉/听觉观测。利用预训练的野生动物基础模型（BioCLIP和BioLingual），通过参数高效微调适配哈希任务。

Result: 在iNaturalist2024（文本-图像检索）和iNatSounds2024（文本-音频检索）等大规模基准测试中，离散超立方体嵌入达到与连续嵌入相当甚至更优的性能，同时大幅降低内存和搜索成本。哈希目标持续改进底层编码器表示，增强检索和零样本泛化能力。

Conclusion: 二进制、基于语言的检索方法能够为生物多样性监测系统提供可扩展且高效的大规模野生动物档案搜索方案，在保持性能的同时显著降低计算开销。

Abstract: Large-scale biodiversity monitoring platforms increasingly rely on multimodal wildlife observations. While recent foundation models enable rich semantic representations across vision, audio, and language, retrieving relevant observations from massive archives remains challenging due to the computational cost of high-dimensional similarity search. In this work, we introduce compact hypercube embeddings for fast text-based wildlife observation retrieval, a framework that enables efficient text-based search over large-scale wildlife image and audio databases using compact binary representations. Building on the cross-view code alignment hashing framework, we extend lightweight hashing beyond a single-modality setup to align natural language descriptions with visual or acoustic observations in a shared Hamming space. Our approach leverages pretrained wildlife foundation models, including BioCLIP and BioLingual, and adapts them efficiently for hashing using parameter-efficient fine-tuning. We evaluate our method on large-scale benchmarks, including iNaturalist2024 for text-to-image retrieval and iNatSounds2024 for text-to-audio retrieval, as well as multiple soundscape datasets to assess robustness under domain shift. Results show that retrieval using discrete hypercube embeddings achieves competitive, and in several cases superior, performance compared to continuous embeddings, while drastically reducing memory and search cost. Moreover, we observe that the hashing objective consistently improves the underlying encoder representations, leading to stronger retrieval and zero-shot generalization. These results demonstrate that binary, language-based retrieval enables scalable and efficient search over large wildlife archives for biodiversity monitoring systems.

</details>


### [6] [BEAR: Towards Beam-Search-Aware Optimization for Recommendation with Large Language Models](https://arxiv.org/abs/2601.22925)
*Weiqin Yang,Bohao Wang,Zhenxiang Xu,Jiawei Chen,Shengjia Zhang,Jingbang Chen,Canghong Jin,Can Wang*

Main category: cs.IR

TL;DR: BEAR提出了一种新的微调目标，通过考虑束搜索行为来解决推荐系统中LLM训练与推理的不一致问题，显著提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的推荐方法存在训练-推理不一致问题：监督微调优化正样本的整体概率，但束搜索的贪婪剪枝机制可能因前缀概率不足而提前丢弃高整体概率的正样本。

Method: 提出BEAR（束搜索感知正则化）微调目标，强制正样本的每个token在每个解码步骤中排名都在前B个候选token内，这是一种松弛的必要条件，计算开销小。

Result: 在四个真实世界数据集上的实验表明，BEAR显著优于强基线方法。

Conclusion: BEAR通过显式考虑束搜索行为解决了训练-推理不一致问题，在推荐任务中取得了显著改进，且计算开销可忽略。

Abstract: Recent years have witnessed a rapid surge in research leveraging Large Language Models (LLMs) for recommendation. These methods typically employ supervised fine-tuning (SFT) to adapt LLMs to recommendation scenarios, and utilize beam search during inference to efficiently retrieve $B$ top-ranked recommended items. However, we identify a critical training-inference inconsistency: while SFT optimizes the overall probability of positive items, it does not guarantee that such items will be retrieved by beam search even if they possess high overall probabilities. Due to the greedy pruning mechanism, beam search can prematurely discard a positive item once its prefix probability is insufficient.
  To address this inconsistency, we propose BEAR (Beam-SEarch-Aware Regularization), a novel fine-tuning objective that explicitly accounts for beam search behavior during training. Rather than directly simulating beam search for each instance during training, which is computationally prohibitive, BEAR enforces a relaxed necessary condition: each token in a positive item must rank within the top-$B$ candidate tokens at each decoding step. This objective effectively mitigates the risk of incorrect pruning while incurring negligible computational overhead compared to standard SFT. Extensive experiments across four real-world datasets demonstrate that BEAR significantly outperforms strong baselines. Code will be released upon acceptance.

</details>


### [7] [OrLog: Resolving Complex Queries with LLMs and Probabilistic Reasoning](https://arxiv.org/abs/2601.23085)
*Mohanna Hoveyda,Jelle Piepenbrock,Arjen P de Vries,Maarten de Rijke,Faegheh Hasibi*

Main category: cs.IR

TL;DR: OrLog是一个神经符号检索框架，将谓词级可能性估计与逻辑推理解耦，使用LLM提供原子谓词的可能性分数，然后通过概率推理引擎计算查询满足的后验概率，实现约束感知检索。


<details>
  <summary>Details</summary>
Motivation: 当前检索系统要么忽略查询中的逻辑约束（如合取、析取、否定），要么在生成推理过程中近似处理这些约束，导致不一致和不可靠。现有的神经符号方法虽然适合结构化推理，但局限于形式逻辑或数学问题，假设查询无歧义且能获得完整证据，这在信息检索中很少见。

Method: OrLog框架将谓词级可能性估计与逻辑推理解耦：1）大型语言模型（LLM）在一次无解码前向传递中提供原子谓词的可能性分数；2）概率推理引擎根据这些分数推导查询满足的后验概率。该方法支持各种逻辑约束，并与多个骨干LLM、不同外部知识访问级别进行评估。

Result: 在提供实体描述的情况下，OrLog相比LLM推理方法显著提升了top-rank精度，特别是在析取查询上增益更大。同时效率更高，每个查询-实体对的平均token数减少约90%。

Conclusion: 无生成的谓词可能性估计结合概率推理能够实现约束感知检索，在性能上优于单一推理方法，同时使用更少的token。OrLog框架为处理复杂信息需求中的逻辑约束提供了有效解决方案。

Abstract: Resolving complex information needs that come with multiple constraints should consider enforcing the logical operators encoded in the query (i.e., conjunction, disjunction, negation) on the candidate answer set. Current retrieval systems either ignore these constraints in neural embeddings or approximate them in a generative reasoning process that can be inconsistent and unreliable. Although well-suited to structured reasoning, existing neuro-symbolic approaches remain confined to formal logic or mathematics problems as they often assume unambiguous queries and access to complete evidence, conditions rarely met in information retrieval. To bridge this gap, we introduce OrLog, a neuro-symbolic retrieval framework that decouples predicate-level plausibility estimation from logical reasoning: a large language model (LLM) provides plausibility scores for atomic predicates in one decoding-free forward pass, from which a probabilistic reasoning engine derives the posterior probability of query satisfaction. We evaluate OrLog across multiple backbone LLMs, varying levels of access to external knowledge, and a range of logical constraints, and compare it against base retrievers and LLM-as-reasoner methods. Provided with entity descriptions, OrLog can significantly boost top-rank precision compared to LLM reasoning with larger gains on disjunctive queries. OrLog is also more efficient, cutting mean tokens by $\sim$90\% per query-entity pair. These results demonstrate that generation-free predicate plausibility estimation combined with probabilistic reasoning enables constraint-aware retrieval that outperforms monolithic reasoning while using far fewer tokens.

</details>
