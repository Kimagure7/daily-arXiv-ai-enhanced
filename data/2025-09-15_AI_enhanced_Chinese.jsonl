{"id": "2509.09681", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09681", "abs": "https://arxiv.org/abs/2509.09681", "authors": ["Yikuan Xia", "Jiazun Chen", "Yirui Zhan", "Suifeng Zhao", "Weipeng Jiang", "Chaorui Zhang", "Wei Han", "Bo Bai", "Jun Gao"], "title": "DB3 Team's Solution For Meta KDD Cup' 25", "comment": null, "summary": "This paper presents the db3 team's winning solution for the Meta CRAG-MM\nChallenge 2025 at KDD Cup'25. Addressing the challenge's unique multi-modal,\nmulti-turn question answering benchmark (CRAG-MM), we developed a comprehensive\nframework that integrates tailored retrieval pipelines for different tasks with\na unified LLM-tuning approach for hallucination control. Our solution features\n(1) domain-specific retrieval pipelines handling image-indexed knowledge\ngraphs, web sources, and multi-turn conversations; and (2) advanced refusal\ntraining using SFT, DPO, and RL. The system achieved 2nd place in Task 1, 2nd\nplace in Task 2, and 1st place in Task 3, securing the grand prize for\nexcellence in ego-centric queries through superior handling of first-person\nperspective challenges.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86db3\u56e2\u961f\u5728KDD Cup'25 Meta CRAG-MM\u6311\u6218\u8d5b\u4e2d\u7684\u83b7\u80dc\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u901a\u8fc7\u591a\u6a21\u6001\u68c0\u7d22\u7ba1\u9053\u548c\u7edf\u4e00LLM\u8c03\u4f18\u65b9\u6cd5\uff0c\u5728\u4e09\u4e2a\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u4f18\u5f02\u6210\u7ee9\u5e76\u8d62\u5f97\u603b\u51a0\u519b\u3002", "motivation": "\u89e3\u51b3CRAG-MM\u6311\u6218\u8d5b\u4e2d\u7684\u591a\u6a21\u6001\u3001\u591a\u8f6e\u95ee\u7b54\u57fa\u51c6\u95ee\u9898\uff0c\u7279\u522b\u662f\u5904\u7406\u56fe\u50cf\u7d22\u5f15\u77e5\u8bc6\u56fe\u8c31\u3001\u7f51\u7edc\u6765\u6e90\u548c\u591a\u8f6e\u5bf9\u8bdd\u7684\u590d\u6742\u68c0\u7d22\u9700\u6c42\uff0c\u4ee5\u53ca\u63a7\u5236LLM\u5e7b\u89c9\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u542b(1)\u9488\u5bf9\u4e0d\u540c\u4efb\u52a1\u7684\u9886\u57df\u7279\u5b9a\u68c0\u7d22\u7ba1\u9053\uff0c\u5904\u7406\u56fe\u50cf\u7d22\u5f15\u77e5\u8bc6\u56fe\u8c31\u3001\u7f51\u7edc\u6765\u6e90\u548c\u591a\u8f6e\u5bf9\u8bdd\uff1b(2)\u4f7f\u7528SFT\u3001DPO\u548cRL\u8fdb\u884c\u9ad8\u7ea7\u62d2\u7edd\u8bad\u7ec3\u7684\u7edf\u4e00\u7684LLM\u8c03\u4f18\u65b9\u6cd5\u3002", "result": "\u5728Task 1\u4e2d\u83b7\u5f97\u7b2c2\u540d\uff0cTask 2\u4e2d\u83b7\u5f97\u7b2c2\u540d\uff0cTask 3\u4e2d\u83b7\u5f97\u7b2c1\u540d\uff0c\u901a\u8fc7\u4f18\u8d8a\u7684\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u5904\u7406\u8d62\u5f97\u4e86\u81ea\u6211\u4e2d\u5fc3\u67e5\u8be2\u65b9\u9762\u7684\u603b\u51a0\u519b\u3002", "conclusion": "\u8be5\u7efc\u5408\u6846\u67b6\u6210\u529f\u6574\u5408\u4e86\u4e13\u95e8\u5316\u68c0\u7d22\u7ba1\u9053\u548c\u5148\u8fdb\u7684\u62d2\u7edd\u8bad\u7ec3\u6280\u672f\uff0c\u5728\u591a\u6a21\u6001\u591a\u8f6e\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u6311\u6218\u65b9\u9762\u5177\u6709\u5353\u8d8a\u80fd\u529b\u3002"}}
{"id": "2509.09682", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.09682", "abs": "https://arxiv.org/abs/2509.09682", "authors": ["Maxim Zhelnin", "Dmitry Redko", "Volkov Daniil", "Anna Volodkevich", "Petr Sokerin", "Valeriy Shevchenko", "Egor Shvetsov", "Alexey Vasilev", "Darya Denisova", "Ruslan Izmailov", "Alexey Zaytsev"], "title": "Faster and Memory-Efficient Training of Sequential Recommendation Models for Large Catalogs", "comment": null, "summary": "Sequential recommendations (SR) with transformer-based architectures are\nwidely adopted in real-world applications, where SR models require frequent\nretraining to adapt to ever-changing user preferences. However, training\ntransformer-based SR models often encounters a high computational cost\nassociated with scoring extensive item catalogs, often exceeding thousands of\nitems. This occurs mainly due to the use of cross-entropy loss, where peak\nmemory scales proportionally to catalog size, batch size, and sequence length.\nRecognizing this, practitioners in the field of recommendation systems\ntypically address memory consumption by integrating the cross-entropy (CE) loss\nwith negative sampling, thereby reducing the explicit memory demands of the\nfinal layer. However, a small number of negative samples would degrade model\nperformance, and as we demonstrate in our work, increasing the number of\nnegative samples and the batch size further improves the model's performance,\nbut rapidly starts to exceed industrial GPUs' size (~40Gb).\n  In this work, we introduce the CCE- method, which offers a GPU-efficient\nimplementation of the CE loss with negative sampling. Our method accelerates\ntraining by up to two times while reducing memory consumption by more than 10\ntimes. Leveraging the memory savings afforded by using CCE- for model training,\nit becomes feasible to enhance its accuracy on datasets with a large item\ncatalog compared to those trained with original PyTorch-implemented loss\nfunctions. Finally, we perform an analysis of key memory-related\nhyperparameters and highlight the necessity of a delicate balance among these\nfactors. We demonstrate that scaling both the number of negative samples and\nbatch size leads to better results rather than maximizing only one of them. To\nfacilitate further adoption of CCE-, we release a Triton kernel that\nefficiently implements the proposed method.", "AI": {"tldr": "\u63d0\u51fa\u4e86CCE-\u65b9\u6cd5\uff0c\u4e00\u79cdGPU\u9ad8\u6548\u7684\u4ea4\u53c9\u71b5\u635f\u5931\u5b9e\u73b0\uff0c\u53ef\u52a0\u901f\u8bad\u7ec32\u500d\u5e76\u51cf\u5c1110\u500d\u4ee5\u4e0a\u5185\u5b58\u6d88\u8017\uff0c\u4f7f\u63a8\u8350\u7cfb\u7edf\u80fd\u591f\u4f7f\u7528\u66f4\u591a\u8d1f\u6837\u672c\u548c\u66f4\u5927\u6279\u6b21\u63d0\u5347\u6a21\u578b\u6027\u80fd", "motivation": "\u4f20\u7edf\u57fa\u4e8etransformer\u7684\u5e8f\u5217\u63a8\u8350\u6a21\u578b\u8bad\u7ec3\u65f6\u7531\u4e8e\u4ea4\u53c9\u71b5\u635f\u5931\u5bfc\u81f4\u5185\u5b58\u6d88\u8017\u4e0e\u5546\u54c1\u76ee\u5f55\u5927\u5c0f\u3001\u6279\u6b21\u5927\u5c0f\u548c\u5e8f\u5217\u957f\u5ea6\u6210\u6b63\u6bd4\uff0c\u5de5\u4e1aGPU\u5185\u5b58\u9650\u5236\u963b\u788d\u4e86\u4f7f\u7528\u66f4\u591a\u8d1f\u6837\u672c\u548c\u66f4\u5927\u6279\u6b21\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd", "method": "\u5f00\u53d1CCE-\u65b9\u6cd5\uff0c\u901a\u8fc7GPU\u9ad8\u6548\u5b9e\u73b0\u4ea4\u53c9\u71b5\u635f\u5931\u4e0e\u8d1f\u91c7\u6837\uff0c\u4f7f\u7528Triton\u5185\u6838\u4f18\u5316\u5b9e\u73b0\uff0c\u51cf\u5c11\u663e\u5f0f\u5185\u5b58\u9700\u6c42", "result": "\u8bad\u7ec3\u52a0\u901f\u8fbe2\u500d\uff0c\u5185\u5b58\u6d88\u8017\u51cf\u5c1110\u500d\u4ee5\u4e0a\uff0c\u5728\u5927\u578b\u5546\u54c1\u76ee\u5f55\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4\u539f\u59cbPyTorch\u5b9e\u73b0\u83b7\u5f97\u66f4\u597d\u7684\u51c6\u786e\u6027", "conclusion": "CCE-\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u63a8\u8350\u7cfb\u7edf\u8bad\u7ec3\u4e2d\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u540c\u65f6\u6269\u5c55\u8d1f\u6837\u672c\u6570\u91cf\u548c\u6279\u6b21\u5927\u5c0f\u6bd4\u53ea\u6700\u5927\u5316\u5176\u4e2d\u4e00\u4e2a\u80fd\u83b7\u5f97\u66f4\u597d\u7684\u7ed3\u679c\uff0c\u5e76\u53d1\u5e03\u4e86\u9ad8\u6548\u5b9e\u73b0\u7684Triton\u5185\u6838"}}
{"id": "2509.09683", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09683", "abs": "https://arxiv.org/abs/2509.09683", "authors": ["Briti Gangopadhyay", "Zhao Wang", "Shingo Takamatsu"], "title": "Forecasting Clicks in Digital Advertising: Multimodal Inputs and Interpretable Outputs", "comment": null, "summary": "Forecasting click volume is a key task in digital advertising, influencing\nboth revenue and campaign strategy. Traditional time series models rely solely\non numerical data, often overlooking rich contextual information embedded in\ntextual elements, such as keyword updates. We present a multimodal forecasting\nframework that combines click data with textual logs from real-world ad\ncampaigns and generates human-interpretable explanations alongside numeric\npredictions. Reinforcement learning is used to improve comprehension of textual\ninformation and enhance fusion of modalities. Experiments on a large-scale\nindustry dataset show that our method outperforms baselines in both accuracy\nand reasoning quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u70b9\u51fb\u6570\u636e\u548c\u6587\u672c\u65e5\u5fd7\u7684\u591a\u6a21\u6001\u9884\u6d4b\u6846\u67b6\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u6587\u672c\u4fe1\u606f\u7406\u89e3\u548c\u6a21\u6001\u878d\u5408\u6548\u679c\uff0c\u5728\u51c6\u786e\u6027\u548c\u63a8\u7406\u8d28\u91cf\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u4f20\u7edf\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u4ec5\u4f9d\u8d56\u6570\u503c\u6570\u636e\uff0c\u5ffd\u7565\u4e86\u5173\u952e\u8bcd\u66f4\u65b0\u7b49\u6587\u672c\u5143\u7d20\u4e2d\u7684\u4e30\u5bcc\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5f71\u54cd\u4e86\u70b9\u51fb\u91cf\u9884\u6d4b\u7684\u51c6\u786e\u6027", "method": "\u591a\u6a21\u6001\u9884\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u70b9\u51fb\u6570\u636e\u548c\u5e7f\u544a\u6d3b\u52a8\u7684\u6587\u672c\u65e5\u5fd7\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6765\u6539\u8fdb\u6587\u672c\u4fe1\u606f\u7406\u89e3\u548c\u6a21\u6001\u878d\u5408\uff0c\u540c\u65f6\u751f\u6210\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u89e3\u91ca", "result": "\u5728\u5927\u89c4\u6a21\u884c\u4e1a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u63a8\u7406\u8d28\u91cf\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b", "conclusion": "\u591a\u6a21\u6001\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5229\u7528\u6587\u672c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u70b9\u51fb\u91cf\u9884\u6d4b\u6027\u80fd\uff0c\u5e76\u4e3a\u9884\u6d4b\u7ed3\u679c\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027"}}
{"id": "2509.09684", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.09684", "abs": "https://arxiv.org/abs/2509.09684", "authors": ["Bruno Yui Yamate", "Thais Rodrigues Neubauer", "Marcelo Fantinato", "Sarajane Marques Peres"], "title": "Text-to-SQL Oriented to the Process Mining Domain: A PT-EN Dataset for Query Translation", "comment": "33 pages", "summary": "This paper introduces text-2-SQL-4-PM, a bilingual (Portuguese-English)\nbenchmark dataset designed for the text-to-SQL task in the process mining\ndomain. Text-to-SQL conversion facilitates natural language querying of\ndatabases, increasing accessibility for users without SQL expertise and\nproductivity for those that are experts. The text-2-SQL-4-PM dataset is\ncustomized to address the unique challenges of process mining, including\nspecialized vocabularies and single-table relational structures derived from\nevent logs. The dataset comprises 1,655 natural language utterances, including\nhuman-generated paraphrases, 205 SQL statements, and ten qualifiers. Methods\ninclude manual curation by experts, professional translations, and a detailed\nannotation process to enable nuanced analyses of task complexity. Additionally,\na baseline study using GPT-3.5 Turbo demonstrates the feasibility and utility\nof the dataset for text-to-SQL applications. The results show that\ntext-2-SQL-4-PM supports evaluation of text-to-SQL implementations, offering\nbroader applicability for semantic parsing and other natural language\nprocessing tasks.", "AI": {"tldr": "text-2-SQL-4-PM\u662f\u4e00\u4e2a\u9762\u5411\u6d41\u7a0b\u6316\u6398\u9886\u57df\u7684\u53cc\u8bed\u6587\u672c\u5230SQL\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b1655\u4e2a\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u548c205\u4e2aSQL\u8bed\u53e5\uff0c\u7528\u4e8e\u8bc4\u4f30\u6587\u672c\u5230SQL\u8f6c\u6362\u7cfb\u7edf\u3002", "motivation": "\u4e3a\u6d41\u7a0b\u6316\u6398\u9886\u57df\u5f00\u53d1\u4e13\u95e8\u7684\u6587\u672c\u5230SQL\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u8be5\u9886\u57df\u7279\u6709\u7684\u4e13\u4e1a\u8bcd\u6c47\u548c\u5355\u8868\u5173\u7cfb\u7ed3\u6784\u6311\u6218\uff0c\u4f7f\u975eSQL\u4e13\u5bb6\u4e5f\u80fd\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u6570\u636e\u5e93\u3002", "method": "\u901a\u8fc7\u4e13\u5bb6\u624b\u52a8\u6574\u7406\u3001\u4e13\u4e1a\u7ffb\u8bd1\u548c\u8be6\u7ec6\u6807\u6ce8\u6d41\u7a0b\u6784\u5efa\u6570\u636e\u96c6\uff0c\u5305\u62ec\u4eba\u7c7b\u751f\u6210\u7684\u91ca\u4e49\u67e5\u8be2\uff0c\u5e76\u4f7f\u7528GPT-3.5 Turbo\u8fdb\u884c\u57fa\u7ebf\u7814\u7a76\u9a8c\u8bc1\u53ef\u884c\u6027\u3002", "result": "\u6570\u636e\u96c6\u652f\u6301\u6587\u672c\u5230SQL\u5b9e\u73b0\u7684\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u5728\u6d41\u7a0b\u6316\u6398\u9886\u57df\u7684\u5e94\u7528\u53ef\u884c\u6027\uff0c\u5e76\u4e3a\u8bed\u4e49\u89e3\u6790\u7b49\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u63d0\u4f9b\u66f4\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002", "conclusion": "text-2-SQL-4-PM\u6570\u636e\u96c6\u4e3a\u6d41\u7a0b\u6316\u6398\u9886\u57df\u7684\u6587\u672c\u5230SQL\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u57fa\u51c6\u8d44\u6e90\uff0c\u4fc3\u8fdb\u4e86\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u6570\u636e\u5e93\u6280\u672f\u7684\u53d1\u5c55\u548c\u5e94\u7528\u3002"}}
{"id": "2509.09699", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09699", "abs": "https://arxiv.org/abs/2509.09699", "authors": ["Mingyang Li", "Viktor Schlegel", "Tingting Mu", "Warren Del-Pinto", "Goran Nenadic"], "title": "Structured Information Matters: Explainable ICD Coding with Patient-Level Knowledge Graphs", "comment": null, "summary": "Mapping clinical documents to standardised clinical vocabularies is an\nimportant task, as it provides structured data for information retrieval and\nanalysis, which is essential to clinical research, hospital administration and\nimproving patient care. However, manual coding is both difficult and\ntime-consuming, making it impractical at scale. Automated coding can\npotentially alleviate this burden, improving the availability and accuracy of\nstructured clinical data. The task is difficult to automate, as it requires\nmapping to high-dimensional and long-tailed target spaces, such as the\nInternational Classification of Diseases (ICD). While external knowledge\nsources have been readily utilised to enhance output code representation, the\nuse of external resources for representing the input documents has been\nunderexplored. In this work, we compute a structured representation of the\ninput documents, making use of document-level knowledge graphs (KGs) that\nprovide a comprehensive structured view of a patient's condition. The resulting\nknowledge graph efficiently represents the patient-centred input documents with\n23\\% of the original text while retaining 90\\% of the information. We assess\nthe effectiveness of this graph for automated ICD-9 coding by integrating it\ninto the state-of-the-art ICD coding architecture PLM-ICD. Our experiments\nyield improved Macro-F1 scores by up to 3.20\\% on popular benchmarks, while\nimproving training efficiency. We attribute this improvement to different types\nof entities and relationships in the KG, and demonstrate the improved\nexplainability potential of the approach over the text-only baseline.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u77e5\u8bc6\u56fe\u8c31\u7ed3\u6784\u5316\u8868\u793a\u4e34\u5e8a\u6587\u6863\uff0c\u7528\u4e8e\u81ea\u52a8\u5316ICD\u7f16\u7801\u4efb\u52a1\uff0c\u5728\u51cf\u5c11\u6587\u672c\u91cf\u7684\u540c\u65f6\u4fdd\u7559\u5173\u952e\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f16\u7801\u6027\u80fd", "motivation": "\u4e34\u5e8a\u6587\u6863\u5230\u6807\u51c6\u5316\u8bcd\u6c47\u7684\u6620\u5c04\u5bf9\u4e8e\u4e34\u5e8a\u7814\u7a76\u548c\u60a3\u8005\u62a4\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u624b\u52a8\u7f16\u7801\u8017\u65f6\u4e14\u96be\u4ee5\u6269\u5c55\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8f93\u51fa\u7f16\u7801\u8868\u793a\uff0c\u800c\u8f93\u5165\u6587\u6863\u7684\u5916\u90e8\u77e5\u8bc6\u8868\u793a\u7814\u7a76\u4e0d\u8db3", "method": "\u6784\u5efa\u6587\u6863\u7ea7\u77e5\u8bc6\u56fe\u8c31\u6765\u7ed3\u6784\u5316\u8868\u793a\u8f93\u5165\u6587\u6863\uff0c\u63d0\u4f9b\u60a3\u8005\u72b6\u51b5\u7684\u5168\u9762\u7ed3\u6784\u5316\u89c6\u56fe\u3002\u5c06\u77e5\u8bc6\u56fe\u8c31\u96c6\u6210\u5230\u6700\u5148\u8fdb\u7684PLM-ICD\u67b6\u6784\u4e2d\u8fdb\u884cICD-9\u7f16\u7801", "result": "\u77e5\u8bc6\u56fe\u8c31\u4ec5\u4f7f\u7528\u539f\u6587\u672c23%\u7684\u5185\u5bb9\u5c31\u4fdd\u7559\u4e8690%\u7684\u4fe1\u606f\uff0c\u5728\u6d41\u884c\u57fa\u51c6\u6d4b\u8bd5\u4e2dMacro-F1\u5206\u6570\u63d0\u5347\u9ad8\u8fbe3.20%\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387", "conclusion": "\u77e5\u8bc6\u56fe\u8c31\u65b9\u6cd5\u901a\u8fc7\u4e0d\u540c\u7c7b\u578b\u7684\u5b9e\u4f53\u548c\u5173\u7cfb\u6709\u6548\u63d0\u5347\u4e86ICD\u7f16\u7801\u6027\u80fd\uff0c\u76f8\u6bd4\u7eaf\u6587\u672c\u57fa\u7ebf\u5177\u6709\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u6f5c\u529b"}}
{"id": "2509.09744", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09744", "abs": "https://arxiv.org/abs/2509.09744", "authors": ["Mujie Liu", "Chenze Wang", "Liping Chen", "Nguyen Linh Dan Le", "Niharika Tewari", "Ting Dang", "Jiangang Ma", "Feng Xia"], "title": "Structure Matters: Brain Graph Augmentation via Learnable Edge Masking for Data-efficient Psychiatric Diagnosis", "comment": null, "summary": "The limited availability of labeled brain network data makes it challenging\nto achieve accurate and interpretable psychiatric diagnoses. While\nself-supervised learning (SSL) offers a promising solution, existing methods\noften rely on augmentation strategies that can disrupt crucial structural\nsemantics in brain graphs. To address this, we propose SAM-BG, a two-stage\nframework for learning brain graph representations with structural semantic\npreservation. In the pre-training stage, an edge masker is trained on a small\nlabeled subset to capture key structural semantics. In the SSL stage, the\nextracted structural priors guide a structure-aware augmentation process,\nenabling the model to learn more semantically meaningful and robust\nrepresentations. Experiments on two real-world psychiatric datasets demonstrate\nthat SAM-BG outperforms state-of-the-art methods, particularly in small-labeled\ndata settings, and uncovers clinically relevant connectivity patterns that\nenhance interpretability. Our code is available at\nhttps://github.com/mjliu99/SAM-BG.", "AI": {"tldr": "SAM-BG\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u8bed\u4e49\u4fdd\u62a4\u6765\u5b66\u4e60\u8111\u56fe\u8868\u793a\uff0c\u5728\u7cbe\u795e\u75be\u75c5\u8bca\u65ad\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u5c0f\u6837\u672c\u6807\u6ce8\u6570\u636e\u573a\u666f\u4e0b\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u8111\u56fe\u6570\u636e\u4e0a\u5f80\u5f80\u4f9d\u8d56\u53ef\u80fd\u7834\u574f\u5173\u952e\u7ed3\u6784\u8bed\u4e49\u7684\u589e\u5f3a\u7b56\u7565\uff0c\u800c\u6807\u6ce8\u8111\u7f51\u7edc\u6570\u636e\u7684\u7a00\u7f3a\u6027\u4f7f\u5f97\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u7cbe\u795e\u75be\u75c5\u8bca\u65ad\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u9884\u8bad\u7ec3\u9636\u6bb5\u5728\u5c0f\u6807\u6ce8\u5b50\u96c6\u4e0a\u8bad\u7ec3\u8fb9\u7f18\u63a9\u7801\u5668\u6355\u83b7\u5173\u952e\u7ed3\u6784\u8bed\u4e49\uff1b2\uff09\u81ea\u76d1\u7763\u5b66\u4e60\u9636\u6bb5\u4f7f\u7528\u63d0\u53d6\u7684\u7ed3\u6784\u5148\u9a8c\u6307\u5bfc\u7ed3\u6784\u611f\u77e5\u589e\u5f3a\u8fc7\u7a0b\uff0c\u5b66\u4e60\u66f4\u5177\u8bed\u4e49\u610f\u4e49\u548c\u9c81\u68d2\u6027\u7684\u8868\u793a\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u7cbe\u795e\u75be\u75c5\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSAM-BG\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5c0f\u6807\u6ce8\u6570\u636e\u8bbe\u7f6e\u4e0b\uff0c\u5e76\u53d1\u73b0\u4e86\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u7684\u4e34\u5e8a\u76f8\u5173\u8fde\u63a5\u6a21\u5f0f\u3002", "conclusion": "SAM-BG\u901a\u8fc7\u7ed3\u6784\u8bed\u4e49\u4fdd\u62a4\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8111\u56fe\u6570\u636e\u6807\u6ce8\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5728\u7cbe\u795e\u75be\u75c5\u8bca\u65ad\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2509.09685", "categories": ["cs.IR", "cs.AI", "cs.MM", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.09685", "abs": "https://arxiv.org/abs/2509.09685", "authors": ["Keunwoo Choi", "Seungheon Doh", "Juhan Nam"], "title": "TalkPlayData 2: An Agentic Synthetic Data Pipeline for Multimodal Conversational Music Recommendation", "comment": null, "summary": "We present TalkPlayData 2, a synthetic dataset for multimodal conversational\nmusic recommendation generated by an agentic data pipeline. In TalkPlayData 2\npipeline, multiple large language model (LLM) agents are created under various\nroles with specialized prompts and access to different parts of information,\nand the chat data is acquired by logging the conversation between the Listener\nLLM and the Recsys LLM. To cover various conversation scenarios, for each\nconversation, the Listener LLM is conditioned on a finetuned conversation goal.\nFinally, all the LLMs are multimodal with audio and images, allowing a\nsimulation of multimodal recommendation and conversation. In the LLM-as-a-judge\nand subjective evaluation experiments, TalkPlayData 2 achieved the proposed\ngoal in various aspects related to training a generative recommendation model\nfor music. TalkPlayData 2 and its generation code are open-sourced at\nhttps://talkpl.ai/talkplaydata2.html.", "AI": {"tldr": "TalkPlayData 2\u662f\u4e00\u4e2a\u901a\u8fc7\u591a\u667a\u80fd\u4f53LLM\u7ba1\u9053\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u5bf9\u8bdd\u5f0f\u97f3\u4e50\u63a8\u8350\uff0c\u5305\u542b\u97f3\u9891\u548c\u56fe\u50cf\u7684\u591a\u6a21\u6001\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u8bad\u7ec3\u751f\u6210\u5f0f\u97f3\u4e50\u63a8\u8350\u6a21\u578b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u591a\u6a21\u6001\u5bf9\u8bdd\u5f0f\u97f3\u4e50\u63a8\u8350\u7cfb\u7edf\u8bad\u7ec3\u6570\u636e\u7684\u7a00\u7f3a\u95ee\u9898\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\u6765\u521b\u5efa\u9ad8\u8d28\u91cf\u7684\u591a\u6a21\u6001\u5bf9\u8bdd\u6570\u636e\u96c6\u3002", "method": "\u4f7f\u7528\u591a\u89d2\u8272LLM\u667a\u80fd\u4f53\uff08Listener\u548cRecsys LLM\uff09\u8fdb\u884c\u5bf9\u8bdd\u751f\u6210\uff0c\u6bcf\u4e2a\u5bf9\u8bdd\u57fa\u4e8e\u5fae\u8c03\u7684\u5bf9\u8bdd\u76ee\u6807\uff0c\u667a\u80fd\u4f53\u5177\u5907\u97f3\u9891\u548c\u56fe\u50cf\u7684\u591a\u6a21\u6001\u5904\u7406\u80fd\u529b\u3002", "result": "\u5728LLM-as-a-judge\u548c\u4e3b\u89c2\u8bc4\u4f30\u5b9e\u9a8c\u4e2d\uff0cTalkPlayData 2\u5728\u8bad\u7ec3\u751f\u6210\u5f0f\u97f3\u4e50\u63a8\u8350\u6a21\u578b\u7684\u591a\u4e2a\u76f8\u5173\u65b9\u9762\u8fbe\u5230\u4e86\u9884\u671f\u76ee\u6807\u3002", "conclusion": "TalkPlayData 2\u6210\u529f\u521b\u5efa\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u591a\u6a21\u6001\u5bf9\u8bdd\u97f3\u4e50\u63a8\u8350\u5408\u6210\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u6e90\u4e86\u6570\u636e\u96c6\u548c\u751f\u6210\u4ee3\u7801\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2509.09700", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09700", "abs": "https://arxiv.org/abs/2509.09700", "authors": ["Malavika Suresh", "Rahaf Aljundi", "Ikechukwu Nkisi-Orji", "Nirmalie Wiratunga"], "title": "Cross-Layer Attention Probing for Fine-Grained Hallucination Detection", "comment": "To be published at the TRUST-AI workshop, ECAI 2025", "summary": "With the large-scale adoption of Large Language Models (LLMs) in various\napplications, there is a growing reliability concern due to their tendency to\ngenerate inaccurate text, i.e. hallucinations. In this work, we propose\nCross-Layer Attention Probing (CLAP), a novel activation probing technique for\nhallucination detection, which processes the LLM activations across the entire\nresidual stream as a joint sequence. Our empirical evaluations using five LLMs\nand three tasks show that CLAP improves hallucination detection compared to\nbaselines on both greedy decoded responses as well as responses sampled at\nhigher temperatures, thus enabling fine-grained detection, i.e. the ability to\ndisambiguate hallucinations and non-hallucinations among different sampled\nresponses to a given prompt. This allows us to propose a detect-then-mitigate\nstrategy using CLAP to reduce hallucinations and improve LLM reliability\ncompared to direct mitigation approaches. Finally, we show that CLAP maintains\nhigh reliability even when applied out-of-distribution.", "AI": {"tldr": "\u63d0\u51faCross-Layer Attention Probing (CLAP)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5904\u7406LLM\u6574\u4e2a\u6b8b\u5dee\u6d41\u7684\u6fc0\u6d3b\u4f5c\u4e3a\u8054\u5408\u5e8f\u5217\uff0c\u663e\u8457\u6539\u8fdb\u4e86\u5e7b\u89c9\u68c0\u6d4b\u6548\u679c\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u68c0\u6d4b\u548c\u68c0\u6d4b\u540e\u7f13\u89e3\u7b56\u7565\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5404\u79cd\u5e94\u7528\u4e2d\u7684\u5927\u89c4\u6a21\u91c7\u7528\uff0c\u5176\u751f\u6210\u4e0d\u51c6\u786e\u6587\u672c\uff08\u5e7b\u89c9\uff09\u7684\u503e\u5411\u5f15\u53d1\u4e86\u53ef\u9760\u6027\u62c5\u5fe7\uff0c\u9700\u8981\u6709\u6548\u7684\u68c0\u6d4b\u548c\u7f13\u89e3\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCLAP\u6fc0\u6d3b\u63a2\u6d4b\u6280\u672f\uff0c\u5c06LLM\u6574\u4e2a\u6b8b\u5dee\u6d41\u7684\u6fc0\u6d3b\u4f5c\u4e3a\u8054\u5408\u5e8f\u5217\u8fdb\u884c\u5904\u7406\uff0c\u7528\u4e8e\u5e7b\u89c9\u68c0\u6d4b\u3002\u5728\u4e94\u4e2aLLM\u548c\u4e09\u4e2a\u4efb\u52a1\u4e0a\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "CLAP\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5728\u8d2a\u5a6a\u89e3\u7801\u548c\u9ad8\u6e29\u91c7\u6837\u54cd\u5e94\u4e2d\u90fd\u6539\u8fdb\u4e86\u5e7b\u89c9\u68c0\u6d4b\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u68c0\u6d4b\u4e0d\u540c\u91c7\u6837\u54cd\u5e94\u4e2d\u7684\u5e7b\u89c9\u548c\u975e\u5e7b\u89c9\uff0c\u5e76\u80fd\u5728\u5206\u5e03\u5916\u4fdd\u6301\u9ad8\u53ef\u9760\u6027\u3002", "conclusion": "CLAP\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u652f\u6301\u68c0\u6d4b\u540e\u7f13\u89e3\u7b56\u7565\uff0c\u76f8\u6bd4\u76f4\u63a5\u7f13\u89e3\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u51cf\u5c11\u5e7b\u89c9\u5e76\u63d0\u9ad8LLM\u53ef\u9760\u6027\u3002"}}
{"id": "2509.09747", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.09747", "abs": "https://arxiv.org/abs/2509.09747", "authors": ["Leen Daher", "Zhaobo Wang", "Malcolm Mielle"], "title": "D-CAT: Decoupled Cross-Attention Transfer between Sensor Modalities for Unimodal Inference", "comment": null, "summary": "Cross-modal transfer learning is used to improve multi-modal classification\nmodels (e.g., for human activity recognition in human-robot collaboration).\nHowever, existing methods require paired sensor data at both training and\ninference, limiting deployment in resource-constrained environments where full\nsensor suites are not economically and technically usable. To address this, we\npropose Decoupled Cross-Attention Transfer (D-CAT), a framework that aligns\nmodality-specific representations without requiring joint sensor modality\nduring inference. Our approach combines a self-attention module for feature\nextraction with a novel cross-attention alignment loss, which enforces the\nalignment of sensors' feature spaces without requiring the coupling of the\nclassification pipelines of both modalities. We evaluate D-CAT on three\nmulti-modal human activity datasets (IMU, video, and audio) under both\nin-distribution and out-of-distribution scenarios, comparing against uni-modal\nmodels. Results show that in in-distribution scenarios, transferring from\nhigh-performing modalities (e.g., video to IMU) yields up to 10% F1-score gains\nover uni-modal training. In out-of-distribution scenarios, even weaker source\nmodalities (e.g., IMU to video) improve target performance, as long as the\ntarget model isn't overfitted on the training data. By enabling single-sensor\ninference with cross-modal knowledge, D-CAT reduces hardware redundancy for\nperception systems while maintaining accuracy, which is critical for\ncost-sensitive or adaptive deployments (e.g., assistive robots in homes with\nvariable sensor availability). Code is available at\nhttps://github.com/Schindler-EPFL-Lab/D-CAT.", "AI": {"tldr": "D-CAT\u662f\u4e00\u79cd\u89e3\u8026\u7684\u8de8\u6ce8\u610f\u529b\u8fc1\u79fb\u6846\u67b6\uff0c\u5141\u8bb8\u5728\u63a8\u7406\u65f6\u4ec5\u4f7f\u7528\u5355\u4e00\u4f20\u611f\u5668\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u77e5\u8bc6\u8fc1\u79fb\u63d0\u5347\u6027\u80fd\uff0c\u51cf\u5c11\u786c\u4ef6\u5197\u4f59\u3002", "motivation": "\u73b0\u6709\u8de8\u6a21\u6001\u8fc1\u79fb\u65b9\u6cd5\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u90fd\u9700\u8981\u914d\u5bf9\u4f20\u611f\u5668\u6570\u636e\uff0c\u9650\u5236\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u80fd\u591f\u5728\u63a8\u7406\u65f6\u4ec5\u4f7f\u7528\u5355\u4e00\u4f20\u611f\u5668\uff0c\u540c\u65f6\u5229\u7528\u5176\u4ed6\u6a21\u6001\u7684\u77e5\u8bc6\u3002", "method": "\u63d0\u51faD-CAT\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u548c\u65b0\u578b\u8de8\u6ce8\u610f\u529b\u5bf9\u9f50\u635f\u5931\uff0c\u5f3a\u5236\u5bf9\u9f50\u4e0d\u540c\u4f20\u611f\u5668\u7684\u7279\u5f81\u7a7a\u95f4\uff0c\u800c\u4e0d\u9700\u8981\u8026\u5408\u4e24\u79cd\u6a21\u6001\u7684\u5206\u7c7b\u6d41\u7a0b\u3002", "result": "\u5728\u4e09\u4e2a\u591a\u6a21\u6001\u4eba\u7c7b\u6d3b\u52a8\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u5728\u5206\u5e03\u5185\u573a\u666f\u4e2d\uff0c\u4ece\u9ad8\u6027\u80fd\u6a21\u6001\uff08\u5982\u89c6\u9891\u5230IMU\uff09\u8fc1\u79fb\u53ef\u83b7\u5f9710%\u7684F1\u5206\u6570\u63d0\u5347\uff1b\u5728\u5206\u5e03\u5916\u573a\u666f\u4e2d\uff0c\u5373\u4f7f\u8f83\u5f31\u7684\u6e90\u6a21\u6001\u4e5f\u80fd\u6539\u5584\u76ee\u6807\u6027\u80fd\u3002", "conclusion": "D-CAT\u901a\u8fc7\u5b9e\u73b0\u5355\u4f20\u611f\u5668\u63a8\u7406\u548c\u8de8\u6a21\u6001\u77e5\u8bc6\u8fc1\u79fb\uff0c\u51cf\u5c11\u4e86\u611f\u77e5\u7cfb\u7edf\u7684\u786c\u4ef6\u5197\u4f59\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u6210\u672c\u654f\u611f\u6216\u81ea\u9002\u5e94\u90e8\u7f72\u573a\u666f\u3002"}}
{"id": "2509.09686", "categories": ["cs.IR", "cs.AI", "I.2; I.7; H.4; H.5"], "pdf": "https://arxiv.org/pdf/2509.09686", "abs": "https://arxiv.org/abs/2509.09686", "authors": ["Fei Huang", "Fan Wu", "Zeqing Zhang", "Qihao Wang", "Long Zhang", "Grant Michael Boquet", "Hongyang Chen"], "title": "GeoGPT.RAG Technical Report", "comment": "19 pages, 10 figures, 10 tables", "summary": "GeoGPT is an open large language model system built to advance research in\nthe geosciences. To enhance its domain-specific capabilities, we integrated\nRetrieval Augmented Generation(RAG), which augments model outputs with relevant\ninformation retrieved from an external knowledge source. GeoGPT uses RAG to\ndraw from the GeoGPT Library, a specialized corpus curated for geoscientific\ncontent, enabling it to generate accurate, context-specific answers. Users can\nalso create personalized knowledge bases by uploading their own publication\nlists, allowing GeoGPT to retrieve and respond using user-provided materials.\nTo further improve retrieval quality and domain alignment, we fine-tuned both\nthe embedding model and a ranking model that scores retrieved passages by\nrelevance to the query. These enhancements optimize RAG for geoscience\napplications and significantly improve the system's ability to deliver precise\nand trustworthy outputs. GeoGPT reflects a strong commitment to open science\nthrough its emphasis on collaboration, transparency, and community driven\ndevelopment. As part of this commitment, we have open-sourced two core RAG\ncomponents-GeoEmbedding and GeoReranker-to support geoscientists, researchers,\nand professionals worldwide with powerful, accessible AI tools.", "AI": {"tldr": "GeoGPT\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5927\u8bed\u8a00\u6a21\u578b\u7cfb\u7edf\uff0c\u4e13\u4e3a\u5730\u7403\u79d1\u5b66\u7814\u7a76\u8bbe\u8ba1\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u6280\u672f\u4ece\u4e13\u4e1a\u77e5\u8bc6\u5e93\u4e2d\u83b7\u53d6\u4fe1\u606f\u6765\u63d0\u5347\u56de\u7b54\u7684\u51c6\u786e\u6027\u548c\u9886\u57df\u76f8\u5173\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5730\u7403\u79d1\u5b66\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u63d0\u4f9b\u51c6\u786e\u3001\u53ef\u4fe1\u8d56\u7684\u5730\u7403\u79d1\u5b66\u9886\u57df\u4e13\u4e1a\u56de\u7b54\u7684AI\u7cfb\u7edf\u3002", "method": "\u96c6\u6210RAG\u6280\u672f\uff0c\u6784\u5efaGeoGPT Library\u4e13\u4e1a\u8bed\u6599\u5e93\uff1b\u5fae\u8c03\u5d4c\u5165\u6a21\u578b\u548c\u6392\u5e8f\u6a21\u578b\u6765\u4f18\u5316\u68c0\u7d22\u8d28\u91cf\uff1b\u652f\u6301\u7528\u6237\u4e0a\u4f20\u4e2a\u6027\u5316\u77e5\u8bc6\u5e93\uff1b\u5f00\u6e90\u6838\u5fc3RAG\u7ec4\u4ef6GeoEmbedding\u548cGeoReranker\u3002", "result": "\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u5728\u5730\u7403\u79d1\u5b66\u5e94\u7528\u4e2d\u7684\u68c0\u7d22\u7cbe\u5ea6\u548c\u8f93\u51fa\u8d28\u91cf\uff0c\u80fd\u591f\u751f\u6210\u51c6\u786e\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u4e13\u4e1a\u56de\u7b54\u3002", "conclusion": "GeoGPT\u901a\u8fc7RAG\u6280\u672f\u548c\u9886\u57df\u4f18\u5316\uff0c\u4e3a\u5730\u7403\u79d1\u5b66\u5bb6\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684AI\u5de5\u5177\uff0c\u4f53\u73b0\u4e86\u5bf9\u5f00\u653e\u79d1\u5b66\u3001\u534f\u4f5c\u548c\u900f\u660e\u5ea6\u7684\u627f\u8bfa\uff0c\u63a8\u52a8\u4e86\u5730\u7403\u79d1\u5b66\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.09701", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09701", "abs": "https://arxiv.org/abs/2509.09701", "authors": ["JungHo Jung", "Junhyun Lee"], "title": "Optimal Multi-Task Learning at Regularization Horizon for Speech Translation Task", "comment": null, "summary": "End-to-end speech-to-text translation typically suffers from the scarcity of\npaired speech-text data. One way to overcome this shortcoming is to utilize the\nbitext data from the Machine Translation (MT) task and perform Multi-Task\nLearning (MTL). In this paper, we formulate MTL from a regularization\nperspective and explore how sequences can be regularized within and across\nmodalities. By thoroughly investigating the effect of consistency\nregularization (different modality) and R-drop (same modality), we show how\nthey respectively contribute to the total regularization. We also demonstrate\nthat the coefficient of MT loss serves as another source of regularization in\nthe MTL setting. With these three sources of regularization, we introduce the\noptimal regularization contour in the high-dimensional space, called the\nregularization horizon. Experiments show that tuning the hyperparameters within\nthe regularization horizon achieves near state-of-the-art performance on the\nMuST-C dataset.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ece\u6b63\u5219\u5316\u89d2\u5ea6\u7814\u7a76\u7aef\u5230\u7aef\u8bed\u97f3\u7ffb\u8bd1\u4e2d\u7684\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u901a\u8fc7\u5206\u6790\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u6b63\u5219\u5316\u3001\u540c\u6a21\u6001R-drop\u548c\u673a\u5668\u7ffb\u8bd1\u635f\u5931\u7cfb\u6570\u4e09\u79cd\u6b63\u5219\u5316\u6e90\uff0c\u63d0\u51fa\u4e86\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u5bfb\u627e\u6700\u4f18\u6b63\u5219\u5316\u8f6e\u5ed3\u7684\u65b9\u6cd5\uff0c\u5728MuST-C\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u63a5\u8fd1\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u7aef\u5230\u7aef\u8bed\u97f3\u5230\u6587\u672c\u7ffb\u8bd1\u9762\u4e34\u914d\u5bf9\u8bed\u97f3-\u6587\u672c\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u9700\u8981\u5229\u7528\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\u7684\u53cc\u8bed\u6587\u672c\u6570\u636e\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u6765\u514b\u670d\u8fd9\u4e00\u5c40\u9650\u6027\u3002", "method": "\u4ece\u6b63\u5219\u5316\u89c6\u89d2\u91cd\u65b0\u8868\u8ff0\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u63a2\u7d22\u5e8f\u5217\u5728\u6a21\u6001\u5185\u548c\u8de8\u6a21\u6001\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u5305\u62ec\u4e00\u81f4\u6027\u6b63\u5219\u5316\uff08\u4e0d\u540c\u6a21\u6001\uff09\u3001R-drop\uff08\u76f8\u540c\u6a21\u6001\uff09\u4ee5\u53ca\u673a\u5668\u7ffb\u8bd1\u635f\u5931\u7cfb\u6570\u7684\u6b63\u5219\u5316\u4f5c\u7528\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u5bfb\u627e\u6700\u4f18\u6b63\u5219\u5316\u8f6e\u5ed3\uff08\u79f0\u4e3a\u6b63\u5219\u5316\u5730\u5e73\u7ebf\uff09\u6765\u8c03\u6574\u8d85\u53c2\u6570\uff0c\u5728MuST-C\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u4e09\u79cd\u6b63\u5219\u5316\u6e90\uff08\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u3001\u540c\u6a21\u6001R-drop\u548cMT\u635f\u5931\u7cfb\u6570\uff09\u5171\u540c\u6784\u6210\u4e86\u6709\u6548\u7684\u6b63\u5219\u5316\u673a\u5236\uff0c\u901a\u8fc7\u4f18\u5316\u8fd9\u4e9b\u6b63\u5219\u5316\u53c2\u6570\u7684\u7ec4\u5408\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u8bed\u97f3\u7ffb\u8bd1\u6027\u80fd\u3002"}}
{"id": "2509.09751", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09751", "abs": "https://arxiv.org/abs/2509.09751", "authors": ["Junqiao Wang", "Zhaoyang Guan", "Guanyu Liu", "Tianze Xia", "Xianzhi Li", "Shuo Yin", "Xinyuan Song", "Chuhan Cheng", "Tianyu Shi", "Alex Lee"], "title": "Meta-Learning Reinforcement Learning for Crypto-Return Prediction", "comment": null, "summary": "Predicting cryptocurrency returns is notoriously difficult: price movements\nare driven by a fast-shifting blend of on-chain activity, news flow, and social\nsentiment, while labeled training data are scarce and expensive. In this paper,\nwe present Meta-RL-Crypto, a unified transformer-based architecture that\nunifies meta-learning and reinforcement learning (RL) to create a fully\nself-improving trading agent. Starting from a vanilla instruction-tuned LLM,\nthe agent iteratively alternates between three roles-actor, judge, and\nmeta-judge-in a closed-loop architecture. This learning process requires no\nadditional human supervision. It can leverage multimodal market inputs and\ninternal preference feedback. The agent in the system continuously refines both\nthe trading policy and evaluation criteria. Experiments across diverse market\nregimes demonstrate that Meta-RL-Crypto shows good performance on the technical\nindicators of the real market and outperforming other LLM-based baselines.", "AI": {"tldr": "Meta-RL-Crypto\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u7edf\u4e00\u67b6\u6784\uff0c\u7ed3\u5408\u5143\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u521b\u5efa\u4e86\u4e00\u4e2a\u5b8c\u5168\u81ea\u6539\u8fdb\u7684\u4ea4\u6613\u4ee3\u7406\uff0c\u7528\u4e8e\u52a0\u5bc6\u8d27\u5e01\u56de\u62a5\u9884\u6d4b\uff0c\u65e0\u9700\u989d\u5916\u4eba\u5de5\u76d1\u7763\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u52a0\u5bc6\u8d27\u5e01\u56de\u62a5\u9884\u6d4b\u6781\u5176\u56f0\u96be\uff0c\u4ef7\u683c\u6ce2\u52a8\u7531\u5feb\u901f\u53d8\u5316\u7684\u94fe\u4e0a\u6d3b\u52a8\u3001\u65b0\u95fb\u6d41\u548c\u793e\u4ea4\u60c5\u7eea\u9a71\u52a8\uff0c\u4e14\u6807\u8bb0\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u6602\u8d35\u3002\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u81ea\u6211\u6539\u8fdb\u3001\u65e0\u9700\u4eba\u5de5\u76d1\u7763\u7684\u4ea4\u6613\u4ee3\u7406\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u7edf\u4e00\u67b6\u6784\uff0c\u7ed3\u5408\u5143\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u3002\u4ece\u6307\u4ee4\u8c03\u4f18\u7684LLM\u5f00\u59cb\uff0c\u4ee3\u7406\u5728\u95ed\u73af\u67b6\u6784\u4e2d\u8fed\u4ee3\u4ea4\u66ff\u626e\u6f14\u4e09\u4e2a\u89d2\u8272\uff08\u6267\u884c\u8005\u3001\u8bc4\u5224\u8005\u548c\u5143\u8bc4\u5224\u8005\uff09\uff0c\u5229\u7528\u591a\u6a21\u6001\u5e02\u573a\u8f93\u5165\u548c\u5185\u90e8\u504f\u597d\u53cd\u9988\uff0c\u6301\u7eed\u6539\u8fdb\u4ea4\u6613\u7b56\u7565\u548c\u8bc4\u4f30\u6807\u51c6\u3002", "result": "\u5728\u4e0d\u540c\u5e02\u573a\u673a\u5236\u4e0b\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMeta-RL-Crypto\u5728\u771f\u5b9e\u5e02\u573a\u7684\u6280\u672f\u6307\u6807\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u5e76\u4e14\u4f18\u4e8e\u5176\u4ed6\u57fa\u4e8eLLM\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Meta-RL-Crypto\u901a\u8fc7\u7ed3\u5408\u5143\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u7edf\u4e00\u67b6\u6784\uff0c\u6210\u529f\u521b\u5efa\u4e86\u4e00\u4e2a\u5b8c\u5168\u81ea\u6539\u8fdb\u7684\u52a0\u5bc6\u8d27\u5e01\u4ea4\u6613\u4ee3\u7406\uff0c\u65e0\u9700\u4eba\u5de5\u76d1\u7763\uff0c\u5728\u590d\u6742\u591a\u53d8\u7684\u5e02\u573a\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2509.09687", "categories": ["cs.IR", "cs.DL"], "pdf": "https://arxiv.org/pdf/2509.09687", "abs": "https://arxiv.org/abs/2509.09687", "authors": ["Hermann Kroll", "Pascal Sackhoff", "Bill Matthias Thang", "Christin Katharina Kreutz", "Wolf-Tilo Balke"], "title": "Demonstrating Narrative Pattern Discovery from Biomedical Literature", "comment": "Accepted Demo at TPDL2025, 10 pages, 3 figures", "summary": "Digital libraries maintain extensive collections of knowledge and need to\nprovide effective access paths for their users. For instance, PubPharm, the\nspecialized information service for Pharmacy in Germany, provides and develops\naccess paths to their underlying biomedical document collection. In brief,\nPubPharm supports traditional keyword-based search, search for chemical\nstructures, as well as novel graph-based discovery workflows, e.g., listing or\nsearching for interactions between different pharmaceutical entities. This\npaper introduces a new search functionality, called narrative pattern mining,\nallowing users to explore context-relevant entities and entity interactions. We\nperformed interviews with five domain experts to verify the usefulness of our\nprototype.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a\u53d9\u4e8b\u6a21\u5f0f\u6316\u6398\u7684\u65b0\u641c\u7d22\u529f\u80fd\uff0c\u5141\u8bb8\u7528\u6237\u63a2\u7d22\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u5b9e\u4f53\u548c\u5b9e\u4f53\u4ea4\u4e92\uff0c\u5e76\u901a\u8fc7\u9886\u57df\u4e13\u5bb6\u8bbf\u8c08\u9a8c\u8bc1\u4e86\u539f\u578b\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u6570\u5b57\u56fe\u4e66\u9986\u9700\u8981\u4e3a\u7528\u6237\u63d0\u4f9b\u6709\u6548\u7684\u8bbf\u95ee\u8def\u5f84\uff0cPubPharm\u4f5c\u4e3a\u5fb7\u56fd\u836f\u5b66\u4e13\u4e1a\u4fe1\u606f\u670d\u52a1\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u641c\u7d22\u529f\u80fd\u6765\u589e\u5f3a\u5bf9\u751f\u7269\u533b\u5b66\u6587\u6863\u96c6\u5408\u7684\u8bbf\u95ee\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u53d9\u4e8b\u6a21\u5f0f\u6316\u6398\u641c\u7d22\u529f\u80fd\uff0c\u5141\u8bb8\u7528\u6237\u63a2\u7d22\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u5b9e\u4f53\u548c\u5b9e\u4f53\u4ea4\u4e92\uff0c\u5e76\u901a\u8fc7\u4e0e\u4e94\u4f4d\u9886\u57df\u4e13\u5bb6\u7684\u8bbf\u8c08\u6765\u9a8c\u8bc1\u539f\u578b\u7684\u6709\u7528\u6027\u3002", "result": "\u901a\u8fc7\u4e13\u5bb6\u8bbf\u8c08\u9a8c\u8bc1\u4e86\u53d9\u4e8b\u6a21\u5f0f\u6316\u6398\u529f\u80fd\u7684\u5b9e\u7528\u6027\uff0c\u8868\u660e\u8be5\u529f\u80fd\u80fd\u591f\u6709\u6548\u652f\u6301\u7528\u6237\u63a2\u7d22\u751f\u7269\u533b\u5b66\u6587\u6863\u4e2d\u7684\u5b9e\u4f53\u5173\u7cfb\u548c\u4ea4\u4e92\u3002", "conclusion": "\u53d9\u4e8b\u6a21\u5f0f\u6316\u6398\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b0\u641c\u7d22\u529f\u80fd\uff0c\u80fd\u591f\u589e\u5f3a\u6570\u5b57\u56fe\u4e66\u9986\u5bf9\u751f\u7269\u533b\u5b66\u6587\u6863\u96c6\u5408\u7684\u8bbf\u95ee\u548c\u63a2\u7d22\u80fd\u529b\uff0c\u5f97\u5230\u4e86\u9886\u57df\u4e13\u5bb6\u7684\u8ba4\u53ef\u3002"}}
{"id": "2509.09702", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.09702", "abs": "https://arxiv.org/abs/2509.09702", "authors": ["Ninad Bhat", "Kieran Browne", "Pip Bingemann"], "title": "Creativity Benchmark: A benchmark for marketing creativity for LLM models", "comment": "30 Pages, 14 figures", "summary": "We introduce Creativity Benchmark, an evaluation framework for large language\nmodels (LLMs) in marketing creativity. The benchmark covers 100 brands (12\ncategories) and three prompt types (Insights, Ideas, Wild Ideas). Human\npairwise preferences from 678 practising creatives over 11,012 anonymised\ncomparisons, analysed with Bradley-Terry models, show tightly clustered\nperformance with no model dominating across brands or prompt types: the\ntop-bottom spread is $\\Delta\\theta \\approx 0.45$, which implies a head-to-head\nwin probability of $0.61$; the highest-rated model beats the lowest only about\n$61\\%$ of the time. We also analyse model diversity using cosine distances to\ncapture intra- and inter-model variation and sensitivity to prompt reframing.\nComparing three LLM-as-judge setups with human rankings reveals weak,\ninconsistent correlations and judge-specific biases, underscoring that\nautomated judges cannot substitute for human evaluation. Conventional\ncreativity tests also transfer only partially to brand-constrained tasks.\nOverall, the results highlight the need for expert human evaluation and\ndiversity-aware workflows.", "AI": {"tldr": "Creativity Benchmark\u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8425\u9500\u521b\u610f\u9886\u57df\u8868\u73b0\u7684\u6846\u67b6\uff0c\u5305\u542b100\u4e2a\u54c1\u724c\u548c3\u79cd\u63d0\u793a\u7c7b\u578b\uff0c\u901a\u8fc7678\u540d\u4e13\u4e1a\u521b\u610f\u4eba\u5458\u768411,012\u6b21\u533f\u540d\u6bd4\u8f83\u663e\u793a\u6a21\u578b\u8868\u73b0\u7d27\u5bc6\u805a\u96c6\uff0c\u6ca1\u6709\u5355\u4e00\u6a21\u578b\u5728\u6240\u6709\u54c1\u724c\u6216\u63d0\u793a\u7c7b\u578b\u4e2d\u5360\u4f18\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u7f3a\u4e4f\u9488\u5bf9\u8425\u9500\u521b\u610f\u9886\u57df\u7684\u4e13\u4e1a\u57fa\u51c6\uff0c\u9700\u8981\u5efa\u7acb\u57fa\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u504f\u597d\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u8861\u91cf\u6a21\u578b\u5728\u54c1\u724c\u7ea6\u675f\u521b\u610f\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528100\u4e2a\u54c1\u724c\uff0812\u4e2a\u7c7b\u522b\uff09\u548c\u4e09\u79cd\u63d0\u793a\u7c7b\u578b\uff08\u6d1e\u5bdf\u3001\u60f3\u6cd5\u3001\u75af\u72c2\u60f3\u6cd5\uff09\uff0c\u901a\u8fc7678\u540d\u4e13\u4e1a\u521b\u610f\u4eba\u5458\u8fdb\u884c11,012\u6b21\u533f\u540d\u6210\u5bf9\u6bd4\u8f83\uff0c\u91c7\u7528Bradley-Terry\u6a21\u578b\u5206\u6790\uff0c\u5e76\u8ba1\u7b97\u4f59\u5f26\u8ddd\u79bb\u6765\u8bc4\u4f30\u6a21\u578b\u591a\u6837\u6027\u548c\u5bf9\u63d0\u793a\u91cd\u6784\u7684\u654f\u611f\u6027\u3002", "result": "\u6a21\u578b\u8868\u73b0\u7d27\u5bc6\u805a\u96c6\uff08\u0394\u03b8\u22480.45\uff09\uff0c\u5934\u5bf9\u5934\u83b7\u80dc\u6982\u7387\u4e3a61%\uff1b\u6700\u9ad8\u8bc4\u5206\u6a21\u578b\u4ec5\u6bd4\u6700\u4f4e\u8bc4\u5206\u6a21\u578b\u9ad861%\u7684\u80dc\u7387\uff1bLLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u4e0e\u4eba\u7c7b\u6392\u540d\u76f8\u5173\u6027\u5f31\u4e14\u4e0d\u4e00\u81f4\uff1b\u4f20\u7edf\u521b\u9020\u529b\u6d4b\u8bd5\u4ec5\u90e8\u5206\u9002\u7528\u4e8e\u54c1\u724c\u7ea6\u675f\u4efb\u52a1\u3002", "conclusion": "\u81ea\u52a8\u5316\u8bc4\u5224\u65e0\u6cd5\u66ff\u4ee3\u4eba\u7c7b\u4e13\u5bb6\u8bc4\u4f30\uff0c\u9700\u8981\u91c7\u7528\u591a\u6837\u6027\u611f\u77e5\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5f3a\u8c03\u4eba\u7c7b\u4e13\u5bb6\u8bc4\u4f30\u5728\u8425\u9500\u521b\u610f\u9886\u57df\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.09754", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09754", "abs": "https://arxiv.org/abs/2509.09754", "authors": ["Yiqun Shen", "Song Yuan", "Zhengze Zhang", "Xiaoliang Wang", "Daxin Jiang", "Nguyen Cam-Tu"], "title": "LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation", "comment": null, "summary": "KV Cache is commonly used to accelerate LLM inference with long contexts, yet\nits high memory demand drives the need for cache compression. Existing\ncompression methods, however, are largely heuristic and lack dynamic budget\nallocation. To address this limitation, we introduce a unified framework for\ncache compression by minimizing information loss in Transformer residual\nstreams. Building on it, we analyze the layer attention output loss and derive\na new metric to compare cache entries across heads, enabling layer-wise\ncompression with dynamic head budgets. Additionally, by contrasting cross-layer\ninformation, we also achieve dynamic layer budgets. LAVa is the first unified\nstrategy for cache eviction and dynamic budget allocation that, unlike prior\nmethods, does not rely on training or the combination of multiple strategies.\nExperiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and\nInfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a\nnew insight: dynamic layer budgets are crucial for generation tasks (e.g., code\ncompletion), while dynamic head budgets play a key role in extraction tasks\n(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently\nmaintains top performance across task types. Our code is available at\nhttps://github.com/MGDDestiny/Lava.", "AI": {"tldr": "LAVa\u662f\u4e00\u4e2a\u7edf\u4e00\u7684KV\u7f13\u5b58\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5c0f\u5316Transformer\u6b8b\u5dee\u6d41\u4e2d\u7684\u4fe1\u606f\u635f\u5931\u6765\u5b9e\u73b0\u52a8\u6001\u9884\u7b97\u5206\u914d\uff0c\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709KV\u7f13\u5b58\u538b\u7f29\u65b9\u6cd5\u5927\u591a\u662f\u542f\u53d1\u5f0f\u7684\uff0c\u7f3a\u4e4f\u52a8\u6001\u9884\u7b97\u5206\u914d\u673a\u5236\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u9ad8\u5185\u5b58\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5c42\u6ce8\u610f\u529b\u8f93\u51fa\u635f\u5931\uff0c\u63d0\u51fa\u65b0\u7684\u5ea6\u91cf\u6807\u51c6\u6765\u6bd4\u8f83\u4e0d\u540c\u5934\u7684\u7f13\u5b58\u6761\u76ee\uff0c\u5b9e\u73b0\u5c42\u95f4\u548c\u5934\u95f4\u7684\u52a8\u6001\u9884\u7b97\u5206\u914d\uff0c\u65e0\u9700\u8bad\u7ec3\u6216\u591a\u7b56\u7565\u7ec4\u5408\u3002", "result": "\u5728LongBench\u3001Needle-In-A-Haystack\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u53d1\u73b0\u52a8\u6001\u5c42\u9884\u7b97\u5bf9\u751f\u6210\u4efb\u52a1\u5173\u952e\uff0c\u52a8\u6001\u5934\u9884\u7b97\u5bf9\u62bd\u53d6\u4efb\u52a1\u91cd\u8981\u3002", "conclusion": "LAVa\u662f\u9996\u4e2a\u7edf\u4e00\u7684\u7f13\u5b58\u6dd8\u6c70\u548c\u52a8\u6001\u9884\u7b97\u5206\u914d\u7b56\u7565\uff0c\u5728\u5404\u79cd\u4efb\u52a1\u7c7b\u578b\u4e2d\u4fdd\u6301\u6700\u4f73\u6027\u80fd\uff0c\u4e3aKV\u7f13\u5b58\u538b\u7f29\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u8df5\u65b9\u6cd5\u3002"}}
{"id": "2509.09688", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09688", "abs": "https://arxiv.org/abs/2509.09688", "authors": ["Mohammad Atif", "Vincent Garonne", "Eric Lancon", "Jerome Lauret", "Alexandr Prozorov", "Michal Vranovsky"], "title": "AI-Powered Assistant for Long-Term Access to RHIC Knowledge", "comment": null, "summary": "As the Relativistic Heavy Ion Collider (RHIC) at Brookhaven National\nLaboratory concludes 25 years of operation, preserving not only its vast data\nholdings ($\\sim$1 ExaByte) but also the embedded scientific knowledge becomes a\ncritical priority. The RHIC Data and Analysis Preservation Plan (DAPP)\nintroduces an AI-powered assistant system that provides natural language access\nto documentation, workflows, and software, with the aim of supporting\nreproducibility, education, and future discovery. Built upon Large Language\nModels using Retrieval-Augmented Generation and the Model Context Protocol,\nthis assistant indexes structured and unstructured content from RHIC\nexperiments and enables domain-adapted interaction. We report on the\ndeployment, computational performance, ongoing multi-experiment integration,\nand architectural features designed for a sustainable and explainable long-term\nAI access. Our experience illustrates how modern AI/ML tools can transform the\nusability and discoverability of scientific legacy data.", "AI": {"tldr": "RHIC\u6570\u636e\u4e0e\u5206\u6790\u4fdd\u5b58\u8ba1\u5212\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684AI\u52a9\u624b\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8bbf\u95ee\u6587\u6863\u3001\u5de5\u4f5c\u6d41\u548c\u8f6f\u4ef6\uff0c\u65e8\u5728\u652f\u6301\u91cd\u79bb\u5b50\u78b0\u649e\u5b9e\u9a8c\u6570\u636e\u7684\u53ef\u91cd\u73b0\u6027\u3001\u6559\u80b2\u548c\u672a\u6765\u53d1\u73b0\u3002", "motivation": "\u968f\u7740RHIC\u8fd0\u884c25\u5e74\u7ed3\u675f\uff0c\u4fdd\u5b58\u5176\u6d77\u91cf\u6570\u636e\uff08\u7ea61EB\uff09\u548c\u5d4c\u5165\u7684\u79d1\u5b66\u77e5\u8bc6\u6210\u4e3a\u5173\u952e\u4f18\u5148\u4e8b\u9879\uff0c\u9700\u8981\u786e\u4fdd\u79d1\u5b66\u9057\u4ea7\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u7528\u6027\u3002", "method": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff0c\u7d22\u5f15RHIC\u5b9e\u9a8c\u7684\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u5185\u5bb9\uff0c\u5b9e\u73b0\u9886\u57df\u9002\u5e94\u7684\u4ea4\u4e92\u3002", "result": "\u7cfb\u7edf\u5df2\u90e8\u7f72\uff0c\u62a5\u544a\u4e86\u8ba1\u7b97\u6027\u80fd\u3001\u591a\u5b9e\u9a8c\u96c6\u6210\u8fdb\u5c55\uff0c\u4ee5\u53ca\u4e3a\u53ef\u6301\u7eed\u548c\u53ef\u89e3\u91ca\u7684\u957f\u671fAI\u8bbf\u95ee\u8bbe\u8ba1\u7684\u67b6\u6784\u7279\u6027\u3002", "conclusion": "\u73b0\u4ee3AI/ML\u5de5\u5177\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u79d1\u5b66\u9057\u4ea7\u6570\u636e\u7684\u53ef\u7528\u6027\u548c\u53ef\u53d1\u73b0\u6027\uff0c\u4e3a\u5927\u578b\u79d1\u5b66\u5b9e\u9a8c\u7684\u6570\u636e\u4fdd\u5b58\u548c\u77e5\u8bc6\u4f20\u627f\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.09703", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09703", "abs": "https://arxiv.org/abs/2509.09703", "authors": ["Zhenhua Xu", "Xixiang Zhao", "Xubin Yue", "Shengwei Tian", "Changting Lin", "Meng Han"], "title": "CTCC: A Robust and Stealthy Fingerprinting Framework for Large Language Models via Cross-Turn Contextual Correlation Backdoor", "comment": "Accepted by EMNLP2025 MainConference", "summary": "The widespread deployment of large language models (LLMs) has intensified\nconcerns around intellectual property (IP) protection, as model theft and\nunauthorized redistribution become increasingly feasible. To address this,\nmodel fingerprinting aims to embed verifiable ownership traces into LLMs.\nHowever, existing methods face inherent trade-offs between stealthness,\nrobustness, and generalizability, being either detectable via distributional\nshifts, vulnerable to adversarial modifications, or easily invalidated once the\nfingerprint is revealed. In this work, we introduce CTCC, a novel rule-driven\nfingerprinting framework that encodes contextual correlations across multiple\ndialogue turns, such as counterfactual, rather than relying on token-level or\nsingle-turn triggers. CTCC enables fingerprint verification under black-box\naccess while mitigating false positives and fingerprint leakage, supporting\ncontinuous construction under a shared semantic rule even if partial triggers\nare exposed. Extensive experiments across multiple LLM architectures\ndemonstrate that CTCC consistently achieves stronger stealth and robustness\nthan prior work. Our findings position CTCC as a reliable and practical\nsolution for ownership verification in real-world LLM deployment scenarios. Our\ncode and data are publicly available at <https://github.com/Xuzhenhua55/CTCC>.", "AI": {"tldr": "CTCC\u662f\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u89c4\u5219\u7684LLM\u6307\u7eb9\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u4e0a\u4e0b\u6587\u5173\u8054\u6765\u5d4c\u5165\u6240\u6709\u6743\u9a8c\u8bc1\u75d5\u8ff9\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u9690\u853d\u6027\u3001\u9c81\u68d2\u6027\u548c\u901a\u7528\u6027\u65b9\u9762\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u5e7f\u6cdb\u90e8\u7f72\u52a0\u5267\u4e86\u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\u62c5\u5fe7\uff0c\u73b0\u6709\u6307\u7eb9\u65b9\u6cd5\u5b58\u5728\u53ef\u68c0\u6d4b\u6027\u3001\u6613\u53d7\u653b\u51fb\u548c\u901a\u7528\u6027\u5dee\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faCTCC\u6846\u67b6\uff0c\u91c7\u7528\u89c4\u5219\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7f16\u7801\u4e0a\u4e0b\u6587\u5173\u8054\uff08\u5982\u53cd\u4e8b\u5b9e\u5173\u7cfb\uff09\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u8bcd\u7ea7\u6216\u5355\u8f6e\u89e6\u53d1\uff0c\u652f\u6301\u9ed1\u76d2\u8bbf\u95ee\u4e0b\u7684\u6307\u7eb9\u9a8c\u8bc1\u3002", "result": "\u5728\u591a\u79cdLLM\u67b6\u6784\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCTCC\u76f8\u6bd4\u5148\u524d\u5de5\u4f5c\u59cb\u7ec8\u5b9e\u73b0\u66f4\u5f3a\u7684\u9690\u853d\u6027\u548c\u9c81\u68d2\u6027\uff0c\u6709\u6548\u51cf\u5c11\u8bef\u62a5\u548c\u6307\u7eb9\u6cc4\u9732\u98ce\u9669\u3002", "conclusion": "CTCC\u4e3a\u73b0\u5b9e\u4e16\u754cLLM\u90e8\u7f72\u573a\u666f\u4e2d\u7684\u6240\u6709\u6743\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u9760\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u5728\u90e8\u5206\u89e6\u53d1\u66b4\u9732\u60c5\u51b5\u4e0b\u57fa\u4e8e\u5171\u4eab\u8bed\u4e49\u89c4\u5219\u7684\u6301\u7eed\u6784\u5efa\u3002"}}
{"id": "2509.09772", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.09772", "abs": "https://arxiv.org/abs/2509.09772", "authors": ["Sanjay Basu", "Sadiq Y. Patel", "Parth Sheth", "Bhairavi Muralidharan", "Namrata Elamaran", "Aakriti Kinra", "Rajaie Batniji"], "title": "Hybrid Adaptive Conformal Offline Reinforcement Learning for Fair Population Health Management", "comment": "10 pages, 5 figures, 4 tables", "summary": "Population health management programs for Medicaid populations coordinate\nlongitudinal outreach and services (e.g., benefits navigation, behavioral\nhealth, social needs support, and clinical scheduling) and must be safe, fair,\nand auditable. We present a Hybrid Adaptive Conformal Offline Reinforcement\nLearning (HACO) framework that separates risk calibration from preference\noptimization to generate conservative action recommendations at scale. In our\nsetting, each step involves choosing among common coordination actions (e.g.,\nwhich member to contact, by which modality, and whether to route to a\nspecialized service) while controlling the near-term risk of adverse\nutilization events (e.g., unplanned emergency department visits or\nhospitalizations). Using a de-identified operational dataset from Waymark\ncomprising 2.77 million sequential decisions across 168,126 patients, HACO (i)\ntrains a lightweight risk model for adverse events, (ii) derives a conformal\nthreshold to mask unsafe actions at a target risk level, and (iii) learns a\npreference policy on the resulting safe subset. We evaluate policies with a\nversion-agnostic fitted Q evaluation (FQE) on stratified subsets and audit\nsubgroup performance across age, sex, and race. HACO achieves strong risk\ndiscrimination (AUC ~0.81) with a calibrated threshold ( {\\tau} ~0.038 at\n{\\alpha} = 0.10), while maintaining high safe coverage. Subgroup analyses\nreveal systematic differences in estimated value across demographics,\nunderscoring the importance of fairness auditing. Our results show that\nconformal risk gating integrates cleanly with offline RL to deliver\nconservative, auditable decision support for population health management\nteams.", "AI": {"tldr": "HACO\u6846\u67b6\u901a\u8fc7\u5206\u79bb\u98ce\u9669\u6821\u51c6\u548c\u504f\u597d\u4f18\u5316\uff0c\u4e3a\u533b\u7597\u8865\u52a9\u4eba\u7fa4\u63d0\u4f9b\u5b89\u5168\u3001\u516c\u5e73\u4e14\u53ef\u5ba1\u8ba1\u7684\u51b3\u7b56\u652f\u6301\uff0c\u5728\u63a7\u5236\u4e0d\u826f\u4e8b\u4ef6\u98ce\u9669\u7684\u540c\u65f6\u4fdd\u6301\u9ad8\u5b89\u5168\u8986\u76d6\u7387", "motivation": "\u4e3a\u533b\u7597\u8865\u52a9\u4eba\u7fa4\u63d0\u4f9b\u5b89\u5168\u3001\u516c\u5e73\u4e14\u53ef\u5ba1\u8ba1\u7684\u4eba\u53e3\u5065\u5eb7\u7ba1\u7406\u670d\u52a1\uff0c\u9700\u8981\u534f\u8c03\u7eb5\u5411\u5916\u5c55\u670d\u52a1\u5e76\u63a7\u5236\u4e0d\u826f\u5229\u7528\u4e8b\u4ef6\u7684\u98ce\u9669", "method": "\u6df7\u5408\u81ea\u9002\u5e94\u7b26\u5408\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60(HACO)\u6846\u67b6\uff1a\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u98ce\u9669\u6a21\u578b\u3001\u63a8\u5bfc\u7b26\u5408\u9608\u503c\u5c4f\u853d\u4e0d\u5b89\u5168\u884c\u4e3a\u3001\u5728\u5b89\u5168\u5b50\u96c6\u4e0a\u5b66\u4e60\u504f\u597d\u7b56\u7565", "result": "\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u98ce\u9669\u533a\u5206\u80fd\u529b(AUC ~0.81)\uff0c\u6821\u51c6\u9608\u503c(\u03c4 ~0.038 at \u03b1=0.10)\uff0c\u4fdd\u6301\u9ad8\u5b89\u5168\u8986\u76d6\u7387\uff0c\u5b50\u7ec4\u5206\u6790\u663e\u793a\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u95f4\u5b58\u5728\u7cfb\u7edf\u6027\u4ef7\u503c\u5dee\u5f02", "conclusion": "\u7b26\u5408\u98ce\u9669\u95e8\u63a7\u4e0e\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65e0\u7f1d\u96c6\u6210\uff0c\u4e3a\u4eba\u53e3\u5065\u5eb7\u7ba1\u7406\u56e2\u961f\u63d0\u4f9b\u4fdd\u5b88\u4e14\u53ef\u5ba1\u8ba1\u7684\u51b3\u7b56\u652f\u6301"}}
{"id": "2509.09689", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09689", "abs": "https://arxiv.org/abs/2509.09689", "authors": ["Himanshu Thakur", "Eshani Agrawal", "Smruthi Mukund"], "title": "Personas within Parameters: Fine-Tuning Small Language Models with Low-Rank Adapters to Mimic User Behaviors", "comment": null, "summary": "A long-standing challenge in developing accurate recommendation models is\nsimulating user behavior, mainly due to the complex and stochastic nature of\nuser interactions. Towards this, one promising line of work has been the use of\nLarge Language Models (LLMs) for simulating user behavior. However, aligning\nthese general-purpose large pre-trained models with user preferences\nnecessitates: (i) effectively and continously parsing large-scale tabular\nuser-item interaction data, (ii) overcoming pre-training-induced inductive\nbiases to accurately learn user specific knowledge, and (iii) achieving the\nformer two at scale for millions of users. While most previous works have\nfocused on complex methods to prompt an LLM or fine-tune it on tabular\ninteraction datasets, our approach shifts the focus to extracting robust\ntextual user representations using a frozen LLM and simulating cost-effective,\nresource-efficient user agents powered by fine-tuned Small Language Models\n(SLMs). Further, we showcase a method for training multiple low-rank adapters\nfor groups of users or \\textit{persona}, striking an optimal balance between\nscalability and performance of user behavior agents. Our experiments provide\ncompelling empirical evidence of the efficacy of our methods, demonstrating\nthat user agents developed using our approach have the potential to bridge the\ngap between offline metrics and real-world performance of recommender systems.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u51bb\u7ed3\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u6587\u672c\u7528\u6237\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u5c0f\u8bed\u8a00\u6a21\u578b\u6765\u6784\u5efa\u9ad8\u6548\u7684\u7528\u6237\u884c\u4e3a\u4ee3\u7406\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u7528\u6237\u884c\u4e3a\u6a21\u62df\u4e2d\u7684\u590d\u6742\u6027\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u63a8\u8350\u7cfb\u7edf\u4e2d\u7528\u6237\u884c\u4e3a\u6a21\u62df\u7684\u957f\u671f\u6311\u6218\uff0c\u7279\u522b\u662f\u5904\u7406\u5927\u89c4\u6a21\u8868\u683c\u7528\u6237-\u7269\u54c1\u4ea4\u4e92\u6570\u636e\u3001\u514b\u670d\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5f52\u7eb3\u504f\u5dee\uff0c\u4ee5\u53ca\u5728\u6570\u767e\u4e07\u7528\u6237\u89c4\u6a21\u4e0a\u5b9e\u73b0\u9ad8\u6548\u6a21\u62df\u3002", "method": "\u4f7f\u7528\u51bb\u7ed3\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u9c81\u68d2\u7684\u6587\u672c\u7528\u6237\u8868\u793a\uff0c\u7136\u540e\u901a\u8fc7\u5fae\u8c03\u5c0f\u8bed\u8a00\u6a21\u578b\u6765\u6784\u5efa\u6210\u672c\u6548\u76ca\u9ad8\u3001\u8d44\u6e90\u6548\u7387\u597d\u7684\u7528\u6237\u4ee3\u7406\uff0c\u5e76\u91c7\u7528\u4f4e\u79e9\u9002\u914d\u5668\u4e3a\u4e0d\u540c\u7528\u6237\u7fa4\u4f53\u8bad\u7ec3\u4e2a\u6027\u5316\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u63d0\u4f9b\u4e86\u6709\u529b\u7684\u7ecf\u9a8c\u8bc1\u636e\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u5f00\u53d1\u51fa\u7684\u7528\u6237\u4ee3\u7406\u80fd\u591f\u6709\u6548\u5f25\u5408\u63a8\u8350\u7cfb\u7edf\u79bb\u7ebf\u6307\u6807\u4e0e\u5b9e\u9645\u6027\u80fd\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\u4e4b\u95f4\u627e\u5230\u4e86\u6700\u4f73\u5e73\u8861\uff0c\u4e3a\u63a8\u8350\u7cfb\u7edf\u7684\u7528\u6237\u884c\u4e3a\u6a21\u62df\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.09704", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.09704", "abs": "https://arxiv.org/abs/2509.09704", "authors": ["Ali Mazyaki", "Mohammad Naghizadeh", "Samaneh Ranjkhah Zonouzaghi", "Hossein Setareh"], "title": "Temporal Preferences in Language Models for Long-Horizon Assistance", "comment": null, "summary": "We study whether language models (LMs) exhibit future- versus\npresent-oriented preferences in intertemporal choice and whether those\npreferences can be systematically manipulated. Using adapted human experimental\nprotocols, we evaluate multiple LMs on time-tradeoff tasks and benchmark them\nagainst a sample of human decision makers. We introduce an operational metric,\nthe Manipulability of Time Orientation (MTO), defined as the change in an LM's\nrevealed time preference between future- and present-oriented prompts. In our\ntests, reasoning-focused models (e.g., DeepSeek-Reasoner and grok-3-mini)\nchoose later options under future-oriented prompts but only partially\npersonalize decisions across identities or geographies. Moreover, models that\ncorrectly reason about time orientation internalize a future orientation for\nthemselves as AI decision makers. We discuss design implications for AI\nassistants that should align with heterogeneous, long-horizon goals and outline\na research agenda on personalized contextual calibration and socially aware\ndeployment.", "AI": {"tldr": "\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u671f\u9009\u62e9\u4e2d\u662f\u5426\u8868\u73b0\u51fa\u672a\u6765\u5bfc\u5411\u4e0e\u73b0\u5728\u5bfc\u5411\u7684\u504f\u597d\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u504f\u597d\u662f\u5426\u53ef\u4ee5\u88ab\u7cfb\u7edf\u6027\u64cd\u7eb5\u3002\u901a\u8fc7\u5f15\u5165MTO\u6307\u6807\u6765\u8861\u91cf\u6a21\u578b\u65f6\u95f4\u504f\u597d\u7684\u53ef\u64cd\u7eb5\u6027\u3002", "motivation": "\u63a2\u7a76\u8bed\u8a00\u6a21\u578b\u7684\u65f6\u95f4\u504f\u597d\u7279\u5f81\uff0c\u4e86\u89e3\u5176\u662f\u5426\u80fd\u591f\u50cf\u4eba\u7c7b\u4e00\u6837\u5728\u8de8\u671f\u51b3\u7b56\u4e2d\u8868\u73b0\u51fa\u7a33\u5b9a\u7684\u65f6\u95f4\u5bfc\u5411\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u504f\u597d\u662f\u5426\u5bb9\u6613\u88ab\u5916\u90e8\u63d0\u793a\u6240\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u6539\u7f16\u7684\u4eba\u7c7b\u5b9e\u9a8c\u534f\u8bae\uff0c\u5728\u65f6\u95f4\u6743\u8861\u4efb\u52a1\u4e2d\u8bc4\u4f30\u591a\u4e2a\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u4e0e\u4eba\u7c7b\u51b3\u7b56\u8005\u8fdb\u884c\u57fa\u51c6\u6bd4\u8f83\u3002\u5f15\u5165MTO\uff08\u65f6\u95f4\u5bfc\u5411\u53ef\u64cd\u7eb5\u6027\uff09\u6307\u6807\u6765\u91cf\u5316\u6a21\u578b\u5728\u9762\u5411\u672a\u6765\u548c\u9762\u5411\u73b0\u5728\u63d0\u793a\u4e0b\u7684\u504f\u597d\u53d8\u5316\u3002", "result": "\u63a8\u7406\u5bfc\u5411\u6a21\u578b\uff08\u5982DeepSeek-Reasoner\u548cgrok-3-mini\uff09\u5728\u9762\u5411\u672a\u6765\u7684\u63d0\u793a\u4e0b\u503e\u5411\u4e8e\u9009\u62e9\u5ef6\u8fdf\u9009\u9879\uff0c\u4f46\u5728\u8de8\u8eab\u4efd\u6216\u5730\u7406\u4f4d\u7f6e\u7684\u4e2a\u6027\u5316\u51b3\u7b56\u65b9\u9762\u8868\u73b0\u6709\u9650\u3002\u80fd\u591f\u6b63\u786e\u63a8\u7406\u65f6\u95f4\u5bfc\u5411\u7684\u6a21\u578b\u4f1a\u4e3a\u81ea\u8eab\u4f5c\u4e3aAI\u51b3\u7b56\u8005\u5185\u5316\u672a\u6765\u5bfc\u5411\u3002", "conclusion": "\u7814\u7a76\u4e3aAI\u52a9\u624b\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\uff0c\u9700\u8981\u4e0e\u5f02\u8d28\u6027\u7684\u957f\u671f\u76ee\u6807\u5bf9\u9f50\uff0c\u5e76\u63d0\u51fa\u4e86\u4e2a\u6027\u5316\u60c5\u5883\u6821\u51c6\u548c\u793e\u4f1a\u610f\u8bc6\u90e8\u7f72\u7684\u7814\u7a76\u8bae\u7a0b\u3002"}}
{"id": "2509.09782", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09782", "abs": "https://arxiv.org/abs/2509.09782", "authors": ["Roshini Pulishetty", "Mani Kishan Ghantasala", "Keerthy Kaushik Dasoju", "Niti Mangwani", "Vishal Garimella", "Aditya Mate", "Somya Chatterjee", "Yue Kang", "Ehi Nosakhare", "Sadid Hasan", "Soundar Srinivasan"], "title": "One Head, Many Models: Cross-Attention Routing for Cost-Aware LLM Selection", "comment": null, "summary": "The proliferation of large language models (LLMs) with varying computational\ncosts and performance profiles presents a critical challenge for scalable,\ncost-effective deployment in real-world applications. We introduce a unified\nrouting framework that leverages a single-head cross-attention mechanism to\njointly model query and model embeddings, enabling dynamic selection of the\noptimal LLM for each input query. Our approach is evaluated on RouterBench, a\nlarge-scale, publicly available benchmark encompassing diverse LLM pools and\ndomains. By explicitly capturing fine-grained query-model interactions, our\nrouter predicts both response quality and generation cost, achieving up to 6.6%\nimprovement in Average Improvement in Quality (AIQ) and 2.9% in maximum\nperformance over existing routers. To robustly balance performance and cost, we\npropose an exponential reward function that enhances stability across user\npreferences. The resulting architecture is lightweight, generalizes effectively\nacross domains, and demonstrates improved efficiency compared to prior methods,\nestablishing a new standard for cost-aware LLM routing.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5355\u5934\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u7684\u7edf\u4e00\u8def\u7531\u6846\u67b6\uff0c\u52a8\u6001\u9009\u62e9\u6700\u4f18LLM\uff0c\u5728RouterBench\u57fa\u51c6\u4e0a\u5b9e\u73b06.6%\u7684\u8d28\u91cf\u63d0\u5347\u548c2.9%\u7684\u6700\u5927\u6027\u80fd\u63d0\u5347", "motivation": "\u89e3\u51b3\u4e0d\u540c\u8ba1\u7b97\u6210\u672c\u548c\u6027\u80fd\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u89c4\u6a21\u5316\u3001\u6210\u672c\u6548\u76ca\u90e8\u7f72\u7684\u6311\u6218", "method": "\u4f7f\u7528\u5355\u5934\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u8054\u5408\u5efa\u6a21\u67e5\u8be2\u548c\u6a21\u578b\u5d4c\u5165\uff0c\u9884\u6d4b\u54cd\u5e94\u8d28\u91cf\u548c\u751f\u6210\u6210\u672c\uff0c\u5e76\u63d0\u51fa\u6307\u6570\u5956\u52b1\u51fd\u6570\u6765\u5e73\u8861\u6027\u80fd\u548c\u6210\u672c", "result": "\u5728RouterBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747\u8d28\u91cf\u6539\u8fdb(AIQ)\u63d0\u53476.6%\uff0c\u6700\u5927\u6027\u80fd\u63d0\u53472.9%\uff0c\u67b6\u6784\u8f7b\u91cf\u4e14\u80fd\u6709\u6548\u8de8\u9886\u57df\u6cdb\u5316", "conclusion": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u7684\u6210\u672c\u611f\u77e5LLM\u8def\u7531\u6807\u51c6\uff0c\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u3001\u9ad8\u6548\u4e14\u80fd\u7a33\u5b9a\u5e73\u8861\u6027\u80fd\u4e0e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.09690", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09690", "abs": "https://arxiv.org/abs/2509.09690", "authors": ["Ping Liu", "Jianqiang Shen", "Qianqi Shen", "Chunnan Yao", "Kevin Kao", "Dan Xu", "Rajat Arora", "Baofen Zheng", "Caleb Johnson", "Liangjie Hong", "Jingwei Wu", "Wenjing Zhang"], "title": "Powering Job Search at Scale: LLM-Enhanced Query Understanding in Job Matching Systems", "comment": "CIKM2025", "summary": "Query understanding is essential in modern relevance systems, where user\nqueries are often short, ambiguous, and highly context-dependent. Traditional\napproaches often rely on multiple task-specific Named Entity Recognition models\nto extract structured facets as seen in job search applications. However, this\nfragmented architecture is brittle, expensive to maintain, and slow to adapt to\nevolving taxonomies and language patterns. In this paper, we introduce a\nunified query understanding framework powered by a Large Language Model (LLM),\ndesigned to address these limitations. Our approach jointly models the user\nquery and contextual signals such as profile attributes to generate structured\ninterpretations that drive more accurate and personalized recommendations. The\nframework improves relevance quality in online A/B testing while significantly\nreducing system complexity and operational overhead. The results demonstrate\nthat our solution provides a scalable and adaptable foundation for query\nunderstanding in dynamic web applications.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7edf\u4e00\u67e5\u8be2\u7406\u89e3\u6846\u67b6\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u591aNER\u6a21\u578b\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u7528\u6237\u67e5\u8be2\u548c\u4e0a\u4e0b\u6587\u4fe1\u53f7\u6765\u751f\u6210\u7ed3\u6784\u5316\u89e3\u91ca\uff0c\u63d0\u5347\u63a8\u8350\u51c6\u786e\u6027\u548c\u4e2a\u6027\u5316\u7a0b\u5ea6", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u591a\u4e2a\u4efb\u52a1\u7279\u5b9a\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u6a21\u578b\uff0c\u5b58\u5728\u67b6\u6784\u8106\u5f31\u3001\u7ef4\u62a4\u6210\u672c\u9ad8\u3001\u96be\u4ee5\u9002\u5e94\u5feb\u901f\u53d8\u5316\u7684\u5206\u7c7b\u4f53\u7cfb\u548c\u8bed\u8a00\u6a21\u5f0f\u7b49\u95ee\u9898", "method": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u7edf\u4e00\u67e5\u8be2\u7406\u89e3\u6846\u67b6\uff0c\u8054\u5408\u5efa\u6a21\u7528\u6237\u67e5\u8be2\u548c\u4e0a\u4e0b\u6587\u4fe1\u53f7\uff08\u5982\u4e2a\u4eba\u8d44\u6599\u5c5e\u6027\uff09\uff0c\u751f\u6210\u7ed3\u6784\u5316\u89e3\u91ca\u6765\u9a71\u52a8\u63a8\u8350\u7cfb\u7edf", "result": "\u5728\u7ebfA/B\u6d4b\u8bd5\u4e2d\u63d0\u5347\u4e86\u76f8\u5173\u6027\u8d28\u91cf\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u7cfb\u7edf\u590d\u6742\u5ea6\u548c\u8fd0\u7ef4\u5f00\u9500", "conclusion": "\u8be5\u89e3\u51b3\u65b9\u6848\u4e3a\u52a8\u6001\u7f51\u7edc\u5e94\u7528\u4e2d\u7684\u67e5\u8be2\u7406\u89e3\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u548c\u9002\u5e94\u6027\u5f3a\u7684\u6280\u672f\u57fa\u7840"}}
{"id": "2509.09705", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09705", "abs": "https://arxiv.org/abs/2509.09705", "authors": ["Claudio Pinhanez", "Paulo Cavalin", "Cassia Sanctos", "Marcelo Grave", "Yago Primerano"], "title": "The Non-Determinism of Small LLMs: Evidence of Low Answer Consistency in Repetition Trials of Standard Multiple-Choice Benchmarks", "comment": null, "summary": "This work explores the consistency of small LLMs (2B-8B parameters) in\nanswering multiple times the same question. We present a study on known,\nopen-source LLMs responding to 10 repetitions of questions from the\nmultiple-choice benchmarks MMLU-Redux and MedQA, considering different\ninference temperatures, small vs. medium models (50B-80B), finetuned vs. base\nmodels, and other parameters. We also look into the effects of requiring\nmulti-trial answer consistency on accuracy and the trade-offs involved in\ndeciding which model best provides both of them. To support those studies, we\npropose some new analytical and graphical tools. Results show that the number\nof questions which can be answered consistently vary considerably among models\nbut are typically in the 50%-80% range for small models at low inference\ntemperatures. Also, accuracy among consistent answers seems to reasonably\ncorrelate with overall accuracy. Results for medium-sized models seem to\nindicate much higher levels of answer consistency.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5c0f\u578bLLM\uff082B-8B\u53c2\u6570\uff09\u5728\u91cd\u590d\u56de\u7b54\u76f8\u540c\u95ee\u9898\u65f6\u7684\u7a33\u5b9a\u6027\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u6e29\u5ea6\u8bbe\u7f6e\u3001\u6a21\u578b\u5927\u5c0f\u3001\u5fae\u8c03\u72b6\u6001\u7b49\u56e0\u7d20\u5bf9\u7b54\u6848\u4e00\u81f4\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u5206\u6790\u5de5\u5177\u3002", "motivation": "\u968f\u7740\u5c0f\u578bLLM\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4e86\u89e3\u5176\u5728\u91cd\u590d\u56de\u7b54\u76f8\u540c\u95ee\u9898\u65f6\u7684\u7a33\u5b9a\u6027\u53d8\u5f97\u91cd\u8981\uff0c\u8fd9\u5173\u7cfb\u5230\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "method": "\u4f7f\u7528\u5f00\u6e90LLM\u5bf9MMLU-Redux\u548cMedQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u95ee\u9898\u8fdb\u884c10\u6b21\u91cd\u590d\u56de\u7b54\uff0c\u7814\u7a76\u4e0d\u540c\u63a8\u7406\u6e29\u5ea6\u3001\u6a21\u578b\u5927\u5c0f\uff08\u5c0f\u578bvs\u4e2d\u578b50B-80B\uff09\u3001\u5fae\u8c03\u72b6\u6001\u7b49\u53c2\u6570\u5bf9\u7b54\u6848\u4e00\u81f4\u6027\u7684\u5f71\u54cd\u3002", "result": "\u5c0f\u578b\u6a21\u578b\u5728\u4f4e\u63a8\u7406\u6e29\u5ea6\u4e0b\uff0c\u80fd\u591f\u4e00\u81f4\u56de\u7b54\u7684\u95ee\u9898\u6bd4\u4f8b\u901a\u5e38\u572850%-80%\u4e4b\u95f4\uff0c\u4e00\u81f4\u6027\u7b54\u6848\u7684\u51c6\u786e\u6027\u4e0e\u603b\u4f53\u51c6\u786e\u6027\u6709\u5408\u7406\u76f8\u5173\u6027\u3002\u4e2d\u578b\u6a21\u578b\u663e\u793a\u51fa\u66f4\u9ad8\u7684\u7b54\u6848\u4e00\u81f4\u6027\u6c34\u5e73\u3002", "conclusion": "\u6a21\u578b\u5927\u5c0f\u548c\u63a8\u7406\u6e29\u5ea6\u663e\u8457\u5f71\u54cd\u7b54\u6848\u4e00\u81f4\u6027\uff0c\u4e2d\u578b\u6a21\u578b\u6bd4\u5c0f\u578b\u6a21\u578b\u8868\u73b0\u66f4\u597d\uff0c\u4e00\u81f4\u6027\u7b54\u6848\u7684\u51c6\u786e\u6027\u53ef\u4ee5\u4f5c\u4e3a\u6a21\u578b\u6574\u4f53\u6027\u80fd\u7684\u53ef\u9760\u6307\u6807\u3002"}}
{"id": "2509.09793", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09793", "abs": "https://arxiv.org/abs/2509.09793", "authors": ["Vincent Herfeld", "Baudouin Denis de Senneville", "Arthur Leclaire", "Nicolas Papadakis"], "title": "From the Gradient-Step Denoiser to the Proximal Denoiser and their associated convergent Plug-and-Play algorithms", "comment": null, "summary": "In this paper we analyze the Gradient-Step Denoiser and its usage in\nPlug-and-Play algorithms. The Plug-and-Play paradigm of optimization algorithms\nuses off the shelf denoisers to replace a proximity operator or a gradient\ndescent operator of an image prior. Usually this image prior is implicit and\ncannot be expressed, but the Gradient-Step Denoiser is trained to be exactly\nthe gradient descent operator or the proximity operator of an explicit\nfunctional while preserving state-of-the-art denoising capabilities.", "AI": {"tldr": "\u5206\u6790\u68af\u5ea6\u6b65\u957f\u53bb\u566a\u5668\u53ca\u5176\u5728\u5373\u63d2\u5373\u7528\u7b97\u6cd5\u4e2d\u7684\u5e94\u7528\uff0c\u8be5\u53bb\u566a\u5668\u88ab\u8bad\u7ec3\u4e3a\u663e\u5f0f\u51fd\u6570\u7684\u6700\u4f18\u68af\u5ea6\u4e0b\u964d\u7b97\u5b50\u6216\u90bb\u8fd1\u7b97\u5b50\uff0c\u540c\u65f6\u4fdd\u6301\u6700\u5148\u8fdb\u7684\u53bb\u566a\u80fd\u529b", "motivation": "\u5373\u63d2\u5373\u7528\u4f18\u5316\u7b97\u6cd5\u901a\u5e38\u4f7f\u7528\u73b0\u6210\u7684\u53bb\u566a\u5668\u6765\u66ff\u4ee3\u56fe\u50cf\u5148\u9a8c\u7684\u90bb\u8fd1\u7b97\u5b50\u6216\u68af\u5ea6\u4e0b\u964d\u7b97\u5b50\uff0c\u4f46\u8fd9\u4e9b\u5148\u9a8c\u901a\u5e38\u662f\u9690\u5f0f\u7684\u3002\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u4f5c\u4e3a\u663e\u5f0f\u51fd\u6570\u7b97\u5b50\u7684\u5148\u8fdb\u53bb\u566a\u5668", "method": "\u8bad\u7ec3\u68af\u5ea6\u6b65\u957f\u53bb\u566a\u5668\uff0c\u4f7f\u5176\u7cbe\u786e\u5730\u6210\u4e3a\u663e\u5f0f\u51fd\u6570\u7684\u6700\u4f18\u68af\u5ea6\u4e0b\u964d\u7b97\u5b50\u6216\u90bb\u8fd1\u7b97\u5b50\uff0c\u540c\u65f6\u4fdd\u6301\u6700\u5148\u8fdb\u7684\u53bb\u566a\u6027\u80fd", "result": "\u6210\u529f\u5f00\u53d1\u51fa\u68af\u5ea6\u6b65\u957f\u53bb\u566a\u5668\uff0c\u8be5\u53bb\u566a\u5668\u65e2\u80fd\u4f5c\u4e3a\u663e\u5f0f\u51fd\u6570\u7684\u4f18\u5316\u7b97\u5b50\uff0c\u53c8\u80fd\u4fdd\u6301state-of-the-art\u7684\u53bb\u566a\u80fd\u529b", "conclusion": "\u68af\u5ea6\u6b65\u957f\u53bb\u566a\u5668\u4e3a\u5373\u63d2\u5373\u7528\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u66f4\u5f3a\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5c06\u9690\u5f0f\u56fe\u50cf\u5148\u9a8c\u8f6c\u5316\u4e3a\u663e\u5f0f\u51fd\u6570\u8868\u793a\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u53bb\u566a\u6027\u80fd"}}
{"id": "2509.09691", "categories": ["cs.IR", "cs.AI", "cs.DB", "68T05 (Primary), 42C10, 94A12 (Secondary)", "I.2.6; H.2.4; H.3.3"], "pdf": "https://arxiv.org/pdf/2509.09691", "abs": "https://arxiv.org/abs/2509.09691", "authors": ["Aleksandr Listopad"], "title": "Wave-Based Semantic Memory with Resonance-Based Retrieval: A Phase-Aware Alternative to Vector Embedding Stores", "comment": "9 pages, 6 figures", "summary": "Conventional vector-based memory systems rely on cosine or inner product\nsimilarity within real-valued embedding spaces. While computationally\nefficient, such approaches are inherently phase-insensitive and limited in\ntheir ability to capture resonance phenomena crucial for meaning\nrepresentation. We propose Wave-Based Semantic Memory, a novel framework that\nmodels knowledge as wave patterns $\\psi(x) = A(x) e^{i\\phi(x)}$ and retrieves\nit through resonance-based interference. This approach preserves both amplitude\nand phase information, enabling more expressive and robust semantic similarity.\nWe demonstrate that resonance-based retrieval achieves higher discriminative\npower in cases where vector methods fail, including phase shifts, negations,\nand compositional queries. Our implementation, ResonanceDB, shows scalability\nto millions of patterns with millisecond latency, positioning wave-based memory\nas a viable alternative to vector stores for AGI-oriented reasoning and\nknowledge representation.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6ce2\u7684\u8bed\u4e49\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u632f\u5e72\u6d89\u8fdb\u884c\u77e5\u8bc6\u68c0\u7d22\uff0c\u76f8\u6bd4\u4f20\u7edf\u5411\u91cf\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u4fdd\u7559\u632f\u5e45\u548c\u76f8\u4f4d\u4fe1\u606f\uff0c\u5728\u8bed\u4e49\u76f8\u4f3c\u6027\u5224\u65ad\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5411\u91cf\u7684\u8bb0\u5fc6\u7cfb\u7edf\u4f9d\u8d56\u4f59\u5f26\u6216\u5185\u79ef\u76f8\u4f3c\u5ea6\uff0c\u867d\u7136\u8ba1\u7b97\u9ad8\u6548\u4f46\u672c\u8d28\u4e0a\u662f\u76f8\u4f4d\u4e0d\u654f\u611f\u7684\uff0c\u65e0\u6cd5\u6355\u6349\u5bf9\u610f\u4e49\u8868\u793a\u81f3\u5173\u91cd\u8981\u7684\u5171\u632f\u73b0\u8c61\u3002", "method": "\u5c06\u77e5\u8bc6\u5efa\u6a21\u4e3a\u6ce2\u6a21\u5f0f\u03c8(x)=A(x)e^{i\u03c6(x)}\uff0c\u901a\u8fc7\u5171\u632f\u5e72\u6d89\u8fdb\u884c\u68c0\u7d22\uff0c\u4fdd\u7559\u632f\u5e45\u548c\u76f8\u4f4d\u4fe1\u606f\u3002\u5b9e\u73b0ResonanceDB\u7cfb\u7edf\uff0c\u652f\u6301\u767e\u4e07\u7ea7\u6a21\u5f0f\u6beb\u79d2\u7ea7\u5ef6\u8fdf\u68c0\u7d22\u3002", "result": "\u5171\u632f\u68c0\u7d22\u5728\u5411\u91cf\u65b9\u6cd5\u5931\u6548\u7684\u60c5\u51b5\u4e0b\uff08\u5982\u76f8\u4f4d\u504f\u79fb\u3001\u5426\u5b9a\u548c\u7ec4\u5408\u67e5\u8be2\uff09\u5c55\u73b0\u51fa\u66f4\u9ad8\u7684\u5224\u522b\u80fd\u529b\u3002", "conclusion": "\u57fa\u4e8e\u6ce2\u7684\u8bb0\u5fc6\u7cfb\u7edf\u662f\u5411\u91cf\u5b58\u50a8\u7684\u53ef\u884c\u66ff\u4ee3\u65b9\u6848\uff0c\u9002\u7528\u4e8eAGI\u5bfc\u5411\u7684\u63a8\u7406\u548c\u77e5\u8bc6\u8868\u793a\u3002"}}
{"id": "2509.09708", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09708", "abs": "https://arxiv.org/abs/2509.09708", "authors": ["Nirmalendu Prakash", "Yeo Wei Jie", "Amir Abdullah", "Ranjan Satapathy", "Erik Cambria", "Roy Ka Wei Lee"], "title": "Beyond I'm Sorry, I Can't: Dissecting Large Language Model Refusal", "comment": null, "summary": "Refusal on harmful prompts is a key safety behaviour in instruction-tuned\nlarge language models (LLMs), yet the internal causes of this behaviour remain\npoorly understood. We study two public instruction-tuned models, Gemma-2-2B-IT\nand LLaMA-3.1-8B-IT, using sparse autoencoders (SAEs) trained on\nresidual-stream activations. Given a harmful prompt, we search the SAE latent\nspace for feature sets whose ablation flips the model from refusal to\ncompliance, demonstrating causal influence and creating a jailbreak. Our search\nproceeds in three stages: (1) Refusal Direction: find a refusal-mediating\ndirection and collect SAE features near that direction; (2) Greedy Filtering:\nprune to a minimal set; and (3) Interaction Discovery: fit a factorization\nmachine (FM) that captures nonlinear interactions among the remaining active\nfeatures and the minimal set. This pipeline yields a broad set of\njailbreak-critical features, offering insight into the mechanistic basis of\nrefusal. Moreover, we find evidence of redundant features that remain dormant\nunless earlier features are suppressed. Our findings highlight the potential\nfor fine-grained auditing and targeted intervention in safety behaviours by\nmanipulating the interpretable latent space.", "AI": {"tldr": "\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5206\u6790\u6307\u4ee4\u8c03\u4f18\u5927\u8bed\u8a00\u6a21\u578b\u7684\u62d2\u7edd\u884c\u4e3a\u673a\u5236\uff0c\u901a\u8fc7\u7279\u5f81\u6d88\u878d\u5b9e\u73b0\u8d8a\u72f1\uff0c\u63ed\u793a\u5b89\u5168\u884c\u4e3a\u7684\u56e0\u679c\u673a\u5236\u548c\u5197\u4f59\u7279\u5f81", "motivation": "\u7406\u89e3\u6307\u4ee4\u8c03\u4f18\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u6709\u5bb3\u63d0\u793a\u62d2\u7edd\u884c\u4e3a\u7684\u5185\u5728\u673a\u5236\uff0c\u76ee\u524d\u5bf9\u6b64\u884c\u4e3a\u7684\u5185\u90e8\u539f\u56e0\u4e86\u89e3\u4e0d\u8db3", "method": "\u5728Gemma-2-2B-IT\u548cLLaMA-3.1-8B-IT\u6a21\u578b\u4e0a\u8bad\u7ec3\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u641c\u7d22\u6d41\u7a0b\uff1a\u62d2\u7edd\u65b9\u5411\u8bc6\u522b\u3001\u8d2a\u5a6a\u8fc7\u6ee4\u548c\u4ea4\u4e92\u53d1\u73b0\uff0c\u5bfb\u627e\u5bfc\u81f4\u62d2\u7edd\u884c\u4e3a\u7684\u5173\u952e\u7279\u5f81", "result": "\u6210\u529f\u8bc6\u522b\u51fa\u5bfc\u81f4\u62d2\u7edd\u884c\u4e3a\u7684\u5173\u952e\u7279\u5f81\u96c6\uff0c\u901a\u8fc7\u7279\u5f81\u6d88\u878d\u5b9e\u73b0\u6a21\u578b\u4ece\u62d2\u7edd\u5230\u5408\u89c4\u7684\u8f6c\u53d8\uff0c\u53d1\u73b0\u5b58\u5728\u5197\u4f59\u7279\u5f81\u673a\u5236", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u901a\u8fc7\u64cd\u4f5c\u53ef\u89e3\u91ca\u6f5c\u5728\u7a7a\u95f4\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5ba1\u8ba1\u548c\u9488\u5bf9\u6027\u5e72\u9884\u5b89\u5168\u884c\u4e3a\u63d0\u4f9b\u4e86\u6f5c\u529b"}}
{"id": "2509.09799", "categories": ["cs.LG", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.09799", "abs": "https://arxiv.org/abs/2509.09799", "authors": ["Mansi Sharma", "Alexandre Duchevet", "Florian Daiber", "Jean-Paul Imbert", "Maurice Rekrut"], "title": "Distinguishing Startle from Surprise Events Based on Physiological Signals", "comment": null, "summary": "Unexpected events can impair attention and delay decision-making, posing\nserious safety risks in high-risk environments such as aviation. In particular,\nreactions like startle and surprise can impact pilot performance in different\nways, yet are often hard to distinguish in practice. Existing research has\nlargely studied these reactions separately, with limited focus on their\ncombined effects or how to differentiate them using physiological data. In this\nwork, we address this gap by distinguishing between startle and surprise events\nbased on physiological signals using machine learning and multi-modal fusion\nstrategies. Our results demonstrate that these events can be reliably\npredicted, achieving a highest mean accuracy of 85.7% with SVM and Late Fusion.\nTo further validate the robustness of our model, we extended the evaluation to\ninclude a baseline condition, successfully differentiating between Startle,\nSurprise, and Baseline states with a highest mean accuracy of 74.9% with\nXGBoost and Late Fusion.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u548c\u591a\u6a21\u6001\u878d\u5408\u7b56\u7565\uff0c\u57fa\u4e8e\u751f\u7406\u4fe1\u53f7\u533a\u5206\u60ca\u5413\u548c\u60ca\u8bb6\u53cd\u5e94\uff0c\u6700\u9ad8\u51c6\u786e\u7387\u8fbe85.7%\uff0c\u5e76\u80fd\u533a\u5206\u4e09\u79cd\u72b6\u6001\uff08\u60ca\u5413\u3001\u60ca\u8bb6\u3001\u57fa\u7ebf\uff09\u51c6\u786e\u7387\u8fbe74.9%", "motivation": "\u610f\u5916\u4e8b\u4ef6\u4f1a\u635f\u5bb3\u6ce8\u610f\u529b\u548c\u5ef6\u8fdf\u51b3\u7b56\uff0c\u5728\u822a\u7a7a\u7b49\u9ad8\u5371\u73af\u5883\u4e2d\u9020\u6210\u4e25\u91cd\u5b89\u5168\u98ce\u9669\u3002\u60ca\u5413\u548c\u60ca\u8bb6\u53cd\u5e94\u4ee5\u4e0d\u540c\u65b9\u5f0f\u5f71\u54cd\u98de\u884c\u5458\u8868\u73b0\uff0c\u4f46\u5b9e\u8df5\u4e2d\u96be\u4ee5\u533a\u5206\uff0c\u73b0\u6709\u7814\u7a76\u591a\u5355\u72ec\u7814\u7a76\u8fd9\u4e24\u79cd\u53cd\u5e94\uff0c\u7f3a\u4e4f\u5bf9\u7ec4\u5408\u6548\u5e94\u548c\u751f\u7406\u6570\u636e\u533a\u5206\u65b9\u6cd5\u7684\u7814\u7a76", "method": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u548c\u591a\u6a21\u6001\u878d\u5408\u7b56\u7565\uff0c\u57fa\u4e8e\u751f\u7406\u4fe1\u53f7\u6570\u636e\u6765\u533a\u5206\u60ca\u5413\u548c\u60ca\u8bb6\u4e8b\u4ef6", "result": "\u80fd\u591f\u53ef\u9760\u9884\u6d4b\u8fd9\u4e9b\u4e8b\u4ef6\uff0cSVM\u548cLate Fusion\u65b9\u6cd5\u83b7\u5f97\u6700\u9ad8\u5e73\u5747\u51c6\u786e\u738785.7%\uff1b\u6269\u5c55\u8bc4\u4f30\u5305\u542b\u57fa\u7ebf\u6761\u4ef6\u540e\uff0cXGBoost\u548cLate Fusion\u65b9\u6cd5\u80fd\u533a\u5206\u60ca\u5413\u3001\u60ca\u8bb6\u548c\u57fa\u7ebf\u4e09\u79cd\u72b6\u6001\uff0c\u6700\u9ad8\u5e73\u5747\u51c6\u786e\u7387\u8fbe74.9%", "conclusion": "\u901a\u8fc7\u751f\u7406\u4fe1\u53f7\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u533a\u5206\u60ca\u5413\u548c\u60ca\u8bb6\u53cd\u5e94\uff0c\u4e3a\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u98de\u884c\u5458\u72b6\u6001\u76d1\u6d4b\u548c\u5b89\u5168\u7ba1\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u6280\u672f\u624b\u6bb5"}}
{"id": "2509.10212", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.10212", "abs": "https://arxiv.org/abs/2509.10212", "authors": ["Alisa Rieger", "Stefan Dietze", "Ran Yu"], "title": "A Research Vision for Web Search on Emerging Topics", "comment": null, "summary": "We regularly encounter information on novel, emerging topics for which the\nbody of knowledge is still evolving, which can be linked, for instance, to\ncurrent events. A primary way to learn more about such topics is through web\nsearch. However, information on emerging topics is sparse and evolves\ndynamically as knowledge grows, making it uncertain and variable in quality and\ntrustworthiness and prone to deliberate or accidental manipulation,\nmisinformation, and bias. In this paper, we outline a research vision towards\nsearch systems and interfaces that support effective knowledge acquisition,\nawareness of the dynamic nature of topics, and responsible opinion formation\namong people searching the web for information on emerging topics. To realize\nthis vision, we propose three overarching research questions, aimed at\nunderstanding the status quo, determining requirements of systems aligned with\nour vision, and building these systems. For each of the three questions, we\nhighlight relevant literature, including pointers on how they could be\naddressed. Lastly, we discuss the challenges that will potentially arise in\npursuing the proposed vision.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7814\u7a76\u613f\u666f\uff0c\u65e8\u5728\u5f00\u53d1\u652f\u6301\u65b0\u5174\u4e3b\u9898\u77e5\u8bc6\u83b7\u53d6\u7684\u641c\u7d22\u5f15\u64ce\u7cfb\u7edf\uff0c\u5e2e\u52a9\u7528\u6237\u4e86\u89e3\u52a8\u6001\u53d8\u5316\u7684\u4fe1\u606f\u5e76\u5f62\u6210\u8d1f\u8d23\u4efb\u7684\u89c2\u70b9\u3002", "motivation": "\u65b0\u5174\u4e3b\u9898\u7684\u4fe1\u606f\u901a\u5e38\u7a00\u758f\u3001\u52a8\u6001\u53d8\u5316\u4e14\u8d28\u91cf\u53c2\u5dee\u4e0d\u9f50\uff0c\u5bb9\u6613\u53d7\u5230\u9519\u8bef\u4fe1\u606f\u548c\u504f\u89c1\u7684\u5f71\u54cd\uff0c\u73b0\u6709\u641c\u7d22\u5f15\u64ce\u96be\u4ee5\u6709\u6548\u652f\u6301\u7528\u6237\u83b7\u53d6\u53ef\u9760\u77e5\u8bc6\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u4e2a\u6838\u5fc3\u7814\u7a76\u95ee\u9898\uff1a\u7406\u89e3\u73b0\u72b6\u3001\u786e\u5b9a\u7cfb\u7edf\u9700\u6c42\u3001\u6784\u5efa\u65b0\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u6307\u51fa\u4e86\u53ef\u80fd\u7684\u89e3\u51b3\u65b9\u5411\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u7814\u7a76\u6846\u67b6\u548c\u613f\u666f\uff0c\u4f46\u6ca1\u6709\u5177\u4f53\u7684\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4e3b\u8981\u8d21\u732e\u5728\u4e8e\u7406\u8bba\u6846\u67b6\u7684\u6784\u5efa\u548c\u7814\u7a76\u65b9\u5411\u7684\u6307\u5f15\u3002", "conclusion": "\u5f00\u53d1\u80fd\u591f\u652f\u6301\u65b0\u5174\u4e3b\u9898\u77e5\u8bc6\u83b7\u53d6\u7684\u641c\u7d22\u5f15\u64ce\u7cfb\u7edf\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f46\u9762\u4e34\u8bf8\u591a\u6311\u6218\uff0c\u9700\u8981\u8de8\u5b66\u79d1\u5408\u4f5c\u6765\u89e3\u51b3\u4fe1\u606f\u8d28\u91cf\u3001\u52a8\u6001\u6027\u548c\u7528\u6237\u8ba4\u77e5\u7b49\u65b9\u9762\u7684\u95ee\u9898\u3002"}}
{"id": "2509.09709", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09709", "abs": "https://arxiv.org/abs/2509.09709", "authors": ["Jing Ren", "Weiqi Wang"], "title": "Assisting Research Proposal Writing with Large Language Models: Evaluation and Refinement", "comment": null, "summary": "Large language models (LLMs) like ChatGPT are increasingly used in academic\nwriting, yet issues such as incorrect or fabricated references raise ethical\nconcerns. Moreover, current content quality evaluations often rely on\nsubjective human judgment, which is labor-intensive and lacks objectivity,\npotentially compromising the consistency and reliability. In this study, to\nprovide a quantitative evaluation and enhance research proposal writing\ncapabilities of LLMs, we propose two key evaluation metrics--content quality\nand reference validity--and an iterative prompting method based on the scores\nderived from these two metrics. Our extensive experiments show that the\nproposed metrics provide an objective, quantitative framework for assessing\nChatGPT's writing performance. Additionally, iterative prompting significantly\nenhances content quality while reducing reference inaccuracies and\nfabrications, addressing critical ethical challenges in academic contexts.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u4e2a\u8bc4\u4f30\u6307\u6807\uff08\u5185\u5bb9\u8d28\u91cf\u548c\u53c2\u8003\u6587\u732e\u6709\u6548\u6027\uff09\u548c\u57fa\u4e8e\u5206\u6570\u7684\u8fed\u4ee3\u63d0\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9a\u91cf\u8bc4\u4f30ChatGPT\u5b66\u672f\u5199\u4f5c\u80fd\u529b\u5e76\u63d0\u5347\u5176\u7814\u7a76\u63d0\u6848\u5199\u4f5c\u8d28\u91cf", "motivation": "\u89e3\u51b3LLMs\u5728\u5b66\u672f\u5199\u4f5c\u4e2d\u5b58\u5728\u7684\u9519\u8bef\u6216\u4f2a\u9020\u53c2\u8003\u6587\u732e\u7b49\u4f26\u7406\u95ee\u9898\uff0c\u4ee5\u53ca\u5f53\u524d\u5185\u5bb9\u8d28\u91cf\u8bc4\u4f30\u4f9d\u8d56\u4e3b\u89c2\u4eba\u5de5\u5224\u65ad\u3001\u7f3a\u4e4f\u5ba2\u89c2\u6027\u7684\u95ee\u9898", "method": "\u63d0\u51fa\u5185\u5bb9\u8d28\u91cf\u548c\u53c2\u8003\u6587\u732e\u6709\u6548\u6027\u4e24\u4e2a\u5173\u952e\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u6307\u6807\u7684\u5206\u6570\u8bbe\u8ba1\u8fed\u4ee3\u63d0\u793a\u65b9\u6cd5", "result": "\u5b9e\u9a8c\u8868\u660e\u63d0\u51fa\u7684\u6307\u6807\u4e3aChatGPT\u5199\u4f5c\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5ba2\u89c2\u5b9a\u91cf\u6846\u67b6\uff0c\u8fed\u4ee3\u63d0\u793a\u663e\u8457\u63d0\u5347\u5185\u5bb9\u8d28\u91cf\u5e76\u51cf\u5c11\u53c2\u8003\u6587\u732e\u9519\u8bef\u548c\u4f2a\u9020", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5b66\u672f\u73af\u5883\u4e0bLLMs\u5199\u4f5c\u7684\u5173\u952e\u4f26\u7406\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5b9a\u91cf\u8bc4\u4f30\u548c\u6539\u8fdb\u673a\u5236"}}
{"id": "2509.09838", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09838", "abs": "https://arxiv.org/abs/2509.09838", "authors": ["Reza Asad", "Reza Babanezhad", "Sharan Vaswani"], "title": "Revisiting Actor-Critic Methods in Discrete Action Off-Policy Reinforcement Learning", "comment": null, "summary": "Value-based approaches such as DQN are the default methods for off-policy\nreinforcement learning with discrete-action environments such as Atari. Common\npolicy-based methods are either on-policy and do not effectively learn from\noff-policy data (e.g. PPO), or have poor empirical performance in the\ndiscrete-action setting (e.g. SAC). Consequently, starting from discrete SAC\n(DSAC), we revisit the design of actor-critic methods in this setting. First,\nwe determine that the coupling between the actor and critic entropy is the\nprimary reason behind the poor performance of DSAC. We demonstrate that by\nmerely decoupling these components, DSAC can have comparable performance as\nDQN. Motivated by this insight, we introduce a flexible off-policy actor-critic\nframework that subsumes DSAC as a special case. Our framework allows using an\nm-step Bellman operator for the critic update, and enables combining standard\npolicy optimization methods with entropy regularization to instantiate the\nresulting actor objective. Theoretically, we prove that the proposed methods\ncan guarantee convergence to the optimal regularized value function in the\ntabular setting. Empirically, we demonstrate that these methods can approach\nthe performance of DQN on standard Atari games, and do so even without entropy\nregularization or explicit exploration.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u79bb\u6563\u52a8\u4f5c\u73af\u5883\u4e2d\u7684actor-critic\u65b9\u6cd5\uff0c\u53d1\u73b0DSAC\u6027\u80fd\u4e0d\u4f73\u7684\u4e3b\u8981\u539f\u56e0\u662factor\u548ccritic\u71b5\u7684\u8026\u5408\u3002\u901a\u8fc7\u89e3\u8026\u8fd9\u4e24\u4e2a\u7ec4\u4ef6\u5e76\u63d0\u51fa\u7075\u6d3b\u7684off-policy\u6846\u67b6\uff0c\u65b9\u6cd5\u5728Atari\u6e38\u620f\u4e2d\u8fbe\u5230\u4e86\u4e0eDQN\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u79bb\u6563\u52a8\u4f5c\u73af\u5883\u4e2doff-policy\u5f3a\u5316\u5b66\u4e60\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u503c\u57fa\u65b9\u6cd5\uff08\u5982DQN\uff09\u662f\u9ed8\u8ba4\u9009\u62e9\uff0c\u800c\u5e38\u89c1\u7684\u7b56\u7565\u57fa\u65b9\u6cd5\u8981\u4e48\u65e0\u6cd5\u6709\u6548\u5229\u7528off-policy\u6570\u636e\uff08\u5982PPO\uff09\uff0c\u8981\u4e48\u5728\u79bb\u6563\u52a8\u4f5c\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4e0d\u4f73\uff08\u5982SAC\uff09\u3002", "method": "\u4ece\u79bb\u6563SAC\uff08DSAC\uff09\u51fa\u53d1\uff0c\u89e3\u8026actor\u548ccritic\u7684\u71b5\u7ec4\u4ef6\uff0c\u63d0\u51fa\u7075\u6d3b\u7684off-policy actor-critic\u6846\u67b6\uff0c\u5141\u8bb8\u4f7f\u7528m\u6b65Bellman\u7b97\u5b50\u8fdb\u884ccritic\u66f4\u65b0\uff0c\u5e76\u5c06\u6807\u51c6\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u4e0e\u71b5\u6b63\u5219\u5316\u7ed3\u5408\u6765\u5b9e\u4f8b\u5316actor\u76ee\u6807\u3002", "result": "\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u5728\u8868\u683c\u8bbe\u7f6e\u4e2d\u53ef\u4ee5\u6536\u655b\u5230\u6700\u4f18\u6b63\u5219\u5316\u503c\u51fd\u6570\uff1b\u5b9e\u8bc1\u8868\u660e\u8fd9\u4e9b\u65b9\u6cd5\u5728\u6807\u51c6Atari\u6e38\u620f\u4e2d\u53ef\u4ee5\u8fbe\u5230DQN\u7684\u6027\u80fd\u6c34\u5e73\uff0c\u751a\u81f3\u5728\u6ca1\u6709\u71b5\u6b63\u5219\u5316\u6216\u663e\u5f0f\u63a2\u7d22\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u3002", "conclusion": "\u901a\u8fc7\u89e3\u8026actor-critic\u7684\u71b5\u7ec4\u4ef6\u5e76\u8bbe\u8ba1\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u79bb\u6563\u52a8\u4f5c\u73af\u5883\u4e2doff-policy actor-critic\u65b9\u6cd5\u7684\u6027\u80fd\u95ee\u9898\uff0c\u4e3a\u79bb\u6563\u52a8\u4f5cRL\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.10245", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10245", "abs": "https://arxiv.org/abs/2509.10245", "authors": ["Irina Ar\u00e9valo", "Jose L Salmeron"], "title": "Model-agnostic post-hoc explainability for recommender systems", "comment": null, "summary": "Recommender systems often benefit from complex feature embeddings and deep\nlearning algorithms, which deliver sophisticated recommendations that enhance\nuser experience, engagement, and revenue. However, these methods frequently\nreduce the interpretability and transparency of the system. In this research,\nwe develop a systematic application, adaptation, and evaluation of deletion\ndiagnostics in the recommender setting. The method compares the performance of\na model to that of a similar model trained without a specific user or item,\nallowing us to quantify how that observation influences the recommender, either\npositively or negatively. To demonstrate its model-agnostic nature, the\nproposal is applied to both Neural Collaborative Filtering (NCF), a widely used\ndeep learning-based recommender, and Singular Value Decomposition (SVD), a\nclassical collaborative filtering technique. Experiments on the MovieLens and\nAmazon Reviews datasets provide insights into model behavior and highlight the\ngenerality of the approach across different recommendation paradigms.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u5220\u9664\u8bca\u65ad\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u63a8\u8350\u7cfb\u7edf\u4e2d\u7279\u5b9a\u7528\u6237\u6216\u9879\u76ee\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u63d0\u9ad8\u63a8\u8350\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u548c\u900f\u660e\u5ea6\u3002", "motivation": "\u63a8\u8350\u7cfb\u7edf\u901a\u5e38\u4f7f\u7528\u590d\u6742\u7684\u7279\u5f81\u5d4c\u5165\u548c\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\uff0c\u867d\u7136\u80fd\u63d0\u4f9b\u7cbe\u51c6\u63a8\u8350\uff0c\u4f46\u964d\u4f4e\u4e86\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u548c\u900f\u660e\u5ea6\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u91cf\u5316\u7279\u5b9a\u89c2\u5bdf\u503c\u5bf9\u63a8\u8350\u7cfb\u7edf\u7684\u5f71\u54cd\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u5220\u9664\u8bca\u65ad\u65b9\u6cd5\uff0c\u901a\u8fc7\u6bd4\u8f83\u5b8c\u6574\u6a21\u578b\u4e0e\u6392\u9664\u7279\u5b9a\u7528\u6237\u6216\u9879\u76ee\u540e\u8bad\u7ec3\u6a21\u578b\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u6765\u91cf\u5316\u8be5\u89c2\u5bdf\u503c\u5bf9\u63a8\u8350\u7cfb\u7edf\u7684\u5f71\u54cd\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u795e\u7ecf\u534f\u540c\u8fc7\u6ee4(NCF)\u548c\u5947\u5f02\u503c\u5206\u89e3(SVD)\u7b49\u4e0d\u540c\u63a8\u8350\u7b97\u6cd5\u3002", "result": "\u5728MovieLens\u548cAmazon Reviews\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u63d0\u4f9b\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u6df1\u5165\u6d1e\u5bdf\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u4e0d\u540c\u63a8\u8350\u8303\u5f0f\u4e2d\u7684\u901a\u7528\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u5220\u9664\u8bca\u65ad\u65b9\u6cd5\u662f\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u5de5\u5177\uff0c\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u63a8\u8350\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e2e\u52a9\u7406\u89e3\u7279\u5b9a\u7528\u6237\u6216\u9879\u76ee\u5bf9\u63a8\u8350\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u63a8\u8350\u7b97\u6cd5\u3002"}}
{"id": "2509.09710", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09710", "abs": "https://arxiv.org/abs/2509.09710", "authors": ["Sepehr Golrokh Amin", "Devin Rhoads", "Fatemeh Fakhrmoosavi", "Nicholas E. Lownes", "John N. Ivan"], "title": "Generating Individual Travel Diaries Using Large Language Models Informed by Census and Land-Use Data", "comment": null, "summary": "This study introduces a Large Language Model (LLM) scheme for generating\nindividual travel diaries in agent-based transportation models. While\ntraditional approaches rely on large quantities of proprietary household travel\nsurveys, the method presented in this study generates personas stochastically\nfrom open-source American Community Survey (ACS) and Smart Location Database\n(SLD) data, then synthesizes diaries through direct prompting. This study\nfeatures a novel one-to-cohort realism score: a composite of four metrics (Trip\nCount Score, Interval Score, Purpose Score, and Mode Score) validated against\nthe Connecticut Statewide Transportation Study (CSTS) diaries, matched across\ndemographic variables. The validation utilizes Jensen-Shannon Divergence to\nmeasure distributional similarities between generated and real diaries. When\ncompared to diaries generated with classical methods (Negative Binomial for\ntrip generation; Multinomial Logit for mode/purpose) calibrated on the\nvalidation set, LLM-generated diaries achieve comparable overall realism (LLM\nmean: 0.485 vs. 0.455). The LLM excels in determining trip purpose and\ndemonstrates greater consistency (narrower realism score distribution), while\nclassical models lead in numerical estimates of trip count and activity\nduration. Aggregate validation confirms the LLM's statistical\nrepresentativeness (LLM mean: 0.612 vs. 0.435), demonstrating LLM's zero-shot\nviability and establishing a quantifiable metric of diary realism for future\nsynthetic diary evaluation systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e2a\u4f53\u51fa\u884c\u65e5\u8bb0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f00\u6e90\u6570\u636e\u751f\u6210\u865a\u62df\u4eba\u7269\u5e76\u5408\u6210\u51fa\u884c\u8bb0\u5f55\uff0c\u9a8c\u8bc1\u663e\u793aLLM\u5728\u51fa\u884c\u76ee\u7684\u786e\u5b9a\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u6574\u4f53\u771f\u5b9e\u6027\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5927\u91cf\u4e13\u6709\u5bb6\u5ead\u51fa\u884c\u8c03\u67e5\u7684\u65b9\u6cd5\u6210\u672c\u9ad8\u6602\u4e14\u6570\u636e\u83b7\u53d6\u56f0\u96be\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u57fa\u4e8e\u5f00\u6e90\u6570\u636e\u7684\u66ff\u4ee3\u65b9\u6848\u6765\u751f\u6210\u771f\u5b9e\u7684\u4e2a\u4f53\u51fa\u884c\u65e5\u8bb0\u3002", "method": "\u4f7f\u7528\u7f8e\u56fd\u793e\u533a\u8c03\u67e5\u548c\u667a\u80fd\u4f4d\u7f6e\u6570\u636e\u5e93\u7684\u5f00\u6e90\u6570\u636e\u968f\u673a\u751f\u6210\u865a\u62df\u4eba\u7269\uff0c\u901a\u8fc7\u76f4\u63a5\u63d0\u793aLLM\u5408\u6210\u51fa\u884c\u65e5\u8bb0\uff0c\u5e76\u91c7\u7528\u5305\u542b\u56db\u4e2a\u6307\u6807\uff08\u51fa\u884c\u6b21\u6570\u3001\u65f6\u95f4\u95f4\u9694\u3001\u51fa\u884c\u76ee\u7684\u3001\u4ea4\u901a\u65b9\u5f0f\uff09\u7684\u7efc\u5408\u771f\u5b9e\u5ea6\u8bc4\u5206\u4f53\u7cfb\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "LLM\u751f\u6210\u7684\u65e5\u8bb0\u6574\u4f53\u771f\u5b9e\u5ea6\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\uff080.485 vs 0.455\uff09\uff0c\u5728\u786e\u5b9a\u51fa\u884c\u76ee\u7684\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u4e14\u4e00\u81f4\u6027\u66f4\u597d\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u51fa\u884c\u6b21\u6570\u548c\u6d3b\u52a8\u65f6\u957f\u4f30\u8ba1\u65b9\u9762\u7565\u80dc\u4e00\u7b79\u3002\u805a\u5408\u9a8c\u8bc1\u663e\u793aLLM\u5177\u6709\u66f4\u597d\u7684\u7edf\u8ba1\u4ee3\u8868\u6027\uff080.612 vs 0.435\uff09\u3002", "conclusion": "LLM\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u6761\u4ef6\u4e0b\u53ef\u884c\uff0c\u4e3a\u672a\u6765\u5408\u6210\u51fa\u884c\u65e5\u8bb0\u8bc4\u4f30\u7cfb\u7edf\u5efa\u7acb\u4e86\u53ef\u91cf\u5316\u7684\u771f\u5b9e\u5ea6\u5ea6\u91cf\u6807\u51c6\uff0c\u5c55\u793a\u4e86\u5728\u4ea4\u901a\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.09843", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09843", "abs": "https://arxiv.org/abs/2509.09843", "authors": ["Jiajun Shen", "Yufei Jin", "Yi He", "Xingquan Zhu"], "title": "HGEN: Heterogeneous Graph Ensemble Networks", "comment": "The paper is in proceedings of the 34th IJCAI Conference, 2025", "summary": "This paper presents HGEN that pioneers ensemble learning for heterogeneous\ngraphs. We argue that the heterogeneity in node types, nodal features, and\nlocal neighborhood topology poses significant challenges for ensemble learning,\nparticularly in accommodating diverse graph learners. Our HGEN framework\nensembles multiple learners through a meta-path and transformation-based\noptimization pipeline to uplift classification accuracy. Specifically, HGEN\nuses meta-path combined with random dropping to create Allele Graph Neural\nNetworks (GNNs), whereby the base graph learners are trained and aligned for\nlater ensembling. To ensure effective ensemble learning, HGEN presents two key\ncomponents: 1) a residual-attention mechanism to calibrate allele GNNs of\ndifferent meta-paths, thereby enforcing node embeddings to focus on more\ninformative graphs to improve base learner accuracy, and 2) a\ncorrelation-regularization term to enlarge the disparity among embedding\nmatrices generated from different meta-paths, thereby enriching base learner\ndiversity. We analyze the convergence of HGEN and attest its higher\nregularization magnitude over simple voting. Experiments on five heterogeneous\nnetworks validate that HGEN consistently outperforms its state-of-the-art\ncompetitors by substantial margin.", "AI": {"tldr": "HGEN\u662f\u9996\u4e2a\u9488\u5bf9\u5f02\u8d28\u56fe\u7684\u96c6\u6210\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5143\u8def\u5f84\u548c\u53d8\u6362\u4f18\u5316\u7ba1\u9053\u96c6\u6210\u591a\u4e2a\u5b66\u4e60\u5668\uff0c\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u7387", "motivation": "\u5f02\u8d28\u56fe\u4e2d\u8282\u70b9\u7c7b\u578b\u3001\u8282\u70b9\u7279\u5f81\u548c\u5c40\u90e8\u90bb\u57df\u62d3\u6251\u7684\u5f02\u8d28\u6027\u7ed9\u96c6\u6210\u5b66\u4e60\u5e26\u6765\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u9002\u5e94\u591a\u6837\u5316\u56fe\u5b66\u4e60\u5668\u65b9\u9762", "method": "\u4f7f\u7528\u5143\u8def\u5f84\u7ed3\u5408\u968f\u673a\u4e22\u5f03\u521b\u5efa\u7b49\u4f4d\u57fa\u56e0GNN\uff0c\u901a\u8fc7\u6b8b\u5dee\u6ce8\u610f\u529b\u673a\u5236\u6821\u51c6\u4e0d\u540c\u5143\u8def\u5f84\u7684\u7b49\u4f4d\u57fa\u56e0GNN\uff0c\u5e76\u4f7f\u7528\u76f8\u5173\u6027\u6b63\u5219\u5316\u9879\u6269\u5927\u4e0d\u540c\u5143\u8def\u5f84\u751f\u6210\u7684\u5d4c\u5165\u77e9\u9635\u5dee\u5f02", "result": "\u5728\u4e94\u4e2a\u5f02\u8d28\u7f51\u7edc\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1HGEN\u59cb\u7ec8\u4ee5\u663e\u8457\u4f18\u52bf\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u7ade\u4e89\u5bf9\u624b", "conclusion": "HGEN\u901a\u8fc7\u6709\u6548\u7684\u96c6\u6210\u5b66\u4e60\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5f02\u8d28\u56fe\u5b66\u4e60\u4e2d\u7684\u6311\u6218\uff0c\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u6027"}}
{"id": "2509.10392", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10392", "abs": "https://arxiv.org/abs/2509.10392", "authors": ["Carole Ibrahim", "Hiba Bederina", "Daniel Cuesta", "Laurent Montier", "Cyrille Delabre", "Jill-J\u00eann Vie"], "title": "Diversified recommendations of cultural activities with personalized determinantal point processes", "comment": "7 pages, accepted at RecSys workshop RecSoGood 2025", "summary": "While optimizing recommendation systems for user engagement is a\nwell-established practice, effectively diversifying recommendations without\nnegatively impacting core business metrics remains a significant industry\nchallenge. In line with our initiative to broaden our audience's cultural\npractices, this study investigates using personalized Determinantal Point\nProcesses (DPPs) to sample diverse and relevant recommendations. We rely on a\nwell-known quality-diversity decomposition of the similarity kernel to give\nmore weight to user preferences. In this paper, we present our implementations\nof the personalized DPP sampling, evaluate the trade-offs between relevance and\ndiversity through both offline and online metrics, and give insights for\npractitioners on their use in a production environment. For the sake of\nreproducibility, we release the full code for our platform and experiments on\nGitHub.", "AI": {"tldr": "\u4f7f\u7528\u4e2a\u6027\u5316\u884c\u5217\u5f0f\u70b9\u8fc7\u7a0b(DPP)\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u5e73\u8861\u76f8\u5173\u6027\u548c\u591a\u6837\u6027\uff0c\u901a\u8fc7\u8d28\u91cf-\u591a\u6837\u6027\u5206\u89e3\u6838\u51fd\u6570\u6765\u4f18\u5148\u8003\u8651\u7528\u6237\u504f\u597d\uff0c\u5e76\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u8bc4\u4f30\u6548\u679c", "motivation": "\u5728\u4f18\u5316\u7528\u6237\u53c2\u4e0e\u5ea6\u7684\u540c\u65f6\uff0c\u6709\u6548\u591a\u6837\u5316\u63a8\u8350\u5185\u5bb9\u800c\u4e0d\u5f71\u54cd\u6838\u5fc3\u4e1a\u52a1\u6307\u6807\u662f\u884c\u4e1a\u91cd\u5927\u6311\u6218\uff0c\u65e8\u5728\u6269\u5927\u53d7\u4f17\u7684\u6587\u5316\u5b9e\u8df5", "method": "\u5b9e\u73b0\u4e2a\u6027\u5316DPP\u91c7\u6837\u65b9\u6cd5\uff0c\u57fa\u4e8e\u8d28\u91cf-\u591a\u6837\u6027\u5206\u89e3\u76f8\u4f3c\u6027\u6838\u51fd\u6570\uff0c\u7ed9\u4e88\u7528\u6237\u504f\u597d\u66f4\u591a\u6743\u91cd\uff0c\u901a\u8fc7\u79bb\u7ebf\u548c\u5728\u7ebf\u6307\u6807\u8bc4\u4f30\u6548\u679c", "result": "\u63d0\u51fa\u4e86\u5b8c\u6574\u7684\u5e73\u53f0\u548c\u5b9e\u9a8c\u4ee3\u7801\u5e76\u5f00\u6e90\u5728GitHub\u4e0a\uff0c\u4e3a\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u4f7f\u7528\u7684\u5b9e\u8df5\u89c1\u89e3", "conclusion": "\u4e2a\u6027\u5316DPP\u662f\u5e73\u8861\u63a8\u8350\u7cfb\u7edf\u76f8\u5173\u6027\u548c\u591a\u6837\u6027\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u635f\u5bb3\u6838\u5fc3\u6307\u6807\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u6587\u5316\u5b9e\u8df5\u7684\u591a\u6837\u5316\u63a8\u8350"}}
{"id": "2509.09711", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09711", "abs": "https://arxiv.org/abs/2509.09711", "authors": ["Aya E. Fouda", "Abdelrahamn A. Hassan", "Radwa J. Hanafy", "Mohammed E. Fouda"], "title": "Psychiatry-Bench: A Multi-Task Benchmark for LLMs in Psychiatry", "comment": null, "summary": "Large language models (LLMs) hold great promise in enhancing psychiatric\npractice, from improving diagnostic accuracy to streamlining clinical\ndocumentation and therapeutic support. However, existing evaluation resources\nheavily rely on small clinical interview corpora, social media posts, or\nsynthetic dialogues, which limits their clinical validity and fails to capture\nthe full complexity of psychiatric reasoning. In this work, we introduce\nPsychiatryBench, a rigorously curated benchmark grounded exclusively in\nauthoritative, expert-validated psychiatric textbooks and casebooks.\nPsychiatryBench comprises eleven distinct question-answering tasks ranging from\ndiagnostic reasoning and treatment planning to longitudinal follow-up,\nmanagement planning, clinical approach, sequential case analysis, and\nmultiple-choice/extended matching formats totaling over 5,300 expert-annotated\nitems. We evaluate a diverse set of frontier LLMs (including Google Gemini,\nDeepSeek, LLaMA 3, and QWQ-32) alongside leading open-source medical models\n(e.g., OpenBiloLLM, MedGemma) using both conventional metrics and an\n\"LLM-as-judge\" similarity scoring framework. Our results reveal substantial\ngaps in clinical consistency and safety, particularly in multi-turn follow-up\nand management tasks, underscoring the need for specialized model tuning and\nmore robust evaluation paradigms. PsychiatryBench offers a modular, extensible\nplatform for benchmarking and improving LLM performance in high-stakes mental\nhealth applications.", "AI": {"tldr": "PsychiatryBench\u662f\u4e00\u4e2a\u57fa\u4e8e\u6743\u5a01\u7cbe\u795e\u75c5\u5b66\u6559\u79d1\u4e66\u548c\u6848\u4f8b\u96c6\u7684\u4e13\u4e1a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b11\u4e2a\u95ee\u7b54\u4efb\u52a1\u548c5300\u591a\u4e2a\u4e13\u5bb6\u6807\u6ce8\u9879\u76ee\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u7cbe\u795e\u75c5\u5b66\u5b9e\u8df5\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u8d44\u6e90\u4e3b\u8981\u4f9d\u8d56\u5c0f\u578b\u4e34\u5e8a\u8bbf\u8c08\u8bed\u6599\u5e93\u3001\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u6216\u5408\u6210\u5bf9\u8bdd\uff0c\u4e34\u5e8a\u6709\u6548\u6027\u6709\u9650\uff0c\u65e0\u6cd5\u6355\u6349\u7cbe\u795e\u75c5\u5b66\u63a8\u7406\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u66f4\u4e13\u4e1a\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u57fa\u4e8e\u6743\u5a01\u4e13\u5bb6\u9a8c\u8bc1\u7684\u7cbe\u795e\u75c5\u5b66\u6559\u79d1\u4e66\u548c\u6848\u4f8b\u96c6\u6784\u5efa\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u8bca\u65ad\u63a8\u7406\u3001\u6cbb\u7597\u8ba1\u5212\u7b4911\u4e2a\u4efb\u52a1\u7c7b\u578b\uff0c\u4f7f\u7528\u4f20\u7edf\u6307\u6807\u548c\"LLM-as-judge\"\u76f8\u4f3c\u6027\u8bc4\u5206\u6846\u67b6\u8bc4\u4f30\u591a\u79cd\u524d\u6cbfLLM\u3002", "result": "\u7ed3\u679c\u663e\u793a\u5728\u4e34\u5e8a\u4e00\u81f4\u6027\u548c\u5b89\u5168\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u591a\u8f6e\u968f\u8bbf\u548c\u7ba1\u7406\u4efb\u52a1\u4e2d\uff0c\u8868\u660e\u9700\u8981\u4e13\u95e8\u7684\u6a21\u578b\u8c03\u4f18\u548c\u66f4\u5f3a\u5927\u7684\u8bc4\u4f30\u8303\u5f0f\u3002", "conclusion": "PsychiatryBench\u4e3a\u9ad8\u98ce\u9669\u5fc3\u7406\u5065\u5eb7\u5e94\u7528\u4e2dLLM\u6027\u80fd\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u6539\u8fdb\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u7684\u5e73\u53f0\u3002"}}
{"id": "2509.09864", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09864", "abs": "https://arxiv.org/abs/2509.09864", "authors": ["Jenny Y. Huang", "Mehul Damani", "Yousef El-Kurdi", "Ramon Astudillo", "Wei Sun"], "title": "Latency and Token-Aware Test-Time Compute", "comment": null, "summary": "Inference-time scaling has emerged as a powerful way to improve large\nlanguage model (LLM) performance by generating multiple candidate responses and\nselecting among them. However, existing work on dynamic allocation for\ntest-time compute typically considers only parallel generation methods such as\nbest-of-N, overlooking incremental decoding methods like beam search, and has\nlargely ignored latency, focusing only on token usage. We formulate\ninference-time scaling as a problem of dynamic compute allocation and method\nselection, where the system must decide which strategy to apply and how much\ncompute to allocate on a per-query basis. Our framework explicitly incorporates\nboth token cost and wall-clock latency, the latter being critical for user\nexperience and particularly for agentic workflows where models must issue\nmultiple queries efficiently. Experiments on reasoning benchmarks show that our\napproach consistently outperforms static strategies, achieving favorable\naccuracy-cost trade-offs while remaining practical for deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u52a8\u6001\u8ba1\u7b97\u5206\u914d\u6846\u67b6\uff0c\u5728\u63a8\u7406\u65f6\u6839\u636e\u67e5\u8be2\u9700\u6c42\u9009\u62e9\u6700\u4f73\u751f\u6210\u7b56\u7565\uff08\u5982beam search\u6216best-of-N\uff09\uff0c\u540c\u65f6\u8003\u8651token\u6210\u672c\u548c\u5ef6\u8fdf\u65f6\u95f4\uff0c\u4ee5\u4f18\u5316LLM\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u65f6\u6269\u5c55\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5e76\u884c\u751f\u6210\u548ctoken\u4f7f\u7528\uff0c\u5ffd\u7565\u4e86\u589e\u91cf\u89e3\u7801\u65b9\u6cd5\u548c\u5ef6\u8fdf\u65f6\u95f4\u7684\u91cd\u8981\u6027\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u9ad8\u6548\u591a\u67e5\u8be2\u7684\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u4e2d\u3002", "method": "\u6784\u5efa\u52a8\u6001\u8ba1\u7b97\u5206\u914d\u6846\u67b6\uff0c\u5728\u6bcf\u67e5\u8be2\u57fa\u7840\u4e0a\u51b3\u5b9a\u5e94\u7528\u54ea\u79cd\u7b56\u7565\uff08beam search\u6216best-of-N\uff09\u4ee5\u53ca\u5206\u914d\u591a\u5c11\u8ba1\u7b97\u8d44\u6e90\uff0c\u540c\u65f6\u663e\u5f0f\u8003\u8651token\u6210\u672c\u548c\u65f6\u949f\u5ef6\u8fdf\u3002", "result": "\u5728\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u9759\u6001\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u826f\u597d\u7684\u51c6\u786e\u7387-\u6210\u672c\u6743\u8861\uff0c\u4e14\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u53ef\u884c\u6027\u3002", "conclusion": "\u52a8\u6001\u8ba1\u7b97\u5206\u914d\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u4f18\u5316LLM\u63a8\u7406\u6027\u80fd\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u9ad8\u6548\u591a\u67e5\u8be2\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2509.10397", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.10397", "abs": "https://arxiv.org/abs/2509.10397", "authors": ["Fei Liu", "Xinyu Lin", "Hanchao Yu", "Mingyuan Wu", "Jianyu Wang", "Qiang Zhang", "Zhuokai Zhao", "Yinglong Xia", "Yao Zhang", "Weiwei Li", "Mingze Gao", "Qifan Wang", "Lizhu Zhang", "Benyu Zhang", "Xiangjun Fan"], "title": "RecoWorld: Building Simulated Environments for Agentic Recommender Systems", "comment": null, "summary": "We present RecoWorld, a blueprint for building simulated environments\ntailored to agentic recommender systems. Such environments give agents a proper\ntraining space where they can learn from errors without impacting real users.\nRecoWorld distinguishes itself with a dual-view architecture: a simulated user\nand an agentic recommender engage in multi-turn interactions aimed at\nmaximizing user retention. The user simulator reviews recommended items,\nupdates its mindset, and when sensing potential user disengagement, generates\nreflective instructions. The agentic recommender adapts its recommendations by\nincorporating these user instructions and reasoning traces, creating a dynamic\nfeedback loop that actively engages users. This process leverages the\nexceptional reasoning capabilities of modern LLMs. We explore diverse content\nrepresentations within the simulator, including text-based, multimodal, and\nsemantic ID modeling, and discuss how multi-turn RL enables the recommender to\nrefine its strategies through iterative interactions. RecoWorld also supports\nmulti-agent simulations, allowing creators to simulate the responses of\ntargeted user populations. It marks an important first step toward recommender\nsystems where users and agents collaboratively shape personalized information\nstreams. We envision new interaction paradigms where \"user instructs,\nrecommender responds,\" jointly optimizing user retention and engagement.", "AI": {"tldr": "RecoWorld\u662f\u4e00\u4e2a\u4e3a\u667a\u80fd\u63a8\u8350\u7cfb\u7edf\u8bbe\u8ba1\u7684\u6a21\u62df\u73af\u5883\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u89c6\u56fe\u67b6\u6784\u8ba9\u7528\u6237\u6a21\u62df\u5668\u548c\u63a8\u8350\u4ee3\u7406\u8fdb\u884c\u591a\u8f6e\u4ea4\u4e92\uff0c\u5229\u7528LLM\u7684\u63a8\u7406\u80fd\u529b\u6765\u6700\u5927\u5316\u7528\u6237\u7559\u5b58\u7387\u3002", "motivation": "\u4e3a\u667a\u80fd\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u5b89\u5168\u7684\u8bad\u7ec3\u73af\u5883\uff0c\u8ba9\u4ee3\u7406\u80fd\u591f\u5728\u4e0d\u5f71\u54cd\u771f\u5b9e\u7528\u6237\u7684\u60c5\u51b5\u4e0b\u4ece\u9519\u8bef\u4e2d\u5b66\u4e60\uff0c\u89e3\u51b3\u4f20\u7edf\u63a8\u8350\u7cfb\u7edf\u7f3a\u4e4f\u7528\u6237\u53cd\u9988\u5faa\u73af\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53cc\u89c6\u56fe\u67b6\u6784\uff1a\u7528\u6237\u6a21\u62df\u5668\u5ba1\u67e5\u63a8\u8350\u9879\u76ee\u5e76\u66f4\u65b0\u5fc3\u6001\uff0c\u5f53\u68c0\u6d4b\u5230\u7528\u6237\u53ef\u80fd\u6d41\u5931\u65f6\u751f\u6210\u53cd\u601d\u6307\u4ee4\uff1b\u63a8\u8350\u4ee3\u7406\u901a\u8fc7\u6574\u5408\u7528\u6237\u6307\u4ee4\u548c\u63a8\u7406\u75d5\u8ff9\u6765\u8c03\u6574\u63a8\u8350\u7b56\u7565\uff0c\u5f62\u6210\u52a8\u6001\u53cd\u9988\u5faa\u73af\u3002\u652f\u6301\u591a\u79cd\u5185\u5bb9\u8868\u793a\u5f62\u5f0f\uff08\u6587\u672c\u3001\u591a\u6a21\u6001\u3001\u8bed\u4e49ID\uff09\u548c\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u652f\u6301\u591a\u4ee3\u7406\u6a21\u62df\u7684\u73af\u5883\u6846\u67b6\uff0c\u80fd\u591f\u6a21\u62df\u76ee\u6807\u7528\u6237\u7fa4\u4f53\u7684\u54cd\u5e94\uff0c\u4e3a\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8bad\u7ec3\u548c\u6d4b\u8bd5\u5e73\u53f0\u3002", "conclusion": "RecoWorld\u662f\u5411\u7528\u6237\u4e0e\u4ee3\u7406\u534f\u540c\u5851\u9020\u4e2a\u6027\u5316\u4fe1\u606f\u6d41\u7684\u91cd\u8981\u7b2c\u4e00\u6b65\uff0c\u5f00\u542f\u4e86\"\u7528\u6237\u6307\u5bfc\u3001\u63a8\u8350\u5668\u54cd\u5e94\"\u7684\u65b0\u4ea4\u4e92\u8303\u5f0f\uff0c\u5171\u540c\u4f18\u5316\u7528\u6237\u7559\u5b58\u548c\u53c2\u4e0e\u5ea6\u3002"}}
{"id": "2509.09712", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09712", "abs": "https://arxiv.org/abs/2509.09712", "authors": ["Talha Tahir"], "title": "The Thinking Therapist: Training Large Language Models to Deliver Acceptance and Commitment Therapy using Supervised Fine-Tuning and Odds Ratio Policy Optimization", "comment": null, "summary": "Acceptance and Commitment Therapy (ACT) is a third-wave cognitive behavioral\ntherapy with emerging evidence of efficacy in several psychiatric conditions.\nThis study investigates the impact of post-training methodology and explicit\nreasoning on the ability of a small open-weight large language model (LLM) to\ndeliver ACT. Using 50 sets of synthetic ACT transcripts generated by\nMistral-Large, we trained Llama-3.2-3b-Instruct with two distinct approaches,\nsupervised fine-tuning (SFT) and odds ratio policy optimization (ORPO), each\nwith and without an explicit chain-of-thought (COT) reasoning step. Performance\nwas evaluated by comparing these four post-trained variants against the base\nInstruct model. These models were benchmarked in simulated therapy sessions,\nwith performance quantitatively assessed on the ACT Fidelity Measure (ACT-FM)\nand the Therapist Empathy Scale (TES) by an LLM judge that had been fine-tuned\non human evaluations. Our findings demonstrate that the ORPO-trained models\nsignificantly outperformed both their SFT and Instruct counterparts on ACT\nfidelity ($\\chi^2(5) = 185.15, p < .001$) and therapeutic empathy ($\\chi^2(5) =\n140.37, p < .001$). The effect of COT was conditional as it provided a\nsignificant benefit to SFT models, improving ACT-FM scores by an average of\n2.68 points ($p < .001$), while offering no discernible advantage to the\nsuperior ORPO or instruct-tuned variants. We posit that the superiority of ORPO\nstems from its ability to learn the therapeutic `process' over imitating\n`content,' a key aspect of ACT, while COT acts as a necessary scaffold for\nmodels trained only via imitation. This study establishes that\npreference-aligned policy optimization can effectively instill ACT competencies\nin small LLMs, and that the utility of explicit reasoning is highly dependent\non the underlying training paradigm.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86SFT\u548cORPO\u4e24\u79cd\u8bad\u7ec3\u65b9\u6cd5\u5bf9\u5c0f\u578bLLM\u8fdb\u884cACT\u6cbb\u7597\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0ORPO\u8bad\u7ec3\u6a21\u578b\u5728\u6cbb\u7597\u4fdd\u771f\u5ea6\u548c\u5171\u60c5\u65b9\u9762\u663e\u8457\u4f18\u4e8eSFT\u548c\u57fa\u7840\u6a21\u578b\uff0c\u800cCOT\u63a8\u7406\u4ec5\u5bf9SFT\u6a21\u578b\u6709\u663e\u8457\u5e2e\u52a9\u3002", "motivation": "\u7814\u7a76\u7b2c\u4e09\u4ee3\u8ba4\u77e5\u884c\u4e3a\u7597\u6cd5ACT\u5728\u5c0f\u578b\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u6548\u679c\uff0c\u63a2\u7d22\u4e0d\u540c\u8bad\u7ec3\u65b9\u6cd5\u548c\u663e\u5f0f\u63a8\u7406\u5bf9\u6a21\u578b\u6cbb\u7597\u80fd\u529b\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528Mistral-Large\u751f\u6210\u768450\u7ec4\u5408\u6210ACT\u8f6c\u5f55\u672c\uff0c\u5bf9Llama-3.2-3b-Instruct\u8fdb\u884c\u4e24\u79cd\u8bad\u7ec3\u65b9\u6cd5\uff08SFT\u548cORPO\uff09\uff0c\u6bcf\u79cd\u65b9\u6cd5\u90fd\u5305\u542b\u6709\u65e0COT\u63a8\u7406\u6b65\u9aa4\u7684\u53d8\u4f53\uff0c\u901a\u8fc7\u6a21\u62df\u6cbb\u7597\u4f1a\u8bdd\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "ORPO\u8bad\u7ec3\u6a21\u578b\u5728ACT\u4fdd\u771f\u5ea6(\u03c7\u00b2=185.15, p<.001)\u548c\u6cbb\u7597\u5171\u60c5(\u03c7\u00b2=140.37, p<.001)\u65b9\u9762\u663e\u8457\u4f18\u4e8eSFT\u548c\u57fa\u7840\u6a21\u578b\uff1bCOT\u4ec5\u5bf9SFT\u6a21\u578b\u6709\u663e\u8457\u6539\u5584(\u5e73\u5747\u63d0\u9ad82.68\u5206\uff0cp<.001)\u3002", "conclusion": "\u504f\u597d\u5bf9\u9f50\u7b56\u7565\u4f18\u5316\u80fd\u6709\u6548\u57f9\u517b\u5c0f\u578bLLM\u7684ACT\u80fd\u529b\uff0c\u663e\u5f0f\u63a8\u7406\u7684\u6548\u7528\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u5e95\u5c42\u8bad\u7ec3\u8303\u5f0f\uff0cORPO\u901a\u8fc7\u5b66\u4e60\u6cbb\u7597\u8fc7\u7a0b\u800c\u975e\u6a21\u4eff\u5185\u5bb9\u83b7\u5f97\u4f18\u52bf\u3002"}}
{"id": "2509.09899", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09899", "abs": "https://arxiv.org/abs/2509.09899", "authors": ["Christopher Eldred", "Fran\u00e7ois Gay-Balmaz", "Vakhtang Putkaradze"], "title": "Variational Neural Networks for Observable Thermodynamics (V-NOTS)", "comment": "26 pages, 6 figures", "summary": "Much attention has recently been devoted to data-based computing of evolution\nof physical systems. In such approaches, information about data points from\npast trajectories in phase space is used to reconstruct the equations of motion\nand to predict future solutions that have not been observed before. However, in\nmany cases, the available data does not correspond to the variables that define\nthe system's phase space. We focus our attention on the important example of\ndissipative dynamical systems. In that case, the phase space consists of\ncoordinates, momenta and entropies; however, the momenta and entropies cannot,\nin general, be observed directly. To address this difficulty, we develop an\nefficient data-based computing framework based exclusively on observable\nvariables, by constructing a novel approach based on the \\emph{thermodynamic\nLagrangian}, and constructing neural networks that respect the thermodynamics\nand guarantees the non-decreasing entropy evolution. We show that our network\ncan provide an efficient description of phase space evolution based on a\nlimited number of data points and a relatively small number of parameters in\nthe system.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u89c2\u6d4b\u53d8\u91cf\u7684\u6570\u636e\u9a71\u52a8\u8ba1\u7b97\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u8017\u6563\u52a8\u529b\u7cfb\u7edf\u7684\u76f8\u7a7a\u95f4\u6f14\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u70ed\u529b\u5b66\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\u548c\u795e\u7ecf\u7f51\u7edc\u6765\u4fdd\u8bc1\u70ed\u529b\u5b66\u7ea6\u675f\u548c\u71b5\u589e\u7279\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u53ef\u4ee5\u76f4\u63a5\u89c2\u6d4b\u5230\u76f8\u7a7a\u95f4\u53d8\u91cf\uff0c\u4f46\u5728\u8017\u6563\u52a8\u529b\u7cfb\u7edf\u4e2d\uff0c\u52a8\u91cf\u548c\u71b5\u7b49\u5173\u952e\u53d8\u91cf\u5f80\u5f80\u65e0\u6cd5\u76f4\u63a5\u89c2\u6d4b\uff0c\u8fd9\u9650\u5236\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u9002\u7528\u6027\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u70ed\u529b\u5b66\u62c9\u683c\u6717\u65e5\u7684\u65b0\u578b\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u795e\u7ecf\u7f51\u7edc\u6765\u4fdd\u8bc1\u70ed\u529b\u5b66\u7ea6\u675f\u548c\u71b5\u7684\u975e\u9012\u51cf\u6f14\u5316\uff0c\u4ec5\u4f7f\u7528\u53ef\u89c2\u6d4b\u53d8\u91cf\u8fdb\u884c\u76f8\u7a7a\u95f4\u6f14\u5316\u63cf\u8ff0\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u57fa\u4e8e\u6709\u9650\u7684\u6570\u636e\u70b9\u548c\u76f8\u5bf9\u8f83\u5c11\u7684\u7cfb\u7edf\u53c2\u6570\uff0c\u6709\u6548\u63cf\u8ff0\u76f8\u7a7a\u95f4\u6f14\u5316\u8fc7\u7a0b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u5904\u7406\u65e0\u6cd5\u76f4\u63a5\u89c2\u6d4b\u76f8\u7a7a\u95f4\u53d8\u91cf\u7684\u8017\u6563\u52a8\u529b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8ba1\u7b97\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u8f83\u597d\u7684\u5b9e\u7528\u6027\u548c\u63a8\u5e7f\u4ef7\u503c\u3002"}}
{"id": "2509.10448", "categories": ["cs.IR", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.10448", "abs": "https://arxiv.org/abs/2509.10448", "authors": ["Kausik Hira", "Mohd Zaki", "Mausam", "N. M. Anoop Krishnan"], "title": "MatSKRAFT: A framework for large-scale materials knowledge extraction from scientific tables", "comment": null, "summary": "Scientific progress increasingly depends on synthesizing knowledge across\nvast literature, yet most experimental data remains trapped in semi-structured\nformats that resist systematic extraction and analysis. Here, we present\nMatSKRAFT, a computational framework that automatically extracts and integrates\nmaterials science knowledge from tabular data at unprecedented scale. Our\napproach transforms tables into graph-based representations processed by\nconstraint-driven GNNs that encode scientific principles directly into model\narchitecture. MatSKRAFT significantly outperforms state-of-the-art large\nlanguage models, achieving F1 scores of 88.68 for property extraction and 71.35\nfor composition extraction, while processing data $19$-$496\\times$ faster than\nthem (compared to the slowest and the fastest models, respectively) with modest\nhardware requirements. Applied to nearly 69,000 tables from more than 47,000\nresearch publications, we construct a comprehensive database containing over\n535,000 entries, including 104,000 compositions that expand coverage beyond\nmajor existing databases, pending manual validation. This systematic approach\nreveals previously overlooked materials with distinct property combinations and\nenables data-driven discovery of composition-property relationships forming the\ncornerstone of materials and scientific discovery.", "AI": {"tldr": "MatSKRAFT\u662f\u4e00\u4e2a\u4ece\u6750\u6599\u79d1\u5b66\u8868\u683c\u6570\u636e\u4e2d\u81ea\u52a8\u63d0\u53d6\u548c\u6574\u5408\u77e5\u8bc6\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u8868\u683c\u6570\u636e\uff0c\u5728\u6027\u80fd\u548c\u901f\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b", "motivation": "\u79d1\u5b66\u8fdb\u6b65\u8d8a\u6765\u8d8a\u4f9d\u8d56\u4e8e\u6574\u5408\u6d77\u91cf\u6587\u732e\u77e5\u8bc6\uff0c\u4f46\u5927\u591a\u6570\u5b9e\u9a8c\u6570\u636e\u4ecd\u88ab\u56f0\u5728\u534a\u7ed3\u6784\u5316\u683c\u5f0f\u4e2d\uff0c\u96be\u4ee5\u8fdb\u884c\u7cfb\u7edf\u63d0\u53d6\u548c\u5206\u6790", "method": "\u5c06\u8868\u683c\u8f6c\u6362\u4e3a\u57fa\u4e8e\u56fe\u7684\u8868\u793a\uff0c\u4f7f\u7528\u7ea6\u675f\u9a71\u52a8\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u5c06\u79d1\u5b66\u539f\u7406\u76f4\u63a5\u7f16\u7801\u5230\u6a21\u578b\u67b6\u6784\u4e2d", "result": "\u5728\u6027\u80fd\u63d0\u53d6\u4e0a\u8fbe\u523088.68 F1\u5206\u6570\uff0c\u5728\u6210\u5206\u63d0\u53d6\u4e0a\u8fbe\u523071.35 F1\u5206\u6570\uff0c\u5904\u7406\u901f\u5ea6\u6bd4\u73b0\u6709\u6a21\u578b\u5feb19-496\u500d\uff0c\u6784\u5efa\u4e86\u5305\u542b\u8d85\u8fc7535,000\u6761\u76ee\u7684\u7efc\u5408\u6570\u636e\u5e93", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u63ed\u793a\u5148\u524d\u88ab\u5ffd\u89c6\u7684\u6750\u6599\u7279\u6027\u7ec4\u5408\uff0c\u652f\u6301\u6570\u636e\u9a71\u52a8\u7684\u6210\u5206-\u7279\u6027\u5173\u7cfb\u53d1\u73b0\uff0c\u4e3a\u6750\u6599\u79d1\u5b66\u53d1\u73b0\u5960\u5b9a\u57fa\u7840"}}
{"id": "2509.09713", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09713", "abs": "https://arxiv.org/abs/2509.09713", "authors": ["Duolin Sun", "Dan Yang", "Yue Shen", "Yihan Jiao", "Zhehao Tan", "Jie Feng", "Lianzhen Zhong", "Jian Wang", "Peng Wei", "Jinjie Gu"], "title": "HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented Generation for Multi-hop Question Answering", "comment": null, "summary": "The Retrieval-Augmented Generation (RAG) approach enhances question-answering\nsystems and dialogue generation tasks by integrating information retrieval (IR)\ntechnologies with large language models (LLMs). This strategy, which retrieves\ninformation from external knowledge bases to bolster the response capabilities\nof generative models, has achieved certain successes. However, current RAG\nmethods still face numerous challenges when dealing with multi-hop queries. For\ninstance, some approaches overly rely on iterative retrieval, wasting too many\nretrieval steps on compound queries. Additionally, using the original complex\nquery for retrieval may fail to capture content relevant to specific\nsub-queries, resulting in noisy retrieved content. If the noise is not managed,\nit can lead to the problem of noise accumulation. To address these issues, we\nintroduce HANRAG, a novel heuristic-based framework designed to efficiently\ntackle problems of varying complexity. Driven by a powerful revelator, HANRAG\nroutes queries, decomposes them into sub-queries, and filters noise from\nretrieved documents. This enhances the system's adaptability and noise\nresistance, making it highly capable of handling diverse queries. We compare\nthe proposed framework against other leading industry methods across various\nbenchmarks. The results demonstrate that our framework obtains superior\nperformance in both single-hop and multi-hop question-answering tasks.", "AI": {"tldr": "HANRAG\u662f\u4e00\u4e2a\u57fa\u4e8e\u542f\u53d1\u5f0f\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u67e5\u8be2\u8def\u7531\u3001\u5b50\u67e5\u8be2\u5206\u89e3\u548c\u566a\u58f0\u8fc7\u6ee4\u6765\u89e3\u51b3\u591a\u8df3\u67e5\u8be2\u4e2d\u7684\u8fed\u4ee3\u68c0\u7d22\u6d6a\u8d39\u548c\u566a\u58f0\u79ef\u7d2f\u95ee\u9898\uff0c\u5728\u5355\u8df3\u548c\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02", "motivation": "\u5f53\u524dRAG\u65b9\u6cd5\u5728\u5904\u7406\u591a\u8df3\u67e5\u8be2\u65f6\u9762\u4e34\u6311\u6218\uff1a\u8fc7\u5ea6\u4f9d\u8d56\u8fed\u4ee3\u68c0\u7d22\u6d6a\u8d39\u6b65\u9aa4\uff0c\u539f\u59cb\u590d\u6742\u67e5\u8be2\u68c0\u7d22\u53ef\u80fd\u65e0\u6cd5\u6355\u83b7\u76f8\u5173\u5b50\u67e5\u8be2\u5185\u5bb9\uff0c\u5bfc\u81f4\u566a\u58f0\u79ef\u7d2f\u95ee\u9898", "method": "\u63d0\u51faHANRAG\u6846\u67b6\uff0c\u4f7f\u7528\u5f3a\u5927\u7684\u63ed\u793a\u5668\u8fdb\u884c\u67e5\u8be2\u8def\u7531\u3001\u5c06\u67e5\u8be2\u5206\u89e3\u4e3a\u5b50\u67e5\u8be2\u3001\u5e76\u4ece\u68c0\u7d22\u6587\u6863\u4e2d\u8fc7\u6ee4\u566a\u58f0\uff0c\u589e\u5f3a\u7cfb\u7edf\u9002\u5e94\u6027\u548c\u6297\u566a\u80fd\u529b", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0e\u9886\u5148\u884c\u4e1a\u65b9\u6cd5\u6bd4\u8f83\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u6846\u67b6\u5728\u5355\u8df3\u548c\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u90fd\u83b7\u5f97\u4e86\u4f18\u8d8a\u6027\u80fd", "conclusion": "HANRAG\u6846\u67b6\u901a\u8fc7\u6709\u6548\u7684\u67e5\u8be2\u5904\u7406\u548c\u566a\u58f0\u7ba1\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86RAG\u7cfb\u7edf\u5904\u7406\u591a\u6837\u5316\u67e5\u8be2\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u591a\u8df3\u67e5\u8be2\u573a\u666f\u4e0b"}}
{"id": "2509.09926", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09926", "abs": "https://arxiv.org/abs/2509.09926", "authors": ["Jiahao Chen", "Zhiyuan Huang", "Yurou Liu", "Bing Su"], "title": "LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios", "comment": null, "summary": "Long-tailed learning has garnered increasing attention due to its wide\napplicability in real-world scenarios. Among existing approaches, Long-Tailed\nSemi-Supervised Learning (LTSSL) has emerged as an effective solution by\nincorporating a large amount of unlabeled data into the imbalanced labeled\ndataset. However, most prior LTSSL methods are designed to train models from\nscratch, which often leads to issues such as overconfidence and low-quality\npseudo-labels. To address these challenges, we extend LTSSL into the foundation\nmodel fine-tuning paradigm and propose a novel framework: LoFT (Long-tailed\nsemi-supervised learning via parameter-efficient Fine-Tuning). We demonstrate\nthat fine-tuned foundation models can generate more reliable pseudolabels,\nthereby benefiting imbalanced learning. Furthermore, we explore a more\npractical setting by investigating semi-supervised learning under open-world\nconditions, where the unlabeled data may include out-of-distribution (OOD)\nsamples. To handle this problem, we propose LoFT-OW (LoFT under Open-World\nscenarios) to improve the discriminative ability. Experimental results on\nmultiple benchmarks demonstrate that our method achieves superior performance\ncompared to previous approaches, even when utilizing only 1\\% of the unlabeled\ndata compared with previous works.", "AI": {"tldr": "\u63d0\u51fa\u4e86LoFT\u6846\u67b6\uff0c\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u57fa\u7840\u6a21\u578b\u6765\u5904\u7406\u957f\u5c3e\u534a\u76d1\u7763\u5b66\u4e60\u95ee\u9898\uff0c\u5e76\u5728\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e0b\u6269\u5c55\u4e3aLoFT-OW\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd", "motivation": "\u73b0\u6709\u7684\u957f\u5c3e\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u4ece\u96f6\u8bad\u7ec3\u6a21\u578b\uff0c\u5bb9\u6613\u4ea7\u751f\u8fc7\u5ea6\u81ea\u4fe1\u548c\u4f4e\u8d28\u91cf\u4f2a\u6807\u7b7e\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u5c06\u957f\u5c3e\u534a\u76d1\u7763\u5b66\u4e60\u6269\u5c55\u5230\u57fa\u7840\u6a21\u578b\u5fae\u8c03\u8303\u5f0f\uff0c\u63d0\u51faLoFT\u6846\u67b6\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u751f\u6210\u66f4\u53ef\u9760\u7684\u4f2a\u6807\u7b7e\uff1b\u9488\u5bf9\u5f00\u653e\u4e16\u754c\u573a\u666f\u63d0\u51faLoFT-OW\u63d0\u5347\u5224\u522b\u80fd\u529b", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u5373\u4f7f\u53ea\u4f7f\u75281%\u7684\u65e0\u6807\u7b7e\u6570\u636e\u4e5f\u80fd\u8d85\u8d8a\u5148\u524d\u65b9\u6cd5", "conclusion": "\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u5fae\u8c03\u7684\u957f\u5c3e\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u6807\u51c6\u8bbe\u7f6e\u548c\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e0b\u90fd\u5177\u6709\u4f18\u5f02\u8868\u73b0"}}
{"id": "2509.10129", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.10129", "abs": "https://arxiv.org/abs/2509.10129", "authors": ["Alessio Chen", "Simone Giovannini", "Andrea Gemelli", "Fabio Coppini", "Simone Marinai"], "title": "Towards Reliable and Interpretable Document Question Answering via VLMs", "comment": null, "summary": "Vision-Language Models (VLMs) have shown strong capabilities in document\nunderstanding, particularly in identifying and extracting textual information\nfrom complex documents. Despite this, accurately localizing answers within\ndocuments remains a major challenge, limiting both interpretability and\nreal-world applicability. To address this, we introduce\n\\textit{DocExplainerV0}, a plug-and-play bounding-box prediction module that\ndecouples answer generation from spatial localization. This design makes it\napplicable to existing VLMs, including proprietary systems where fine-tuning is\nnot feasible. Through systematic evaluation, we provide quantitative insights\ninto the gap between textual accuracy and spatial grounding, showing that\ncorrect answers often lack reliable localization. Our standardized framework\nhighlights these shortcomings and establishes a benchmark for future research\ntoward more interpretable and robust document information extraction VLMs.", "AI": {"tldr": "DocExplainerV0\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u8fb9\u754c\u6846\u9884\u6d4b\u6a21\u5757\uff0c\u5c06\u7b54\u6848\u751f\u6210\u4e0e\u7a7a\u95f4\u5b9a\u4f4d\u89e3\u8026\uff0c\u7528\u4e8e\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6587\u6863\u4e2d\u7684\u7b54\u6848\u5b9a\u4f4d\u80fd\u529b", "motivation": "\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6587\u6863\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u51c6\u786e\u5728\u6587\u6863\u4e2d\u5b9a\u4f4d\u7b54\u6848\u4ecd\u7136\u662f\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u5b9e\u9645\u5e94\u7528", "method": "\u5f15\u5165DocExplainerV0\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u4e0e\u73b0\u6709VLM\u89e3\u8026\uff0c\u53ef\u4ee5\u5e94\u7528\u4e8e\u5305\u62ec\u4e13\u6709\u7cfb\u7edf\u5728\u5185\u7684\u5404\u79cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u7a7a\u95f4\u5b9a\u4f4d", "result": "\u7cfb\u7edf\u8bc4\u4f30\u63ed\u793a\u4e86\u6587\u672c\u51c6\u786e\u6027\u4e0e\u7a7a\u95f4\u5b9a\u4f4d\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u663e\u793a\u6b63\u786e\u7b54\u6848\u5f80\u5f80\u7f3a\u4e4f\u53ef\u9760\u7684\u7a7a\u95f4\u5b9a\u4f4d\uff0c\u5efa\u7acb\u4e86\u672a\u6765\u7814\u7a76\u7684\u57fa\u51c6", "conclusion": "\u8be5\u6846\u67b6\u7a81\u51fa\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5f00\u53d1\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u7684\u6587\u6863\u4fe1\u606f\u63d0\u53d6\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840"}}
{"id": "2509.09714", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09714", "abs": "https://arxiv.org/abs/2509.09714", "authors": ["Serge Lionel Nikiema", "Alb\u00e9rick Euraste Djire", "Abdoul Aziz Bonkoungou", "Micheline B\u00e9n\u00e9dicte Moumoula", "Jordan Samhi", "Abdoul Kader Kabore", "Jacques Klein", "Tegawend\u00e9 F. Bissyande"], "title": "How Small Transformation Expose the Weakness of Semantic Similarity Measures", "comment": null, "summary": "This research examines how well different methods measure semantic\nsimilarity, which is important for various software engineering applications\nsuch as code search, API recommendations, automated code reviews, and\nrefactoring tools. While large language models are increasingly used for these\nsimilarity assessments, questions remain about whether they truly understand\nsemantic relationships or merely recognize surface patterns.\n  The study tested 18 different similarity measurement approaches, including\nword-based methods, embedding techniques, LLM-based systems, and\nstructure-aware algorithms. The researchers created a systematic testing\nframework that applies controlled changes to text and code to evaluate how well\neach method handles different types of semantic relationships.\n  The results revealed significant issues with commonly used metrics. Some\nembedding-based methods incorrectly identified semantic opposites as similar up\nto 99.9 percent of the time, while certain transformer-based approaches\noccasionally rated opposite meanings as more similar than synonymous ones. The\nstudy found that embedding methods' poor performance often stemmed from how\nthey calculate distances; switching from Euclidean distance to cosine\nsimilarity improved results by 24 to 66 percent. LLM-based approaches performed\nbetter at distinguishing semantic differences, producing low similarity scores\n(0.00 to 0.29) for genuinely different meanings, compared to embedding methods\nthat incorrectly assigned high scores (0.82 to 0.99) to dissimilar content.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e8618\u79cd\u8bed\u4e49\u76f8\u4f3c\u5ea6\u6d4b\u91cf\u65b9\u6cd5\uff0c\u53d1\u73b0\u5e38\u7528\u6307\u6807\u5b58\u5728\u4e25\u91cd\u95ee\u9898\uff0c\u67d0\u4e9b\u65b9\u6cd5\u9519\u8bef\u5730\u5c06\u8bed\u4e49\u76f8\u53cd\u5185\u5bb9\u8bc6\u522b\u4e3a\u9ad8\u5ea6\u76f8\u4f3c\uff0c\u800cLLM\u65b9\u6cd5\u5728\u533a\u5206\u8bed\u4e49\u5dee\u5f02\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u641c\u7d22\u3001API\u63a8\u8350\u7b49\u8f6f\u4ef6\u5de5\u7a0b\u5e94\u7528\u4e2d\u5e7f\u6cdb\u7528\u4e8e\u8bed\u4e49\u76f8\u4f3c\u5ea6\u8bc4\u4f30\uff0c\u9700\u8981\u9a8c\u8bc1\u8fd9\u4e9b\u65b9\u6cd5\u662f\u5426\u771f\u6b63\u7406\u89e3\u8bed\u4e49\u5173\u7cfb\u8fd8\u662f\u4ec5\u8bc6\u522b\u8868\u9762\u6a21\u5f0f\u3002", "method": "\u7814\u7a76\u6d4b\u8bd5\u4e8618\u79cd\u4e0d\u540c\u65b9\u6cd5\uff08\u57fa\u4e8e\u8bcd\u3001\u5d4c\u5165\u6280\u672f\u3001LLM\u7cfb\u7edf\u548c\u7ed3\u6784\u611f\u77e5\u7b97\u6cd5\uff09\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u6d4b\u8bd5\u6846\u67b6\u5bf9\u6587\u672c\u548c\u4ee3\u7801\u5e94\u7528\u53d7\u63a7\u53d8\u5316\u6765\u8bc4\u4f30\u5404\u65b9\u6cd5\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u8bed\u4e49\u5173\u7cfb\u7684\u80fd\u529b\u3002", "result": "\u7ed3\u679c\u663e\u793a\u5e38\u7528\u6307\u6807\u5b58\u5728\u663e\u8457\u95ee\u9898\uff1a\u67d0\u4e9b\u5d4c\u5165\u65b9\u6cd5\u9519\u8bef\u8bc6\u522b\u8bed\u4e49\u76f8\u53cd\u5185\u5bb9\u4e3a\u76f8\u4f3c\uff08\u9ad8\u8fbe99.9%\uff09\uff0c\u67d0\u4e9b\u57fa\u4e8etransformer\u7684\u65b9\u6cd5\u5076\u5c14\u5c06\u76f8\u53cd\u542b\u4e49\u8bc4\u4e3a\u6bd4\u540c\u4e49\u8bcd\u66f4\u76f8\u4f3c\u3002\u5d4c\u5165\u65b9\u6cd5\u6027\u80fd\u5dee\u5e38\u6e90\u4e8e\u8ddd\u79bb\u8ba1\u7b97\u65b9\u5f0f\uff0c\u4ece\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u5207\u6362\u5230\u4f59\u5f26\u76f8\u4f3c\u6027\u4f7f\u7ed3\u679c\u63d0\u534724-66%\u3002LLM\u65b9\u6cd5\u5728\u533a\u5206\u8bed\u4e49\u5dee\u5f02\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u5bf9\u771f\u6b63\u4e0d\u540c\u542b\u4e49\u7ed9\u51fa\u4f4e\u76f8\u4f3c\u5ea6\u5206\u6570\uff080.00-0.29\uff09\uff0c\u800c\u5d4c\u5165\u65b9\u6cd5\u9519\u8bef\u5730\u4e3a\u4e0d\u76f8\u4f3c\u5185\u5bb9\u5206\u914d\u9ad8\u5206\uff080.82-0.99\uff09\u3002", "conclusion": "\u5f53\u524d\u5e38\u7528\u7684\u8bed\u4e49\u76f8\u4f3c\u5ea6\u6d4b\u91cf\u65b9\u6cd5\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0cLLM\u65b9\u6cd5\u5728\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u5d4c\u5165\u65b9\u6cd5\uff0c\u4f46\u6240\u6709\u65b9\u6cd5\u90fd\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u51c6\u786e\u6355\u6349\u8bed\u4e49\u5173\u7cfb\u3002"}}
{"id": "2509.09933", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09933", "abs": "https://arxiv.org/abs/2509.09933", "authors": ["Shintaro Nakamura", "Yuko Kuroki", "Wei Chen"], "title": "Multi-Play Combinatorial Semi-Bandit Problem", "comment": null, "summary": "In the combinatorial semi-bandit (CSB) problem, a player selects an action\nfrom a combinatorial action set and observes feedback from the base arms\nincluded in the action. While CSB is widely applicable to combinatorial\noptimization problems, its restriction to binary decision spaces excludes\nimportant cases involving non-negative integer flows or allocations, such as\nthe optimal transport and knapsack problems.To overcome this limitation, we\npropose the multi-play combinatorial semi-bandit (MP-CSB), where a player can\nselect a non-negative integer action and observe multiple feedbacks from a\nsingle arm in each round. We propose two algorithms for the MP-CSB. One is a\nThompson-sampling-based algorithm that is computationally feasible even when\nthe action space is exponentially large with respect to the number of arms, and\nattains $O(\\log T)$ distribution-dependent regret in the stochastic regime,\nwhere $T$ is the time horizon. The other is a best-of-both-worlds algorithm,\nwhich achieves $O(\\log T)$ variance-dependent regret in the stochastic regime\nand the worst-case $\\tilde{\\mathcal{O}}\\left( \\sqrt{T} \\right)$ regret in the\nadversarial regime. Moreover, its regret in adversarial one is data-dependent,\nadapting to the cumulative loss of the optimal action, the total quadratic\nvariation, and the path-length of the loss sequence. Finally, we numerically\nshow that the proposed algorithms outperform existing methods in the CSB\nliterature.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u591a\u64ad\u653e\u7ec4\u5408\u534a\u8d4c\u535a\u673a(MP-CSB)\u6a21\u578b\uff0c\u6269\u5c55\u4e86\u4f20\u7edf\u7ec4\u5408\u534a\u8d4c\u535a\u673a\u95ee\u9898\uff0c\u652f\u6301\u975e\u8d1f\u6574\u6570\u52a8\u4f5c\u7a7a\u95f4\uff0c\u89e3\u51b3\u4e86\u6700\u4f18\u8fd0\u8f93\u548c\u80cc\u5305\u7b49\u95ee\u9898\u7684\u5c40\u9650\u6027\u3002\u63d0\u51fa\u4e86\u4e24\u79cd\u7b97\u6cd5\uff1a\u57fa\u4e8eThompson\u91c7\u6837\u7684\u7b97\u6cd5\u548c\u6700\u4f73\u4e24\u7528\u7b97\u6cd5\uff0c\u5206\u522b\u5728\u968f\u673a\u548c\u5bf9\u6297\u73af\u5883\u4e0b\u5b9e\u73b0\u5bf9\u6570\u7ea7\u548c\u6b21\u7ebf\u6027\u9057\u61be\u3002", "motivation": "\u4f20\u7edf\u7ec4\u5408\u534a\u8d4c\u535a\u673a(CSB)\u95ee\u9898\u4ec5\u9650\u4e8e\u4e8c\u5143\u51b3\u7b56\u7a7a\u95f4\uff0c\u65e0\u6cd5\u5904\u7406\u6d89\u53ca\u975e\u8d1f\u6574\u6570\u6d41\u6216\u5206\u914d\u7684\u91cd\u8981\u5e94\u7528\u573a\u666f\uff0c\u5982\u6700\u4f18\u8fd0\u8f93\u548c\u80cc\u5305\u95ee\u9898\u3002\u9700\u8981\u6269\u5c55\u6a21\u578b\u4ee5\u652f\u6301\u66f4\u5e7f\u6cdb\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86MP-CSB\u6a21\u578b\uff0c\u5141\u8bb8\u9009\u62e9\u975e\u8d1f\u6574\u6570\u52a8\u4f5c\u5e76\u89c2\u5bdf\u5355\u4e2a\u81c2\u7684\u591a\u4e2a\u53cd\u9988\u3002\u8bbe\u8ba1\u4e86\u4e24\u79cd\u7b97\u6cd5\uff1a1)\u57fa\u4e8eThompson\u91c7\u6837\u7684\u7b97\u6cd5\uff0c\u8ba1\u7b97\u53ef\u884c\u4e14\u5b9e\u73b0O(log T)\u9057\u61be\uff1b2)\u6700\u4f73\u4e24\u7528\u7b97\u6cd5\uff0c\u5728\u968f\u673a\u73af\u5883\u4e0b\u5b9e\u73b0\u65b9\u5dee\u4f9d\u8d56\u9057\u61be\uff0c\u5728\u5bf9\u6297\u73af\u5883\u4e0b\u5b9e\u73b0\u6570\u636e\u4f9d\u8d56\u7684\u6b21\u7ebf\u6027\u9057\u61be\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0cThompson\u91c7\u6837\u7b97\u6cd5\u5728\u968f\u673a\u73af\u5883\u4e0b\u8fbe\u5230O(log T)\u5206\u5e03\u4f9d\u8d56\u9057\u61be\uff0c\u6700\u4f73\u4e24\u7528\u7b97\u6cd5\u5728\u968f\u673a\u73af\u5883\u4e0b\u5b9e\u73b0O(log T)\u65b9\u5dee\u4f9d\u8d56\u9057\u61be\uff0c\u5728\u5bf9\u6297\u73af\u5883\u4e0b\u5b9e\u73b0\u00d5(\u221aT)\u6700\u574f\u60c5\u51b5\u9057\u61be\uff0c\u4e14\u9057\u61be\u5177\u6709\u6570\u636e\u4f9d\u8d56\u6027\u3002\u6570\u503c\u5b9e\u9a8c\u663e\u793a\u63d0\u51fa\u7684\u7b97\u6cd5\u4f18\u4e8e\u73b0\u6709CSB\u65b9\u6cd5\u3002", "conclusion": "MP-CSB\u6a21\u578b\u6210\u529f\u6269\u5c55\u4e86\u7ec4\u5408\u534a\u8d4c\u535a\u673a\u6846\u67b6\uff0c\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5904\u7406\u975e\u8d1f\u6574\u6570\u52a8\u4f5c\u7a7a\u95f4\u7684\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.09715", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09715", "abs": "https://arxiv.org/abs/2509.09715", "authors": ["Naveen Lamba", "Sanju Tiwari", "Manas Gaur"], "title": "Investigating Symbolic Triggers of Hallucination in Gemma Models Across HaluEval and TruthfulQA", "comment": null, "summary": "Hallucination in Large Language Models (LLMs) is a well studied problem.\nHowever, the properties that make LLM intrinsically vulnerable to\nhallucinations have not been identified and studied. This research identifies\nand characterizes the key properties, allowing us to pinpoint vulnerabilities\nwithin the model's internal mechanisms. To solidify on these properties, we\nutilized two established datasets, HaluEval and TruthfulQA and convert their\nexisting format of question answering into various other formats to narrow down\nthese properties as the reason for the hallucinations. Our findings reveal that\nhallucination percentages across symbolic properties are notably high for\nGemma-2-2B, averaging 79.0% across tasks and datasets. With increased model\nscale, hallucination drops to 73.6% for Gemma-2-9B and 63.9% for Gemma-2-27B,\nreflecting a 15 percentage point reduction overall. Although the hallucination\nrate decreases as the model size increases, a substantial amount of\nhallucination caused by symbolic properties still persists. This is especially\nevident for modifiers (ranging from 84.76% to 94.98%) and named entities\n(ranging from 83.87% to 93.96%) across all Gemma models and both datasets.\nThese findings indicate that symbolic elements continue to confuse the models,\npointing to a fundamental weakness in how these LLMs process such\ninputs--regardless of their scale.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc6\u522b\u5e76\u8868\u5f81\u4e86\u5bfc\u81f4\u5927\u8bed\u8a00\u6a21\u578b\u4ea7\u751f\u5e7b\u89c9\u7684\u5173\u952e\u7b26\u53f7\u5c5e\u6027\uff0c\u53d1\u73b0\u5373\u4f7f\u6a21\u578b\u89c4\u6a21\u589e\u5927\uff0c\u7b26\u53f7\u5143\u7d20\uff08\u5982\u4fee\u9970\u8bed\u548c\u547d\u540d\u5b9e\u4f53\uff09\u4ecd\u7136\u662f\u5bfc\u81f4\u5e7b\u89c9\u7684\u4e3b\u8981\u56e0\u7d20\u3002", "motivation": "\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5bfc\u81f4\u6a21\u578b\u5185\u5728\u6613\u4ea7\u751f\u5e7b\u89c9\u7684\u5c5e\u6027\u5c1a\u672a\u88ab\u8bc6\u522b\u548c\u7814\u7a76\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc6\u522b\u8fd9\u4e9b\u5173\u952e\u5c5e\u6027\uff0c\u5e76\u5b9a\u4f4d\u6a21\u578b\u5185\u90e8\u673a\u5236\u7684\u8106\u5f31\u70b9\u3002", "method": "\u4f7f\u7528HaluEval\u548cTruthfulQA\u4e24\u4e2a\u6570\u636e\u96c6\uff0c\u5c06\u73b0\u6709\u7684\u95ee\u7b54\u683c\u5f0f\u8f6c\u6362\u4e3a\u591a\u79cd\u5176\u4ed6\u683c\u5f0f\uff0c\u4ee5\u786e\u5b9a\u7b26\u53f7\u5c5e\u6027\u662f\u5bfc\u81f4\u5e7b\u89c9\u7684\u539f\u56e0\u3002\u6d4b\u8bd5\u4e86Gemma-2\u7cfb\u5217\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b\uff082B\u30019B\u300127B\uff09\u3002", "result": "Gemma-2-2B\u7684\u5e73\u5747\u5e7b\u89c9\u7387\u4e3a79.0%\uff0c\u968f\u7740\u6a21\u578b\u89c4\u6a21\u589e\u5927\uff0c\u5e7b\u89c9\u7387\u4e0b\u964d\u81f373.6%\uff089B\uff09\u548c63.9%\uff0827B\uff09\u3002\u4f46\u4fee\u9970\u8bed\uff0884.76%-94.98%\uff09\u548c\u547d\u540d\u5b9e\u4f53\uff0883.87%-93.96%\uff09\u5728\u6240\u6709\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u4ecd\u7136\u4fdd\u6301\u5f88\u9ad8\u7684\u5e7b\u89c9\u7387\u3002", "conclusion": "\u7b26\u53f7\u5143\u7d20\u6301\u7eed\u6df7\u6dc6\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u8fd9\u8868\u660e\u65e0\u8bba\u6a21\u578b\u89c4\u6a21\u5982\u4f55\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u5904\u7406\u6b64\u7c7b\u8f93\u5165\u65f6\u5b58\u5728\u6839\u672c\u6027\u5f31\u70b9\uff0c\u7b26\u53f7\u5c5e\u6027\u662f\u5bfc\u81f4\u5e7b\u89c9\u7684\u5185\u5728\u8106\u5f31\u6027\u56e0\u7d20\u3002"}}
{"id": "2509.09936", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2509.09936", "abs": "https://arxiv.org/abs/2509.09936", "authors": ["Saarth Gaonkar", "Xiang Zheng", "Haocheng Xi", "Rishabh Tiwari", "Kurt Keutzer", "Dmitriy Morozov", "Michael W. Mahoney", "Amir Gholami"], "title": "SciML Agents: Write the Solver, Not the Solution", "comment": null, "summary": "Recent work in scientific machine learning aims to tackle scientific tasks\ndirectly by predicting target values with neural networks (e.g.,\nphysics-informed neural networks, neural ODEs, neural operators, etc.), but\nattaining high accuracy and robustness has been challenging. We explore an\nalternative view: use LLMs to write code that leverages decades of numerical\nalgorithms. This shifts the burden from learning a solution function to making\ndomain-aware numerical choices. We ask whether LLMs can act as SciML agents\nthat, given a natural-language ODE description, generate runnable code that is\nscientifically appropriate, selecting suitable solvers (stiff vs. non-stiff),\nand enforcing stability checks. There is currently no benchmark to measure this\nkind of capability for scientific computing tasks. As such, we first introduce\ntwo new datasets: a diagnostic dataset of adversarial \"misleading\" problems;\nand a large-scale benchmark of 1,000 diverse ODE tasks. The diagnostic set\ncontains problems whose superficial appearance suggests stiffness, and that\nrequire algebraic simplification to demonstrate non-stiffness; and the\nlarge-scale benchmark spans stiff and non-stiff ODE regimes. We evaluate open-\nand closed-source LLM models along two axes: (i) unguided versus guided\nprompting with domain-specific knowledge; and (ii) off-the-shelf versus\nfine-tuned variants. Our evaluation measures both executability and numerical\nvalidity against reference solutions. We find that with sufficient context and\nguided prompts, newer instruction-following models achieve high accuracy on\nboth criteria. In many cases, recent open-source systems perform strongly\nwithout fine-tuning, while older or smaller models still benefit from\nfine-tuning. Overall, our preliminary results indicate that careful prompting\nand fine-tuning can yield a specialized LLM agent capable of reliably solving\nsimple ODE problems.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4f7f\u7528LLMs\u751f\u6210\u79d1\u5b66\u8ba1\u7b97\u4ee3\u7801\u800c\u975e\u76f4\u63a5\u9884\u6d4b\u89e3\u51fd\u6570\uff0c\u5f15\u5165\u4e24\u4e2a\u65b0\u6570\u636e\u96c6\u8bc4\u4f30LLMs\u4f5c\u4e3aSciML\u4ee3\u7406\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u9002\u5f53\u63d0\u793a\u548c\u5fae\u8c03\u53ef\u4f7fLLMs\u53ef\u9760\u89e3\u51b3\u7b80\u5355ODE\u95ee\u9898", "motivation": "\u4f20\u7edf\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u76f4\u63a5\u9884\u6d4b\u76ee\u6807\u503c\u5b58\u5728\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u6311\u6218\uff0c\u672c\u6587\u63a2\u7d22\u4f7f\u7528LLMs\u7f16\u5199\u57fa\u4e8e\u6570\u503c\u7b97\u6cd5\u7684\u4ee3\u7801\uff0c\u5c06\u5b66\u4e60\u8d1f\u62c5\u4ece\u89e3\u51fd\u6570\u8f6c\u79fb\u5230\u9886\u57df\u611f\u77e5\u7684\u6570\u503c\u9009\u62e9", "method": "\u5f15\u5165\u8bca\u65ad\u6570\u636e\u96c6\u548c\u5927\u89c4\u6a21ODE\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u5f00\u6e90\u548c\u95ed\u6e90LLM\u5728\u65e0\u5f15\u5bfcvs\u9886\u57df\u77e5\u8bc6\u5f15\u5bfc\u63d0\u793a\u3001\u73b0\u6210vs\u5fae\u8c03\u53d8\u4f53\u4e0b\u7684\u8868\u73b0\uff0c\u6d4b\u91cf\u53ef\u6267\u884c\u6027\u548c\u6570\u503c\u6709\u6548\u6027", "result": "\u5728\u5145\u5206\u4e0a\u4e0b\u6587\u548c\u5f15\u5bfc\u63d0\u793a\u4e0b\uff0c\u8f83\u65b0\u7684\u6307\u4ee4\u8ddf\u968f\u6a21\u578b\u5728\u4e24\u4e2a\u8bc4\u4f30\u6807\u51c6\u4e0a\u90fd\u8fbe\u5230\u9ad8\u7cbe\u5ea6\uff0c\u5f00\u6e90\u7cfb\u7edf\u65e0\u9700\u5fae\u8c03\u8868\u73b0\u5f3a\u52b2\uff0c\u8f83\u8001\u6216\u8f83\u5c0f\u6a21\u578b\u4ecd\u80fd\u4ece\u5fae\u8c03\u4e2d\u53d7\u76ca", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u548c\u5fae\u8c03\u53ef\u4ee5\u4ea7\u751f\u80fd\u591f\u53ef\u9760\u89e3\u51b3\u7b80\u5355ODE\u95ee\u9898\u7684\u4e13\u7528LLM\u4ee3\u7406\uff0c\u4e3a\u79d1\u5b66\u8ba1\u7b97\u4efb\u52a1\u4e2d\u7684LLM\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2"}}
{"id": "2509.09723", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ME", "I.2.6; J.4; I.5.1; H.3.3; H.2.8"], "pdf": "https://arxiv.org/pdf/2509.09723", "abs": "https://arxiv.org/abs/2509.09723", "authors": ["Kai R. Larsen", "Sen Yan", "Roland M\u00fcller", "Lan Sang", "Mikko R\u00f6nkk\u00f6", "Ravi Starzl", "Donald Edmondson"], "title": "ALIGNS: Unlocking nomological networks in psychological measurement through a large language model", "comment": null, "summary": "Psychological measurement is critical to many disciplines. Despite advances\nin measurement, building nomological networks, theoretical maps of how concepts\nand measures relate to establish validity, remains a challenge 70 years after\nCronbach and Meehl proposed them as fundamental to validation. This limitation\nhas practical consequences: clinical trials may fail to detect treatment\neffects, and public policy may target the wrong outcomes. We introduce Analysis\nof Latent Indicators to Generate Nomological Structures (ALIGNS), a large\nlanguage model-based system trained with validated questionnaire measures.\nALIGNS provides three comprehensive nomological networks containing over\n550,000 indicators across psychology, medicine, social policy, and other\nfields. This represents the first application of large language models to solve\na foundational problem in measurement validation. We report classification\naccuracy tests used to develop the model, as well as three evaluations. In the\nfirst evaluation, the widely used NIH PROMIS anxiety and depression instruments\nare shown to converge into a single dimension of emotional distress. The second\nevaluation examines child temperament measures and identifies four potential\ndimensions not captured by current frameworks, and questions one existing\ndimension. The third evaluation, an applicability check, engages expert\npsychometricians who assess the system's importance, accessibility, and\nsuitability. ALIGNS is freely available at nomologicalnetwork.org,\ncomplementing traditional validation methods with large-scale nomological\nanalysis.", "AI": {"tldr": "ALIGNS\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u751f\u6210\u5305\u542b55\u4e07+\u6307\u6807\u7684\u7efc\u5408nomological\u7f51\u7edc\uff0c\u89e3\u51b3\u5fc3\u7406\u5b66\u6d4b\u91cf\u4e2d\u6784\u5efa\u7406\u8bba\u7f51\u7edc\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u5927\u89c4\u6a21nomological\u5206\u6790\u6765\u8865\u5145\u4f20\u7edf\u9a8c\u8bc1\u65b9\u6cd5\u3002", "motivation": "\u5fc3\u7406\u6d4b\u91cf\u5bf9\u8bb8\u591a\u5b66\u79d1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6784\u5efanomological\u7f51\u7edc\uff08\u6982\u5ff5\u548c\u6d4b\u91cf\u5173\u7cfb\u7684\u7406\u8bba\u56fe\u8c31\uff09\u4ecd\u7136\u662f70\u5e74\u6765\u7684\u6311\u6218\uff0c\u8fd9\u5bfc\u81f4\u4e34\u5e8a\u8bd5\u9a8c\u53ef\u80fd\u65e0\u6cd5\u68c0\u6d4b\u6cbb\u7597\u6548\u679c\uff0c\u516c\u5171\u653f\u7b56\u53ef\u80fd\u9488\u5bf9\u9519\u8bef\u7684\u7ed3\u679c\u3002", "method": "\u5f00\u53d1ALIGNS\u7cfb\u7edf\uff0c\u4f7f\u7528\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u95ee\u5377\u6d4b\u91cf\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u751f\u6210\u4e09\u4e2a\u7efc\u5408nomological\u7f51\u7edc\uff0c\u5305\u542b\u5fc3\u7406\u5b66\u3001\u533b\u5b66\u3001\u793e\u4f1a\u653f\u7b56\u7b49\u9886\u57df\u768455\u4e07+\u6307\u6807\u3002", "result": "1) NIH PROMIS\u7126\u8651\u548c\u6291\u90c1\u5de5\u5177\u6536\u655b\u4e3a\u5355\u4e00\u60c5\u7eea\u56f0\u6270\u7ef4\u5ea6\uff1b2) \u513f\u7ae5\u6c14\u8d28\u6d4b\u91cf\u8bc6\u522b\u51fa\u56db\u4e2a\u73b0\u6709\u6846\u67b6\u672a\u6355\u6349\u7684\u6f5c\u5728\u7ef4\u5ea6\uff0c\u5e76\u8d28\u7591\u4e00\u4e2a\u73b0\u6709\u7ef4\u5ea6\uff1b3) \u5fc3\u7406\u6d4b\u91cf\u5b66\u4e13\u5bb6\u8bc4\u4f30\u663e\u793a\u7cfb\u7edf\u5177\u6709\u91cd\u8981\u6027\u3001\u53ef\u8bbf\u95ee\u6027\u548c\u9002\u7528\u6027\u3002", "conclusion": "ALIGNS\u662f\u9996\u4e2a\u5e94\u7528\u5927\u8bed\u8a00\u6a21\u578b\u89e3\u51b3\u6d4b\u91cf\u9a8c\u8bc1\u57fa\u7840\u95ee\u9898\u7684\u7cfb\u7edf\uff0c\u53ef\u514d\u8d39\u4f7f\u7528\uff0c\u4e3a\u4f20\u7edf\u9a8c\u8bc1\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21nomological\u5206\u6790\u7684\u8865\u5145\u3002"}}
{"id": "2509.09940", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09940", "abs": "https://arxiv.org/abs/2509.09940", "authors": ["Yifei Wang", "Wenbin Wang", "Yong Luo"], "title": "DyKen-Hyena: Dynamic Kernel Generation via Cross-Modal Attention for Multimodal Intent Recognition", "comment": "8 pages, 2 figures", "summary": "Though Multimodal Intent Recognition (MIR) proves effective by utilizing rich\ninformation from multiple sources (e.g., language, video, and audio), the\npotential for intent-irrelevant and conflicting information across modalities\nmay hinder performance from being further improved. Most current models attempt\nto fuse modalities by applying mechanisms like multi-head attention to unimodal\nfeature sequences and then adding the result back to the original\nrepresentation. This process risks corrupting the primary linguistic features\nwith noisy or irrelevant non-verbal signals, as it often fails to capture the\nfine-grained, token-level influence where non-verbal cues should modulate, not\njust augment, textual meaning. To address this, we introduce DyKen-Hyena, which\nreframes the problem from feature fusion to processing modulation. Our model\ntranslates audio-visual cues into dynamic, per-token convolutional kernels that\ndirectly modulate textual feature extraction. This fine-grained approach\nachieves state-of-the-art results on the MIntRec and MIntRec2.0 benchmarks.\nNotably, it yields a +10.46% F1-score improvement in out-of-scope detection,\nvalidating that our method creates a fundamentally more robust intent\nrepresentation.", "AI": {"tldr": "DyKen-Hyena\u6a21\u578b\u901a\u8fc7\u5c06\u591a\u6a21\u6001\u4fe1\u606f\u8f6c\u5316\u4e3a\u52a8\u6001\u5377\u79ef\u6838\u6765\u8c03\u5236\u6587\u672c\u7279\u5f81\u63d0\u53d6\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u878d\u5408\u7279\u5f81\uff0c\u5728\u591a\u6a21\u6001\u610f\u56fe\u8bc6\u522b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86SOTA\u6548\u679c", "motivation": "\u4f20\u7edf\u591a\u6a21\u6001\u610f\u56fe\u8bc6\u522b\u65b9\u6cd5\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u591a\u6a21\u6001\u7279\u5f81\uff0c\u4f46\u53ef\u80fd\u5f15\u5165\u610f\u56fe\u65e0\u5173\u7684\u566a\u58f0\u4fe1\u606f\uff0c\u7834\u574f\u4e3b\u8981\u8bed\u8a00\u7279\u5f81\uff0c\u4e14\u65e0\u6cd5\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684token\u7ea7\u8c03\u5236", "method": "\u63d0\u51faDyKen-Hyena\u6a21\u578b\uff0c\u5c06\u97f3\u9891-\u89c6\u89c9\u7ebf\u7d22\u8f6c\u5316\u4e3a\u52a8\u6001\u7684\u6bcftoken\u5377\u79ef\u6838\uff0c\u76f4\u63a5\u8c03\u5236\u6587\u672c\u7279\u5f81\u63d0\u53d6\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4ece\u7279\u5f81\u878d\u5408\u5230\u5904\u7406\u8c03\u5236\u7684\u8f6c\u53d8", "result": "\u5728MIntRec\u548cMIntRec2.0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5728out-of-scope\u68c0\u6d4b\u4e2d\u83b7\u5f97+10.46%\u7684F1\u5206\u6570\u63d0\u5347", "conclusion": "\u8be5\u65b9\u6cd5\u521b\u5efa\u4e86\u66f4\u9c81\u68d2\u7684\u610f\u56fe\u8868\u793a\uff0c\u9a8c\u8bc1\u4e86\u7ec6\u7c92\u5ea6\u8c03\u5236\u65b9\u6cd5\u7684\u6709\u6548\u6027"}}
{"id": "2509.09724", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T09"], "pdf": "https://arxiv.org/pdf/2509.09724", "abs": "https://arxiv.org/abs/2509.09724", "authors": ["Wonyoung Kim", "Sujeong Seo", "Juhyun Lee"], "title": "DiTTO-LLM: Framework for Discovering Topic-based Technology Opportunities via Large Language Model", "comment": "5 figures", "summary": "Technology opportunities are critical information that serve as a foundation\nfor advancements in technology, industry, and innovation. This paper proposes a\nframework based on the temporal relationships between technologies to identify\nemerging technology opportunities. The proposed framework begins by extracting\ntext from a patent dataset, followed by mapping text-based topics to discover\ninter-technology relationships. Technology opportunities are then identified by\ntracking changes in these topics over time. To enhance efficiency, the\nframework leverages a large language model to extract topics and employs a\nprompt for a chat-based language model to support the discovery of technology\nopportunities. The framework was evaluated using an artificial intelligence\npatent dataset provided by the United States Patent and Trademark Office. The\nexperimental results suggest that artificial intelligence technology is\nevolving into forms that facilitate everyday accessibility. This approach\ndemonstrates the potential of the proposed framework to identify future\ntechnology opportunities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6280\u672f\u95f4\u65f6\u95f4\u5173\u7cfb\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b\u65b0\u5174\u6280\u672f\u673a\u4f1a\u3002\u8be5\u6846\u67b6\u5229\u7528\u4e13\u5229\u6570\u636e\u3001\u6587\u672c\u6316\u6398\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u5206\u6790\u6280\u672f\u4e3b\u9898\u7684\u6f14\u53d8\uff0c\u5e76\u901a\u8fc7AI\u4e13\u5229\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u6280\u672f\u673a\u4f1a\u662f\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3001\u4ea7\u4e1a\u53d1\u5c55\u548c\u521b\u65b0\u7684\u5173\u952e\u4fe1\u606f\u3002\u4e3a\u4e86\u6709\u6548\u8bc6\u522b\u65b0\u5174\u6280\u672f\u673a\u4f1a\uff0c\u9700\u8981\u5efa\u7acb\u7cfb\u7edf\u6027\u7684\u5206\u6790\u6846\u67b6\u6765\u6355\u6349\u6280\u672f\u95f4\u7684\u65f6\u5e8f\u5173\u7cfb\u548c\u6f14\u53d8\u6a21\u5f0f\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4e13\u5229\u6570\u636e\u7684\u5206\u6790\u6846\u67b6\uff1a1\uff09\u4ece\u4e13\u5229\u6570\u636e\u96c6\u4e2d\u63d0\u53d6\u6587\u672c\uff1b2\uff09\u5c06\u6587\u672c\u4e3b\u9898\u6620\u5c04\u4ee5\u53d1\u73b0\u6280\u672f\u95f4\u5173\u7cfb\uff1b3\uff09\u901a\u8fc7\u8ffd\u8e2a\u4e3b\u9898\u968f\u65f6\u95f4\u53d8\u5316\u6765\u8bc6\u522b\u6280\u672f\u673a\u4f1a\uff1b4\uff09\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u4e3b\u9898\uff0c\u5e76\u4f7f\u7528\u63d0\u793a\u5de5\u7a0b\u652f\u6301\u673a\u4f1a\u53d1\u73b0\u3002", "result": "\u4f7f\u7528\u7f8e\u56fd\u4e13\u5229\u5546\u6807\u5c40\u63d0\u4f9b\u7684AI\u4e13\u5229\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u4eba\u5de5\u667a\u80fd\u6280\u672f\u6b63\u5728\u5411\u4fbf\u4e8e\u65e5\u5e38\u8bbf\u95ee\u7684\u5f62\u5f0f\u6f14\u53d8\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u8bc6\u522b\u672a\u6765\u6280\u672f\u673a\u4f1a\u7684\u6f5c\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u8bc6\u522b\u65b0\u5174\u6280\u672f\u673a\u4f1a\uff0c\u5c55\u793a\u4e86\u7ed3\u5408\u4e13\u5229\u6570\u636e\u5206\u6790\u3001\u6587\u672c\u6316\u6398\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6280\u672f\u673a\u4f1a\u53d1\u73b0\u65b9\u9762\u7684\u5e94\u7528\u4ef7\u503c\uff0c\u4e3a\u6280\u672f\u53d1\u5c55\u548c\u521b\u65b0\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\u3002"}}
{"id": "2509.09955", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.09955", "abs": "https://arxiv.org/abs/2509.09955", "authors": ["Omar Erak", "Omar Alhussein", "Hatem Abou-Zeid", "Mehdi Bennis", "Sami Muhaidat"], "title": "Adaptive Token Merging for Efficient Transformer Semantic Communication at the Edge", "comment": "Submitted to IEEE Journals", "summary": "Large-scale transformers are central to modern semantic communication, yet\ntheir high computational and communication costs hinder deployment on\nresource-constrained edge devices. This paper introduces a training-free\nframework for adaptive token merging, a novel mechanism that compresses\ntransformer representations at runtime by selectively merging semantically\nredundant tokens under per-layer similarity thresholds. Unlike prior\nfixed-ratio reduction, our approach couples merging directly to input\nredundancy, enabling data-dependent adaptation that balances efficiency and\ntask relevance without retraining. We cast the discovery of merging strategies\nas a multi-objective optimization problem and leverage Bayesian optimization to\nobtain Pareto-optimal trade-offs between accuracy, inference cost, and\ncommunication cost. On ImageNet classification, we match the accuracy of the\nunmodified transformer with 30\\% fewer floating-point operations per second and\nunder 20\\% of the original communication cost, while for visual question\nanswering our method achieves performance competitive with the full LLaVA model\nat less than one-third of the compute and one-tenth of the bandwidth. Finally,\nwe show that our adaptive merging is robust across varying channel conditions\nand provides inherent privacy benefits, substantially degrading the efficacy of\nmodel inversion attacks. Our framework provides a practical and versatile\nsolution for deploying powerful transformer models in resource-limited edge\nintelligence scenarios.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u7684token\u5408\u5e76\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5408\u5e76\u8bed\u4e49\u5197\u4f59token\u6765\u538b\u7f29transformer\u8868\u793a\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u548c\u901a\u4fe1\u6210\u672c", "motivation": "\u5927\u89c4\u6a21transformer\u5728\u8bed\u4e49\u901a\u4fe1\u4e2d\u8ba1\u7b97\u548c\u901a\u4fe1\u6210\u672c\u8fc7\u9ad8\uff0c\u96be\u4ee5\u90e8\u7f72\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u9ad8\u6548\u538b\u7f29\u65b9\u6cd5", "method": "\u57fa\u4e8e\u6bcf\u5c42\u76f8\u4f3c\u5ea6\u9608\u503c\u9009\u62e9\u6027\u5408\u5e76\u8bed\u4e49\u5197\u4f59token\uff0c\u5c06\u5408\u5e76\u7b56\u7565\u53d1\u73b0\u5efa\u6a21\u4e3a\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u83b7\u5f97\u7cbe\u5ea6\u3001\u63a8\u7406\u6210\u672c\u548c\u901a\u4fe1\u6210\u672c\u4e4b\u95f4\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u6743\u8861", "result": "\u5728ImageNet\u5206\u7c7b\u4e0a\u4ee530%\u66f4\u5c11\u7684FLOPs\u548c\u4f4e\u4e8e20%\u7684\u539f\u59cb\u901a\u4fe1\u6210\u672c\u8fbe\u5230\u539f\u59cbtransformer\u7cbe\u5ea6\uff1b\u5728VQA\u4efb\u52a1\u4e0a\u4ee5\u4e0d\u52301/3\u8ba1\u7b97\u91cf\u548c1/10\u5e26\u5bbd\u8fbe\u5230\u4e0e\u5b8c\u6574LLaVA\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u667a\u80fd\u573a\u666f\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u8de8\u4fe1\u9053\u6761\u4ef6\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u63d0\u4f9b\u56fa\u6709\u7684\u9690\u79c1\u4fdd\u62a4\u4f18\u52bf"}}
{"id": "2509.09725", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09725", "abs": "https://arxiv.org/abs/2509.09725", "authors": ["Chunyu Li", "Xindi Zheng", "Siqi Liu"], "title": "BIBERT-Pipe on Biomedical Nested Named Entity Linking at BioASQ 2025", "comment": null, "summary": "Entity linking (EL) for biomedical text is typically benchmarked on\nEnglish-only corpora with flat mentions, leaving the more realistic scenario of\nnested and multilingual mentions largely unexplored. We present our system for\nthe BioNNE 2025 Multilingual Biomedical Nested Named Entity Linking shared task\n(English & Russian), closing this gap with a lightweight pipeline that keeps\nthe original EL model intact and modifies only three task-aligned components:\nTwo-stage retrieval-ranking. We leverage the same base encoder model in both\nstages: the retrieval stage uses the original pre-trained model, while the\nranking stage applies domain-specific fine-tuning. Boundary cues. In the\nranking stage, we wrap each mention with learnable [Ms] / [Me] tags, providing\nthe encoder with an explicit, language-agnostic span before robustness to\noverlap and nesting. Dataset augmentation. We also automatically expand the\nranking training corpus with three complementary data sources, enhancing\ncoverage without extra manual annotation. On the BioNNE 2025 leaderboard, our\ntwo stage system, bilingual bert (BIBERT-Pipe), ranks third in the multilingual\ntrack, demonstrating the effectiveness and competitiveness of these minimal yet\nprincipled modifications. Code are publicly available at\nhttps://github.com/Kaggle-Competitions-Code/BioNNE-L.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u4e24\u9636\u6bb5\u5b9e\u4f53\u94fe\u63a5\u7cfb\u7edfBIBERT-Pipe\uff0c\u7528\u4e8e\u5904\u7406\u751f\u7269\u533b\u5b66\u6587\u672c\u4e2d\u7684\u591a\u8bed\u8a00\u5d4c\u5957\u547d\u540d\u5b9e\u4f53\u94fe\u63a5\u4efb\u52a1\uff0c\u5728BioNNE 2025\u8bc4\u6d4b\u4e2d\u6392\u540d\u7b2c\u4e09", "motivation": "\u73b0\u6709\u7684\u751f\u7269\u533b\u5b66\u5b9e\u4f53\u94fe\u63a5\u57fa\u51c6\u4e3b\u8981\u9488\u5bf9\u82f1\u8bed\u548c\u5e73\u5766\u63d0\u53ca\uff0c\u7f3a\u4e4f\u5bf9\u591a\u8bed\u8a00\u548c\u5d4c\u5957\u63d0\u53ca\u7684\u73b0\u5b9e\u573a\u666f\u7814\u7a76", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u68c0\u7d22-\u6392\u5e8f\u67b6\u6784\uff1a\u68c0\u7d22\u9636\u6bb5\u4f7f\u7528\u539f\u59cb\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u6392\u5e8f\u9636\u6bb5\u8fdb\u884c\u9886\u57df\u7279\u5b9a\u5fae\u8c03\uff1b\u4f7f\u7528\u53ef\u5b66\u4e60\u7684[Ms]/[Me]\u6807\u7b7e\u5305\u88c5\u63d0\u53ca\uff1b\u901a\u8fc7\u4e09\u79cd\u6570\u636e\u6e90\u81ea\u52a8\u6269\u5c55\u8bad\u7ec3\u8bed\u6599", "result": "\u5728BioNNE 2025\u591a\u8bed\u8a00\u8d5b\u9053\u4e2d\u6392\u540d\u7b2c\u4e09\uff0c\u8bc1\u660e\u4e86\u8fd9\u4e9b\u6700\u5c0f\u4f46\u539f\u5219\u6027\u4fee\u6539\u7684\u6709\u6548\u6027\u548c\u7ade\u4e89\u529b", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4fdd\u6301\u539f\u59cbEL\u6a21\u578b\u5b8c\u6574\uff0c\u4ec5\u4fee\u6539\u4e09\u4e2a\u4efb\u52a1\u5bf9\u9f50\u7ec4\u4ef6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u591a\u8bed\u8a00\u751f\u7269\u533b\u5b66\u5d4c\u5957\u5b9e\u4f53\u94fe\u63a5\u95ee\u9898\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2509.09960", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09960", "abs": "https://arxiv.org/abs/2509.09960", "authors": ["Mingxuan Jiang", "Yongxin Wang", "Ziyue Dai", "Yicun Liu", "Hongyi Nie", "Sen Liu", "Hongfeng Chai"], "title": "Limited Reference, Reliable Generation: A Two-Component Framework for Tabular Data Generation in Low-Data Regimes", "comment": null, "summary": "Synthetic tabular data generation is increasingly essential in data\nmanagement, supporting downstream applications when real-world and high-quality\ntabular data is insufficient. Existing tabular generation approaches, such as\ngenerative adversarial networks (GANs), diffusion models, and fine-tuned Large\nLanguage Models (LLMs), typically require sufficient reference data, limiting\ntheir effectiveness in domain-specific databases with scarce records. While\nprompt-based LLMs offer flexibility without parameter tuning, they often fail\nto capture dataset-specific feature-label dependencies and generate redundant\ndata, leading to degradation in downstream task performance. To overcome these\nissues, we propose ReFine, a framework that (i) derives symbolic \"if-then\"\nrules from interpretable models and embeds them into prompts to explicitly\nguide generation toward domain-specific feature distribution, and (ii) applies\na dual-granularity filtering strategy that suppresses over-sampling patterns\nand selectively refines rare but informative samples to reduce distributional\nimbalance. Extensive experiments on various regression and classification\nbenchmarks demonstrate that ReFine consistently outperforms state-of-the-art\nmethods, achieving up to 0.44 absolute improvement in R-squared for regression\nand 10.0 percent relative improvement in F1 score for classification tasks.", "AI": {"tldr": "ReFine\u662f\u4e00\u4e2a\u5408\u6210\u8868\u683c\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u53ef\u89e3\u91ca\u6a21\u578b\u63d0\u53d6\u7b26\u53f7\u89c4\u5219\u5d4c\u5165\u63d0\u793a\u8bcd\u6765\u6307\u5bfc\u751f\u6210\uff0c\u5e76\u91c7\u7528\u53cc\u7c92\u5ea6\u8fc7\u6ee4\u7b56\u7565\u51cf\u5c11\u5206\u5e03\u4e0d\u5e73\u8861\uff0c\u5728\u56de\u5f52\u548c\u5206\u7c7b\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8868\u683c\u751f\u6210\u65b9\u6cd5\u9700\u8981\u5145\u8db3\u53c2\u8003\u6570\u636e\uff0c\u5728\u9886\u57df\u7279\u5b9a\u6570\u636e\u5e93\u4e2d\u6548\u679c\u6709\u9650\uff1b\u57fa\u4e8e\u63d0\u793a\u7684LLM\u96be\u4ee5\u6355\u6349\u6570\u636e\u96c6\u7279\u5b9a\u7684\u7279\u5f81-\u6807\u7b7e\u4f9d\u8d56\u5173\u7cfb\u4e14\u751f\u6210\u5197\u4f59\u6570\u636e\uff0c\u5bfc\u81f4\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u4ece\u53ef\u89e3\u91ca\u6a21\u578b\u63a8\u5bfc\u7b26\u53f7\"if-then\"\u89c4\u5219\u5e76\u5d4c\u5165\u63d0\u793a\u8bcd\uff0c\u660e\u786e\u6307\u5bfc\u751f\u6210\u671d\u5411\u9886\u57df\u7279\u5b9a\u7279\u5f81\u5206\u5e03\uff1b\u5e94\u7528\u53cc\u7c92\u5ea6\u8fc7\u6ee4\u7b56\u7565\u6291\u5236\u8fc7\u91c7\u6837\u6a21\u5f0f\u5e76\u9009\u62e9\u6027\u7cbe\u70bc\u7a00\u6709\u4f46\u4fe1\u606f\u4e30\u5bcc\u7684\u6837\u672c\u3002", "result": "\u5728\u5404\u79cd\u56de\u5f52\u548c\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cReFine\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u56de\u5f52\u4efb\u52a1R\u5e73\u65b9\u7edd\u5bf9\u63d0\u53470.44\uff0c\u5206\u7c7b\u4efb\u52a1F1\u5206\u6570\u76f8\u5bf9\u63d0\u534710.0%\u3002", "conclusion": "ReFine\u6846\u67b6\u901a\u8fc7\u89c4\u5219\u5f15\u5bfc\u548c\u667a\u80fd\u8fc7\u6ee4\u6709\u6548\u89e3\u51b3\u4e86\u8868\u683c\u6570\u636e\u751f\u6210\u4e2d\u7684\u9886\u57df\u9002\u5e94\u6027\u548c\u5206\u5e03\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u6570\u636e\u7684\u8d28\u91cf\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2509.09726", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09726", "abs": "https://arxiv.org/abs/2509.09726", "authors": ["Seiji Hattori", "Takuya Matsuzaki", "Makoto Fujiwara"], "title": "Natural Language Translation of Formal Proofs through Informalization of Proof Steps and Recursive Summarization along Proof Structure", "comment": "Submitted to INLG 2025 (accepted)", "summary": "This paper proposes a natural language translation method for\nmachine-verifiable formal proofs that leverages the informalization\n(verbalization of formal language proof steps) and summarization capabilities\nof LLMs. For evaluation, it was applied to formal proof data created in\naccordance with natural language proofs taken from an undergraduate-level\ntextbook, and the quality of the generated natural language proofs was analyzed\nin comparison with the original natural language proofs. Furthermore, we will\ndemonstrate that this method can output highly readable and accurate natural\nlanguage proofs by applying it to existing formal proof library of the Lean\nproof assistant.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528LLM\u7684\u975e\u6b63\u5f0f\u5316\u548c\u6458\u8981\u80fd\u529b\u5c06\u673a\u5668\u53ef\u9a8c\u8bc1\u7684\u5f62\u5f0f\u8bc1\u660e\u7ffb\u8bd1\u6210\u81ea\u7136\u8bed\u8a00\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u6559\u79d1\u4e66\u8bc1\u660e\u548cLean\u8bc1\u660e\u5e93\u4e0a\u9a8c\u8bc1\u4e86\u5176\u53ef\u8bfb\u6027\u548c\u51c6\u786e\u6027", "motivation": "\u4e3a\u4e86\u5c06\u5f62\u5f0f\u5316\u7684\u673a\u5668\u53ef\u9a8c\u8bc1\u8bc1\u660e\u8f6c\u6362\u4e3a\u66f4\u6613\u7406\u89e3\u7684\u81ea\u7136\u8bed\u8a00\u5f62\u5f0f\uff0c\u63d0\u9ad8\u5f62\u5f0f\u8bc1\u660e\u7684\u53ef\u8bfb\u6027\u548c\u53ef\u8bbf\u95ee\u6027", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u7684\u975e\u6b63\u5f0f\u5316(\u5f62\u5f0f\u8bed\u8a00\u8bc1\u660e\u6b65\u9aa4\u7684\u8a00\u8bed\u5316)\u548c\u6458\u8981\u80fd\u529b\uff0c\u5bf9\u5f62\u5f0f\u8bc1\u660e\u6570\u636e\u8fdb\u884c\u81ea\u7136\u8bed\u8a00\u7ffb\u8bd1", "result": "\u5728\u672c\u79d1\u6559\u79d1\u4e66\u7684\u5f62\u5f0f\u8bc1\u660e\u6570\u636e\u4e0a\u5e94\u7528\u8be5\u65b9\u6cd5\uff0c\u751f\u6210\u7684\u81ea\u7136\u8bed\u8a00\u8bc1\u660e\u4e0e\u539f\u59cb\u81ea\u7136\u8bed\u8a00\u8bc1\u660e\u76f8\u6bd4\u8d28\u91cf\u826f\u597d\uff1b\u5728Lean\u8bc1\u660e\u52a9\u624b\u7684\u73b0\u6709\u5f62\u5f0f\u8bc1\u660e\u5e93\u4e0a\u5e94\u7528\uff0c\u80fd\u591f\u8f93\u51fa\u9ad8\u53ef\u8bfb\u6027\u548c\u51c6\u786e\u6027\u7684\u81ea\u7136\u8bed\u8a00\u8bc1\u660e", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5c06\u5f62\u5f0f\u8bc1\u660e\u8f6c\u6362\u4e3a\u9ad8\u8d28\u91cf\u7684\u81ea\u7136\u8bed\u8a00\u8bc1\u660e\uff0c\u63d0\u9ad8\u4e86\u5f62\u5f0f\u8bc1\u660e\u7684\u53ef\u7406\u89e3\u6027\u548c\u5b9e\u7528\u6027"}}
{"id": "2509.09991", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09991", "abs": "https://arxiv.org/abs/2509.09991", "authors": ["Amandip Sangha"], "title": "Data-Driven Energy Estimation for Virtual Servers Using Combined System Metrics and Machine Learning", "comment": null, "summary": "This paper presents a machine learning-based approach to estimate the energy\nconsumption of virtual servers without access to physical power measurement\ninterfaces. Using resource utilization metrics collected from guest virtual\nmachines, we train a Gradient Boosting Regressor to predict energy consumption\nmeasured via RAPL on the host. We demonstrate, for the first time, guest-only\nresource-based energy estimation without privileged host access with\nexperiments across diverse workloads, achieving high predictive accuracy and\nvariance explained ($0.90 \\leq R^2 \\leq 0.97$), indicating the feasibility of\nguest-side energy estimation. This approach can enable energy-aware scheduling,\ncost optimization and physical host independent energy estimates in virtualized\nenvironments. Our approach addresses a critical gap in virtualized environments\n(e.g. cloud) where direct energy measurement is infeasible.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u865a\u62df\u670d\u52a1\u5668\u80fd\u8017\u4f30\u8ba1\u65b9\u6cd5\uff0c\u65e0\u9700\u7269\u7406\u529f\u7387\u6d4b\u91cf\u63a5\u53e3\uff0c\u4ec5\u4f7f\u7528\u865a\u62df\u673a\u8d44\u6e90\u5229\u7528\u7387\u6307\u6807\u5373\u53ef\u51c6\u786e\u9884\u6d4b\u80fd\u8017", "motivation": "\u89e3\u51b3\u865a\u62df\u5316\u73af\u5883\uff08\u5982\u4e91\u5e73\u53f0\uff09\u4e2d\u65e0\u6cd5\u76f4\u63a5\u6d4b\u91cf\u80fd\u8017\u7684\u5173\u952e\u95ee\u9898\uff0c\u4e3a\u80fd\u6e90\u611f\u77e5\u8c03\u5ea6\u548c\u6210\u672c\u4f18\u5316\u63d0\u4f9b\u652f\u6301", "method": "\u4f7f\u7528\u68af\u5ea6\u63d0\u5347\u56de\u5f52\u5668\uff08Gradient Boosting Regressor\uff09\uff0c\u57fa\u4e8e\u865a\u62df\u673a\u6536\u96c6\u7684\u8d44\u6e90\u5229\u7528\u7387\u6307\u6807\u6765\u9884\u6d4b\u901a\u8fc7RAPL\u6d4b\u91cf\u7684\u4e3b\u673a\u80fd\u8017", "result": "\u5728\u591a\u6837\u5316\u5de5\u4f5c\u8d1f\u8f7d\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u9884\u6d4b\u7cbe\u5ea6\uff080.90 \u2264 R\u00b2 \u2264 0.97\uff09\uff0c\u9996\u6b21\u8bc1\u660e\u4e86\u65e0\u9700\u7279\u6743\u4e3b\u673a\u8bbf\u95ee\u7684\u7eaf\u5ba2\u6237\u7aef\u80fd\u8017\u4f30\u8ba1\u7684\u53ef\u884c\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u865a\u62df\u5316\u73af\u5883\u4e2d\u7684\u80fd\u6e90\u611f\u77e5\u8c03\u5ea6\u3001\u6210\u672c\u4f18\u5316\u548c\u7269\u7406\u4e3b\u673a\u72ec\u7acb\u7684\u80fd\u8017\u4f30\u8ba1\uff0c\u586b\u8865\u4e86\u865a\u62df\u5316\u73af\u5883\u4e2d\u76f4\u63a5\u80fd\u8017\u6d4b\u91cf\u4e0d\u53ef\u884c\u7684\u5173\u952e\u7a7a\u767d"}}
{"id": "2509.09727", "categories": ["cs.CL", "cs.CE"], "pdf": "https://arxiv.org/pdf/2509.09727", "abs": "https://arxiv.org/abs/2509.09727", "authors": ["Andy Zhu", "Yingjun Du"], "title": "A Role-Aware Multi-Agent Framework for Financial Education Question Answering with LLMs", "comment": "8 pages, 6 figures, Underreview", "summary": "Question answering (QA) plays a central role in financial education, yet\nexisting large language model (LLM) approaches often fail to capture the\nnuanced and specialized reasoning required for financial problem-solving. The\nfinancial domain demands multistep quantitative reasoning, familiarity with\ndomain-specific terminology, and comprehension of real-world scenarios. We\npresent a multi-agent framework that leverages role-based prompting to enhance\nperformance on domain-specific QA. Our framework comprises a Base Generator, an\nEvidence Retriever, and an Expert Reviewer agent that work in a single-pass\niteration to produce a refined answer. We evaluated our framework on a set of\n3,532 expert-designed finance education questions from Study.com, an online\nlearning platform. We leverage retrieval-augmented generation (RAG) for\ncontextual evidence from 6 finance textbooks and prompting strategies for a\ndomain-expert reviewer. Our experiments indicate that critique-based refinement\nimproves answer accuracy by 6.6-8.3% over zero-shot Chain-of-Thought baselines,\nwith the highest performance from Gemini-2.0-Flash. Furthermore, our method\nenables GPT-4o-mini to achieve performance comparable to the finance-tuned\nFinGPT-mt_Llama3-8B_LoRA. Our results show a cost-effective approach to\nenhancing financial QA and offer insights for further research in multi-agent\nfinancial LLM systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u4ee3\u7406\u6846\u67b6\u7684\u91d1\u878d\u95ee\u7b54\u7cfb\u7edf\uff0c\u901a\u8fc7\u89d2\u8272\u63d0\u793a\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91d1\u878d\u9886\u57df\u95ee\u7b54\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u95ee\u7b54\u4e2d\u96be\u4ee5\u5904\u7406\u590d\u6742\u7684\u591a\u6b65\u5b9a\u91cf\u63a8\u7406\u548c\u4e13\u4e1a\u672f\u8bed\uff0c\u9700\u8981\u4e13\u95e8\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u91d1\u878d\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u5305\u62ec\u57fa\u7840\u751f\u6210\u5668\u3001\u8bc1\u636e\u68c0\u7d22\u5668\u548c\u4e13\u5bb6\u8bc4\u5ba1\u5668\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u4ece6\u672c\u91d1\u878d\u6559\u79d1\u4e66\u4e2d\u83b7\u53d6\u4e0a\u4e0b\u6587\u8bc1\u636e\u3002", "result": "\u76f8\u6bd4\u96f6\u6837\u672c\u601d\u7ef4\u94fe\u57fa\u7ebf\uff0c\u6279\u5224\u6027\u7cbe\u70bc\u4f7f\u7b54\u6848\u51c6\u786e\u7387\u63d0\u9ad8\u4e866.6-8.3%\uff0cGemini-2.0-Flash\u8868\u73b0\u6700\u4f73\uff0cGPT-4o-mini\u8fbe\u5230\u4e0e\u4e13\u4e1a\u91d1\u878d\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u91d1\u878d\u95ee\u7b54\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u591a\u4ee3\u7406\u91d1\u878dLLM\u7cfb\u7edf\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2509.10000", "categories": ["cs.LG", "cond-mat.other"], "pdf": "https://arxiv.org/pdf/2509.10000", "abs": "https://arxiv.org/abs/2509.10000", "authors": ["Tilen Cadez", "Kyoung-Min Kim"], "title": "Neural Scaling Laws for Deep Regression", "comment": "Supplementary Information will be provided with the published\n  manuscript", "summary": "Neural scaling laws--power-law relationships between generalization errors\nand characteristics of deep learning models--are vital tools for developing\nreliable models while managing limited resources. Although the success of large\nlanguage models highlights the importance of these laws, their application to\ndeep regression models remains largely unexplored. Here, we empirically\ninvestigate neural scaling laws in deep regression using a parameter estimation\nmodel for twisted van der Waals magnets. We observe power-law relationships\nbetween the loss and both training dataset size and model capacity across a\nwide range of values, employing various architectures--including fully\nconnected networks, residual networks, and vision transformers. Furthermore,\nthe scaling exponents governing these relationships range from 1 to 2, with\nspecific values depending on the regressed parameters and model details. The\nconsistent scaling behaviors and their large scaling exponents suggest that the\nperformance of deep regression models can improve substantially with increasing\ndata size.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5b9e\u8bc1\u7814\u7a76\u4e86\u6df1\u5ea6\u56de\u5f52\u6a21\u578b\u4e2d\u7684\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\uff0c\u53d1\u73b0\u5728\u626d\u66f2\u8303\u5fb7\u74e6\u5c14\u65af\u78c1\u4f53\u7684\u53c2\u6570\u4f30\u8ba1\u6a21\u578b\u4e2d\uff0c\u635f\u5931\u4e0e\u8bad\u7ec3\u6570\u636e\u96c6\u5927\u5c0f\u548c\u6a21\u578b\u5bb9\u91cf\u4e4b\u95f4\u5b58\u5728\u5e42\u5f8b\u5173\u7cfb\uff0c\u7f29\u653e\u6307\u6570\u57281\u52302\u4e4b\u95f4\u3002", "motivation": "\u867d\u7136\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u5728\u6df1\u5ea6\u56de\u5f52\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u7814\u7a76\u8fd9\u4e9b\u5b9a\u5f8b\u5728\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u626d\u66f2\u8303\u5fb7\u74e6\u5c14\u65af\u78c1\u4f53\u7684\u53c2\u6570\u4f30\u8ba1\u6a21\u578b\uff0c\u91c7\u7528\u5168\u8fde\u63a5\u7f51\u7edc\u3001\u6b8b\u5dee\u7f51\u7edc\u548c\u89c6\u89c9\u53d8\u6362\u5668\u7b49\u591a\u79cd\u67b6\u6784\uff0c\u5728\u4e0d\u540c\u6570\u636e\u96c6\u5927\u5c0f\u548c\u6a21\u578b\u5bb9\u91cf\u4e0b\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u89c2\u5bdf\u5230\u635f\u5931\u4e0e\u8bad\u7ec3\u6570\u636e\u96c6\u5927\u5c0f\u548c\u6a21\u578b\u5bb9\u91cf\u4e4b\u95f4\u5b58\u5728\u5e42\u5f8b\u5173\u7cfb\uff0c\u7f29\u653e\u6307\u6570\u8303\u56f4\u4e3a1\u52302\uff0c\u5177\u4f53\u503c\u53d6\u51b3\u4e8e\u56de\u5f52\u53c2\u6570\u548c\u6a21\u578b\u7ec6\u8282\u3002", "conclusion": "\u4e00\u81f4\u7684\u7f29\u653e\u884c\u4e3a\u548c\u5927\u7f29\u653e\u6307\u6570\u8868\u660e\uff0c\u6df1\u5ea6\u56de\u5f52\u6a21\u578b\u7684\u6027\u80fd\u53ef\u4ee5\u968f\u7740\u6570\u636e\u91cf\u7684\u589e\u52a0\u800c\u663e\u8457\u63d0\u9ad8\u3002"}}
{"id": "2509.09728", "categories": ["cs.CL", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.09728", "abs": "https://arxiv.org/abs/2509.09728", "authors": ["Elena Rohde", "Jonas Klingwort", "Christian Borgs"], "title": "A meta-analysis on the performance of machine-learning based language models for sentiment analysis", "comment": null, "summary": "This paper presents a meta-analysis evaluating ML performance in sentiment\nanalysis for Twitter data. The study aims to estimate the average performance,\nassess heterogeneity between and within studies, and analyze how study\ncharacteristics influence model performance. Using PRISMA guidelines, we\nsearched academic databases and selected 195 trials from 20 studies with 12\nstudy features. Overall accuracy, the most reported performance metric, was\nanalyzed using double arcsine transformation and a three-level random effects\nmodel. The average overall accuracy of the AIC-optimized model was 0.80 [0.76,\n0.84]. This paper provides two key insights: 1) Overall accuracy is widely used\nbut often misleading due to its sensitivity to class imbalance and the number\nof sentiment classes, highlighting the need for normalization. 2) Standardized\nreporting of model performance, including reporting confusion matrices for\nindependent test sets, is essential for reliable comparisons of ML classifiers\nacross studies, which seems far from common practice.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u835f\u8403\u5206\u6790\u8bc4\u4f30\u4e86\u673a\u5668\u5b66\u4e60\u5728Twitter\u60c5\u611f\u5206\u6790\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u5e73\u5747\u51c6\u786e\u7387\u4e3a0.80\uff0c\u5e76\u6307\u51fa\u6574\u4f53\u51c6\u786e\u7387\u6307\u6807\u5b58\u5728\u8bef\u5bfc\u6027\uff0c\u9700\u8981\u6807\u51c6\u5316\u62a5\u544a\u89c4\u8303\u3002", "motivation": "\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\u5728Twitter\u60c5\u611f\u5206\u6790\u4e2d\u7684\u5e73\u5747\u6027\u80fd\u8868\u73b0\uff0c\u5206\u6790\u7814\u7a76\u95f4\u7684\u5f02\u8d28\u6027\uff0c\u5e76\u63a2\u8ba8\u7814\u7a76\u7279\u5f81\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u4ee5\u4fc3\u8fdb\u66f4\u53ef\u9760\u7684\u8de8\u7814\u7a76\u6bd4\u8f83\u3002", "method": "\u91c7\u7528PRISMA\u6307\u5357\u8fdb\u884c\u6587\u732e\u68c0\u7d22\uff0c\u4ece20\u9879\u7814\u7a76\u4e2d\u7b5b\u9009\u51fa195\u4e2a\u8bd5\u9a8c\uff0c\u4f7f\u7528\u53cc\u53cd\u6b63\u5f26\u53d8\u6362\u548c\u4e09\u6c34\u5e73\u968f\u673a\u6548\u5e94\u6a21\u578b\u5206\u6790\u6574\u4f53\u51c6\u786e\u7387\u7b49\u6027\u80fd\u6307\u6807\u3002", "result": "AIC\u4f18\u5316\u6a21\u578b\u7684\u5e73\u5747\u6574\u4f53\u51c6\u786e\u7387\u4e3a0.80 [0.76, 0.84]\uff0c\u53d1\u73b0\u6574\u4f53\u51c6\u786e\u7387\u56e0\u5bf9\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u60c5\u611f\u7c7b\u522b\u6570\u91cf\u7684\u654f\u611f\u6027\u800c\u7ecf\u5e38\u4ea7\u751f\u8bef\u5bfc\u3002", "conclusion": "\u9700\u8981\u89c4\u8303\u6a21\u578b\u6027\u80fd\u62a5\u544a\u6807\u51c6\uff0c\u5305\u62ec\u62a5\u544a\u72ec\u7acb\u6d4b\u8bd5\u96c6\u7684\u6df7\u6dc6\u77e9\u9635\uff0c\u4ee5\u5b9e\u73b0\u8de8\u7814\u7a76\u7684\u53ef\u9760\u6bd4\u8f83\uff0c\u4f46\u76ee\u524d\u8fd9\u79cd\u505a\u6cd5\u8fdc\u672a\u666e\u53ca\u3002"}}
{"id": "2509.10011", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2509.10011", "abs": "https://arxiv.org/abs/2509.10011", "authors": ["Antoine Orioua", "Philipp Krah", "Julian Koellermeier"], "title": "Intrinsic Dimension Estimating Autoencoder (IDEA) Using CancelOut Layer and a Projected Loss", "comment": "Preprint with 12 pages and 12 figures", "summary": "This paper introduces the Intrinsic Dimension Estimating Autoencoder (IDEA),\nwhich identifies the underlying intrinsic dimension of a wide range of datasets\nwhose samples lie on either linear or nonlinear manifolds. Beyond estimating\nthe intrinsic dimension, IDEA is also able to reconstruct the original dataset\nafter projecting it onto the corresponding latent space, which is structured\nusing re-weighted double CancelOut layers. Our key contribution is the\nintroduction of the projected reconstruction loss term, guiding the training of\nthe model by continuously assessing the reconstruction quality under the\nremoval of an additional latent dimension. We first assess the performance of\nIDEA on a series of theoretical benchmarks to validate its robustness. These\nexperiments allow us to test its reconstruction ability and compare its\nperformance with state-of-the-art intrinsic dimension estimators. The\nbenchmarks show good accuracy and high versatility of our approach.\nSubsequently, we apply our model to data generated from the numerical solution\nof a vertically resolved one-dimensional free-surface flow, following a\npointwise discretization of the vertical velocity profile in the horizontal\ndirection, vertical direction, and time. IDEA succeeds in estimating the\ndataset's intrinsic dimension and then reconstructs the original solution by\nworking directly within the projection space identified by the network.", "AI": {"tldr": "IDEA\u662f\u4e00\u79cd\u80fd\u591f\u4f30\u8ba1\u6570\u636e\u96c6\u5185\u5728\u7ef4\u5ea6\u5e76\u91cd\u5efa\u539f\u59cb\u6570\u636e\u7684\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u6295\u5f71\u91cd\u5efa\u635f\u5931\u9879\u548c\u91cd\u52a0\u6743\u53ccCancelOut\u5c42\u7ed3\u6784\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u9645\u6d41\u4f53\u52a8\u529b\u5b66\u6570\u636e\u4e0a\u90fd\u8868\u73b0\u51fa\u826f\u597d\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4f30\u8ba1\u975e\u7ebf\u6027\u6d41\u5f62\u6570\u636e\u96c6\u7684\u5185\u5728\u7ef4\u5ea6\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u51c6\u786e\u4f30\u8ba1\u5185\u5728\u7ef4\u5ea6\u53c8\u80fd\u6709\u6548\u91cd\u5efa\u539f\u59cb\u6570\u636e\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u63d0\u51faIDEA\u81ea\u7f16\u7801\u5668\uff0c\u4f7f\u7528\u91cd\u52a0\u6743\u53ccCancelOut\u5c42\u6784\u5efa\u6f5c\u5728\u7a7a\u95f4\uff0c\u5f15\u5165\u6295\u5f71\u91cd\u5efa\u635f\u5931\u9879\u901a\u8fc7\u8fde\u7eed\u79fb\u9664\u6f5c\u5728\u7ef4\u5ea6\u6765\u6307\u5bfc\u8bad\u7ec3\u3002", "result": "\u5728\u7406\u8bba\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u51c6\u786e\u6027\u548c\u901a\u7528\u6027\uff0c\u5728\u6d41\u4f53\u52a8\u529b\u5b66\u6570\u503c\u89e3\u6570\u636e\u96c6\u4e0a\u6210\u529f\u4f30\u8ba1\u5185\u5728\u7ef4\u5ea6\u5e76\u91cd\u5efa\u539f\u59cb\u89e3\u3002", "conclusion": "IDEA\u662f\u4e00\u4e2a\u6709\u6548\u4e14\u901a\u7528\u7684\u5185\u5728\u7ef4\u5ea6\u4f30\u8ba1\u548c\u91cd\u5efa\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u6d41\u5f62\u6570\u636e\u96c6\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2509.09729", "categories": ["cs.CL", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.09729", "abs": "https://arxiv.org/abs/2509.09729", "authors": ["Gerard Sant", "Zifan Jiang", "Carlos Escolano", "Amit Moryossef", "Mathias M\u00fcller", "Rico Sennrich", "Sarah Ebling"], "title": "MultimodalHugs: Enabling Sign Language Processing in Hugging Face", "comment": null, "summary": "In recent years, sign language processing (SLP) has gained importance in the\ngeneral field of Natural Language Processing. However, compared to research on\nspoken languages, SLP research is hindered by complex ad-hoc code,\ninadvertently leading to low reproducibility and unfair comparisons. Existing\ntools that are built for fast and reproducible experimentation, such as Hugging\nFace, are not flexible enough to seamlessly integrate sign language\nexperiments. This view is confirmed by a survey we conducted among SLP\nresearchers.\n  To address these challenges, we introduce MultimodalHugs, a framework built\non top of Hugging Face that enables more diverse data modalities and tasks,\nwhile inheriting the well-known advantages of the Hugging Face ecosystem. Even\nthough sign languages are our primary focus, MultimodalHugs adds a layer of\nabstraction that makes it more widely applicable to other use cases that do not\nfit one of the standard templates of Hugging Face. We provide quantitative\nexperiments to illustrate how MultimodalHugs can accommodate diverse modalities\nsuch as pose estimation data for sign languages, or pixel data for text\ncharacters.", "AI": {"tldr": "MultimodalHugs\u662f\u4e00\u4e2a\u57fa\u4e8eHugging Face\u6784\u5efa\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u4e13\u95e8\u89e3\u51b3\u624b\u8bed\u5904\u7406\u7814\u7a76\u4e2d\u7684\u53ef\u590d\u73b0\u6027\u548c\u7075\u6d3b\u6027\u4e0d\u8db3\u95ee\u9898\uff0c\u652f\u6301\u59ff\u6001\u4f30\u8ba1\u6570\u636e\u548c\u50cf\u7d20\u6570\u636e\u7b49\u591a\u79cd\u6a21\u6001\u3002", "motivation": "\u624b\u8bed\u5904\u7406\u7814\u7a76\u9762\u4e34\u590d\u6742\u4e34\u65f6\u4ee3\u7801\u3001\u4f4e\u53ef\u590d\u73b0\u6027\u548c\u4e0d\u516c\u5e73\u6bd4\u8f83\u7684\u95ee\u9898\uff0c\u73b0\u6709\u5de5\u5177\u5982Hugging Face\u4e0d\u591f\u7075\u6d3b\uff0c\u65e0\u6cd5\u65e0\u7f1d\u96c6\u6210\u624b\u8bed\u5b9e\u9a8c\u3002", "method": "\u5728Hugging Face\u57fa\u7840\u4e0a\u6784\u5efaMultimodalHugs\u6846\u67b6\uff0c\u589e\u52a0\u62bd\u8c61\u5c42\u4ee5\u652f\u6301\u66f4\u591a\u6837\u5316\u7684\u6570\u636e\u6a21\u6001\u548c\u4efb\u52a1\uff0c\u7ee7\u627fHugging Face\u751f\u6001\u7cfb\u7edf\u7684\u4f18\u52bf\u3002", "result": "\u6846\u67b6\u80fd\u591f\u5bb9\u7eb3\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u5305\u62ec\u624b\u8bed\u59ff\u6001\u4f30\u8ba1\u6570\u636e\u548c\u6587\u672c\u5b57\u7b26\u50cf\u7d20\u6570\u636e\uff0c\u901a\u8fc7\u5b9a\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u9002\u7528\u6027\u3002", "conclusion": "MultimodalHugs\u4e3a\u624b\u8bed\u5904\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u3001\u53ef\u590d\u73b0\u7684\u5b9e\u9a8c\u6846\u67b6\uff0c\u540c\u65f6\u4e5f\u9002\u7528\u4e8e\u5176\u4ed6\u4e0d\u7b26\u5408Hugging Face\u6807\u51c6\u6a21\u677f\u7684\u591a\u6a21\u6001\u7528\u4f8b\u3002"}}
{"id": "2509.10025", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10025", "abs": "https://arxiv.org/abs/2509.10025", "authors": ["Strahinja Nikolic", "Ilker Oguz", "Demetri Psaltis"], "title": "Exploring Expert Specialization through Unsupervised Training in Sparse Mixture of Experts", "comment": "14 pages, 7 figures", "summary": "Understanding the internal organization of neural networks remains a\nfundamental challenge in deep learning interpretability. We address this\nchallenge by exploring a novel Sparse Mixture of Experts Variational\nAutoencoder (SMoE-VAE) architecture. We test our model on the QuickDraw\ndataset, comparing unsupervised expert routing against a supervised baseline\nguided by ground-truth labels. Surprisingly, we find that unsupervised routing\nconsistently achieves superior reconstruction performance. The experts learn to\nidentify meaningful sub-categorical structures that often transcend\nhuman-defined class boundaries. Through t-SNE visualizations and reconstruction\nanalysis, we investigate how MoE models uncover fundamental data structures\nthat are more aligned with the model's objective than predefined labels.\nFurthermore, our study on the impact of dataset size provides insights into the\ntrade-offs between data quantity and expert specialization, offering guidance\nfor designing efficient MoE architectures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08SMoE-VAE\uff09\u67b6\u6784\uff0c\u53d1\u73b0\u5728\u65e0\u76d1\u7763\u4e13\u5bb6\u8def\u7531\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u6709\u76d1\u7763\u57fa\u7ebf\uff0c\u4e13\u5bb6\u80fd\u591f\u5b66\u4e60\u5230\u8d85\u8d8a\u4eba\u5de5\u5b9a\u4e49\u7c7b\u522b\u8fb9\u754c\u7684\u6709\u610f\u4e49\u5b50\u7c7b\u522b\u7ed3\u6784\u3002", "motivation": "\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u7684\u5185\u90e8\u7ec4\u7ec7\u662f\u6df1\u5ea6\u5b66\u4e60\u53ef\u89e3\u91ca\u6027\u7684\u57fa\u672c\u6311\u6218\uff0c\u9700\u8981\u63a2\u7d22\u65b0\u7684\u67b6\u6784\u6765\u63ed\u793a\u6570\u636e\u7684\u5185\u5728\u7ed3\u6784\u3002", "method": "\u91c7\u7528\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08SMoE-VAE\uff09\u67b6\u6784\uff0c\u5728QuickDraw\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u6bd4\u8f83\u65e0\u76d1\u7763\u4e13\u5bb6\u8def\u7531\u4e0e\u57fa\u4e8e\u771f\u5b9e\u6807\u7b7e\u7684\u6709\u76d1\u7763\u57fa\u7ebf\u3002", "result": "\u65e0\u76d1\u7763\u8def\u7531\u59cb\u7ec8\u83b7\u5f97\u66f4\u597d\u7684\u91cd\u5efa\u6027\u80fd\uff0c\u4e13\u5bb6\u5b66\u4f1a\u8bc6\u522b\u6709\u610f\u4e49\u5b50\u7c7b\u522b\u7ed3\u6784\uff0c\u8fd9\u4e9b\u7ed3\u6784\u5f80\u5f80\u8d85\u8d8a\u4eba\u5de5\u5b9a\u4e49\u7684\u7c7b\u522b\u8fb9\u754c\u3002t-SNE\u53ef\u89c6\u5316\u548c\u91cd\u5efa\u5206\u6790\u663e\u793aMoE\u6a21\u578b\u53d1\u73b0\u66f4\u7b26\u5408\u6a21\u578b\u76ee\u6807\u7684\u57fa\u672c\u6570\u636e\u7ed3\u6784\u3002", "conclusion": "\u7814\u7a76\u6570\u636e\u96c6\u5927\u5c0f\u7684\u5f71\u54cd\u63ed\u793a\u4e86\u6570\u636e\u91cf\u4e0e\u4e13\u5bb6\u4e13\u4e1a\u5316\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4e3a\u8bbe\u8ba1\u9ad8\u6548\u7684MoE\u67b6\u6784\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u8868\u660e\u65e0\u76d1\u7763\u65b9\u6cd5\u5728\u53d1\u73b0\u6570\u636e\u5185\u5728\u7ed3\u6784\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2509.09731", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09731", "abs": "https://arxiv.org/abs/2509.09731", "authors": ["Haiyang Yu", "Yuchuan Wu", "Fan Shi", "Lei Liao", "Jinghui Lu", "Xiaodong Ge", "Han Wang", "Minghan Zhuo", "Xuecheng Wu", "Xiang Fei", "Hao Feng", "Guozhi Tang", "An-Lan Wang", "Hanshen Zhu", "Yangfan He", "Quanhuan Liang", "Liyuan Meng", "Chao Feng", "Can Huang", "Jingqun Tang", "Bin Li"], "title": "Benchmarking Vision-Language Models on Chinese Ancient Documents: From OCR to Knowledge Reasoning", "comment": null, "summary": "Chinese ancient documents, invaluable carriers of millennia of Chinese\nhistory and culture, hold rich knowledge across diverse fields but face\nchallenges in digitization and understanding, i.e., traditional methods only\nscan images, while current Vision-Language Models (VLMs) struggle with their\nvisual and linguistic complexity. Existing document benchmarks focus on English\nprinted texts or simplified Chinese, leaving a gap for evaluating VLMs on\nancient Chinese documents. To address this, we present AncientDoc, the first\nbenchmark for Chinese ancient documents, designed to assess VLMs from OCR to\nknowledge reasoning. AncientDoc includes five tasks (page-level OCR, vernacular\ntranslation, reasoning-based QA, knowledge-based QA, linguistic variant QA) and\ncovers 14 document types, over 100 books, and about 3,000 pages. Based on\nAncientDoc, we evaluate mainstream VLMs using multiple metrics, supplemented by\na human-aligned large language model for scoring.", "AI": {"tldr": "AncientDoc\u662f\u9996\u4e2a\u9488\u5bf9\u4e2d\u6587\u53e4\u7c4d\u6587\u6863\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b5\u4e2a\u4efb\u52a1\uff0c\u6db5\u76d614\u79cd\u6587\u6863\u7c7b\u578b\u3001100\u591a\u672c\u4e66\u7c4d\u548c\u7ea63000\u9875\u5185\u5bb9\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728OCR\u5230\u77e5\u8bc6\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u4e2d\u6587\u53e4\u7c4d\u4f5c\u4e3a\u4e2d\u534e\u5386\u53f2\u6587\u5316\u7684\u91cd\u8981\u8f7d\u4f53\uff0c\u5728\u6570\u5b57\u5316\u548c\u7406\u89e3\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u82f1\u6587\u5370\u5237\u6587\u672c\u6216\u7b80\u4f53\u4e2d\u6587\uff0c\u7f3a\u4e4f\u5bf9\u4e2d\u6587\u53e4\u7c4d\u6587\u6863\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "method": "\u6784\u5efaAncientDoc\u57fa\u51c6\uff0c\u5305\u542b\u9875\u9762\u7ea7OCR\u3001\u767d\u8bdd\u7ffb\u8bd1\u3001\u63a8\u7406\u95ee\u7b54\u3001\u77e5\u8bc6\u95ee\u7b54\u548c\u8bed\u8a00\u53d8\u4f53\u95ee\u7b54\u4e94\u4e2a\u4efb\u52a1\uff0c\u4f7f\u7528\u591a\u6307\u6807\u8bc4\u4f30\u4e3b\u6d41\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u8f85\u4ee5\u4eba\u5de5\u5bf9\u9f50\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bc4\u5206\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b14\u79cd\u6587\u6863\u7c7b\u578b\u3001100\u591a\u672c\u4e66\u7c4d\u3001\u7ea63000\u9875\u5185\u5bb9\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u96c6\u3002", "conclusion": "AncientDoc\u586b\u8865\u4e86\u4e2d\u6587\u53e4\u7c4d\u6587\u6863\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u53e4\u7c4d\u5904\u7406\u65b9\u9762\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2509.10033", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10033", "abs": "https://arxiv.org/abs/2509.10033", "authors": ["Boya Ma", "Abram Magner", "Maxwell McNeil", "Petko Bogdanov"], "title": "Sparse Coding Representation of 2-way Data", "comment": null, "summary": "Sparse dictionary coding represents signals as linear combinations of a few\ndictionary atoms. It has been applied to images, time series, graph signals and\nmulti-way spatio-temporal data by jointly employing temporal and spatial\ndictionaries. Data-agnostic analytical dictionaries, such as the discrete\nFourier transform, wavelets and graph Fourier, have seen wide adoption due to\nefficient implementations and good practical performance. On the other hand,\ndictionaries learned from data offer sparser and more accurate solutions but\nrequire learning of both the dictionaries and the coding coefficients. This\nbecomes especially challenging for multi-dictionary scenarios since encoding\ncoefficients correspond to all atom combinations from the dictionaries. To\naddress this challenge, we propose a low-rank coding model for 2-dictionary\nscenarios and study its data complexity. Namely, we establish a bound on the\nnumber of samples needed to learn dictionaries that generalize to unseen\nsamples from the same distribution. We propose a convex relaxation solution,\ncalled AODL, whose exact solution we show also solves the original problem. We\nthen solve this relaxation via alternating optimization between the sparse\ncoding matrices and the learned dictionaries, which we prove to be convergent.\nWe demonstrate its quality for data reconstruction and missing value imputation\nin both synthetic and real-world datasets. For a fixed reconstruction quality,\nAODL learns up to 90\\% sparser solutions compared to non-low-rank and\nanalytical (fixed) dictionary baselines. In addition, the learned dictionaries\nreveal interpretable insights into patterns present within the samples used for\ntraining.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7a00\u758f\u5b57\u5178\u7f16\u7801\u7684\u4f4e\u79e9\u7f16\u7801\u6a21\u578bAODL\uff0c\u901a\u8fc7\u51f8\u677e\u5f1b\u548c\u4ea4\u66ff\u4f18\u5316\u89e3\u51b3\u591a\u5b57\u5178\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u91cd\u5efa\u8d28\u91cf\u7684\u540c\u65f6\u83b7\u5f97\u66f4\u7a00\u758f\u7684\u89e3", "motivation": "\u7a00\u758f\u5b57\u5178\u7f16\u7801\u5728\u5904\u7406\u591a\u7ef4\u65f6\u7a7a\u6570\u636e\u65f6\u9700\u8981\u540c\u65f6\u5b66\u4e60\u591a\u4e2a\u5b57\u5178\u548c\u7f16\u7801\u7cfb\u6570\uff0c\u8fd9\u5728\u5927\u89c4\u6a21\u591a\u5b57\u5178\u573a\u666f\u4e2d\u53d8\u5f97\u7279\u522b\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u7f16\u7801\u7cfb\u6570\u5bf9\u5e94\u4e8e\u6240\u6709\u5b57\u5178\u539f\u5b50\u7684\u7ec4\u5408", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4f4e\u79e9\u7f16\u7801\u6a21\u578b\u6765\u5904\u7406\u53cc\u5b57\u5178\u573a\u666f\uff0c\u5f00\u53d1\u4e86AODL\u51f8\u677e\u5f1b\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7a00\u758f\u7f16\u7801\u77e9\u9635\u548c\u5b66\u4e60\u5b57\u5178\u4e4b\u95f4\u7684\u4ea4\u66ff\u4f18\u5316\u6765\u6c42\u89e3\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u6536\u655b\u6027", "result": "AODL\u5728\u56fa\u5b9a\u91cd\u5efa\u8d28\u91cf\u4e0b\u76f8\u6bd4\u975e\u4f4e\u79e9\u548c\u56fa\u5b9a\u5206\u6790\u5b57\u5178\u57fa\u7ebf\u5b66\u4e60\u5230\u9ad8\u8fbe90%\u66f4\u7a00\u758f\u7684\u89e3\uff0c\u5b66\u4e60\u5230\u7684\u5b57\u5178\u63ed\u793a\u4e86\u8bad\u7ec3\u6837\u672c\u4e2d\u5b58\u5728\u7684\u53ef\u89e3\u91ca\u6a21\u5f0f", "conclusion": "AODL\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u5b57\u5178\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u4f18\u5f02\u7684\u6570\u636e\u91cd\u5efa\u548c\u7f3a\u5931\u503c\u63d2\u8865\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u7a00\u758f\u6027\u548c\u53ef\u89e3\u91ca\u6027"}}
{"id": "2509.09734", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09734", "abs": "https://arxiv.org/abs/2509.09734", "authors": ["Zikang Guo", "Benfeng Xu", "Chiwei Zhu", "Wentao Hong", "Xiaorui Wang", "Zhendong Mao"], "title": "MCP-AgentBench: Evaluating Real-World Language Agent Performance with MCP-Mediated Tools", "comment": null, "summary": "The Model Context Protocol (MCP) is rapidly emerging as a pivotal open\nstandard, designed to enhance agent-tool integration and interoperability, and\nis positioned to unlock a new era of powerful, interconnected, and genuinely\nutilitarian agentic AI. However, despite MCP's growing adoption, existing\nbenchmarks often fail to capture real-world agent performance within this new\nparadigm, leading to a distorted perception of their true operational value and\nan inability to reliably differentiate proficiencies. To bridge this critical\nevaluation gap, we introduce MCP-AgentBench -- a comprehensive benchmark\nspecifically engineered to rigorously assess language agent capabilities in\nMCP-mediated tool interactions. Core contributions of MCP-AgentBench include:\nthe establishment of a robust MCP testbed comprising 33 operational servers\nwith 188 distinct tools; the development of a benchmark featuring 600\nsystematically designed queries distributed across 6 distinct categories of\nvarying interaction complexity; and the introduction of MCP-Eval, a novel\noutcome-oriented evaluation methodology prioritizing real-world task success.\nThrough extensive empirical evaluation of leading language agents, we provide\nfoundational insights. MCP-AgentBench aims to equip the research community with\na standardized and reliable framework to build, validate, and advance agents\ncapable of fully leveraging MCP's transformative benefits, thereby accelerating\nprogress toward truly capable and interoperable AI systems.", "AI": {"tldr": "MCP-AgentBench\u662f\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9MCP\u534f\u8bae\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b33\u4e2a\u670d\u52a1\u5668\u3001188\u4e2a\u5de5\u5177\u548c600\u4e2a\u67e5\u8be2\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u4ee3\u7406\u5728\u5de5\u5177\u4ea4\u4e92\u4e2d\u7684\u771f\u5b9e\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u51c6\u786e\u8bc4\u4f30\u5728MCP\u65b0\u8303\u5f0f\u4e0b\u7684\u771f\u5b9e\u4ee3\u7406\u6027\u80fd\uff0c\u5bfc\u81f4\u5bf9\u5176\u5b9e\u9645\u64cd\u4f5c\u4ef7\u503c\u7684\u626d\u66f2\u8ba4\u77e5\u548c\u65e0\u6cd5\u53ef\u9760\u533a\u5206\u80fd\u529b\u5dee\u5f02\u3002", "method": "\u5efa\u7acb\u4e86\u5305\u542b33\u4e2a\u64cd\u4f5c\u670d\u52a1\u5668\u548c188\u4e2a\u4e0d\u540c\u5de5\u5177\u7684MCP\u6d4b\u8bd5\u5e8a\uff1b\u5f00\u53d1\u4e86600\u4e2a\u7cfb\u7edf\u8bbe\u8ba1\u7684\u67e5\u8be2\uff0c\u5206\u5e03\u57286\u4e2a\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u4ea4\u4e92\u7c7b\u522b\u4e2d\uff1b\u5f15\u5165\u4e86MCP-Eval\u8fd9\u79cd\u4ee5\u7ed3\u679c\u4e3a\u5bfc\u5411\u7684\u65b0\u578b\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u5bf9\u9886\u5148\u8bed\u8a00\u4ee3\u7406\u7684\u5e7f\u6cdb\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u63d0\u4f9b\u4e86\u57fa\u7840\u6027\u89c1\u89e3\uff0c\u5c55\u793a\u4e86\u4e0d\u540c\u4ee3\u7406\u5728MCP\u73af\u5883\u4e0b\u7684\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "MCP-AgentBench\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u548c\u53ef\u9760\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u3001\u9a8c\u8bc1\u548c\u63a8\u8fdb\u80fd\u591f\u5145\u5206\u5229\u7528MCP\u53d8\u9769\u6027\u4f18\u52bf\u7684\u4ee3\u7406\uff0c\u4ece\u800c\u52a0\u901f\u771f\u6b63\u80fd\u529b\u548c\u4e92\u64cd\u4f5c\u6027AI\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.10034", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10034", "abs": "https://arxiv.org/abs/2509.10034", "authors": ["Sahil Rajesh Dhayalkar"], "title": "Symbolic Feedforward Networks for Probabilistic Finite Automata: Exact Simulation and Learnability", "comment": "19 pages, 2 figures", "summary": "We present a formal and constructive theory showing that probabilistic finite\nautomata (PFAs) can be exactly simulated using symbolic feedforward neural\nnetworks. Our architecture represents state distributions as vectors and\ntransitions as stochastic matrices, enabling probabilistic state propagation\nvia matrix-vector products. This yields a parallel, interpretable, and\ndifferentiable simulation of PFA dynamics using soft updates-without\nrecurrence. We formally characterize probabilistic subset construction,\n$\\varepsilon$-closure, and exact simulation via layered symbolic computation,\nand prove equivalence between PFAs and specific classes of neural networks. We\nfurther show that these symbolic simulators are not only expressive but\nlearnable: trained with standard gradient descent-based optimization on labeled\nsequence data, they recover the exact behavior of ground-truth PFAs. This\nlearnability, formalized in Proposition 5.1, is the crux of this work. Our\nresults unify probabilistic automata theory with neural architectures under a\nrigorous algebraic framework, bridging the gap between symbolic computation and\ndeep learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f62\u5f0f\u5316\u7406\u8bba\uff0c\u8bc1\u660e\u6982\u7387\u6709\u9650\u81ea\u52a8\u673a(PFAs)\u53ef\u4ee5\u901a\u8fc7\u7b26\u53f7\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u7cbe\u786e\u6a21\u62df\u3002\u901a\u8fc7\u5c06\u72b6\u6001\u5206\u5e03\u8868\u793a\u4e3a\u5411\u91cf\u3001\u8f6c\u79fb\u8868\u793a\u4e3a\u968f\u673a\u77e9\u9635\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u9012\u5f52\u7684\u5e76\u884c\u3001\u53ef\u89e3\u91ca\u3001\u53ef\u5fae\u5206PFA\u6a21\u62df\u3002", "motivation": "\u7edf\u4e00\u6982\u7387\u81ea\u52a8\u673a\u7406\u8bba\u4e0e\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5728\u4e25\u683c\u7684\u4ee3\u6570\u6846\u67b6\u4e0b\u5f25\u5408\u7b26\u53f7\u8ba1\u7b97\u4e0e\u6df1\u5ea6\u5b66\u4e60\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5b9e\u73b0\u6982\u7387\u81ea\u52a8\u673a\u7684\u53ef\u5b66\u4e60\u795e\u7ecf\u7f51\u7edc\u6a21\u62df\u3002", "method": "\u4f7f\u7528\u7b26\u53f7\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5c06\u72b6\u6001\u5206\u5e03\u8868\u793a\u4e3a\u5411\u91cf\uff0c\u8f6c\u79fb\u8868\u793a\u4e3a\u968f\u673a\u77e9\u9635\uff0c\u901a\u8fc7\u77e9\u9635-\u5411\u91cf\u4e58\u79ef\u5b9e\u73b0\u6982\u7387\u72b6\u6001\u4f20\u64ad\uff0c\u91c7\u7528\u8f6f\u66f4\u65b0\u800c\u975e\u9012\u5f52\u65b9\u5f0f\u3002", "result": "\u8bc1\u660e\u4e86PFAs\u4e0e\u7279\u5b9a\u7c7b\u522b\u795e\u7ecf\u7f51\u7edc\u7684\u7b49\u4ef7\u6027\uff0c\u5c55\u793a\u4e86\u8fd9\u4e9b\u7b26\u53f7\u6a21\u62df\u5668\u4e0d\u4ec5\u5177\u6709\u8868\u8fbe\u6027\u800c\u4e14\u53ef\u5b66\u4e60\uff1a\u901a\u8fc7\u6807\u51c6\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u5728\u6807\u8bb0\u5e8f\u5217\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u80fd\u591f\u6062\u590d\u771f\u5b9ePFAs\u7684\u7cbe\u786e\u884c\u4e3a\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u547d\u98985.1\u5f62\u5f0f\u5316\u4e86\u53ef\u5b66\u4e60\u6027\u8fd9\u4e00\u6838\u5fc3\u8d21\u732e\uff0c\u4e3a\u6982\u7387\u81ea\u52a8\u673a\u7406\u8bba\u4e0e\u795e\u7ecf\u67b6\u6784\u7684\u7edf\u4e00\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u4ee3\u6570\u6846\u67b6\uff0c\u6210\u529f\u8fde\u63a5\u4e86\u7b26\u53f7\u8ba1\u7b97\u4e0e\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\u3002"}}
{"id": "2509.09735", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09735", "abs": "https://arxiv.org/abs/2509.09735", "authors": ["Willem Huijzer", "Jieying Chen"], "title": "Discrimination by LLMs: Cross-lingual Bias Assessment and Mitigation in Decision-Making and Summarisation", "comment": "7 pages", "summary": "The rapid integration of Large Language Models (LLMs) into various domains\nraises concerns about societal inequalities and information bias. This study\nexamines biases in LLMs related to background, gender, and age, with a focus on\ntheir impact on decision-making and summarization tasks. Additionally, the\nresearch examines the cross-lingual propagation of these biases and evaluates\nthe effectiveness of prompt-instructed mitigation strategies. Using an adapted\nversion of the dataset by Tamkin et al. (2023) translated into Dutch, we\ncreated 151,200 unique prompts for the decision task and 176,400 for the\nsummarisation task. Various demographic variables, instructions, salience\nlevels, and languages were tested on GPT-3.5 and GPT-4o. Our analysis revealed\nthat both models were significantly biased during decision-making, favouring\nfemale gender, younger ages, and certain backgrounds such as the\nAfrican-American background. In contrast, the summarisation task showed minimal\nevidence of bias, though significant age-related differences emerged for\nGPT-3.5 in English. Cross-lingual analysis showed that bias patterns were\nbroadly similar between English and Dutch, though notable differences were\nobserved across specific demographic categories. The newly proposed mitigation\ninstructions, while unable to eliminate biases completely, demonstrated\npotential in reducing them. The most effective instruction achieved a 27\\% mean\nreduction in the gap between the most and least favorable demographics.\nNotably, contrary to GPT-3.5, GPT-4o displayed reduced biases for all prompts\nin English, indicating the specific potential for prompt-based mitigation\nwithin newer models. This research underscores the importance of cautious\nadoption of LLMs and context-specific bias testing, highlighting the need for\ncontinued development of effective mitigation strategies to ensure responsible\ndeployment of AI.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86GPT-3.5\u548cGPT-4o\u5728\u51b3\u7b56\u548c\u6458\u8981\u4efb\u52a1\u4e2d\u5b58\u5728\u7684\u80cc\u666f\u3001\u6027\u522b\u548c\u5e74\u9f84\u504f\u89c1\uff0c\u53d1\u73b0\u51b3\u7b56\u4efb\u52a1\u4e2d\u5b58\u5728\u663e\u8457\u504f\u89c1\uff0c\u800c\u6458\u8981\u4efb\u52a1\u504f\u89c1\u8f83\u5c0f\u3002\u7814\u7a76\u8fd8\u8bc4\u4f30\u4e86\u8de8\u8bed\u8a00\u504f\u89c1\u4f20\u64ad\u548c\u63d0\u793a\u6307\u4ee4\u7f13\u89e3\u7b56\u7565\u7684\u6548\u679c\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5404\u9886\u57df\u7684\u5feb\u901f\u5e94\u7528\uff0c\u4eba\u4eec\u5bf9\u5176\u53ef\u80fd\u52a0\u5267\u793e\u4f1a\u4e0d\u5e73\u7b49\u548c\u4fe1\u606f\u504f\u89c1\u7684\u62c5\u5fe7\u65e5\u76ca\u589e\u52a0\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u6a21\u578b\u504f\u89c1\u53ca\u5176\u7f13\u89e3\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528Tamkin\u7b49\u4eba(2023)\u6570\u636e\u96c6\u7684\u8377\u5170\u8bed\u7ffb\u8bd1\u7248\u672c\uff0c\u521b\u5efa\u4e86151,200\u4e2a\u51b3\u7b56\u4efb\u52a1\u63d0\u793a\u548c176,400\u4e2a\u6458\u8981\u4efb\u52a1\u63d0\u793a\uff0c\u6d4b\u8bd5\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u53d8\u91cf\u3001\u6307\u4ee4\u3001\u663e\u8457\u5ea6\u6c34\u5e73\u548c\u8bed\u8a00\u5728GPT-3.5\u548cGPT-4o\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u4e24\u4e2a\u6a21\u578b\u5728\u51b3\u7b56\u4efb\u52a1\u4e2d\u90fd\u5b58\u5728\u663e\u8457\u504f\u89c1\uff0c\u504f\u5411\u5973\u6027\u3001\u5e74\u8f7b\u5e74\u9f84\u548c\u7279\u5b9a\u80cc\u666f\uff08\u5982\u975e\u88d4\u7f8e\u56fd\u4eba\uff09\u3002\u6458\u8981\u4efb\u52a1\u504f\u89c1\u8f83\u5c0f\uff0c\u4f46GPT-3.5\u5728\u82f1\u8bed\u4e2d\u663e\u793a\u51fa\u663e\u8457\u7684\u5e74\u9f84\u76f8\u5173\u5dee\u5f02\u3002\u8de8\u8bed\u8a00\u5206\u6790\u663e\u793a\u82f1\u8377\u504f\u89c1\u6a21\u5f0f\u5927\u4f53\u76f8\u4f3c\u3002\u6700\u6709\u6548\u7684\u7f13\u89e3\u6307\u4ee4\u4f7f\u6700\u6709\u5229\u548c\u6700\u4e0d\u5229\u4eba\u53e3\u7edf\u8ba1\u7fa4\u4f53\u95f4\u7684\u5dee\u8ddd\u5e73\u5747\u51cf\u5c1127%\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u9700\u8981\u8c28\u614e\u91c7\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5e76\u8fdb\u884c\u7279\u5b9a\u60c5\u5883\u7684\u504f\u89c1\u6d4b\u8bd5\uff0c\u540c\u65f6\u9700\u8981\u6301\u7eed\u5f00\u53d1\u6709\u6548\u7684\u7f13\u89e3\u7b56\u7565\u4ee5\u786e\u4fddAI\u7684\u8d1f\u8d23\u4efb\u90e8\u7f72\u3002GPT-4o\u76f8\u6bd4GPT-3.5\u5728\u6240\u6709\u82f1\u8bed\u63d0\u793a\u4e2d\u90fd\u663e\u793a\u51fa\u504f\u89c1\u51cf\u5c11\uff0c\u8868\u660e\u65b0\u6a21\u578b\u5728\u57fa\u4e8e\u63d0\u793a\u7684\u7f13\u89e3\u65b9\u9762\u5177\u6709\u7279\u5b9a\u6f5c\u529b\u3002"}}
{"id": "2509.10041", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10041", "abs": "https://arxiv.org/abs/2509.10041", "authors": ["Mohammad Hasan Narimani", "Mostafa Tavassolipour"], "title": "FedRP: A Communication-Efficient Approach for Differentially Private Federated Learning Using Random Projection", "comment": null, "summary": "Federated learning (FL) offers an innovative paradigm for collaborative model\ntraining across decentralized devices, such as smartphones, balancing enhanced\npredictive performance with the protection of user privacy in sensitive areas\nlike Internet of Things (IoT) and medical data analysis. Despite its\nadvantages, FL encounters significant challenges related to user privacy\nprotection against potential attacks and the management of communication costs.\nThis paper introduces a novel federated learning algorithm called FedRP, which\nintegrates random projection techniques with the Alternating Direction Method\nof Multipliers (ADMM) optimization framework. This approach enhances privacy by\nemploying random projection to reduce the dimensionality of model parameters\nprior to their transmission to a central server, reducing the communication\ncost. The proposed algorithm offers a strong $(\\epsilon, \\delta)$-differential\nprivacy guarantee, demonstrating resilience against data reconstruction\nattacks. Experimental results reveal that FedRP not only maintains high model\naccuracy but also outperforms existing methods, including conventional\ndifferential privacy approaches and FedADMM, in terms of both privacy\npreservation and communication efficiency.", "AI": {"tldr": "FedRP\u662f\u4e00\u79cd\u65b0\u9896\u7684\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\uff0c\u7ed3\u5408\u968f\u673a\u6295\u5f71\u548cADMM\u4f18\u5316\u6846\u67b6\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u964d\u4f4e\u901a\u4fe1\u6210\u672c\uff0c\u63d0\u4f9b(\u03b5,\u03b4)-\u5dee\u5206\u9690\u79c1\u4fdd\u8bc1\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u7269\u8054\u7f51\u548c\u533b\u7597\u6570\u636e\u5206\u6790\u7b49\u654f\u611f\u9886\u57df\u9762\u4e34\u7528\u6237\u9690\u79c1\u4fdd\u62a4\u548c\u901a\u4fe1\u6210\u672c\u7ba1\u7406\u7684\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u4fdd\u62a4\u9690\u79c1\u53c8\u9ad8\u6548\u7684\u7b97\u6cd5\u3002", "method": "\u63d0\u51faFedRP\u7b97\u6cd5\uff0c\u96c6\u6210\u968f\u673a\u6295\u5f71\u6280\u672f\u548cADMM\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u968f\u673a\u6295\u5f71\u964d\u4f4e\u6a21\u578b\u53c2\u6570\u7ef4\u5ea6\u540e\u518d\u4f20\u8f93\u5230\u4e2d\u592e\u670d\u52a1\u5668\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aFedRP\u4e0d\u4ec5\u4fdd\u6301\u9ad8\u6a21\u578b\u7cbe\u5ea6\uff0c\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u901a\u4fe1\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5305\u62ec\u4f20\u7edf\u5dee\u5206\u9690\u79c1\u65b9\u6cd5\u548cFedADMM\u3002", "conclusion": "FedRP\u7b97\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u548c\u901a\u4fe1\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u654f\u611f\u6570\u636e\u573a\u666f\u4e0b\u7684\u534f\u4f5c\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.09801", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T07, 68T50, 68T05", "I.2.7; I.2.6; C.4"], "pdf": "https://arxiv.org/pdf/2509.09801", "abs": "https://arxiv.org/abs/2509.09801", "authors": ["Brennen Hill"], "title": "HEFT: A Coarse-to-Fine Hierarchy for Enhancing the Efficiency and Accuracy of Language Model Reasoning", "comment": null, "summary": "The adaptation of large language models (LLMs) to specialized reasoning tasks\nis fundamentally constrained by computational resources. Parameter-Efficient\nFine-Tuning (PEFT) methods have emerged as a powerful solution, yet the\nlandscape of these techniques is diverse, with distinct methods operating in\neither the model's weight space or its representation space. This paper\ninvestigates the hypothesis that a synergistic combination of these paradigms\ncan unlock superior performance and efficiency. We introduce HEFT (Hierarchical\nEfficient Fine-Tuning), a novel hierarchical adaptation strategy that composes\ntwo distinct PEFT methods in a coarse-to-fine manner: first, a broad,\nfoundational adaptation in the weight space using Low-Rank Adaptation (LoRA),\nfollowed by a precise, surgical refinement of internal activations using\nRepresentation Fine-Tuning (ReFT). We evaluate this approach by fine-tuning a\nLlama-2-7B model on the BoolQ benchmark, a challenging dataset for inferential\nreasoning. Our results reveal a profound synergistic effect. A model fine-tuned\nfor only three epochs with our HEFT strategy achieves an accuracy of 85.17\\%,\nexceeding the performance of models trained for 20 epochs with either LoRA-only\n(85.05\\%) or ReFT-only (83.36\\%) methodologies. This work demonstrates that the\nthoughtful composition of PEFT methods is a potent algorithmic innovation,\noffering a more efficient and effective path toward advancing the reasoning\ncapabilities of language models. By achieving superior results with a fraction\nof the computational budget, our findings present a principled approach to\novercoming the obstacles inherent in adapting large-scale models for complex\ncognitive tasks.", "AI": {"tldr": "HEFT\u662f\u4e00\u79cd\u5206\u5c42\u9ad8\u6548\u5fae\u8c03\u7b56\u7565\uff0c\u7ed3\u5408\u4e86LoRA\uff08\u6743\u91cd\u7a7a\u95f4\uff09\u548cReFT\uff08\u8868\u793a\u7a7a\u95f4\uff09\u4e24\u79cdPEFT\u65b9\u6cd5\uff0c\u5728BoolQ\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4ec5\u75283\u4e2aepoch\u5c31\u8fbe\u5230\u4e8685.17%\u7684\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u4e86\u5355\u72ec\u4f7f\u7528LoRA\uff0820\u4e2aepoch\uff0c85.05%\uff09\u6216ReFT\uff0820\u4e2aepoch\uff0c83.36%\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e13\u95e8\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u9002\u914d\u53d7\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\uff0c\u867d\u7136\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u4e0d\u540c\u65b9\u6cd5\u5728\u6743\u91cd\u7a7a\u95f4\u6216\u8868\u793a\u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff0c\u672c\u6587\u7814\u7a76\u8fd9\u4e9b\u8303\u5f0f\u7684\u534f\u540c\u7ec4\u5408\u662f\u5426\u80fd\u89e3\u9501\u66f4\u597d\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "method": "\u63d0\u51faHEFT\u5206\u5c42\u9002\u914d\u7b56\u7565\uff1a\u9996\u5148\u4f7f\u7528LoRA\u5728\u6743\u91cd\u7a7a\u95f4\u8fdb\u884c\u5e7f\u6cdb\u7684\u57fa\u7840\u9002\u914d\uff0c\u7136\u540e\u4f7f\u7528ReFT\u5bf9\u5185\u90e8\u6fc0\u6d3b\u8fdb\u884c\u7cbe\u786e\u7684\u5916\u79d1\u5f0f\u7cbe\u70bc\uff0c\u5f62\u6210\u4ece\u7c97\u5230\u7ec6\u7684\u5fae\u8c03\u65b9\u5f0f\u3002", "result": "\u5728Llama-2-7B\u6a21\u578b\u548cBoolQ\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHEFT\u4ec5\u75283\u4e2aepoch\u5c31\u8fbe\u523085.17%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8eLoRA-only\uff0820\u4e2aepoch\uff0c85.05%\uff09\u548cReFT-only\uff0820\u4e2aepoch\uff0c83.36%\uff09\u65b9\u6cd5\u3002", "conclusion": "PEFT\u65b9\u6cd5\u7684\u7cbe\u5fc3\u7ec4\u5408\u662f\u4e00\u79cd\u5f3a\u5927\u7684\u7b97\u6cd5\u521b\u65b0\uff0c\u4e3a\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u6709\u6548\u7684\u8def\u5f84\uff0c\u4ee5\u66f4\u5c11\u7684\u8ba1\u7b97\u9884\u7b97\u83b7\u5f97\u66f4\u4f18\u7ed3\u679c\uff0c\u4e3a\u9002\u5e94\u5927\u89c4\u6a21\u6a21\u578b\u7684\u590d\u6742\u8ba4\u77e5\u4efb\u52a1\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u65b9\u6cd5\u3002"}}
{"id": "2509.10048", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10048", "abs": "https://arxiv.org/abs/2509.10048", "authors": ["Madhushan Ramalingam"], "title": "Uncertainty-Aware Tabular Prediction: Evaluating VBLL-Enhanced TabPFN in Safety-Critical Medical Data", "comment": null, "summary": "Predictive models are being increasingly used across a wide range of domains,\nincluding safety-critical applications such as medical diagnosis and criminal\njustice. Reliable uncertainty estimation is a crucial task in such settings.\nTabular Prior-data Fitted Network (TabPFN) is a recently proposed machine\nlearning foundation model for tabular dataset, which uses a generative\ntransformer architecture. Variational Bayesian Last Layers (VBLL) is a\nstate-of-the-art lightweight variational formulation that effectively improves\nuncertainty estimation with minimal computational overhead. In this work we aim\nto evaluate the performance of VBLL integrated with the recently proposed\nTabPFN in uncertainty calibration. Our experiments, conducted on three\nbenchmark medical tabular datasets, compare the performance of the original\nTabPFN and the VBLL-integrated version. Contrary to expectations, we observed\nthat original TabPFN consistently outperforms VBLL integrated TabPFN in\nuncertainty calibration across all datasets.", "AI": {"tldr": "\u5728\u4e09\u4e2a\u533b\u7597\u8868\u683c\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30VBLL\u4e0eTabPFN\u96c6\u6210\u7684\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u6027\u80fd\uff0c\u53d1\u73b0\u539f\u59cbTabPFN\u59cb\u7ec8\u4f18\u4e8e\u96c6\u6210\u7248\u672c", "motivation": "\u9884\u6d4b\u6a21\u578b\u5728\u533b\u7597\u8bca\u65ad\u7b49\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u9700\u8981\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0cTabPFN\u662f\u65b0\u5174\u7684\u8868\u683c\u6570\u636e\u57fa\u7840\u6a21\u578b\uff0cVBLL\u662f\u6700\u5148\u8fdb\u7684\u8f7b\u91cf\u7ea7\u53d8\u5206\u65b9\u6cd5\uff0c\u7814\u7a76\u4e24\u8005\u96c6\u6210\u7684\u6821\u51c6\u6027\u80fd", "method": "\u5728\u4e09\u4e2a\u57fa\u51c6\u533b\u7597\u8868\u683c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u539f\u59cbTabPFN\u548cVBLL\u96c6\u6210\u7248\u672c\u7684\u6027\u80fd", "result": "\u4e0e\u9884\u671f\u76f8\u53cd\uff0c\u539f\u59cbTabPFN\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u7684\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u6027\u80fd\u90fd\u4f18\u4e8eVBLL\u96c6\u6210\u7248\u672c", "conclusion": "VBLL\u96c6\u6210\u5e76\u672a\u6539\u5584TabPFN\u7684\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u6027\u80fd\uff0c\u539f\u59cbTabPFN\u8868\u73b0\u66f4\u4f73"}}
{"id": "2509.09804", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09804", "abs": "https://arxiv.org/abs/2509.09804", "authors": ["Helen de Andrade Abreu", "Tiago Timponi Torrent", "Ely Edison da Silva Matos"], "title": "Pragmatic Frames Evoked by Gestures: A FrameNet Brasil Approach to Multimodality in Turn Organization", "comment": "Paper submitted to Language Sciences Journal", "summary": "This paper proposes a framework for modeling multimodal conversational turn\norganization via the proposition of correlations between language and\ninteractive gestures, based on analysis as to how pragmatic frames are\nconceptualized and evoked by communicators. As a means to provide evidence for\nthe analysis, we developed an annotation methodology to enrich a multimodal\ndataset (annotated for semantic frames) with pragmatic frames modeling\nconversational turn organization. Although conversational turn organization has\nbeen studied by researchers from diverse fields, the specific strategies,\nespecially gestures used by communicators, had not yet been encoded in a\ndataset that can be used for machine learning. To fill this gap, we enriched\nthe Frame2 dataset with annotations of gestures used for turn organization. The\nFrame2 dataset features 10 episodes from the Brazilian TV series Pedro Pelo\nMundo annotated for semantic frames evoked in both video and text. This dataset\nallowed us to closely observe how communicators use interactive gestures\noutside a laboratory, in settings, to our knowledge, not previously recorded in\nrelated literature. Our results have confirmed that communicators involved in\nface-to-face conversation make use of gestures as a tool for passing, taking\nand keeping conversational turns, and also revealed variations of some gestures\nthat had not been documented before. We propose that the use of these gestures\narises from the conceptualization of pragmatic frames, involving mental spaces,\nblending and conceptual metaphors. In addition, our data demonstrate that the\nannotation of pragmatic frames contributes to a deeper understanding of human\ncognition and language.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u8fc7\u8bed\u8a00\u548c\u4ea4\u4e92\u624b\u52bf\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u6765\u5efa\u6a21\u591a\u6a21\u6001\u5bf9\u8bdd\u8f6e\u6b21\u7ec4\u7ec7\u7684\u6846\u67b6\uff0c\u57fa\u4e8e\u8bed\u7528\u6846\u67b6\u5982\u4f55\u88ab\u6982\u5ff5\u5316\u548c\u5524\u8d77\u7684\u5206\u6790\u3002", "motivation": "\u586b\u8865\u5bf9\u8bdd\u8f6e\u6b21\u7ec4\u7ec7\u4e2d\u7279\u5b9a\u7b56\u7565\uff08\u5c24\u5176\u662f\u624b\u52bf\uff09\u5728\u673a\u5668\u5b66\u4e60\u53ef\u7528\u6570\u636e\u96c6\u4e2d\u7684\u7f16\u7801\u7a7a\u767d\uff0c\u7814\u7a76\u9762\u5bf9\u9762\u5bf9\u8bdd\u4e2d\u624b\u52bf\u5728\u4f20\u9012\u3001\u83b7\u53d6\u548c\u4fdd\u6301\u5bf9\u8bdd\u8f6e\u6b21\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u6ce8\u91ca\u65b9\u6cd5\uff0c\u5728Frame2\u591a\u6a21\u6001\u6570\u636e\u96c6\uff08\u5df2\u6807\u6ce8\u8bed\u4e49\u6846\u67b6\uff09\u57fa\u7840\u4e0a\u589e\u52a0\u8bed\u7528\u6846\u67b6\u6807\u6ce8\uff0c\u5206\u6790\u5df4\u897f\u7535\u89c6\u5267\u4e2d\u768410\u4e2a\u7247\u6bb5\uff0c\u89c2\u5bdf\u975e\u5b9e\u9a8c\u5ba4\u73af\u5883\u4e0b\u7684\u4ea4\u4e92\u624b\u52bf\u4f7f\u7528\u3002", "result": "\u786e\u8ba4\u9762\u5bf9\u9762\u5bf9\u8bdd\u4e2d\u4f7f\u7528\u624b\u52bf\u4f5c\u4e3a\u4f20\u9012\u3001\u83b7\u53d6\u548c\u4fdd\u6301\u5bf9\u8bdd\u8f6e\u6b21\u7684\u5de5\u5177\uff0c\u53d1\u73b0\u4e86\u4e4b\u524d\u672a\u8bb0\u5f55\u7684\u624b\u52bf\u53d8\u4f53\uff0c\u8bc1\u660e\u8bed\u7528\u6846\u67b6\u6807\u6ce8\u6709\u52a9\u4e8e\u66f4\u6df1\u5165\u7406\u89e3\u4eba\u7c7b\u8ba4\u77e5\u548c\u8bed\u8a00\u3002", "conclusion": "\u624b\u52bf\u4f7f\u7528\u6e90\u4e8e\u8bed\u7528\u6846\u67b6\u7684\u6982\u5ff5\u5316\uff0c\u6d89\u53ca\u5fc3\u7406\u7a7a\u95f4\u3001\u6982\u5ff5\u6574\u5408\u548c\u6982\u5ff5\u9690\u55bb\uff0c\u8bed\u7528\u6846\u67b6\u6807\u6ce8\u4e3a\u7406\u89e3\u4eba\u7c7b\u8ba4\u77e5\u548c\u8bed\u8a00\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2509.10089", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10089", "abs": "https://arxiv.org/abs/2509.10089", "authors": ["Marco Andrea B\u00fchler", "Gonzalo Guill\u00e9n-Gos\u00e1lbez"], "title": "KAN-SR: A Kolmogorov-Arnold Network Guided Symbolic Regression Framework", "comment": null, "summary": "We introduce a novel symbolic regression framework, namely KAN-SR, built on\nKolmogorov Arnold Networks (KANs) which follows a divide-and-conquer approach.\nSymbolic regression searches for mathematical equations that best fit a given\ndataset and is commonly solved with genetic programming approaches. We show\nthat by using deep learning techniques, more specific KANs, and combining them\nwith simplification strategies such as translational symmetries and\nseparabilities, we are able to recover ground-truth equations of the Feynman\nSymbolic Regression for Scientific Discovery (SRSD) dataset. Additionally, we\nshow that by combining the proposed framework with neural controlled\ndifferential equations, we are able to model the dynamics of an in-silico\nbioprocess system precisely, opening the door for the dynamic modeling of other\nengineering systems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eKolmogorov-Arnold\u7f51\u7edc(KAN)\u7684\u7b26\u53f7\u56de\u5f52\u6846\u67b6KAN-SR\uff0c\u91c7\u7528\u5206\u6cbb\u7b56\u7565\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u548c\u7b80\u5316\u7b56\u7565\uff0c\u80fd\u591f\u51c6\u786e\u6062\u590dFeynman SRSD\u6570\u636e\u96c6\u7684\u771f\u5b9e\u65b9\u7a0b\uff0c\u5e76\u80fd\u7cbe\u786e\u5efa\u6a21\u751f\u7269\u8fc7\u7a0b\u7cfb\u7edf\u52a8\u529b\u5b66\u3002", "motivation": "\u4f20\u7edf\u7b26\u53f7\u56de\u5f52\u901a\u5e38\u4f7f\u7528\u9057\u4f20\u7f16\u7a0b\u65b9\u6cd5\uff0c\u672c\u6587\u65e8\u5728\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u6539\u8fdb\u7b26\u53f7\u56de\u5f52\uff0c\u901a\u8fc7KAN\u7f51\u7edc\u548c\u7b80\u5316\u7b56\u7565\u63d0\u9ad8\u65b9\u7a0b\u53d1\u73b0\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u4f7f\u7528Kolmogorov-Arnold\u7f51\u7edc(KAN)\u6784\u5efa\u7b26\u53f7\u56de\u5f52\u6846\u67b6\uff0c\u91c7\u7528\u5206\u6cbb\u65b9\u6cd5\uff0c\u7ed3\u5408\u5e73\u79fb\u5bf9\u79f0\u6027\u548c\u53ef\u5206\u79bb\u6027\u7b49\u7b80\u5316\u7b56\u7565\uff0c\u5e76\u4e0e\u795e\u7ecf\u63a7\u5236\u5fae\u5206\u65b9\u7a0b\u7ed3\u5408\u5efa\u6a21\u52a8\u529b\u5b66\u7cfb\u7edf\u3002", "result": "\u6210\u529f\u6062\u590dFeynman SRSD\u6570\u636e\u96c6\u7684\u771f\u5b9e\u65b9\u7a0b\uff0c\u5e76\u7cbe\u786e\u5efa\u6a21\u4e86\u7845\u5185\u751f\u7269\u8fc7\u7a0b\u7cfb\u7edf\u7684\u52a8\u529b\u5b66\uff0c\u4e3a\u5176\u4ed6\u5de5\u7a0b\u7cfb\u7edf\u7684\u52a8\u6001\u5efa\u6a21\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002", "conclusion": "KAN-SR\u6846\u67b6\u5728\u7b26\u53f7\u56de\u5f52\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u548c\u7b80\u5316\u7b56\u7565\uff0c\u80fd\u591f\u6709\u6548\u53d1\u73b0\u6570\u5b66\u65b9\u7a0b\u5e76\u7cbe\u786e\u5efa\u6a21\u590d\u6742\u52a8\u529b\u5b66\u7cfb\u7edf\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5de5\u7a0b\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.09852", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09852", "abs": "https://arxiv.org/abs/2509.09852", "authors": ["Chuyuan Li", "Austin Xu", "Shafiq Joty", "Giuseppe Carenini"], "title": "Topic-Guided Reinforcement Learning with LLMs for Enhancing Multi-Document Summarization", "comment": null, "summary": "A key challenge in Multi-Document Summarization (MDS) is effectively\nintegrating information from multiple sources while maintaining coherence and\ntopical relevance. While Large Language Models have shown impressive results in\nsingle-document summarization, their performance on MDS still leaves room for\nimprovement. In this paper, we propose a topic-guided reinforcement learning\napproach to improve content selection in MDS. We first show that explicitly\nprompting models with topic labels enhances the informativeness of the\ngenerated summaries. Building on this insight, we propose a novel topic reward\nwithin the Group Relative Policy Optimization (GRPO) framework to measure topic\nalignment between the generated summary and source documents. Experimental\nresults on the Multi-News and Multi-XScience datasets demonstrate that our\nmethod consistently outperforms strong baselines, highlighting the\neffectiveness of leveraging topical cues in MDS.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4e3b\u9898\u5f15\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6539\u8fdb\u591a\u6587\u6863\u6458\u8981\u7684\u5185\u5bb9\u9009\u62e9\uff0c\u901a\u8fc7\u4e3b\u9898\u5956\u52b1\u673a\u5236\u5728GRPO\u6846\u67b6\u4e2d\u63d0\u5347\u6458\u8981\u4e0e\u6e90\u6587\u6863\u7684\u4e3b\u9898\u5bf9\u9f50\u5ea6", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5355\u6587\u6863\u6458\u8981\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u591a\u6587\u6863\u6458\u8981\u4e2d\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u9700\u8981\u66f4\u597d\u5730\u6574\u5408\u591a\u6e90\u4fe1\u606f\u5e76\u4fdd\u6301\u4e3b\u9898\u76f8\u5173\u6027", "method": "\u4f7f\u7528\u4e3b\u9898\u6807\u7b7e\u660e\u786e\u63d0\u793a\u6a21\u578b\uff0c\u5728GRPO\u6846\u67b6\u4e2d\u5f15\u5165\u65b0\u9896\u7684\u4e3b\u9898\u5956\u52b1\u673a\u5236\u6765\u8861\u91cf\u751f\u6210\u6458\u8981\u4e0e\u6e90\u6587\u6863\u7684\u4e3b\u9898\u5bf9\u9f50\u7a0b\u5ea6", "result": "\u5728Multi-News\u548cMulti-XScience\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5 consistently\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u6a21\u578b", "conclusion": "\u5229\u7528\u4e3b\u9898\u7ebf\u7d22\u5728\u591a\u6587\u6863\u6458\u8981\u4e2d\u5177\u6709\u663e\u8457\u6548\u679c\uff0c\u4e3b\u9898\u5f15\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u5185\u5bb9\u9009\u62e9\u8d28\u91cf"}}
{"id": "2509.10132", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10132", "abs": "https://arxiv.org/abs/2509.10132", "authors": ["Nour Jamoussi", "Giuseppe Serra", "Photios A. Stavrou", "Marios Kountouris"], "title": "Cost-Free Personalization via Information-Geometric Projection in Bayesian Federated Learning", "comment": null, "summary": "Bayesian Federated Learning (BFL) combines uncertainty modeling with\ndecentralized training, enabling the development of personalized and reliable\nmodels under data heterogeneity and privacy constraints. Existing approaches\ntypically rely on Markov Chain Monte Carlo (MCMC) sampling or variational\ninference, often incorporating personalization mechanisms to better adapt to\nlocal data distributions. In this work, we propose an information-geometric\nprojection framework for personalization in parametric BFL. By projecting the\nglobal model onto a neighborhood of the user's local model, our method enables\na tunable trade-off between global generalization and local specialization.\nUnder mild assumptions, we show that this projection step is equivalent to\ncomputing a barycenter on the statistical manifold, allowing us to derive\nclosed-form solutions and achieve cost-free personalization. We apply the\nproposed approach to a variational learning setup using the Improved\nVariational Online Newton (IVON) optimizer and extend its application to\ngeneral aggregation schemes in BFL. Empirical evaluations under heterogeneous\ndata distributions confirm that our method effectively balances global and\nlocal performance with minimal computational overhead.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u51e0\u4f55\u6295\u5f71\u7684\u8d1d\u53f6\u65af\u8054\u90a6\u5b66\u4e60\u4e2a\u6027\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5168\u5c40\u6a21\u578b\u6295\u5f71\u5230\u7528\u6237\u672c\u5730\u6a21\u578b\u7684\u90bb\u57df\uff0c\u5b9e\u73b0\u5168\u5c40\u6cdb\u5316\u4e0e\u672c\u5730\u7279\u5316\u7684\u53ef\u8c03\u8282\u6743\u8861\u3002", "motivation": "\u73b0\u6709\u8d1d\u53f6\u65af\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56MCMC\u91c7\u6837\u6216\u53d8\u5206\u63a8\u65ad\uff0c\u9700\u8981\u4e2a\u6027\u5316\u673a\u5236\u6765\u9002\u5e94\u5f02\u6784\u6570\u636e\u5206\u5e03\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u4e14\u80fd\u7075\u6d3b\u6743\u8861\u5168\u5c40\u4e0e\u672c\u5730\u6027\u80fd\u7684\u4e2a\u6027\u5316\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4fe1\u606f\u51e0\u4f55\u6295\u5f71\u6846\u67b6\uff0c\u5c06\u5168\u5c40\u6a21\u578b\u6295\u5f71\u5230\u7528\u6237\u672c\u5730\u6a21\u578b\u7684\u7edf\u8ba1\u6d41\u5f62\u90bb\u57df\uff0c\u8bc1\u660e\u8be5\u6295\u5f71\u7b49\u4ef7\u4e8e\u8ba1\u7b97\u7edf\u8ba1\u6d41\u5f62\u4e0a\u7684\u91cd\u5fc3\uff0c\u4ece\u800c\u83b7\u5f97\u95ed\u5f0f\u89e3\u548c\u96f6\u6210\u672c\u4e2a\u6027\u5316\u3002\u7ed3\u5408IVON\u4f18\u5316\u5668\u5e94\u7528\u4e8e\u53d8\u5206\u5b66\u4e60\u8bbe\u7f6e\u3002", "result": "\u5728\u5f02\u6784\u6570\u636e\u5206\u5e03\u4e0b\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5e73\u8861\u5168\u5c40\u548c\u672c\u5730\u6027\u80fd\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002", "conclusion": "\u63d0\u51fa\u7684\u4fe1\u606f\u51e0\u4f55\u6295\u5f71\u6846\u67b6\u4e3a\u8d1d\u53f6\u65af\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u3001\u53ef\u8c03\u8282\u7684\u4e2a\u6027\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5168\u5c40\u6a21\u578b\u4e0e\u672c\u5730\u7279\u5316\u7684\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2509.09871", "categories": ["cs.CL", "cs.AI", "68T50 (Primary) 91F10 (Secondary)"], "pdf": "https://arxiv.org/pdf/2509.09871", "abs": "https://arxiv.org/abs/2509.09871", "authors": ["Basti\u00e1n Gonz\u00e1lez-Bustamante", "Nando Verelst", "Carla Cisternas"], "title": "Emulating Public Opinion: A Proof-of-Concept of AI-Generated Synthetic Survey Responses for the Chilean Case", "comment": "Working paper: 18 pages, 4 tables, 2 figures", "summary": "Large Language Models (LLMs) offer promising avenues for methodological and\napplied innovations in survey research by using synthetic respondents to\nemulate human answers and behaviour, potentially mitigating measurement and\nrepresentation errors. However, the extent to which LLMs recover aggregate item\ndistributions remains uncertain and downstream applications risk reproducing\nsocial stereotypes and biases inherited from training data. We evaluate the\nreliability of LLM-generated synthetic survey responses against ground-truth\nhuman responses from a Chilean public opinion probabilistic survey.\nSpecifically, we benchmark 128 prompt-model-question triplets, generating\n189,696 synthetic profiles, and pool performance metrics (i.e., accuracy,\nprecision, recall, and F1-score) in a meta-analysis across 128\nquestion-subsample pairs to test for biases along key sociodemographic\ndimensions. The evaluation spans OpenAI's GPT family and o-series reasoning\nmodels, as well as Llama and Qwen checkpoints. Three results stand out. First,\nsynthetic responses achieve excellent performance on trust items (F1-score and\naccuracy > 0.90). Second, GPT-4o, GPT-4o-mini and Llama 4 Maverick perform\ncomparably on this task. Third, synthetic-human alignment is highest among\nrespondents aged 45-59. Overall, LLM-based synthetic samples approximate\nresponses from a probabilistic sample, though with substantial item-level\nheterogeneity. Capturing the full nuance of public opinion remains challenging\nand requires careful calibration and additional distributional tests to ensure\nalgorithmic fidelity and reduce errors.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u751f\u6210\u5408\u6210\u8c03\u67e5\u56de\u590d\u7684\u53ef\u9760\u6027\uff0c\u901a\u8fc7\u4e0e\u667a\u5229\u6982\u7387\u62bd\u6837\u8c03\u67e5\u7684\u771f\u5b9e\u4eba\u7c7b\u56de\u590d\u5bf9\u6bd4\uff0c\u53d1\u73b0LLM\u5728\u4fe1\u4efb\u7c7b\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02(F1\u5206\u6570>0.90)\uff0c\u4f46\u5728\u6355\u6349\u516c\u4f17\u610f\u89c1\u7684\u7ec6\u5fae\u5dee\u522b\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\u3002", "motivation": "LLM\u4e3a\u8c03\u67e5\u7814\u7a76\u63d0\u4f9b\u4e86\u4f7f\u7528\u5408\u6210\u53d7\u8bbf\u8005\u6a21\u62df\u4eba\u7c7b\u7b54\u6848\u7684\u65b0\u65b9\u6cd5\uff0c\u53ef\u80fd\u51cf\u5c11\u6d4b\u91cf\u548c\u4ee3\u8868\u6027\u8bef\u5dee\uff0c\u4f46\u5176\u6062\u590d\u805a\u5408\u9879\u76ee\u5206\u5e03\u7684\u7a0b\u5ea6\u4ee5\u53ca\u53ef\u80fd\u590d\u5236\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u793e\u4f1a\u523b\u677f\u5370\u8c61\u548c\u504f\u89c1\u7684\u95ee\u9898\u4ecd\u9700\u9a8c\u8bc1\u3002", "method": "\u901a\u8fc7128\u4e2a\u63d0\u793a-\u6a21\u578b-\u95ee\u9898\u4e09\u5143\u7ec4\u751f\u6210189,696\u4e2a\u5408\u6210\u914d\u7f6e\u6587\u4ef6\uff0c\u4f7f\u7528\u51c6\u786e\u6027\u3001\u7cbe\u786e\u5ea6\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u7b49\u6027\u80fd\u6307\u6807\u8fdb\u884c\u5143\u5206\u6790\uff0c\u6d4b\u8bd5\u5173\u952e\u793e\u4f1a\u4eba\u53e3\u7edf\u8ba1\u7ef4\u5ea6\u4e0a\u7684\u504f\u89c1\uff0c\u8bc4\u4f30\u4e86OpenAI\u7684GPT\u7cfb\u5217\u3001o\u7cfb\u5217\u63a8\u7406\u6a21\u578b\u4ee5\u53caLlama\u548cQwen\u68c0\u67e5\u70b9\u3002", "result": "1) \u5408\u6210\u56de\u590d\u5728\u4fe1\u4efb\u9879\u76ee\u4e0a\u8868\u73b0\u4f18\u5f02(F1\u5206\u6570\u548c\u51c6\u786e\u6027>0.90)\uff1b2) GPT-4o\u3001GPT-4o-mini\u548cLlama 4 Maverick\u5728\u6b64\u4efb\u52a1\u4e0a\u8868\u73b0\u76f8\u5f53\uff1b3) 45-59\u5c81\u53d7\u8bbf\u8005\u7684\u5408\u6210-\u4eba\u7c7b\u5bf9\u9f50\u5ea6\u6700\u9ad8\u3002LLM\u5408\u6210\u6837\u672c\u80fd\u591f\u8fd1\u4f3c\u6982\u7387\u6837\u672c\u7684\u56de\u590d\uff0c\u4f46\u5b58\u5728\u663e\u8457\u7684\u9879\u76ee\u7ea7\u5f02\u8d28\u6027\u3002", "conclusion": "\u57fa\u4e8eLLM\u7684\u5408\u6210\u6837\u672c\u53ef\u4ee5\u8fd1\u4f3c\u6982\u7387\u6837\u672c\u7684\u56de\u590d\uff0c\u4f46\u8981\u5b8c\u5168\u6355\u6349\u516c\u4f17\u610f\u89c1\u7684\u7ec6\u5fae\u5dee\u522b\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u4ed4\u7ec6\u6821\u51c6\u548c\u989d\u5916\u7684\u5206\u5e03\u6d4b\u8bd5\u6765\u786e\u4fdd\u7b97\u6cd5\u4fdd\u771f\u5ea6\u548c\u51cf\u5c11\u8bef\u5dee\u3002"}}
{"id": "2509.10151", "categories": ["cs.LG", "cs.AI", "I.2.1"], "pdf": "https://arxiv.org/pdf/2509.10151", "abs": "https://arxiv.org/abs/2509.10151", "authors": ["Riccardo Lunelli", "Angus Nicolson", "Samuel Martin Pr\u00f6ll", "Sebastian Johannes Reinstadler", "Axel Bauer", "Clemens Dlaska"], "title": "BenchECG and xECG: a benchmark and baseline for ECG foundation models", "comment": "32 pages, 4 figures, 22 tables", "summary": "Electrocardiograms (ECGs) are inexpensive, widely used, and well-suited to\ndeep learning. Recently, interest has grown in developing foundation models for\nECGs - models that generalise across diverse downstream tasks. However,\nconsistent evaluation has been lacking: prior work often uses narrow task\nselections and inconsistent datasets, hindering fair comparison. Here, we\nintroduce BenchECG, a standardised benchmark comprising a comprehensive suite\nof publicly available ECG datasets and versatile tasks. We also propose xECG,\nan xLSTM-based recurrent model trained with SimDINOv2 self-supervised learning,\nwhich achieves the best BenchECG score compared to publicly available\nstate-of-the-art models. In particular, xECG is the only publicly available\nmodel to perform strongly on all datasets and tasks. By standardising\nevaluation, BenchECG enables rigorous comparison and aims to accelerate\nprogress in ECG representation learning. xECG achieves superior performance\nover earlier approaches, defining a new baseline for future ECG foundation\nmodels.", "AI": {"tldr": "\u63d0\u51fa\u4e86BenchECG\u6807\u51c6\u5316\u57fa\u51c6\u548cxECG\u6a21\u578b\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\u89e3\u51b3ECG\u57fa\u7840\u6a21\u578b\u7f3a\u4e4f\u516c\u5e73\u6bd4\u8f83\u7684\u95ee\u9898\uff0cxECG\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u4efb\u52a1\u4e0a\u8868\u73b0\u6700\u4f18\u3002", "motivation": "ECG\u57fa\u7840\u6a21\u578b\u7f3a\u4e4f\u4e00\u81f4\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u5148\u524d\u7814\u7a76\u4f7f\u7528\u4e0d\u540c\u7684\u4efb\u52a1\u548c\u6570\u636e\u96c6\uff0c\u963b\u788d\u4e86\u516c\u5e73\u6bd4\u8f83\uff0c\u9700\u8981\u5efa\u7acb\u6807\u51c6\u5316\u57fa\u51c6\u6765\u63a8\u52a8ECG\u8868\u793a\u5b66\u4e60\u7684\u53d1\u5c55\u3002", "method": "\u5f00\u53d1BenchECG\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u5305\u542b\u516c\u5f00ECG\u6570\u636e\u96c6\u548c\u591a\u6837\u5316\u4efb\u52a1\uff1b\u63d0\u51fa\u57fa\u4e8exLSTM\u7684xECG\u6a21\u578b\uff0c\u91c7\u7528SimDINOv2\u81ea\u76d1\u7763\u5b66\u4e60\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "xECG\u5728BenchECG\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6700\u4f73\u5206\u6570\uff0c\u662f\u552f\u4e00\u5728\u6240\u6709\u6570\u636e\u96c6\u548c\u4efb\u52a1\u4e0a\u90fd\u8868\u73b0\u4f18\u5f02\u7684\u516c\u5f00\u53ef\u7528\u6a21\u578b\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "BenchECG\u901a\u8fc7\u6807\u51c6\u5316\u8bc4\u4f30\u5b9e\u73b0\u4e86\u4e25\u683c\u6bd4\u8f83\uff0cxECG\u4e3a\u672a\u6765ECG\u57fa\u7840\u6a21\u578b\u8bbe\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u57fa\u51c6\uff0c\u5c06\u52a0\u901fECG\u8868\u793a\u5b66\u4e60\u9886\u57df\u7684\u8fdb\u5c55\u3002"}}
{"id": "2509.09969", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09969", "abs": "https://arxiv.org/abs/2509.09969", "authors": ["Zhitian Hou", "Zihan Ye", "Nanli Zeng", "Tianyong Hao", "Kun Zeng"], "title": "Large Language Models Meet Legal Artificial Intelligence: A Survey", "comment": null, "summary": "Large Language Models (LLMs) have significantly advanced the development of\nLegal Artificial Intelligence (Legal AI) in recent years, enhancing the\nefficiency and accuracy of legal tasks. To advance research and applications of\nLLM-based approaches in legal domain, this paper provides a comprehensive\nreview of 16 legal LLMs series and 47 LLM-based frameworks for legal tasks, and\nalso gather 15 benchmarks and 29 datasets to evaluate different legal\ncapabilities. Additionally, we analyse the challenges and discuss future\ndirections for LLM-based approaches in the legal domain. We hope this paper\nprovides a systematic introduction for beginners and encourages future research\nin this field. Resources are available at\nhttps://github.com/ZhitianHou/LLMs4LegalAI.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6cd5\u5f8bAI\u9886\u57df\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u6db5\u76d6\u4e8616\u4e2a\u6cd5\u5f8bLLM\u7cfb\u5217\u300147\u4e2aLLM\u6846\u67b6\u300115\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c29\u4e2a\u6570\u636e\u96c6\uff0c\u5e76\u5206\u6790\u4e86\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u63a8\u52a8\u6cd5\u5f8b\u4eba\u5de5\u667a\u80fd\u9886\u57df\u7684\u53d1\u5c55\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6cd5\u5f8b\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u7cfb\u7edf\u6027\u7684\u6307\u5bfc\u548c\u8d44\u6e90\u652f\u6301\uff0c\u5e2e\u52a9\u521d\u5b66\u8005\u5165\u95e8\u5e76\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u5168\u9762\u7efc\u8ff0\u548c\u5206\u6790\u73b0\u6709\u6cd5\u5f8bLLM\u6a21\u578b\u3001\u6846\u67b6\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u68b3\u7406\u6cd5\u5f8bAI\u9886\u57df\u7684\u6280\u672f\u73b0\u72b6\u548c\u53d1\u5c55\u8d8b\u52bf\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u6cd5\u5f8bAI\u8d44\u6e90\u4f53\u7cfb\uff0c\u5305\u62ec\u6a21\u578b\u3001\u6846\u67b6\u3001\u8bc4\u4f30\u5de5\u5177\u548c\u6570\u636e\u8d44\u6e90\uff0c\u4e3a\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6280\u672f\u53c2\u8003\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6cd5\u5f8bAI\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u4ecd\u9762\u4e34\u8bf8\u591a\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u53d1\u5c55\uff0c\u672c\u6587\u4e3a\u8fd9\u4e00\u9886\u57df\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u57fa\u7840\u548c\u652f\u6301\u3002"}}
{"id": "2509.10161", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.10161", "abs": "https://arxiv.org/abs/2509.10161", "authors": ["Shiwei Li", "Qunwei Li", "Haozhao Wang", "Ruixuan Li", "Jianbin Lin", "Wenliang Zhong"], "title": "FedBiF: Communication-Efficient Federated Learning via Bits Freezing", "comment": "Accepted by TPDS", "summary": "Federated learning (FL) is an emerging distributed machine learning paradigm\nthat enables collaborative model training without sharing local data. Despite\nits advantages, FL suffers from substantial communication overhead, which can\naffect training efficiency. Recent efforts have mitigated this issue by\nquantizing model updates to reduce communication costs. However, most existing\nmethods apply quantization only after local training, introducing quantization\nerrors into the trained parameters and potentially degrading model accuracy. In\nthis paper, we propose Federated Bit Freezing (FedBiF), a novel FL framework\nthat directly learns quantized model parameters during local training. In each\ncommunication round, the server first quantizes the model parameters and\ntransmits them to the clients. FedBiF then allows each client to update only a\nsingle bit of the multi-bit parameter representation, freezing the remaining\nbits. This bit-by-bit update strategy reduces each parameter update to one bit\nwhile maintaining high precision in parameter representation. Extensive\nexperiments are conducted on five widely used datasets under both IID and\nNon-IID settings. The results demonstrate that FedBiF not only achieves\nsuperior communication compression but also promotes sparsity in the resulting\nmodels. Notably, FedBiF attains accuracy comparable to FedAvg, even when using\nonly 1 bit-per-parameter (bpp) for uplink and 3 bpp for downlink communication.\nThe code is available at https://github.com/Leopold1423/fedbif-tpds25.", "AI": {"tldr": "\u63d0\u51fa\u4e86FedBiF\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u672c\u5730\u8bad\u7ec3\u671f\u95f4\u76f4\u63a5\u5b66\u4e60\u91cf\u5316\u6a21\u578b\u53c2\u6570\uff0c\u9010\u6bd4\u7279\u66f4\u65b0\u53c2\u6570\u6765\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5b58\u5728\u663e\u8457\u7684\u901a\u4fe1\u5f00\u9500\u95ee\u9898\uff0c\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u901a\u5e38\u5728\u672c\u5730\u8bad\u7ec3\u540e\u5e94\u7528\u91cf\u5316\uff0c\u5bfc\u81f4\u91cf\u5316\u8bef\u5dee\u5f71\u54cd\u6a21\u578b\u7cbe\u5ea6\u3002", "method": "\u670d\u52a1\u5668\u5148\u91cf\u5316\u6a21\u578b\u53c2\u6570\u5e76\u53d1\u9001\u7ed9\u5ba2\u6237\u7aef\uff0c\u6bcf\u4e2a\u5ba2\u6237\u7aef\u6bcf\u6b21\u53ea\u66f4\u65b0\u591a\u6bd4\u7279\u53c2\u6570\u8868\u793a\u4e2d\u7684\u5355\u4e2a\u6bd4\u7279\uff0c\u51bb\u7ed3\u5176\u4f59\u6bd4\u7279\uff0c\u5b9e\u73b0\u9010\u6bd4\u7279\u66f4\u65b0\u7b56\u7565\u3002", "result": "\u57285\u4e2a\u5e38\u7528\u6570\u636e\u96c6\u4e0a\u7684IID\u548c\u975eIID\u8bbe\u7f6e\u4e0b\uff0cFedBiF\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u901a\u4fe1\u538b\u7f29\uff0c\u4fc3\u8fdb\u6a21\u578b\u7a00\u758f\u6027\uff0c\u4ec5\u4f7f\u75281bpp\u4e0a\u884c\u548c3bpp\u4e0b\u884c\u901a\u4fe1\u5373\u53ef\u8fbe\u5230\u4e0eFedAvg\u76f8\u5f53\u7684\u7cbe\u5ea6\u3002", "conclusion": "FedBiF\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u7684\u901a\u4fe1\u5f00\u9500\u95ee\u9898\uff0c\u901a\u8fc7\u76f4\u63a5\u5b66\u4e60\u91cf\u5316\u53c2\u6570\u548c\u9010\u6bd4\u7279\u66f4\u65b0\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u901a\u4fe1\u6210\u672c\u3002"}}
{"id": "2509.09990", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09990", "abs": "https://arxiv.org/abs/2509.09990", "authors": ["Guixian Xu", "Zeli Su", "Ziyin Zhang", "Jianing Liu", "XU Han", "Ting Zhang", "Yushuang Dong"], "title": "CMHG: A Dataset and Benchmark for Headline Generation of Minority Languages in China", "comment": null, "summary": "Minority languages in China, such as Tibetan, Uyghur, and Traditional\nMongolian, face significant challenges due to their unique writing systems,\nwhich differ from international standards. This discrepancy has led to a severe\nlack of relevant corpora, particularly for supervised tasks like headline\ngeneration. To address this gap, we introduce a novel dataset, Chinese Minority\nHeadline Generation (CMHG), which includes 100,000 entries for Tibetan, and\n50,000 entries each for Uyghur and Mongolian, specifically curated for headline\ngeneration tasks. Additionally, we propose a high-quality test set annotated by\nnative speakers, designed to serve as a benchmark for future research in this\ndomain. We hope this dataset will become a valuable resource for advancing\nheadline generation in Chinese minority languages and contribute to the\ndevelopment of related benchmarks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u4e2d\u56fd\u5c11\u6570\u6c11\u65cf\u8bed\u8a00\uff08\u85cf\u8bed\u3001\u7ef4\u543e\u5c14\u8bed\u3001\u8499\u53e4\u8bed\uff09\u6807\u9898\u751f\u6210\u4efb\u52a1\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86CMHG\u6570\u636e\u96c6\uff0c\u5305\u542b20\u4e07\u6761\u6807\u6ce8\u6570\u636e\uff0c\u5e76\u5efa\u7acb\u4e86\u9ad8\u8d28\u91cf\u6d4b\u8bd5\u96c6\u4f5c\u4e3a\u57fa\u51c6\u3002", "motivation": "\u4e2d\u56fd\u5c11\u6570\u6c11\u65cf\u8bed\u8a00\u56e0\u5176\u72ec\u7279\u7684\u4e66\u5199\u7cfb\u7edf\u4e0e\u56fd\u9645\u6807\u51c6\u4e0d\u7b26\uff0c\u5bfc\u81f4\u76f8\u5173\u8bed\u6599\u5e93\u4e25\u91cd\u7f3a\u4e4f\uff0c\u7279\u522b\u662f\u5728\u6807\u9898\u751f\u6210\u7b49\u76d1\u7763\u4efb\u52a1\u4e2d\u3002\u8fd9\u79cd\u6570\u636e\u7a00\u7f3a\u6027\u963b\u788d\u4e86\u76f8\u5173NLP\u6280\u672f\u7684\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u4e86Chinese Minority Headline Generation (CMHG)\u6570\u636e\u96c6\uff0c\u5305\u542b10\u4e07\u6761\u85cf\u8bed\u30015\u4e07\u6761\u7ef4\u543e\u5c14\u8bed\u548c5\u4e07\u6761\u8499\u53e4\u8bed\u6570\u636e\uff0c\u4e13\u95e8\u7528\u4e8e\u6807\u9898\u751f\u6210\u4efb\u52a1\u3002\u540c\u65f6\u521b\u5efa\u4e86\u7531\u6bcd\u8bed\u8005\u6807\u6ce8\u7684\u9ad8\u8d28\u91cf\u6d4b\u8bd5\u96c6\u4f5c\u4e3a\u57fa\u51c6\u3002", "result": "\u6210\u529f\u5efa\u7acb\u4e86\u5305\u542b20\u4e07\u6761\u6807\u6ce8\u6570\u636e\u7684\u5c11\u6570\u6c11\u65cf\u8bed\u8a00\u6807\u9898\u751f\u6210\u6570\u636e\u96c6\uff0c\u4e3a\u85cf\u8bed\u3001\u7ef4\u543e\u5c14\u8bed\u548c\u8499\u53e4\u8bed\u63d0\u4f9b\u4e86\u9996\u4e2a\u4e13\u95e8\u7684\u6807\u9898\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u96c6\u3002", "conclusion": "CMHG\u6570\u636e\u96c6\u5c06\u6210\u4e3a\u63a8\u52a8\u4e2d\u56fd\u5c11\u6570\u6c11\u65cf\u8bed\u8a00\u6807\u9898\u751f\u6210\u7814\u7a76\u7684\u91cd\u8981\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u76f8\u5173\u57fa\u51c6\u6d4b\u8bd5\u7684\u53d1\u5c55\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u6570\u636e\u7a7a\u767d\u3002"}}
{"id": "2509.10163", "categories": ["cs.LG", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.10163", "abs": "https://arxiv.org/abs/2509.10163", "authors": ["Francisco Javier Esono Nkulu Andong", "Qi Min"], "title": "Federated Multi-Agent Reinforcement Learning for Privacy-Preserving and Energy-Aware Resource Management in 6G Edge Networks", "comment": null, "summary": "As sixth-generation (6G) networks move toward ultra-dense, intelligent edge\nenvironments, efficient resource management under stringent privacy, mobility,\nand energy constraints becomes critical. This paper introduces a novel\nFederated Multi-Agent Reinforcement Learning (Fed-MARL) framework that\nincorporates cross-layer orchestration of both the MAC layer and application\nlayer for energy-efficient, privacy-preserving, and real-time resource\nmanagement across heterogeneous edge devices. Each agent uses a Deep Recurrent\nQ-Network (DRQN) to learn decentralized policies for task offloading, spectrum\naccess, and CPU energy adaptation based on local observations (e.g., queue\nlength, energy, CPU usage, and mobility). To protect privacy, we introduce a\nsecure aggregation protocol based on elliptic curve Diffie Hellman key\nexchange, which ensures accurate model updates without exposing raw data to\nsemi-honest adversaries. We formulate the resource management problem as a\npartially observable multi-agent Markov decision process (POMMDP) with a\nmulti-objective reward function that jointly optimizes latency, energy\nefficiency, spectral efficiency, fairness, and reliability under 6G-specific\nservice requirements such as URLLC, eMBB, and mMTC. Simulation results\ndemonstrate that Fed-MARL outperforms centralized MARL and heuristic baselines\nin task success rate, latency, energy efficiency, and fairness, while ensuring\nrobust privacy protection and scalability in dynamic, resource-constrained 6G\nedge networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8054\u90a6\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08Fed-MARL\uff09\u76846G\u7f51\u7edc\u8d44\u6e90\u7ba1\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u5c42\u7f16\u6392\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u3001\u80fd\u6548\u4f18\u5316\u548c\u5b9e\u65f6\u8d44\u6e90\u5206\u914d", "motivation": "6G\u7f51\u7edc\u5411\u8d85\u5bc6\u96c6\u667a\u80fd\u8fb9\u7f18\u73af\u5883\u53d1\u5c55\uff0c\u9700\u8981\u5728\u4e25\u683c\u9690\u79c1\u3001\u79fb\u52a8\u6027\u548c\u80fd\u8017\u7ea6\u675f\u4e0b\u5b9e\u73b0\u9ad8\u6548\u8d44\u6e90\u7ba1\u7406", "method": "\u4f7f\u7528\u6df1\u5ea6\u5faa\u73afQ\u7f51\u7edc\uff08DRQN\uff09\u5b66\u4e60\u53bb\u4e2d\u5fc3\u5316\u7b56\u7565\uff0c\u7ed3\u5408\u692d\u5706\u66f2\u7ebfDiffie-Hellman\u5bc6\u94a5\u4ea4\u6362\u7684\u5b89\u5168\u805a\u5408\u534f\u8bae\u4fdd\u62a4\u9690\u79c1\uff0c\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u591a\u667a\u80fd\u4f53\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b", "result": "\u4eff\u771f\u663e\u793aFed-MARL\u5728\u4efb\u52a1\u6210\u529f\u7387\u3001\u5ef6\u8fdf\u3001\u80fd\u6548\u548c\u516c\u5e73\u6027\u65b9\u9762\u4f18\u4e8e\u96c6\u4e2d\u5f0fMARL\u548c\u542f\u53d1\u5f0f\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u786e\u4fdd\u9690\u79c1\u4fdd\u62a4\u548c\u53ef\u6269\u5c55\u6027", "conclusion": "\u8be5\u6846\u67b6\u4e3a6G\u5f02\u6784\u8fb9\u7f18\u8bbe\u5907\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u9690\u79c1\u4fdd\u62a4\u5b9e\u65f6\u8d44\u6e90\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6ee1\u8db3URLLC\u3001eMBB\u548cmMTC\u7b496G\u7279\u5b9a\u670d\u52a1\u9700\u6c42"}}
{"id": "2509.10004", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10004", "abs": "https://arxiv.org/abs/2509.10004", "authors": ["Ponhvoan Srey", "Xiaobao Wu", "Anh Tuan Luu"], "title": "Unsupervised Hallucination Detection by Inspecting Reasoning Processes", "comment": "To appear in EMNLP 2025", "summary": "Unsupervised hallucination detection aims to identify hallucinated content\ngenerated by large language models (LLMs) without relying on labeled data.\nWhile unsupervised methods have gained popularity by eliminating\nlabor-intensive human annotations, they frequently rely on proxy signals\nunrelated to factual correctness. This misalignment biases detection probes\ntoward superficial or non-truth-related aspects, limiting generalizability\nacross datasets and scenarios. To overcome these limitations, we propose IRIS,\nan unsupervised hallucination detection framework, leveraging internal\nrepresentations intrinsic to factual correctness. IRIS prompts the LLM to\ncarefully verify the truthfulness of a given statement, and obtain its\ncontextualized embedding as informative features for training. Meanwhile, the\nuncertainty of each response is considered a soft pseudolabel for truthfulness.\nExperimental results demonstrate that IRIS consistently outperforms existing\nunsupervised methods. Our approach is fully unsupervised, computationally low\ncost, and works well even with few training data, making it suitable for\nreal-time detection.", "AI": {"tldr": "IRIS\u662f\u4e00\u4e2a\u65e0\u76d1\u7763\u5e7b\u89c9\u68c0\u6d4b\u6846\u67b6\uff0c\u5229\u7528LLM\u5185\u90e8\u8868\u5f81\u6765\u8bc6\u522b\u751f\u6210\u5185\u5bb9\u4e2d\u7684\u4e8b\u5b9e\u9519\u8bef\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\uff0c\u8ba1\u7b97\u6210\u672c\u4f4e\u4e14\u9002\u7528\u4e8e\u5b9e\u65f6\u68c0\u6d4b", "motivation": "\u73b0\u6709\u65e0\u76d1\u7763\u65b9\u6cd5\u4f9d\u8d56\u4e0e\u4e8b\u5b9e\u6b63\u786e\u6027\u65e0\u5173\u7684\u4ee3\u7406\u4fe1\u53f7\uff0c\u5bfc\u81f4\u68c0\u6d4b\u504f\u5411\u8868\u9762\u7279\u5f81\uff0c\u9650\u5236\u4e86\u8de8\u6570\u636e\u96c6\u548c\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b", "method": "\u901a\u8fc7\u63d0\u793aLLM\u4ed4\u7ec6\u9a8c\u8bc1\u9648\u8ff0\u7684\u771f\u5b9e\u6027\uff0c\u83b7\u53d6\u5176\u60c5\u5883\u5316\u5d4c\u5165\u4f5c\u4e3a\u7279\u5f81\uff0c\u5e76\u5c06\u54cd\u5e94\u4e0d\u786e\u5b9a\u6027\u4f5c\u4e3a\u771f\u5b9e\u6027\u7684\u8f6f\u4f2a\u6807\u7b7e", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eIRIS consistently outperforms existing unsupervised methods\uff0c\u5728\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u4e0b\u4e5f\u80fd\u826f\u597d\u5de5\u4f5c", "conclusion": "IRIS\u662f\u4e00\u4e2a\u5b8c\u5168\u65e0\u76d1\u7763\u3001\u8ba1\u7b97\u6210\u672c\u4f4e\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u68c0\u6d4bLLM\u751f\u6210\u7684\u5e7b\u89c9\u5185\u5bb9\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\u573a\u666f"}}
{"id": "2509.10164", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2509.10164", "abs": "https://arxiv.org/abs/2509.10164", "authors": ["Hoshitaro Ohnishi", "Hideo Mukai"], "title": "A Symmetry-Integrated Approach to Surface Code Decoding", "comment": "12 pages, 6 figures", "summary": "Quantum error correction, which utilizes logical qubits that are encoded as\nredundant multiple physical qubits to find and correct errors in physical\nqubits, is indispensable for practical quantum computing. Surface code is\nconsidered to be a promising encoding method with a high error threshold that\nis defined by stabilizer generators. However, previous methods have suffered\nfrom the problem that the decoder acquires solely the error probability\ndistribution because of the non-uniqueness of correct prediction obtained from\nthe input. To circumvent this problem, we propose a technique to reoptimize the\ndecoder model by approximating syndrome measurements with a continuous function\nthat is mathematically interpolated by neural network. We evaluated the\nimprovement in accuracy of a multilayer perceptron based decoder for code\ndistances of 5 and 7 as well as for decoders based on convolutional and\nrecurrent neural networks and transformers for a code distance of 5. In all\ncases, the reoptimized decoder gave better accuracy than the original models,\ndemonstrating the universal effectiveness of the proposed method that is\nindependent of code distance or network architecture. These results suggest\nthat re-framing the problem of surface code decoding into a regression problem\nthat can be tackled by deep learning is a useful strategy.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u7efc\u5408\u5f81\u6d4b\u91cf\u7684\u8fde\u7eed\u51fd\u6570\u6765\u91cd\u65b0\u4f18\u5316\u8868\u9762\u7801\u89e3\u7801\u5668\u7684\u6280\u672f\uff0c\u63d0\u9ad8\u4e86\u91cf\u5b50\u7ea0\u9519\u89e3\u7801\u7684\u51c6\u786e\u6027", "motivation": "\u4f20\u7edf\u89e3\u7801\u5668\u53ea\u80fd\u83b7\u53d6\u8bef\u5dee\u6982\u7387\u5206\u5e03\uff0c\u7531\u4e8e\u8f93\u5165\u9884\u6d4b\u7ed3\u679c\u7684\u4e0d\u552f\u4e00\u6027\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u4ee5\u63d0\u9ad8\u89e3\u7801\u7cbe\u5ea6", "method": "\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u6570\u5b66\u63d2\u503c\u8fd1\u4f3c\u7efc\u5408\u5f81\u6d4b\u91cf\uff0c\u5c06\u89e3\u7801\u95ee\u9898\u91cd\u6784\u4e3a\u56de\u5f52\u95ee\u9898\uff0c\u91c7\u7528\u591a\u5c42\u611f\u77e5\u673a\u3001\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3001\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u548cTransformer\u7b49\u67b6\u6784", "result": "\u5728\u7801\u8ddd5\u548c7\u7684\u6240\u6709\u60c5\u51b5\u4e0b\uff0c\u91cd\u65b0\u4f18\u5316\u7684\u89e3\u7801\u5668\u90fd\u6bd4\u539f\u59cb\u6a21\u578b\u7cbe\u5ea6\u66f4\u9ad8\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u666e\u9002\u6709\u6548\u6027", "conclusion": "\u5c06\u8868\u9762\u7801\u89e3\u7801\u95ee\u9898\u91cd\u65b0\u6784\u5efa\u4e3a\u53ef\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u89e3\u51b3\u7684\u56de\u5f52\u95ee\u9898\u662f\u4e00\u4e2a\u6709\u7528\u7684\u7b56\u7565"}}
{"id": "2509.10010", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.10010", "abs": "https://arxiv.org/abs/2509.10010", "authors": ["Adnan Ahmad", "Philine Kowol", "Stefan Hillmann", "Sebastian M\u00f6ller"], "title": "Multi-Intent Recognition in Dialogue Understanding: A Comparison Between Smaller Open-Source LLMs", "comment": null, "summary": "In this paper, we provide an extensive analysis of multi-label intent\nclassification using Large Language Models (LLMs) that are open-source,\npublicly available, and can be run in consumer hardware. We use the MultiWOZ\n2.1 dataset, a benchmark in the dialogue system domain, to investigate the\nefficacy of three popular open-source pre-trained LLMs, namely LLama2-7B-hf,\nMistral-7B-v0.1, and Yi-6B. We perform the classification task in a few-shot\nsetup, giving 20 examples in the prompt with some instructions. Our approach\nfocuses on the differences in performance of these models across several\nperformance metrics by methodically assessing these models on multi-label\nintent classification tasks. Additionally, we compare the performance of the\ninstruction-based fine-tuning approach with supervised learning using the\nsmaller transformer model BertForSequenceClassification as a baseline. To\nevaluate the performance of the models, we use evaluation metrics like\naccuracy, precision, and recall as well as micro, macro, and weighted F1 score.\nWe also report the inference time, VRAM requirements, etc. The Mistral-7B-v0.1\noutperforms two other generative models on 11 intent classes out of 14 in terms\nof F-Score, with a weighted average of 0.50. It also has relatively lower\nHumming Loss and higher Jaccard Similarity, making it the winning model in the\nfew-shot setting. We find BERT based supervised classifier having superior\nperformance compared to the best performing few-shot generative LLM. The study\nprovides a framework for small open-source LLMs in detecting complex\nmulti-intent dialogues, enhancing the Natural Language Understanding aspect of\ntask-oriented chatbots.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6807\u7b7e\u610f\u56fe\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\uff0c\u5728MultiWOZ 2.1\u6570\u636e\u96c6\u4e0a\u6bd4\u8f83\u4e86LLama2-7B\u3001Mistral-7B\u548cYi-6B\u6a21\u578b\uff0c\u53d1\u73b0Mistral-7B\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u76d1\u7763\u5b66\u4e60\u7684BERT\u6a21\u578b\u4ecd\u4f18\u4e8e\u751f\u6210\u5f0fLLM\u3002", "motivation": "\u7814\u7a76\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u5904\u7406\u591a\u6807\u7b7e\u610f\u56fe\u5206\u7c7b\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u4e3a\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u63d0\u4f9b\u5b9e\u7528\u6846\u67b6\u3002", "method": "\u4f7f\u7528MultiWOZ 2.1\u6570\u636e\u96c6\uff0c\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\uff08\u63d0\u793a\u4e2d\u5305\u542b20\u4e2a\u793a\u4f8b\uff09\u6d4b\u8bd5\u4e09\u4e2a\u5f00\u6e90LLM\uff0c\u5e76\u4e0e\u57fa\u4e8eBERT\u7684\u76d1\u7763\u5b66\u4e60\u57fa\u7ebf\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\uff0c\u8bc4\u4f30\u591a\u4e2a\u6027\u80fd\u6307\u6807\u3002", "result": "Mistral-7B-v0.1\u572814\u4e2a\u610f\u56fe\u7c7b\u522b\u4e2d\u768411\u4e2a\u4e0aF\u5206\u6570\u8868\u73b0\u6700\u4f73\uff0c\u52a0\u6743\u5e73\u5747F\u5206\u6570\u4e3a0.50\uff0c\u4f46\u5728\u6574\u4f53\u6027\u80fd\u4e0a\u4ecd\u4e0d\u53ca\u76d1\u7763\u5b66\u4e60\u7684BERT\u5206\u7c7b\u5668\u3002", "conclusion": "\u5f00\u6e90LLM\u5728\u591a\u6807\u7b7e\u610f\u56fe\u5206\u7c7b\u4efb\u52a1\u4e0a\u5c55\u73b0\u51fa\u6f5c\u529b\uff0cMistral-7B\u662f\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2d\u7684\u6700\u4f73\u9009\u62e9\uff0c\u4f46\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u5f53\u524d\u4ecd\u5177\u6709\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2509.10167", "categories": ["cs.LG", "68T07, 60H30, 34F05"], "pdf": "https://arxiv.org/pdf/2509.10167", "abs": "https://arxiv.org/abs/2509.10167", "authors": ["L\u00e9na\u00efc Chizat"], "title": "The Hidden Width of Deep ResNets: Tight Error Bounds and Phase Diagrams", "comment": null, "summary": "We study the gradient-based training of large-depth residual networks\n(ResNets) from standard random initializations. We show that with a diverging\ndepth $L$, a fixed embedding dimension $D$, and an arbitrary hidden width $M$,\nthe training dynamics converges to a Neural Mean ODE training dynamics.\nRemarkably, the limit is independent of the scaling of $M$, covering practical\ncases of, say, Transformers, where $M$ (the number of hidden units or attention\nheads per layer) is typically of the order of $D$. For a residual scale\n$\\Theta_D\\big(\\frac{\\alpha}{LM}\\big)$, we obtain the error bound\n$O_D\\big(\\frac{1}{L}+ \\frac{\\alpha}{\\sqrt{LM}}\\big)$ between the model's output\nand its limit after a fixed number gradient of steps, and we verify empirically\nthat this rate is tight. When $\\alpha=\\Theta(1)$, the limit exhibits complete\nfeature learning, i.e. the Mean ODE is genuinely non-linearly parameterized. In\ncontrast, we show that $\\alpha \\to \\infty$ yields a \\lazy ODE regime where the\nMean ODE is linearly parameterized. We then focus on the particular case of\nResNets with two-layer perceptron blocks, for which we study how these scalings\ndepend on the embedding dimension $D$. We show that for this model, the only\nresidual scale that leads to complete feature learning is\n$\\Theta\\big(\\frac{\\sqrt{D}}{LM}\\big)$. In this regime, we prove the error bound\n$O\\big(\\frac{1}{L}+ \\frac{\\sqrt{D}}{\\sqrt{LM}}\\big)$ between the ResNet and its\nlimit after a fixed number of gradient steps, which is also empirically tight.\nOur convergence results rely on a novel mathematical perspective on ResNets :\n(i) due to the randomness of the initialization, the forward and backward pass\nthrough the ResNet behave as the stochastic approximation of certain mean ODEs,\nand (ii) by propagation of chaos (that is, asymptotic independence of the\nunits) this behavior is preserved through the training dynamics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u6df1\u5ea6\u6b8b\u5dee\u7f51\u7edc(ResNets)\u7684\u68af\u5ea6\u8bad\u7ec3\u52a8\u6001\uff0c\u8bc1\u660e\u4e86\u5728\u6df1\u5ea6\u8d8b\u4e8e\u65e0\u7a77\u65f6\u8bad\u7ec3\u8fc7\u7a0b\u6536\u655b\u5230\u795e\u7ecf\u5e73\u5747ODE\u52a8\u6001\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u6b8b\u5dee\u7f29\u653e\u53c2\u6570\u5bf9\u7279\u5f81\u5b66\u4e60\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u5927\u6df1\u5ea6\u6b8b\u5dee\u7f51\u7edc\u4ece\u6807\u51c6\u968f\u673a\u521d\u59cb\u5316\u5f00\u59cb\u7684\u8bad\u7ec3\u52a8\u6001\uff0c\u7279\u522b\u662f\u6df1\u5ea6\u8d8b\u4e8e\u65e0\u7a77\u65f6\u7684\u6781\u9650\u884c\u4e3a\uff0c\u4ee5\u53ca\u4e0d\u540c\u53c2\u6570\u7f29\u653e\u5bf9\u7279\u5f81\u5b66\u4e60\u80fd\u529b\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u6570\u5b66\u5206\u6790\u6846\u67b6\uff0c\u5c06ResNet\u7684\u524d\u5411\u548c\u540e\u5411\u4f20\u64ad\u89c6\u4e3a\u968f\u673a\u5e73\u5747ODE\u7684\u8fd1\u4f3c\uff0c\u901a\u8fc7\u6df7\u6c8c\u4f20\u64ad\u7406\u8bba\u8bc1\u660e\u8bad\u7ec3\u52a8\u6001\u7684\u6536\u655b\u6027\uff0c\u5e76\u8fdb\u884c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u7ed3\u679c\u3002", "result": "\u8bc1\u660e\u4e86\u6df1\u5ea6\u8d8b\u4e8e\u65e0\u7a77\u65f6\u8bad\u7ec3\u52a8\u6001\u6536\u655b\u5230\u795e\u7ecf\u5e73\u5747ODE\uff1b\u786e\u5b9a\u4e86\u5bfc\u81f4\u5b8c\u5168\u7279\u5f81\u5b66\u4e60\u7684\u6b8b\u5dee\u7f29\u653e\u53c2\u6570\u4e3a\u0398(\u221aD/LM)\uff1b\u7ed9\u51fa\u4e86\u6a21\u578b\u8f93\u51fa\u4e0e\u6781\u9650\u4e4b\u95f4\u7684\u8bef\u5dee\u754c\u9650O(1/L + \u221aD/\u221aLM)\u3002", "conclusion": "\u6df1\u5ea6ResNets\u7684\u8bad\u7ec3\u52a8\u6001\u5728\u6df1\u5ea6\u8d8b\u4e8e\u65e0\u7a77\u65f6\u6536\u655b\u5230\u5e73\u5747ODE\uff0c\u7279\u5b9a\u7684\u6b8b\u5dee\u7f29\u653e\u53c2\u6570\u80fd\u591f\u5b9e\u73b0\u5b8c\u5168\u7279\u5f81\u5b66\u4e60\uff0c\u800c\u8fc7\u5927\u53c2\u6570\u4f1a\u5bfc\u81f4\u61d2\u60f0\u8bad\u7ec3\u673a\u5236\u3002"}}
{"id": "2509.10035", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10035", "abs": "https://arxiv.org/abs/2509.10035", "authors": ["Laurin Plank", "Armin Zlomuzica"], "title": "Linguistic trajectories of bipolar disorder on social media", "comment": "Pre-print", "summary": "Language provides valuable markers of affective disorders such as bipolar\ndisorder (BD), yet clinical assessments remain limited in scale. In response,\nanalyses of social media (SM) language have gained prominence due to their high\ntemporal resolution and longitudinal scope. Here, we introduce a method to\ndetermine the timing of users' diagnoses and apply it to study language\ntrajectories from 3 years before to 21 years after BD diagnosis - contrasted\nwith uses reporting unipolar depression (UD) and non-affected users (HC). We\nshow that BD diagnosis is accompanied by pervasive linguistic alterations\nreflecting mood disturbance, psychiatric comorbidity, substance abuse,\nhospitalization, medical comorbidities, unusual thought content, and\ndisorganized thought. We further observe recurring mood-related language\nchanges across two decades after the diagnosis, with a pronounced 12-month\nperiodicity suggestive of seasonal mood episodes. Finally, trend-level evidence\nsuggests an increased periodicity in users estimated to be female. In sum, our\nfindings provide evidence for language alterations in the acute and chronic\nphase of BD. This validates and extends recent efforts leveraging SM for\nscalable monitoring of mental health.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u793e\u4ea4\u5a92\u4f53\u8bed\u8a00\u5206\u6790\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u786e\u5b9a\u53cc\u76f8\u60c5\u611f\u969c\u788d\u8bca\u65ad\u65f6\u95f4\u7684\u65b9\u6cd5\uff0c\u8ffd\u8e2a\u4e86\u8bca\u65ad\u524d\u540e\u957f\u8fbe24\u5e74\u7684\u8bed\u8a00\u53d8\u5316\u8f68\u8ff9\uff0c\u53d1\u73b0\u4e86\u4e0e\u60c5\u7eea\u969c\u788d\u3001\u5171\u75c5\u7b49\u76f8\u5173\u7684\u8bed\u8a00\u7279\u5f81\uff0c\u5e76\u89c2\u5bdf\u5230\u8bca\u65ad\u540e\u4e8c\u5341\u5e74\u95f4\u5177\u670912\u4e2a\u6708\u5468\u671f\u6027\u7684\u60c5\u7eea\u76f8\u5173\u8bed\u8a00\u53d8\u5316\u3002", "motivation": "\u8bed\u8a00\u662f\u60c5\u611f\u969c\u788d\u7684\u91cd\u8981\u6807\u5fd7\u7269\uff0c\u4f46\u4e34\u5e8a\u8bc4\u4f30\u89c4\u6a21\u6709\u9650\u3002\u793e\u4ea4\u5a92\u4f53\u8bed\u8a00\u5206\u6790\u56e0\u5176\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u7eb5\u5411\u8303\u56f4\u800c\u53d7\u5230\u5173\u6ce8\uff0c\u9700\u8981\u5f00\u53d1\u65b9\u6cd5\u6765\u5229\u7528\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u8fdb\u884c\u5927\u89c4\u6a21\u5fc3\u7406\u5065\u5eb7\u76d1\u6d4b\u3002", "method": "\u5f15\u5165\u786e\u5b9a\u7528\u6237\u8bca\u65ad\u65f6\u95f4\u7684\u65b9\u6cd5\uff0c\u5206\u6790\u53cc\u76f8\u60c5\u611f\u969c\u788d\u60a3\u8005\u4ece\u8bca\u65ad\u524d3\u5e74\u5230\u8bca\u65ad\u540e21\u5e74\u7684\u8bed\u8a00\u8f68\u8ff9\uff0c\u4e0e\u5355\u76f8\u6291\u90c1\u75c7\u60a3\u8005\u548c\u975e\u53d7\u5f71\u54cd\u7528\u6237\u8fdb\u884c\u5bf9\u6bd4\u7814\u7a76\u3002", "result": "\u53d1\u73b0\u53cc\u76f8\u60c5\u611f\u969c\u788d\u8bca\u65ad\u4f34\u968f\u666e\u904d\u7684\u8bed\u8a00\u6539\u53d8\uff0c\u53cd\u6620\u60c5\u7eea\u969c\u788d\u3001\u7cbe\u795e\u5171\u75c5\u3001\u7269\u8d28\u6ee5\u7528\u7b49\u7279\u5f81\uff1b\u89c2\u5bdf\u5230\u8bca\u65ad\u540e\u4e8c\u5341\u5e74\u95f4\u53cd\u590d\u51fa\u73b0\u7684\u60c5\u7eea\u76f8\u5173\u8bed\u8a00\u53d8\u5316\uff0c\u5177\u6709\u660e\u663e\u768412\u4e2a\u6708\u5468\u671f\u6027\uff1b\u8d8b\u52bf\u8bc1\u636e\u8868\u660e\u5973\u6027\u7528\u6237\u7684\u5468\u671f\u6027\u53d8\u5316\u66f4\u660e\u663e\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63d0\u4f9b\u4e86\u53cc\u76f8\u60c5\u611f\u969c\u788d\u6025\u6027\u548c\u6162\u6027\u9636\u6bb5\u8bed\u8a00\u6539\u53d8\u7684\u8bc1\u636e\uff0c\u9a8c\u8bc1\u5e76\u6269\u5c55\u4e86\u5229\u7528\u793e\u4ea4\u5a92\u4f53\u8fdb\u884c\u53ef\u6269\u5c55\u5fc3\u7406\u5065\u5eb7\u76d1\u6d4b\u7684\u6700\u65b0\u52aa\u529b\u3002"}}
{"id": "2509.10186", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10186", "abs": "https://arxiv.org/abs/2509.10186", "authors": ["Benjamin Holzschuh", "Georg Kohl", "Florian Redinger", "Nils Thuerey"], "title": "P3D: Scalable Neural Surrogates for High-Resolution 3D Physics Simulations with Global Context", "comment": null, "summary": "We present a scalable framework for learning deterministic and probabilistic\nneural surrogates for high-resolution 3D physics simulations. We introduce a\nhybrid CNN-Transformer backbone architecture targeted for 3D physics\nsimulations, which significantly outperforms existing architectures in terms of\nspeed and accuracy. Our proposed network can be pretrained on small patches of\nthe simulation domain, which can be fused to obtain a global solution,\noptionally guided via a fast and scalable sequence-to-sequence model to include\nlong-range dependencies. This setup allows for training large-scale models with\nreduced memory and compute requirements for high-resolution datasets. We\nevaluate our backbone architecture against a large set of baseline methods with\nthe objective to simultaneously learn the dynamics of 14 different types of\nPDEs in 3D. We demonstrate how to scale our model to high-resolution isotropic\nturbulence with spatial resolutions of up to $512^3$. Finally, we demonstrate\nthe versatility of our network by training it as a diffusion model to produce\nprobabilistic samples of highly turbulent 3D channel flows across varying\nReynolds numbers, accurately capturing the underlying flow statistics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u9ad8\u5206\u8fa8\u73873D\u7269\u7406\u6a21\u62df\u7684\u786e\u5b9a\u6027\u548c\u6982\u7387\u6027\u795e\u7ecf\u4ee3\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u6df7\u5408CNN-Transformer\u67b6\u6784\u5728\u901f\u5ea6\u548c\u7cbe\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u9ad8\u5206\u8fa8\u73873D\u7269\u7406\u6a21\u62df\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u7684\u795e\u7ecf\u4ee3\u7406\u6a21\u578b\u6765\u66ff\u4ee3\u4f20\u7edf\u6570\u503c\u6a21\u62df\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6df7\u5408CNN-Transformer\u9aa8\u5e72\u67b6\u6784\uff0c\u652f\u6301\u5728\u5c0f\u5757\u6a21\u62df\u57df\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u878d\u5408\u83b7\u5f97\u5168\u5c40\u89e3\uff0c\u5e76\u901a\u8fc7\u5e8f\u5217\u5230\u5e8f\u5217\u6a21\u578b\u5305\u542b\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u572814\u79cd\u4e0d\u540c\u7c7b\u578b3D PDE\u52a8\u529b\u5b66\u5b66\u4e60\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u53ef\u6269\u5c55\u5230512^3\u7a7a\u95f4\u5206\u8fa8\u7387\u7684\u9ad8\u5206\u8fa8\u7387\u5404\u5411\u540c\u6027\u6e4d\u6d41\uff0c\u5e76\u80fd\u4f5c\u4e3a\u6269\u6563\u6a21\u578b\u751f\u6210\u4e0d\u540c\u96f7\u8bfa\u6570\u4e0b\u9ad8\u5ea6\u6e4d\u6d413D\u901a\u9053\u6d41\u7684\u6982\u7387\u6837\u672c\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u9ad8\u5206\u8fa8\u73873D\u7269\u7406\u6a21\u62df\u63d0\u4f9b\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u795e\u7ecf\u4ee3\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u3002"}}
{"id": "2509.10040", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10040", "abs": "https://arxiv.org/abs/2509.10040", "authors": ["Mohamed Basem", "Mohamed Younes", "Seif Ahmed", "Abdelrahman Moustafa"], "title": "!MSA at BAREC Shared Task 2025: Ensembling Arabic Transformers for Readability Assessment", "comment": "10 Pages , 8 figures , ArabicNLP 2025 , Co-located with EMNLP 2025", "summary": "We present MSAs winning system for the BAREC 2025 Shared Task on fine-grained\nArabic readability assessment, achieving first place in six of six tracks. Our\napproach is a confidence-weighted ensemble of four complementary transformer\nmodels (AraBERTv2, AraELECTRA, MARBERT, and CAMeLBERT) each fine-tuned with\ndistinct loss functions to capture diverse readability signals. To tackle\nsevere class imbalance and data scarcity, we applied weighted training,\nadvanced preprocessing, SAMER corpus relabeling with our strongest model, and\nsynthetic data generation via Gemini 2.5 Flash, adding about 10,000 rare-level\nsamples. A targeted post-processing step corrected prediction distribution\nskew, delivering a 6.3 percent Quadratic Weighted Kappa (QWK) gain. Our system\nreached 87.5 percent QWK at the sentence level and 87.4 percent at the document\nlevel, demonstrating the power of model and loss diversity, confidence-informed\nfusion, and intelligent augmentation for robust Arabic readability prediction.", "AI": {"tldr": "MSA\u56e2\u961f\u5728BAREC 2025\u963f\u62c9\u4f2f\u8bed\u7ec6\u7c92\u5ea6\u53ef\u8bfb\u6027\u8bc4\u4f30\u4efb\u52a1\u4e2d\u83b7\u80dc\uff0c\u4f7f\u7528\u56db\u4e2aTransformer\u6a21\u578b\u7684\u7f6e\u4fe1\u5ea6\u52a0\u6743\u96c6\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u540e\u5904\u7406\u6280\u672f\uff0c\u5728\u516d\u4e2a\u8d5b\u9053\u4e2d\u5747\u83b7\u5f97\u7b2c\u4e00\u540d\u3002", "motivation": "\u89e3\u51b3\u963f\u62c9\u4f2f\u8bed\u53ef\u8bfb\u6027\u8bc4\u4f30\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u63d0\u5347\u7ec6\u7c92\u5ea6\u53ef\u8bfb\u6027\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528AraBERTv2\u3001AraELECTRA\u3001MARBERT\u548cCAMeLBERT\u56db\u4e2a\u6a21\u578b\uff0c\u91c7\u7528\u4e0d\u540c\u635f\u5931\u51fd\u6570\u8fdb\u884c\u5fae\u8c03\uff1b\u5e94\u7528\u52a0\u6743\u8bad\u7ec3\u3001\u9ad8\u7ea7\u9884\u5904\u7406\u3001SAMER\u8bed\u6599\u5e93\u91cd\u65b0\u6807\u6ce8\u548cGemini 2.5 Flash\u751f\u6210\u7ea610,000\u4e2a\u7a00\u6709\u7ea7\u522b\u6837\u672c\u7684\u6570\u636e\u589e\u5f3a\uff1b\u901a\u8fc7\u9488\u5bf9\u6027\u540e\u5904\u7406\u6821\u6b63\u9884\u6d4b\u5206\u5e03\u504f\u5dee\u3002", "result": "\u5728\u53e5\u5b50\u7ea7\u522b\u8fbe\u523087.5%\u7684\u4e8c\u6b21\u52a0\u6743Kappa\uff08QWK\uff09\uff0c\u5728\u6587\u6863\u7ea7\u522b\u8fbe\u523087.4%\u7684QWK\uff0c\u540e\u5904\u7406\u5e26\u67656.3%\u7684QWK\u589e\u76ca\u3002", "conclusion": "\u6a21\u578b\u548c\u635f\u5931\u51fd\u6570\u7684\u591a\u6837\u6027\u3001\u7f6e\u4fe1\u5ea6\u4fe1\u606f\u878d\u5408\u4ee5\u53ca\u667a\u80fd\u6570\u636e\u589e\u5f3a\u5bf9\u4e8e\u9c81\u68d2\u7684\u963f\u62c9\u4f2f\u8bed\u53ef\u8bfb\u6027\u9884\u6d4b\u5177\u6709\u5f3a\u5927\u6548\u679c\u3002"}}
{"id": "2509.10189", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10189", "abs": "https://arxiv.org/abs/2509.10189", "authors": ["Zexu Jin"], "title": "Hadamard-Riemannian Optimization for Margin-Variance Ensemble", "comment": null, "summary": "Ensemble learning has been widely recognized as a pivotal technique for\nboosting predictive performance by combining multiple base models.\nNevertheless, conventional margin-based ensemble methods predominantly focus on\nmaximizing the expected margin while neglecting the critical role of margin\nvariance, which inherently restricts the generalization capability of the model\nand heightens its vulnerability to overfitting, particularly in noisy or\nimbalanced datasets. Additionally, the conventional approach of optimizing\nensemble weights within the probability simplex often introduces computational\ninefficiency and scalability challenges, complicating its application to\nlarge-scale problems. To tackle these limitations, this paper introduces a\nnovel ensemble learning framework that explicitly incorporates margin variance\ninto the loss function. Our method jointly optimizes the negative expected\nmargin and its variance, leading to enhanced robustness and improved\ngeneralization performance. Moreover, by reparameterizing the ensemble weights\nonto the unit sphere, we substantially simplify the optimization process and\nimprove computational efficiency. Extensive experiments conducted on multiple\nbenchmark datasets demonstrate that the proposed approach consistently\noutperforms traditional margin-based ensemble techniques, underscoring its\neffectiveness and practical utility.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u96c6\u6210\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8fb9\u9645\u65b9\u5dee\u663e\u5f0f\u7eb3\u5165\u635f\u5931\u51fd\u6570\uff0c\u8054\u5408\u4f18\u5316\u8d1f\u671f\u671b\u8fb9\u9645\u548c\u5176\u65b9\u5dee\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u8fb9\u9645\u7684\u96c6\u6210\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6700\u5927\u5316\u671f\u671b\u8fb9\u9645\uff0c\u4f46\u5ffd\u89c6\u4e86\u8fb9\u9645\u65b9\u5dee\u7684\u5173\u952e\u4f5c\u7528\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u5e76\u5728\u566a\u58f0\u6216\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e2d\u5bb9\u6613\u8fc7\u62df\u5408\u3002\u6b64\u5916\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u6982\u7387\u5355\u7eaf\u5f62\u4e2d\u4f18\u5316\u96c6\u6210\u6743\u91cd\u5b58\u5728\u8ba1\u7b97\u6548\u7387\u4f4e\u548c\u53ef\u6269\u5c55\u6027\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5c06\u8fb9\u9645\u65b9\u5dee\u663e\u5f0f\u7eb3\u5165\u635f\u5931\u51fd\u6570\uff0c\u8054\u5408\u4f18\u5316\u8d1f\u671f\u671b\u8fb9\u9645\u548c\u5176\u65b9\u5dee\uff1b\u901a\u8fc7\u5c06\u96c6\u6210\u6743\u91cd\u91cd\u65b0\u53c2\u6570\u5316\u5230\u5355\u4f4d\u7403\u9762\u4e0a\uff0c\u7b80\u5316\u4f18\u5316\u8fc7\u7a0b\u5e76\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u4f20\u7edf\u7684\u57fa\u4e8e\u8fb9\u9645\u7684\u96c6\u6210\u6280\u672f\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u8003\u8651\u8fb9\u9645\u65b9\u5dee\u548c\u4f18\u5316\u6743\u91cd\u53c2\u6570\u5316\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u96c6\u6210\u5b66\u4e60\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u80fd\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.10078", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10078", "abs": "https://arxiv.org/abs/2509.10078", "authors": ["Dongmin Choi", "Woojung Song", "Jongwook Han", "Eun-Ju Lee", "Yohan Jo"], "title": "Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models", "comment": "17 pages, 4 figures", "summary": "Researchers have applied established psychometric questionnaires (e.g., BFI,\nPVQ) to measure the personality traits and values reflected in the responses of\nLarge Language Models (LLMs). However, concerns have been raised about applying\nthese human-designed questionnaires to LLMs. One such concern is their lack of\necological validity--the extent to which survey questions adequately reflect\nand resemble real-world contexts in which LLMs generate texts in response to\nuser queries. However, it remains unclear how established questionnaires and\necologically valid questionnaires differ in their outcomes, and what insights\nthese differences may provide. In this paper, we conduct a comprehensive\ncomparative analysis of the two types of questionnaires. Our analysis reveals\nthat established questionnaires (1) yield substantially different profiles of\nLLMs from ecologically valid ones, deviating from the psychological\ncharacteristics expressed in the context of user queries, (2) suffer from\ninsufficient items for stable measurement, (3) create misleading impressions\nthat LLMs possess stable constructs, and (4) yield exaggerated profiles for\npersona-prompted LLMs. Overall, our work cautions against the use of\nestablished psychological questionnaires for LLMs. Our code will be released\nupon publication.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5bf9\u6bd4\u5206\u6790\u53d1\u73b0\uff0c\u4f20\u7edf\u5fc3\u7406\u6d4b\u91cf\u95ee\u5377\u5728\u6d4b\u91cf\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u5b58\u5728\u751f\u6001\u6548\u5ea6\u4e0d\u8db3\u3001\u7ed3\u679c\u504f\u5dee\u3001\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5efa\u8bae\u907f\u514d\u4f7f\u7528", "motivation": "\u5f53\u524d\u7814\u7a76\u4e2d\u5e38\u7528\u4f20\u7edf\u5fc3\u7406\u6d4b\u91cf\u95ee\u5377\uff08\u5982BFI\u3001PVQ\uff09\u6765\u6d4b\u91cf\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4eba\u683c\u7279\u8d28\uff0c\u4f46\u8fd9\u4e9b\u4eba\u7c7b\u8bbe\u8ba1\u7684\u95ee\u5377\u662f\u5426\u5177\u6709\u751f\u6001\u6548\u5ea6\uff08ecological validity\uff09\u5b58\u5728\u7591\u95ee\uff0c\u9700\u8981\u6bd4\u8f83\u5206\u6790\u4f20\u7edf\u95ee\u5377\u4e0e\u751f\u6001\u6709\u6548\u95ee\u5377\u7684\u5dee\u5f02", "method": "\u8fdb\u884c\u7efc\u5408\u6027\u7684\u5bf9\u6bd4\u5206\u6790\uff0c\u6bd4\u8f83\u4f20\u7edf\u5fc3\u7406\u6d4b\u91cf\u95ee\u5377\u4e0e\u751f\u6001\u6709\u6548\u95ee\u5377\u5728\u6d4b\u91cf\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u7684\u7ed3\u679c\u5dee\u5f02", "result": "\u53d1\u73b0\u4f20\u7edf\u95ee\u5377\uff1a1\uff09\u4ea7\u751f\u4e0e\u751f\u6001\u6709\u6548\u95ee\u5377\u663e\u8457\u4e0d\u540c\u7684\u6a21\u578b\u4eba\u683c\u4ea4\u5dee\uff0c\u504f\u79bb\u7528\u6237\u67e5\u8be2\u4e0a\u4e0b\u6587\u4e2d\u8868\u8fbe\u7684\u5fc3\u7406\u7279\u5f81\uff1b2\uff09\u95ee\u9879\u4e0d\u591f\u5bfc\u81f4\u6d4b\u91cf\u4e0d\u7a33\u5b9a\uff1b3\uff09\u9020\u6210\u6a21\u578b\u62e5\u6709\u7a33\u5b9a\u6784\u5efa\u7684\u8bef\u5bfc\u5370\u8c61\uff1b4\uff09\u5bf9\u4eba\u8bbe\u63d0\u793a\u7684\u6a21\u578b\u4ea7\u751f\u5938\u5f20\u7684\u4eba\u683c\u63cf\u8ff0", "conclusion": "\u5efa\u8bae\u8c28\u614e\u4f7f\u7528\u4f20\u7edf\u5fc3\u7406\u6d4b\u91cf\u95ee\u5377\u6765\u6d4b\u91cf\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u8fd9\u4e9b\u95ee\u5377\u5728\u751f\u6001\u6548\u5ea6\u3001\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u95ee\u9898"}}
{"id": "2509.10227", "categories": ["cs.LG", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2509.10227", "abs": "https://arxiv.org/abs/2509.10227", "authors": ["\u00c1ngel Ladr\u00f3n", "Miguel S\u00e1nchez-Dom\u00ednguez", "Javier Rozal\u00e9n", "Fernando R. S\u00e1nchez", "Javier de Vicente", "Lucas Lacasa", "Eusebio Valero", "Gonzalo Rubio"], "title": "A Certifiable Machine Learning-Based Pipeline to Predict Fatigue Life of Aircraft Structures", "comment": "29 pages, 15 figures", "summary": "Fatigue life prediction is essential in both the design and operational\nphases of any aircraft, and in this sense safety in the aerospace industry\nrequires early detection of fatigue cracks to prevent in-flight failures.\nRobust and precise fatigue life predictors are thus essential to ensure safety.\nTraditional engineering methods, while reliable, are time consuming and involve\ncomplex workflows, including steps such as conducting several Finite Element\nMethod (FEM) simulations, deriving the expected loading spectrum, and applying\ncycle counting techniques like peak-valley or rainflow counting. These steps\noften require collaboration between multiple teams and tools, added to the\ncomputational time and effort required to achieve fatigue life predictions.\nMachine learning (ML) offers a promising complement to traditional fatigue life\nestimation methods, enabling faster iterations and generalization, providing\nquick estimates that guide decisions alongside conventional simulations.\n  In this paper, we present a ML-based pipeline that aims to estimate the\nfatigue life of different aircraft wing locations given the flight parameters\nof the different missions that the aircraft will be operating throughout its\noperational life. We validate the pipeline in a realistic use case of fatigue\nlife estimation, yielding accurate predictions alongside a thorough statistical\nvalidation and uncertainty quantification. Our pipeline constitutes a\ncomplement to traditional methodologies by reducing the amount of costly\nsimulations and, thereby, lowering the required computational and human\nresources.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u98de\u673a\u673a\u7ffc\u75b2\u52b3\u5bff\u547d\u9884\u6d4b\u7ba1\u9053\uff0c\u901a\u8fc7\u98de\u884c\u53c2\u6570\u5feb\u901f\u4f30\u8ba1\u4e0d\u540c\u673a\u7ffc\u4f4d\u7f6e\u7684\u75b2\u52b3\u5bff\u547d\uff0c\u4f5c\u4e3a\u4f20\u7edf\u5de5\u7a0b\u65b9\u6cd5\u7684\u8865\u5145\u3002", "motivation": "\u4f20\u7edf\u75b2\u52b3\u5bff\u547d\u9884\u6d4b\u65b9\u6cd5\u867d\u7136\u53ef\u9760\u4f46\u8017\u65f6\u4e14\u6d41\u7a0b\u590d\u6742\uff0c\u9700\u8981\u591a\u56e2\u961f\u534f\u4f5c\u548c\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u3002\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u63d0\u4f9b\u5feb\u901f\u8fed\u4ee3\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4f5c\u4e3a\u4f20\u7edf\u6a21\u62df\u7684\u8865\u5145\u6307\u5bfc\u51b3\u7b56\u3002", "method": "\u5f00\u53d1\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u7ba1\u9053\uff0c\u6839\u636e\u98de\u673a\u6574\u4e2a\u8fd0\u884c\u5bff\u547d\u4e2d\u4e0d\u540c\u4efb\u52a1\u7684\u98de\u884c\u53c2\u6570\uff0c\u4f30\u8ba1\u673a\u7ffc\u4e0d\u540c\u4f4d\u7f6e\u7684\u75b2\u52b3\u5bff\u547d\u3002", "result": "\u5728\u771f\u5b9e\u7684\u75b2\u52b3\u5bff\u547d\u4f30\u8ba1\u7528\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u7ba1\u9053\u7684\u51c6\u786e\u6027\uff0c\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u7edf\u8ba1\u9a8c\u8bc1\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u80fd\u591f\u51cf\u5c11\u6602\u8d35\u7684\u6a21\u62df\u9700\u6c42\u3002", "conclusion": "\u8be5\u673a\u5668\u5b66\u4e60\u7ba1\u9053\u901a\u8fc7\u964d\u4f4e\u8ba1\u7b97\u548c\u4eba\u529b\u8d44\u6e90\u9700\u6c42\uff0c\u6709\u6548\u8865\u5145\u4e86\u4f20\u7edf\u75b2\u52b3\u5bff\u547d\u9884\u6d4b\u65b9\u6cd5\uff0c\u4e3a\u822a\u7a7a\u822a\u5929\u5b89\u5168\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.10087", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10087", "abs": "https://arxiv.org/abs/2509.10087", "authors": ["Mustapha Adamu", "Qi Zhang", "Huitong Pan", "Longin Jan Latecki", "Eduard C. Dragut"], "title": "Querying Climate Knowledge: Semantic Retrieval for Scientific Discovery", "comment": "ACM SIGIR 2025 Workshop MANILA", "summary": "The growing complexity and volume of climate science literature make it\nincreasingly difficult for researchers to find relevant information across\nmodels, datasets, regions, and variables. This paper introduces a\ndomain-specific Knowledge Graph (KG) built from climate publications and\nbroader scientific texts, aimed at improving how climate knowledge is accessed\nand used. Unlike keyword based search, our KG supports structured, semantic\nqueries that help researchers discover precise connections such as which models\nhave been validated in specific regions or which datasets are commonly used\nwith certain teleconnection patterns. We demonstrate how the KG answers such\nquestions using Cypher queries, and outline its integration with large language\nmodels in RAG systems to improve transparency and reliability in\nclimate-related question answering. This work moves beyond KG construction to\nshow its real world value for climate researchers, model developers, and others\nwho rely on accurate, contextual scientific information.", "AI": {"tldr": "\u6784\u5efa\u4e86\u4e00\u4e2a\u9762\u5411\u6c14\u5019\u79d1\u5b66\u9886\u57df\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u652f\u6301\u8bed\u4e49\u67e5\u8be2\u548cLLM\u96c6\u6210\uff0c\u63d0\u5347\u6c14\u5019\u7814\u7a76\u7684\u4fe1\u606f\u68c0\u7d22\u6548\u7387", "motivation": "\u6c14\u5019\u79d1\u5b66\u6587\u732e\u65e5\u76ca\u590d\u6742\u548c\u5e9e\u5927\uff0c\u7814\u7a76\u4eba\u5458\u96be\u4ee5\u8de8\u6a21\u578b\u3001\u6570\u636e\u96c6\u3001\u533a\u57df\u548c\u53d8\u91cf\u627e\u5230\u76f8\u5173\u4fe1\u606f\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u77e5\u8bc6\u8bbf\u95ee\u65b9\u5f0f", "method": "\u4ece\u6c14\u5019\u51fa\u7248\u7269\u548c\u79d1\u5b66\u6587\u672c\u6784\u5efa\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u56fe\u8c31\uff0c\u652f\u6301Cypher\u67e5\u8be2\u8bed\u8a00\u8fdb\u884c\u7ed3\u6784\u5316\u8bed\u4e49\u67e5\u8be2\uff0c\u5e76\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u6784\u5efaRAG\u7cfb\u7edf", "result": "\u77e5\u8bc6\u56fe\u8c31\u80fd\u591f\u56de\u7b54\u7cbe\u786e\u7684\u8fde\u63a5\u95ee\u9898\uff0c\u5982\u7279\u5b9a\u533a\u57df\u9a8c\u8bc1\u7684\u6a21\u578b\u3001\u4e0e\u7279\u5b9a\u9065\u76f8\u5173\u6a21\u5f0f\u5e38\u7528\u7684\u6570\u636e\u96c6\u7b49\uff0c\u63d0\u9ad8\u4e86\u4fe1\u606f\u53d1\u73b0\u7684\u51c6\u786e\u6027", "conclusion": "\u8be5\u77e5\u8bc6\u56fe\u8c31\u8d85\u8d8a\u4e86\u4f20\u7edf\u6784\u5efa\u5de5\u4f5c\uff0c\u5c55\u793a\u4e86\u5176\u5728\u6c14\u5019\u7814\u7a76\u4eba\u5458\u3001\u6a21\u578b\u5f00\u53d1\u8005\u7b49\u4f9d\u8d56\u51c6\u786e\u79d1\u5b66\u4fe1\u606f\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u63d0\u5347\u4e86\u900f\u660e\u5ea6\u548c\u53ef\u9760\u6027"}}
{"id": "2509.10248", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10248", "abs": "https://arxiv.org/abs/2509.10248", "authors": ["Janis Keuper"], "title": "Prompt Injection Attacks on LLM Generated Reviews of Scientific Publications", "comment": null, "summary": "The ongoing intense discussion on rising LLM usage in the scientific\npeer-review process has recently been mingled by reports of authors using\nhidden prompt injections to manipulate review scores. Since the existence of\nsuch \"attacks\" - although seen by some commentators as \"self-defense\" - would\nhave a great impact on the further debate, this paper investigates the\npracticability and technical success of the described manipulations. Our\nsystematic evaluation uses 1k reviews of 2024 ICLR papers generated by a wide\nrange of LLMs shows two distinct results: I) very simple prompt injections are\nindeed highly effective, reaching up to 100% acceptance scores. II) LLM reviews\nare generally biased toward acceptance (>95% in many models). Both results have\ngreat impact on the ongoing discussions on LLM usage in peer-review.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u53d1\u73b0\uff1a\u7b80\u5355\u7684\u63d0\u793a\u8bcd\u6ce8\u5165\u653b\u51fb\u5bf9LLM\u5ba1\u7a3f\u8fc7\u7a0b\u9ad8\u5ea6\u6709\u6548\uff08\u53ef\u8fbe100%\u63a5\u53d7\u7387\uff09\uff0c\u4e14LLM\u5ba1\u7a3f\u666e\u904d\u5b58\u5728\u63a5\u53d7\u504f\u5411\uff08>95%\uff09\uff0c\u8fd9\u5bf9LLM\u5728\u540c\u884c\u8bc4\u5ba1\u4e2d\u7684\u4f7f\u7528\u8ba8\u8bba\u5177\u6709\u91cd\u8981\u5f71\u54cd\u3002", "motivation": "\u9488\u5bf9\u8fd1\u671f\u5173\u4e8e\u4f5c\u8005\u4f7f\u7528\u9690\u85cf\u63d0\u793a\u8bcd\u6ce8\u5165\u64cd\u7eb5\u5ba1\u7a3f\u5206\u6570\u7684\u62a5\u9053\uff0c\u7814\u7a76\u8fd9\u79cd\u653b\u51fb\u7684\u53ef\u884c\u6027\u548c\u6280\u672f\u6210\u529f\u7387\uff0c\u4ee5\u5f71\u54cd\u5173\u4e8eLLM\u5728\u79d1\u5b66\u540c\u884c\u8bc4\u5ba1\u4e2d\u4f7f\u7528\u7684\u91cd\u8981\u8ba8\u8bba\u3002", "method": "\u4f7f\u7528\u591a\u79cdLLM\u5bf92024\u5e74ICLR\u4f1a\u8bae\u76841000\u7bc7\u8bba\u6587\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\uff0c\u6d4b\u8bd5\u7b80\u5355\u63d0\u793a\u8bcd\u6ce8\u5165\u7684\u6548\u679c\u3002", "result": "1) \u975e\u5e38\u7b80\u5355\u63d0\u793a\u8bcd\u6ce8\u5165\u9ad8\u5ea6\u6709\u6548\uff0c\u8fbe\u5230100%\u63a5\u53d7\u7387\uff1b2) LLM\u5ba1\u7a3f\u666e\u904d\u504f\u5411\u63a5\u53d7\uff08\u8bb8\u591a\u6a21\u578b>95%\uff09\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5bf9LLM\u5728\u540c\u884c\u8bc4\u5ba1\u4e2d\u7684\u4f7f\u7528\u8ba8\u8bba\u5177\u6709\u91cd\u5927\u5f71\u54cd\uff0c\u63ed\u793a\u4e86\u7cfb\u7edf\u6f0f\u6d1e\u548c\u56fa\u6709\u504f\u89c1\u3002"}}
{"id": "2509.10095", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10095", "abs": "https://arxiv.org/abs/2509.10095", "authors": ["Abdulrahman Allam", "Seif Ahmed", "Ali Hamdi", "Ammar Mohammed"], "title": "Arabic Large Language Models for Medical Text Generation", "comment": "Published in 2025 4th International Conference on Computer\n  Technologies (ICCTech)", "summary": "Efficient hospital management systems (HMS) are critical worldwide to address\nchallenges such as overcrowding, limited resources, and poor availability of\nurgent health care. Existing methods often lack the ability to provide\naccurate, real-time medical advice, particularly for irregular inputs and\nunderrepresented languages. To overcome these limitations, this study proposes\nan approach that fine-tunes large language models (LLMs) for Arabic medical\ntext generation. The system is designed to assist patients by providing\naccurate medical advice, diagnoses, drug recommendations, and treatment plans\nbased on user input. The research methodology required the collection of a\nunique dataset from social media platforms, capturing real-world medical\nconversations between patients and doctors. The dataset, which includes patient\ncomplaints together with medical advice, was properly cleaned and preprocessed\nto account for multiple Arabic dialects. Fine-tuning state-of-the-art\ngenerative models, such as Mistral-7B-Instruct-v0.2, LLaMA-2-7B, and GPT-2\nMedium, optimized the system's ability to generate reliable medical text.\nResults from evaluations indicate that the fine-tuned Mistral-7B model\noutperformed the other models, achieving average BERT (Bidirectional Encoder\nRepresentations from Transformers) Score values in precision, recall, and\nF1-scores of 68.5\\%, 69.08\\%, and 68.5\\%, respectively. Comparative\nbenchmarking and qualitative assessments validate the system's ability to\nproduce coherent and relevant medical replies to informal input. This study\nhighlights the potential of generative artificial intelligence (AI) in\nadvancing HMS, offering a scalable and adaptable solution for global healthcare\nchallenges, especially in linguistically and culturally diverse environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982Mistral-7B\u3001LLaMA-2-7B\u548cGPT-2\uff09\u6765\u751f\u6210\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u6587\u672c\uff0c\u65e8\u5728\u89e3\u51b3\u533b\u9662\u7ba1\u7406\u7cfb\u7edf\u4e2d\u7684\u5b9e\u65f6\u533b\u7597\u5efa\u8bae\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u975e\u6b63\u89c4\u8f93\u5165\u548c underrepresented \u8bed\u8a00\u65b9\u9762\u3002\u5fae\u8c03\u540e\u7684Mistral-7B\u6a21\u578b\u5728BERT\u8bc4\u5206\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u5e73\u5747\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u5206\u522b\u4e3a68.5%\u300169.08%\u548c68.5%\u3002", "motivation": "\u73b0\u6709\u7684\u533b\u9662\u7ba1\u7406\u7cfb\u7edf\u5f80\u5f80\u65e0\u6cd5\u63d0\u4f9b\u51c6\u786e\u3001\u5b9e\u65f6\u7684\u533b\u7597\u5efa\u8bae\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u975e\u6b63\u89c4\u8f93\u5165\u548c underrepresented \u8bed\u8a00\uff08\u5982\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\uff09\u65f6\u5b58\u5728\u5c40\u9650\u3002\u5168\u7403\u533b\u7597\u7cfb\u7edf\u9762\u4e34 overcrowding\u3001\u8d44\u6e90\u6709\u9650\u548c\u7d27\u6025\u533b\u7597\u53ef\u7528\u6027\u5dee\u7b49\u6311\u6218\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u6536\u96c6\u4e86\u6765\u81ea\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u7684\u771f\u5b9e\u533b\u60a3\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u591a\u79cd\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u3002\u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u6e05\u7406\u548c\u9884\u5904\u7406\u540e\uff0c\u5fae\u8c03\u4e86Mistral-7B-Instruct-v0.2\u3001LLaMA-2-7B\u548cGPT-2 Medium\u7b49\u5148\u8fdb\u751f\u6210\u6a21\u578b\uff0c\u4ee5\u4f18\u5316\u533b\u7597\u6587\u672c\u751f\u6210\u80fd\u529b\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u5fae\u8c03\u540e\u7684Mistral-7B\u6a21\u578b\u5728BERT\u8bc4\u5206\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u5e73\u5747\u7cbe\u786e\u738768.5%\u3001\u53ec\u56de\u738769.08%\u3001F1\u5206\u657068.5%\u3002\u6bd4\u8f83\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9a\u6027\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u80fd\u591f\u5bf9\u975e\u6b63\u89c4\u8f93\u5165\u4ea7\u751f\u8fde\u8d2f\u4e14\u76f8\u5173\u7684\u533b\u7597\u56de\u590d\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5728\u533b\u9662\u7ba1\u7406\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u5168\u7403\u533b\u7597\u6311\u6218\uff08\u7279\u522b\u662f\u5728\u8bed\u8a00\u548c\u6587\u5316\u591a\u6837\u5316\u7684\u73af\u5883\u4e2d\uff09\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u548c\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u80fd\u591f\u63d0\u4f9b\u51c6\u786e\u7684\u533b\u7597\u5efa\u8bae\u3001\u8bca\u65ad\u3001\u836f\u7269\u63a8\u8350\u548c\u6cbb\u7597\u8ba1\u5212\u3002"}}
{"id": "2509.10273", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10273", "abs": "https://arxiv.org/abs/2509.10273", "authors": ["Sahil Sethi", "Kai Sundmacher", "Caroline Ganzer"], "title": "Property prediction for ionic liquids without prior structural knowledge using limited experimental data: A data-driven neural recommender system leveraging transfer learning", "comment": null, "summary": "Ionic liquids (ILs) have emerged as versatile replacements for traditional\nsolvents because their physicochemical properties can be precisely tailored to\nvarious applications. However, accurately predicting key thermophysical\nproperties remains challenging due to the vast chemical design space and the\nlimited availability of experimental data. In this study, we present a\ndata-driven transfer learning framework that leverages a neural recommender\nsystem (NRS) to enable reliable property prediction for ILs using sparse\nexperimental datasets. The approach involves a two-stage process: first,\npre-training NRS models on COSMO-RS-based simulated data at fixed temperature\nand pressure to learn property-specific structural embeddings for cations and\nanions; and second, fine-tuning simple feedforward neural networks using these\nembeddings with experimental data at varying temperatures and pressures. In\nthis work, five essential IL properties are considered: density, viscosity,\nsurface tension, heat capacity, and melting point. The framework supports both\nwithin-property and cross-property knowledge transfer. Notably, pre-trained\nmodels for density, viscosity, and heat capacity are used to fine-tune models\nfor all five target properties, achieving improved performance by a substantial\nmargin for four of them. The model exhibits robust extrapolation to previously\nunseen ILs. Moreover, the final trained models enable property prediction for\nover 700,000 IL combinations, offering a scalable solution for IL screening in\nprocess design. This work highlights the effectiveness of combining simulated\ndata and transfer learning to overcome sparsity in the experimental data.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u795e\u7ecf\u63a8\u8350\u7cfb\u7edf\u7684\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528COSMO-RS\u6a21\u62df\u6570\u636e\u548c\u7a00\u758f\u5b9e\u9a8c\u6570\u636e\uff0c\u51c6\u786e\u9884\u6d4b\u79bb\u5b50\u6db2\u4f53\u7684\u4e94\u79cd\u5173\u952e\u70ed\u7269\u7406\u6027\u8d28", "motivation": "\u79bb\u5b50\u6db2\u4f53\u5177\u6709\u53ef\u5b9a\u5236\u7684\u7269\u7406\u5316\u5b66\u6027\u8d28\uff0c\u4f46\u7531\u4e8e\u5316\u5b66\u8bbe\u8ba1\u7a7a\u95f4\u5de8\u5927\u548c\u5b9e\u9a8c\u6570\u636e\u6709\u9650\uff0c\u51c6\u786e\u9884\u6d4b\u5176\u70ed\u7269\u7406\u6027\u8d28\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027", "method": "\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u9996\u5148\u5728\u56fa\u5b9a\u6e29\u5ea6\u538b\u529b\u4e0b\u4f7f\u7528COSMO-RS\u6a21\u62df\u6570\u636e\u9884\u8bad\u7ec3\u795e\u7ecf\u63a8\u8350\u7cfb\u7edf\u6a21\u578b\uff0c\u5b66\u4e60\u9633\u79bb\u5b50\u548c\u9634\u79bb\u5b50\u7684\u6027\u8d28\u7279\u5f02\u6027\u7ed3\u6784\u5d4c\u5165\uff1b\u7136\u540e\u4f7f\u7528\u8fd9\u4e9b\u5d4c\u5165\u548c\u4e0d\u540c\u6e29\u5ea6\u538b\u529b\u4e0b\u7684\u5b9e\u9a8c\u6570\u636e\u5fae\u8c03\u7b80\u5355\u524d\u9988\u795e\u7ecf\u7f51\u7edc", "result": "\u8be5\u6846\u67b6\u652f\u6301\u6027\u8d28\u5185\u548c\u8de8\u6027\u8d28\u77e5\u8bc6\u8fc1\u79fb\uff0c\u9884\u8bad\u7ec3\u7684\u5bc6\u5ea6\u3001\u7c98\u5ea6\u548c\u70ed\u5bb9\u6a21\u578b\u7528\u4e8e\u5fae\u8c03\u6240\u6709\u4e94\u4e2a\u76ee\u6807\u6027\u8d28\u6a21\u578b\uff0c\u5176\u4e2d\u56db\u4e2a\u6027\u8d28\u7684\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002\u6a21\u578b\u5bf9\u672a\u89c1\u8fc7\u7684\u79bb\u5b50\u6db2\u4f53\u5177\u6709\u9c81\u68d2\u5916\u63a8\u80fd\u529b\uff0c\u53ef\u4e3a\u8d85\u8fc770\u4e07\u79cd\u79bb\u5b50\u6db2\u4f53\u7ec4\u5408\u63d0\u4f9b\u6027\u8d28\u9884\u6d4b", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u7ed3\u5408\u6a21\u62df\u6570\u636e\u548c\u8fc1\u79fb\u5b66\u4e60\u6765\u514b\u670d\u5b9e\u9a8c\u6570\u636e\u7a00\u758f\u6027\u7684\u6709\u6548\u6027\uff0c\u4e3a\u8fc7\u7a0b\u8bbe\u8ba1\u4e2d\u7684\u79bb\u5b50\u6db2\u4f53\u7b5b\u9009\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.10108", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10108", "abs": "https://arxiv.org/abs/2509.10108", "authors": ["Abdulrahman Allam", "Seif Ahmed", "Ali Hamdi", "Khaled Shaban"], "title": "Scaling Arabic Medical Chatbots Using Synthetic Data: Enhancing Generative AI with Synthetic Patient Records", "comment": "Accepted in AICCSA 2025", "summary": "The development of medical chatbots in Arabic is significantly constrained by\nthe scarcity of large-scale, high-quality annotated datasets. While prior\nefforts compiled a dataset of 20,000 Arabic patient-doctor interactions from\nsocial media to fine-tune large language models (LLMs), model scalability and\ngeneralization remained limited. In this study, we propose a scalable synthetic\ndata augmentation strategy to expand the training corpus to 100,000 records.\nUsing advanced generative AI systems ChatGPT-4o and Gemini 2.5 Pro we generated\n80,000 contextually relevant and medically coherent synthetic question-answer\npairs grounded in the structure of the original dataset. These synthetic\nsamples were semantically filtered, manually validated, and integrated into the\ntraining pipeline. We fine-tuned five LLMs, including Mistral-7B and AraGPT2,\nand evaluated their performance using BERTScore metrics and expert-driven\nqualitative assessments. To further analyze the effectiveness of synthetic\nsources, we conducted an ablation study comparing ChatGPT-4o and\nGemini-generated data independently. The results showed that ChatGPT-4o data\nconsistently led to higher F1-scores and fewer hallucinations across all\nmodels. Overall, our findings demonstrate the viability of synthetic\naugmentation as a practical solution for enhancing domain-specific language\nmodels in-low resource medical NLP, paving the way for more inclusive,\nscalable, and accurate Arabic healthcare chatbot systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u5408\u6210\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u4f7f\u7528ChatGPT-4o\u548cGemini 2.5 Pro\u751f\u6210\u4e8680,000\u4e2a\u963f\u62c9\u4f2f\u8bed\u533b\u5b66\u95ee\u7b54\u5bf9\uff0c\u5c06\u8bad\u7ec3\u8bed\u6599\u6269\u5c55\u5230100,000\u6761\u8bb0\u5f55\uff0c\u663e\u8457\u63d0\u5347\u4e86\u963f\u62c9\u4f2f\u533b\u7597\u804a\u5929\u673a\u5668\u4eba\u7684\u6027\u80fd\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u804a\u5929\u673a\u5668\u4eba\u7684\u53d1\u5c55\u53d7\u5230\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u9650\u5236\uff0c\u5148\u524d\u57fa\u4e8e20,000\u6761\u793e\u4ea4\u5a92\u4f53\u533b\u60a3\u4ea4\u4e92\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u4f7f\u7528ChatGPT-4o\u548cGemini 2.5 Pro\u751f\u621080,000\u4e2a\u4e0a\u4e0b\u6587\u76f8\u5173\u4e14\u533b\u5b66\u8fde\u8d2f\u7684\u5408\u6210\u95ee\u7b54\u5bf9\uff0c\u7ecf\u8fc7\u8bed\u4e49\u8fc7\u6ee4\u548c\u4eba\u5de5\u9a8c\u8bc1\u540e\u6574\u5408\u5230\u8bad\u7ec3\u6d41\u7a0b\u4e2d\u3002\u5bf9\u5305\u62ecMistral-7B\u548cAraGPT2\u5728\u5185\u7684\u4e94\u4e2aLLM\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u4f7f\u7528BERTScore\u6307\u6807\u548c\u4e13\u5bb6\u5b9a\u6027\u8bc4\u4f30\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\u3002", "result": "ChatGPT-4o\u751f\u6210\u7684\u6570\u636e\u5728\u6240\u6709\u6a21\u578b\u4e2d consistently \u83b7\u5f97\u66f4\u9ad8\u7684F1\u5206\u6570\u548c\u66f4\u5c11\u7684\u5e7b\u89c9\u73b0\u8c61\u3002\u5408\u6210\u6570\u636e\u589e\u5f3a\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u589e\u5f3a\u662f\u63d0\u5347\u4f4e\u8d44\u6e90\u533b\u5b66NLP\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u6a21\u578b\u7684\u6709\u6548\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u6784\u5efa\u66f4\u5177\u5305\u5bb9\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u51c6\u786e\u6027\u7684\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u804a\u5929\u673a\u5668\u4eba\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.10291", "categories": ["cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.10291", "abs": "https://arxiv.org/abs/2509.10291", "authors": ["Salih Toprak", "Muge Erel-Ozcevik"], "title": "Proof of AutoML: SDN based Secure Energy Trading with Blockchain in Disaster Case", "comment": "6 pages, 3 figures, 7th International Conference on Blockchain\n  Computing and Applications (BCCA 2025), \\c{opyright}2025 IEEE", "summary": "In disaster scenarios where conventional energy infrastructure is\ncompromised, secure and traceable energy trading between solar-powered\nhouseholds and mobile charging units becomes a necessity. To ensure the\nintegrity of such transactions over a blockchain network, robust and\nunpredictable nonce generation is vital. This study proposes an SDN-enabled\narchitecture where machine learning regressors are leveraged not for their\naccuracy, but for their potential to generate randomized values suitable as\nnonce candidates. Therefore, it is newly called Proof of AutoML. Here, SDN\nallows flexible control over data flows and energy routing policies even in\nfragmented or degraded networks, ensuring adaptive response during emergencies.\nUsing a 9000-sample dataset, we evaluate five AutoML-selected regression models\n- Gradient Boosting, LightGBM, Random Forest, Extra Trees, and K-Nearest\nNeighbors - not by their prediction accuracy, but by their ability to produce\ndiverse and non-deterministic outputs across shuffled data inputs. Randomness\nanalysis reveals that Random Forest and Extra Trees regressors exhibit complete\ndependency on randomness, whereas Gradient Boosting, K-Nearest Neighbors and\nLightGBM show strong but slightly lower randomness scores (97.6%, 98.8% and\n99.9%, respectively). These findings highlight that certain machine learning\nmodels, particularly tree-based ensembles, may serve as effective and\nlightweight nonce generators within blockchain-secured, SDN-based energy\ntrading infrastructures resilient to disaster conditions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eSDN\u548cAutoML\u7684Proof of AutoML\u67b6\u6784\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u56de\u5f52\u6a21\u578b\u751f\u6210\u968f\u673a\u6570\u4f5c\u4e3a\u533a\u5757\u94fenonce\uff0c\u7528\u4e8e\u707e\u96be\u573a\u666f\u4e0b\u7684\u5b89\u5168\u80fd\u6e90\u4ea4\u6613", "motivation": "\u5728\u4f20\u7edf\u80fd\u6e90\u57fa\u7840\u8bbe\u65bd\u53d7\u635f\u7684\u707e\u96be\u573a\u666f\u4e2d\uff0c\u9700\u8981\u786e\u4fdd\u592a\u9633\u80fd\u5bb6\u5ead\u4e0e\u79fb\u52a8\u5145\u7535\u5355\u5143\u4e4b\u95f4\u80fd\u6e90\u4ea4\u6613\u7684\u5b89\u5168\u6027\u548c\u53ef\u8ffd\u6eaf\u6027\uff0c\u800c\u533a\u5757\u94fe\u7f51\u7edc\u9700\u8981\u5f3a\u5927\u7684\u968f\u673anonce\u751f\u6210\u673a\u5236", "method": "\u91c7\u7528SDN\u67b6\u6784\u5b9e\u73b0\u7075\u6d3b\u7684\u6570\u636e\u6d41\u548c\u80fd\u6e90\u8def\u7531\u63a7\u5236\uff0c\u4f7f\u7528\u4e94\u79cdAutoML\u9009\u62e9\u7684\u56de\u5f52\u6a21\u578b\uff08\u68af\u5ea6\u63d0\u5347\u3001LightGBM\u3001\u968f\u673a\u68ee\u6797\u3001\u989d\u5916\u6811\u548cK\u8fd1\u90bb\uff09\u751f\u6210\u968f\u673anonce\uff0c\u901a\u8fc79000\u6837\u672c\u6570\u636e\u96c6\u8bc4\u4f30\u6a21\u578b\u7684\u968f\u673a\u6027\u800c\u975e\u51c6\u786e\u6027", "result": "\u968f\u673a\u6027\u5206\u6790\u663e\u793a\u968f\u673a\u68ee\u6797\u548c\u989d\u5916\u6811\u56de\u5f52\u5668\u5b8c\u5168\u4f9d\u8d56\u968f\u673a\u6027\uff0c\u68af\u5ea6\u63d0\u5347\u3001K\u8fd1\u90bb\u548cLightGBM\u5206\u522b\u8fbe\u523097.6%\u300198.8%\u548c99.9%\u7684\u968f\u673a\u6027\u5f97\u5206\uff0c\u6811\u57fa\u96c6\u6210\u6a21\u578b\u8868\u73b0\u6700\u4f73", "conclusion": "\u67d0\u4e9b\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u7279\u522b\u662f\u6811\u57fa\u96c6\u6210\u6a21\u578b\uff0c\u53ef\u4f5c\u4e3a\u533a\u5757\u94fe\u5b89\u5168\u3001SDN\u57fa\u7840\u7684\u80fd\u6e90\u4ea4\u6613\u57fa\u7840\u8bbe\u65bd\u4e2d\u6709\u6548\u4e14\u8f7b\u91cf\u7ea7\u7684nonce\u751f\u6210\u5668\uff0c\u9002\u7528\u4e8e\u707e\u96be\u6062\u590d\u573a\u666f"}}
{"id": "2509.10116", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.10116", "abs": "https://arxiv.org/abs/2509.10116", "authors": ["Julian Linke", "Barbara Schuppler"], "title": "Prominence-aware automatic speech recognition for conversational speech", "comment": null, "summary": "This paper investigates prominence-aware automatic speech recognition (ASR)\nby combining prominence detection and speech recognition for conversational\nAustrian German. First, prominence detectors were developed by fine-tuning\nwav2vec2 models to classify word-level prominence. The detector was then used\nto automatically annotate prosodic prominence in a large corpus. Based on those\nannotations, we trained novel prominence-aware ASR systems that simultaneously\ntranscribe words and their prominence levels. The integration of prominence\ninformation did not change performance compared to our baseline ASR system,\nwhile reaching a prominence detection accuracy of 85.53% for utterances where\nthe recognized word sequence was correct. This paper shows that\ntransformer-based models can effectively encode prosodic information and\nrepresents a novel contribution to prosody-enhanced ASR, with potential\napplications for linguistic research and prosody-informed dialogue systems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7ed3\u5408\u91cd\u97f3\u68c0\u6d4b\u548c\u8bed\u97f3\u8bc6\u522b\u7684\u663e\u8457\u6027\u611f\u77e5\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\uff0c\u5728\u5965\u5730\u5229\u5fb7\u8bed\u5bf9\u8bdd\u6570\u636e\u4e0a\u5b9e\u73b0\u4e8685.53%\u7684\u91cd\u97f3\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u4f46\u672a\u663e\u8457\u63d0\u5347ASR\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5c06\u97f5\u5f8b\u91cd\u97f3\u4fe1\u606f\u6574\u5408\u5230\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u4e2d\uff0c\u4ee5\u63d0\u5347\u8bed\u97f3\u7406\u89e3\u7684\u51c6\u786e\u6027\uff0c\u5e76\u4e3a\u8bed\u8a00\u5b66\u7814\u7a76\u548c\u57fa\u4e8e\u97f5\u5f8b\u7684\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u652f\u6301\u3002", "method": "\u901a\u8fc7\u5fae\u8c03wav2vec2\u6a21\u578b\u5f00\u53d1\u8bcd\u7ea7\u91cd\u97f3\u68c0\u6d4b\u5668\uff0c\u81ea\u52a8\u6807\u6ce8\u5927\u578b\u8bed\u6599\u5e93\u7684\u97f5\u5f8b\u91cd\u97f3\uff0c\u7136\u540e\u8bad\u7ec3\u540c\u65f6\u8f6c\u5f55\u5355\u8bcd\u53ca\u5176\u91cd\u97f3\u6c34\u5e73\u7684\u663e\u8457\u6027\u611f\u77e5ASR\u7cfb\u7edf\u3002", "result": "\u96c6\u6210\u91cd\u97f3\u4fe1\u606f\u540eASR\u6027\u80fd\u4e0e\u57fa\u7ebf\u7cfb\u7edf\u76f8\u5f53\uff0c\u4f46\u5728\u8bc6\u522b\u6b63\u786e\u7684\u8bed\u53e5\u4e2d\u91cd\u97f3\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe\u523085.53%\u3002", "conclusion": "\u57fa\u4e8etransformer\u7684\u6a21\u578b\u80fd\u6709\u6548\u7f16\u7801\u97f5\u5f8b\u4fe1\u606f\uff0c\u4e3a\u97f5\u5f8b\u589e\u5f3a\u7684ASR\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u8d21\u732e\uff0c\u5177\u6709\u8bed\u8a00\u5b66\u7814\u7a76\u548c\u97f5\u5f8b\u611f\u77e5\u5bf9\u8bdd\u7cfb\u7edf\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.10303", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10303", "abs": "https://arxiv.org/abs/2509.10303", "authors": ["Jesse van Remmerden", "Zaharah Bukhsh", "Yingqian Zhang"], "title": "Generalizing Beyond Suboptimality: Offline Reinforcement Learning Learns Effective Scheduling through Random Data", "comment": null, "summary": "The Job-Shop Scheduling Problem (JSP) and Flexible Job-Shop Scheduling\nProblem (FJSP), are canonical combinatorial optimization problems with\nwide-ranging applications in industrial operations. In recent years, many\nonline reinforcement learning (RL) approaches have been proposed to learn\nconstructive heuristics for JSP and FJSP. Although effective, these online RL\nmethods require millions of interactions with simulated environments that may\nnot capture real-world complexities, and their random policy initialization\nleads to poor sample efficiency. To address these limitations, we introduce\nConservative Discrete Quantile Actor-Critic (CDQAC), a novel offline RL\nalgorithm that learns effective scheduling policies directly from historical\ndata, eliminating the need for costly online interactions, while maintaining\nthe ability to improve upon suboptimal training data. CDQAC couples a\nquantile-based critic with a delayed policy update, estimating the return\ndistribution of each machine-operation pair rather than selecting pairs\noutright. Our extensive experiments demonstrate CDQAC's remarkable ability to\nlearn from diverse data sources. CDQAC consistently outperforms the original\ndata-generating heuristics and surpasses state-of-the-art offline and online RL\nbaselines. In addition, CDQAC is highly sample efficient, requiring only 10-20\ntraining instances to learn high-quality policies. Surprisingly, we find that\nCDQAC performs better when trained on data generated by a random heuristic than\nwhen trained on higher-quality data from genetic algorithms and priority\ndispatching rules.", "AI": {"tldr": "\u63d0\u51faCDQAC\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u76f4\u63a5\u4ece\u5386\u53f2\u6570\u636e\u5b66\u4e60\u4f5c\u4e1a\u8f66\u95f4\u8c03\u5ea6\u7b56\u7565\uff0c\u65e0\u9700\u5728\u7ebf\u4ea4\u4e92\uff0c\u5728\u6837\u672c\u6548\u7387\u548c\u6570\u636e\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u4f18\u5f02", "motivation": "\u4f20\u7edf\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6a21\u62df\u73af\u5883\u4ea4\u4e92\u4e14\u6837\u672c\u6548\u7387\u4f4e\uff0c\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u4e16\u754c\u590d\u6742\u6027\uff0c\u9700\u8981\u76f4\u63a5\u4ece\u5386\u53f2\u6570\u636e\u5b66\u4e60\u8c03\u5ea6\u7b56\u7565\u7684\u65b9\u6cd5", "method": "CDQAC\u7b97\u6cd5\u7ed3\u5408\u5206\u4f4d\u6570critic\u548c\u5ef6\u8fdf\u7b56\u7565\u66f4\u65b0\uff0c\u4f30\u8ba1\u6bcf\u4e2a\u673a\u5668-\u64cd\u4f5c\u5bf9\u7684\u56de\u62a5\u5206\u5e03\u800c\u975e\u76f4\u63a5\u9009\u62e9\uff0c\u80fd\u591f\u4ece\u591a\u6837\u5316\u6570\u636e\u6e90\u5b66\u4e60", "result": "CDQAC\u663e\u8457\u4f18\u4e8e\u539f\u59cb\u6570\u636e\u751f\u6210\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u79bb\u7ebf\u548c\u5728\u7ebfRL\u57fa\u7ebf\uff0c\u4ec5\u970010-20\u4e2a\u8bad\u7ec3\u5b9e\u4f8b\u5373\u53ef\u5b66\u4e60\u9ad8\u8d28\u91cf\u7b56\u7565", "conclusion": "CDQAC\u662f\u9ad8\u6548\u7684\u79bb\u7ebfRL\u8c03\u5ea6\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u5408\u4ece\u968f\u673a\u542f\u53d1\u5f0f\u751f\u6210\u7684\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u5728\u6837\u672c\u6548\u7387\u548c\u6570\u636e\u8d28\u91cf\u8981\u6c42\u65b9\u9762\u5177\u6709\u4f18\u52bf"}}
{"id": "2509.10127", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10127", "abs": "https://arxiv.org/abs/2509.10127", "authors": ["Zhengyu Hu", "Zheyuan Xiao", "Max Xiong", "Yuxuan Lei", "Tianfu Wang", "Jianxun Lian", "Kaize Ding", "Ziang Xiao", "Nicholas Jing Yuan", "Xing Xie"], "title": "Population-Aligned Persona Generation for LLM-based Social Simulation", "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled human-like\nsocial simulations at unprecedented scale and fidelity, offering new\nopportunities for computational social science. A key challenge, however, is\nthe construction of persona sets that authentically represent the diversity and\ndistribution of real-world populations. Most existing LLM-based social\nsimulation studies focus primarily on designing agentic frameworks and\nsimulation environments, often overlooking the complexities of persona\ngeneration and the potential biases introduced by unrepresentative persona\nsets. In this paper, we propose a systematic framework for synthesizing\nhigh-quality, population-aligned persona sets for LLM-driven social simulation.\nOur approach begins by leveraging LLMs to generate narrative personas from\nlong-term social media data, followed by rigorous quality assessment to filter\nout low-fidelity profiles. We then apply importance sampling to achieve global\nalignment with reference psychometric distributions, such as the Big Five\npersonality traits. To address the needs of specific simulation contexts, we\nfurther introduce a task-specific module that adapts the globally aligned\npersona set to targeted subpopulations. Extensive experiments demonstrate that\nour method significantly reduces population-level bias and enables accurate,\nflexible social simulation for a wide range of research and policy\napplications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u4e0e\u4eba\u53e3\u5206\u5e03\u5bf9\u9f50\u7684AI\u89d2\u8272\u96c6\uff0c\u4ee5\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u793e\u4f1a\u6a21\u62df\u4e2d\u7684\u4eba\u53e3\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u793e\u4f1a\u6a21\u62df\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u667a\u80fd\u4f53\u6846\u67b6\u548c\u6a21\u62df\u73af\u5883\u8bbe\u8ba1\uff0c\u5f80\u5f80\u5ffd\u89c6\u4e86\u89d2\u8272\u751f\u6210\u7684\u590d\u6742\u6027\u4ee5\u53ca\u975e\u4ee3\u8868\u6027\u89d2\u8272\u96c6\u53ef\u80fd\u5f15\u5165\u7684\u504f\u89c1\u3002\u9700\u8981\u6784\u5efa\u80fd\u591f\u771f\u5b9e\u53cd\u6620\u73b0\u5b9e\u4e16\u754c\u4eba\u53e3\u591a\u6837\u6027\u548c\u5206\u5e03\u7684\u89d2\u8272\u96c6\u5408\u3002", "method": "1\uff09\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u957f\u671f\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u751f\u6210\u53d9\u4e8b\u89d2\u8272\uff1b2\uff09\u901a\u8fc7\u4e25\u683c\u8d28\u91cf\u8bc4\u4f30\u8fc7\u6ee4\u4f4e\u8d28\u91cf\u6863\u6848\uff1b3\uff09\u5e94\u7528\u91cd\u8981\u6027\u91c7\u6837\u5b9e\u73b0\u4e0e\u53c2\u8003\u5fc3\u7406\u6d4b\u91cf\u5206\u5e03\uff08\u5982\u5927\u4e94\u4eba\u683c\u7279\u8d28\uff09\u7684\u5168\u5c40\u5bf9\u9f50\uff1b4\uff09\u5f15\u5165\u4efb\u52a1\u7279\u5b9a\u6a21\u5757\u5c06\u5168\u5c40\u5bf9\u9f50\u7684\u89d2\u8272\u96c6\u9002\u914d\u5230\u76ee\u6807\u5b50\u7fa4\u4f53\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u53e3\u5c42\u9762\u7684\u504f\u5dee\uff0c\u80fd\u591f\u4e3a\u5e7f\u6cdb\u7684\u7814\u7a76\u548c\u653f\u7b56\u5e94\u7528\u63d0\u4f9b\u51c6\u786e\u3001\u7075\u6d3b\u7684\u793e\u4f1a\u6a21\u62df\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aLLM\u9a71\u52a8\u7684\u793e\u4f1a\u6a21\u62df\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u3001\u4eba\u53e3\u5bf9\u9f50\u7684\u89d2\u8272\u96c6\u751f\u6210\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7814\u7a76\u4e2d\u89d2\u8272\u96c6\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5177\u6709\u91cd\u8981\u7684\u7814\u7a76\u548c\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.10308", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10308", "abs": "https://arxiv.org/abs/2509.10308", "authors": ["Joshua Dimasaka", "Christian Gei\u00df", "Robert Muir-Wood", "Emily So"], "title": "GraphCSVAE: Graph Categorical Structured Variational Autoencoder for Spatiotemporal Auditing of Physical Vulnerability Towards Sustainable Post-Disaster Risk Reduction", "comment": "Accepted full paper at the 8th International Disaster and Risk\n  Conference, IDRC 2025 | Keywords: weakly supervised, graph deep learning,\n  categorical distribution, physical vulnerability, remote sensing,\n  spatiotemporal disaster risk, transition matrix | The data and code are\n  respectively available at https://doi.org/10.5281/zenodo.16656471 and\n  https://github.com/riskaudit/GraphCSVAE", "summary": "In the aftermath of disasters, many institutions worldwide face challenges in\ncontinually monitoring changes in disaster risk, limiting the ability of key\ndecision-makers to assess progress towards the UN Sendai Framework for Disaster\nRisk Reduction 2015-2030. While numerous efforts have substantially advanced\nthe large-scale modeling of hazard and exposure through Earth observation and\ndata-driven methods, progress remains limited in modeling another equally\nimportant yet challenging element of the risk equation: physical vulnerability.\nTo address this gap, we introduce Graph Categorical Structured Variational\nAutoencoder (GraphCSVAE), a novel probabilistic data-driven framework for\nmodeling physical vulnerability by integrating deep learning, graph\nrepresentation, and categorical probabilistic inference, using time-series\nsatellite-derived datasets and prior expert belief systems. We introduce a\nweakly supervised first-order transition matrix that reflects the changes in\nthe spatiotemporal distribution of physical vulnerability in two\ndisaster-stricken and socioeconomically disadvantaged areas: (1) the\ncyclone-impacted coastal Khurushkul community in Bangladesh and (2) the\nmudslide-affected city of Freetown in Sierra Leone. Our work reveals\npost-disaster regional dynamics in physical vulnerability, offering valuable\ninsights into localized spatiotemporal auditing and sustainable strategies for\npost-disaster risk reduction.", "AI": {"tldr": "\u63d0\u51fa\u4e86GraphCSVAE\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u3001\u56fe\u8868\u793a\u548c\u5206\u7c7b\u6982\u7387\u63a8\u7406\u6574\u5408\u536b\u661f\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u548c\u4e13\u5bb6\u77e5\u8bc6\uff0c\u7528\u4e8e\u5efa\u6a21\u707e\u5bb3\u7269\u7406\u8106\u5f31\u6027\uff0c\u5e76\u5728\u5b5f\u52a0\u62c9\u56fd\u548c\u585e\u62c9\u5229\u6602\u7684\u707e\u5bb3\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u707e\u5bb3\u98ce\u9669\u8bc4\u4f30\u65b9\u6cd5\u5728\u7269\u7406\u8106\u5f31\u6027\u5efa\u6a21\u65b9\u9762\u8fdb\u5c55\u6709\u9650\uff0c\u9650\u5236\u4e86\u51b3\u7b56\u8005\u5bf9\u8054\u5408\u56fd\u4ed9\u53f0\u6846\u67b6\u8fdb\u5c55\u7684\u8bc4\u4f30\u80fd\u529b\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u6570\u636e\u9a71\u52a8\u6846\u67b6\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f00\u53d1\u4e86Graph Categorical Structured Variational Autoencoder (GraphCSVAE)\u6982\u7387\u6846\u67b6\uff0c\u6574\u5408\u6df1\u5ea6\u5b66\u4e60\u3001\u56fe\u8868\u793a\u548c\u5206\u7c7b\u6982\u7387\u63a8\u7406\uff0c\u4f7f\u7528\u65f6\u95f4\u5e8f\u5217\u536b\u661f\u6570\u636e\u548c\u4e13\u5bb6\u5148\u9a8c\u77e5\u8bc6\uff0c\u5f15\u5165\u5f31\u76d1\u7763\u4e00\u9636\u8f6c\u79fb\u77e9\u9635\u6765\u53cd\u6620\u7269\u7406\u8106\u5f31\u6027\u7684\u65f6\u7a7a\u5206\u5e03\u53d8\u5316\u3002", "result": "\u5728\u4e24\u4e2a\u707e\u5bb3\u9891\u53d1\u4e14\u793e\u4f1a\u7ecf\u6d4e\u5f31\u52bf\u5730\u533a\uff08\u5b5f\u52a0\u62c9\u56fd\u6cbf\u6d77\u793e\u533a\u548c\u585e\u62c9\u5229\u6602\u5f17\u91cc\u6566\u5e02\uff09\u6210\u529f\u63ed\u793a\u4e86\u707e\u540e\u7269\u7406\u8106\u5f31\u6027\u7684\u533a\u57df\u52a8\u6001\uff0c\u63d0\u4f9b\u4e86\u5c40\u90e8\u65f6\u7a7a\u5ba1\u8ba1\u548c\u53ef\u6301\u7eed\u51cf\u707e\u7b56\u7565\u7684\u5b9d\u8d35\u89c1\u89e3\u3002", "conclusion": "GraphCSVAE\u6846\u67b6\u80fd\u591f\u6709\u6548\u5efa\u6a21\u7269\u7406\u8106\u5f31\u6027\uff0c\u4e3a\u707e\u540e\u98ce\u9669\u8bc4\u4f30\u548c\u51cf\u707e\u7b56\u7565\u5236\u5b9a\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u8054\u5408\u56fd\u4ed9\u53f0\u6846\u67b6\u7684\u76ee\u6807\u3002"}}
{"id": "2509.10324", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10324", "abs": "https://arxiv.org/abs/2509.10324", "authors": ["Myung Jin Kim", "YeongHyeon Park", "Il Dong Yun"], "title": "ARMA Block: A CNN-Based Autoregressive and Moving Average Module for Long-Term Time Series Forecasting", "comment": null, "summary": "This paper proposes a simple yet effective convolutional module for long-term\ntime series forecasting. The proposed block, inspired by the Auto-Regressive\nIntegrated Moving Average (ARIMA) model, consists of two convolutional\ncomponents: one for capturing the trend (autoregression) and the other for\nrefining local variations (moving average). Unlike conventional ARIMA, which\nrequires iterative multi-step forecasting, the block directly performs\nmulti-step forecasting, making it easily extendable to multivariate settings.\nExperiments on nine widely used benchmark datasets demonstrate that our method\nARMA achieves competitive accuracy, particularly on datasets exhibiting strong\ntrend variations, while maintaining architectural simplicity. Furthermore,\nanalysis shows that the block inherently encodes absolute positional\ninformation, suggesting its potential as a lightweight replacement for\npositional embeddings in sequential models.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eARIMA\u6a21\u578b\u542f\u53d1\u7684\u7b80\u5355\u5377\u79ef\u6a21\u5757ARMA\uff0c\u7528\u4e8e\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u5305\u542b\u8d8b\u52bf\u6355\u6349\u548c\u5c40\u90e8\u53d8\u5316\u4fee\u6b63\u4e24\u4e2a\u7ec4\u4ef6\uff0c\u80fd\u76f4\u63a5\u8fdb\u884c\u591a\u6b65\u9884\u6d4b\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u7ade\u4e89\u6027\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edfARIMA\u6a21\u578b\u9700\u8981\u8fed\u4ee3\u591a\u6b65\u9884\u6d4b\u4e14\u96be\u4ee5\u6269\u5c55\u5230\u591a\u53d8\u91cf\u8bbe\u7f6e\uff0c\u9700\u8981\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u6a21\u5757\u6765\u5b9e\u73b0\u76f4\u63a5\u591a\u6b65\u9884\u6d4b\u5e76\u4fdd\u6301\u67b6\u6784\u7b80\u6d01\u6027\u3002", "method": "\u8bbe\u8ba1\u5305\u542b\u4e24\u4e2a\u5377\u79ef\u7ec4\u4ef6\u7684\u6a21\u5757\uff1a\u4e00\u4e2a\u7528\u4e8e\u6355\u6349\u8d8b\u52bf\uff08\u81ea\u56de\u5f52\uff09\uff0c\u53e6\u4e00\u4e2a\u7528\u4e8e\u7ec6\u5316\u5c40\u90e8\u53d8\u5316\uff08\u79fb\u52a8\u5e73\u5747\uff09\uff0c\u76f4\u63a5\u8fdb\u884c\u591a\u6b65\u9884\u6d4b\u5e76\u6613\u4e8e\u6269\u5c55\u5230\u591a\u53d8\u91cf\u573a\u666f\u3002", "result": "\u5728\u4e5d\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cARMA\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u7cbe\u5ea6\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u5f3a\u8d8b\u52bf\u53d8\u5316\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u67b6\u6784\u7b80\u6d01\u6027\u3002", "conclusion": "\u8be5\u6a21\u5757\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u8fd8\u56fa\u6709\u5730\u7f16\u7801\u4e86\u7edd\u5bf9\u4f4d\u7f6e\u4fe1\u606f\uff0c\u6709\u6f5c\u529b\u4f5c\u4e3a\u5e8f\u5217\u6a21\u578b\u4e2d\u4f4d\u7f6e\u5d4c\u5165\u7684\u8f7b\u91cf\u7ea7\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2509.10179", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10179", "abs": "https://arxiv.org/abs/2509.10179", "authors": ["Ji\u0159\u00ed Mili\u010dka", "Anna Marklov\u00e1", "V\u00e1clav Cvr\u010dek"], "title": "Benchmark of stylistic variation in LLM-generated texts", "comment": null, "summary": "This study investigates the register variation in texts written by humans and\ncomparable texts produced by large language models (LLMs). Biber's\nmultidimensional analysis (MDA) is applied to a sample of human-written texts\nand AI-created texts generated to be their counterparts to find the dimensions\nof variation in which LLMs differ most significantly and most systematically\nfrom humans. As textual material, a new LLM-generated corpus AI-Brown is used,\nwhich is comparable to BE-21 (a Brown family corpus representing contemporary\nBritish English). Since all languages except English are underrepresented in\nthe training data of frontier LLMs, similar analysis is replicated on Czech\nusing AI-Koditex corpus and Czech multidimensional model. Examined were 16\nfrontier models in various settings and prompts, with emphasis placed on the\ndifference between base models and instruction-tuned models. Based on this, a\nbenchmark is created through which models can be compared with each other and\nranked in interpretable dimensions.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528Biber\u7684\u591a\u7ef4\u5206\u6790\u6cd5\u6bd4\u8f83\u4eba\u7c7b\u5199\u4f5c\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6587\u672c\u7684\u8bed\u57df\u5dee\u5f02\uff0c\u521b\u5efa\u4e86AI-Brown\u548cAI-Koditex\u8bed\u6599\u5e93\uff0c\u5206\u6790\u4e8616\u4e2a\u524d\u6cbf\u6a21\u578b\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u5efa\u7acb\u4e86\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u8bc4\u4f30\u57fa\u51c6\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6587\u672c\u4e0e\u4eba\u7c7b\u5199\u4f5c\u5728\u8bed\u57df\u7279\u5f81\u4e0a\u7684\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u7279\u522b\u662f\u5728\u82f1\u8bed\u548c\u6377\u514b\u8bed\u7b49\u8bad\u7ec3\u6570\u636e\u8f83\u5c11\u7684\u8bed\u8a00\u4e2d\uff0c\u4e3a\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u7ef4\u5ea6\u57fa\u51c6\u3002", "method": "\u5e94\u7528Biber\u7684\u591a\u7ef4\u5206\u6790\u6cd5(MDA)\uff0c\u4f7f\u7528AI-Brown\u8bed\u6599\u5e93\uff08\u5bf9\u5e94BE-21\u5e03\u6717\u5bb6\u65cf\u8bed\u6599\u5e93\uff09\u548cAI-Koditex\u6377\u514b\u8bed\u6599\u5e93\uff0c\u5206\u679016\u4e2a\u524d\u6cbfLLM\u5728\u4e0d\u540c\u8bbe\u7f6e\u548c\u63d0\u793a\u4e0b\u7684\u6587\u672c\u751f\u6210\u7279\u5f81\u3002", "result": "\u7814\u7a76\u53d1\u73b0LLM\u5728\u591a\u4e2a\u8bed\u57df\u7ef4\u5ea6\u4e0a\u4e0e\u4eba\u7c7b\u5199\u4f5c\u5b58\u5728\u663e\u8457\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u7279\u522b\u662f\u57fa\u7840\u6a21\u578b\u4e0e\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u4e4b\u95f4\u7684\u8868\u73b0\u5dee\u5f02\u660e\u663e\uff0c\u4e3a\u6a21\u578b\u6bd4\u8f83\u63d0\u4f9b\u4e86\u53ef\u91cf\u5316\u7684\u8bc4\u4f30\u7ef4\u5ea6\u3002", "conclusion": "\u7814\u7a76\u6210\u529f\u5efa\u7acb\u4e86\u57fa\u4e8e\u591a\u7ef4\u5206\u6790\u7684LLM\u8bc4\u4f30\u57fa\u51c6\uff0c\u80fd\u591f\u7cfb\u7edf\u6027\u5730\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u5728\u8bed\u57df\u7279\u5f81\u4e0a\u7684\u8868\u73b0\uff0c\u4e3a\u7406\u89e3LLM\u6587\u672c\u751f\u6210\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u7684\u5206\u6790\u6846\u67b6\u3002"}}
{"id": "2509.10363", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2509.10363", "abs": "https://arxiv.org/abs/2509.10363", "authors": ["Benjamin David Shaffer", "Brooks Kinch", "Joseph Klobusicky", "M. Ani Hsieh", "Nathaniel Trask"], "title": "Physics-informed sensor coverage through structure preserving machine learning", "comment": null, "summary": "We present a machine learning framework for adaptive source localization in\nwhich agents use a structure-preserving digital twin of a coupled\nhydrodynamic-transport system for real-time trajectory planning and data\nassimilation. The twin is constructed with conditional neural Whitney forms\n(CNWF), coupling the numerical guarantees of finite element exterior calculus\n(FEEC) with transformer-based operator learning. The resulting model preserves\ndiscrete conservation, and adapts in real time to streaming sensor data. It\nemploys a conditional attention mechanism to identify: a reduced Whitney-form\nbasis; reduced integral balance equations; and a source field, each compatible\nwith given sensor measurements. The induced reduced-order environmental model\nretains the stability and consistency of standard finite-element simulation,\nyielding a physically realizable, regular mapping from sensor data to the\nsource field. We propose a staggered scheme that alternates between evaluating\nthe digital twin and applying Lloyd's algorithm to guide sensor placement, with\nanalysis providing conditions for monotone improvement of a coverage\nfunctional. Using the predicted source field as an importance function within\nan optimal-recovery scheme, we demonstrate recovery of point sources under\ncontinuity assumptions, highlighting the role of regularity as a sufficient\ncondition for localization. Experimental comparisons with physics-agnostic\ntransformer architectures show improved accuracy in complex geometries when\nphysical constraints are enforced, indicating that structure preservation\nprovides an effective inductive bias for source identification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u795e\u7ecfWhitney\u5f62\u5f0f\u7684\u6570\u5b57\u5b6a\u751f\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u9002\u5e94\u6e90\u5b9a\u4f4d\uff0c\u7ed3\u5408\u6709\u9650\u5143\u5916\u5fae\u79ef\u5206\u548cTransformer\u7b97\u5b50\u5b66\u4e60\uff0c\u4fdd\u6301\u79bb\u6563\u5b88\u6052\u6027\u5e76\u5b9e\u65f6\u9002\u5e94\u4f20\u611f\u5668\u6570\u636e\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u590d\u6742\u6d41\u4f53\u8f93\u8fd0\u7cfb\u7edf\u4e2d\u6e90\u5b9a\u4f4d\u95ee\u9898\uff0c\u9700\u8981\u6784\u5efa\u80fd\u591f\u4fdd\u6301\u7269\u7406\u7ed3\u6784\u3001\u5b9e\u65f6\u9002\u5e94\u4f20\u611f\u5668\u6570\u636e\u5e76\u4fdd\u8bc1\u6570\u503c\u7a33\u5b9a\u6027\u7684\u6570\u5b57\u5b6a\u751f\u6a21\u578b\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u795e\u7ecfWhitney\u5f62\u5f0f(CNWF)\u6784\u5efa\u6570\u5b57\u5b6a\u751f\uff0c\u7ed3\u5408FEEC\u7684\u6570\u503c\u4fdd\u8bc1\u548cTransformer\u7b97\u5b50\u5b66\u4e60\u3002\u91c7\u7528\u4ea4\u9519\u65b9\u6848\u5728\u6570\u5b57\u5b6a\u751f\u8bc4\u4f30\u548cLloyd\u7b97\u6cd5\u5f15\u5bfc\u4f20\u611f\u5668\u5e03\u7f6e\u4e4b\u95f4\u4ea4\u66ff\u8fdb\u884c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u7269\u7406\u65e0\u5173\u7684Transformer\u67b6\u6784\u76f8\u6bd4\uff0c\u5728\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u4e2d\u5f3a\u5236\u7269\u7406\u7ea6\u675f\u65f6\u7cbe\u5ea6\u66f4\u9ad8\uff0c\u7ed3\u6784\u4fdd\u6301\u4e3a\u6e90\u8bc6\u522b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5f52\u7eb3\u504f\u7f6e\u3002", "conclusion": "\u7ed3\u6784\u4fdd\u6301\u7684\u6570\u5b57\u5b6a\u751f\u6846\u67b6\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u81ea\u9002\u5e94\u6e90\u5b9a\u4f4d\uff0c\u7269\u7406\u7ea6\u675f\u7684\u5f3a\u5236\u6267\u884c\u63d0\u9ad8\u4e86\u590d\u6742\u51e0\u4f55\u4e2d\u7684\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u6b63\u5219\u6027\u4f5c\u4e3a\u5b9a\u4f4d\u7684\u5145\u5206\u6761\u4ef6\u53d1\u6325\u4e86\u91cd\u8981\u4f5c\u7528\u3002"}}
{"id": "2509.10184", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10184", "abs": "https://arxiv.org/abs/2509.10184", "authors": ["Leen Almajed", "Abeer ALdayel"], "title": "Incongruent Positivity: When Miscalibrated Positivity Undermines Online Supportive Conversations", "comment": "This paper is under review", "summary": "In emotionally supportive conversations, well-intended positivity can\nsometimes misfire, leading to responses that feel dismissive, minimizing, or\nunrealistically optimistic. We examine this phenomenon of incongruent\npositivity as miscalibrated expressions of positive support in both human and\nLLM generated responses. To this end, we collected real user-assistant\ndialogues from Reddit across a range of emotional intensities and generated\nadditional responses using large language models for the same context. We\ncategorize these conversations by intensity into two levels: Mild, which covers\nrelationship tension and general advice, and Severe, which covers grief and\nanxiety conversations. This level of categorization enables a comparative\nanalysis of how supportive responses vary across lower and higher stakes\ncontexts. Our analysis reveals that LLMs are more prone to unrealistic\npositivity through dismissive and minimizing tone, particularly in high-stakes\ncontexts. To further study the underlying dimensions of this phenomenon, we\nfinetune LLMs on datasets with strong and weak emotional reactions. Moreover,\nwe developed a weakly supervised multilabel classifier ensemble (DeBERTa and\nMentalBERT) that shows improved detection of incongruent positivity types\nacross two sorts of concerns (Mild and Severe). Our findings shed light on the\nneed to move beyond merely generating generic positive responses and instead\nstudy the congruent support measures to balance positive affect with emotional\nacknowledgment. This approach offers insights into aligning large language\nmodels with affective expectations in the online supportive dialogue, paving\nthe way toward context-aware and trust preserving online conversation systems.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\u4e2d\u4e0d\u534f\u8c03\u7684\u79ef\u6781\u56de\u5e94\u95ee\u9898\uff0c\u5206\u6790\u4eba\u7c7b\u548cLLM\u751f\u6210\u56de\u5e94\u4e2d\u7684\u4e0d\u6070\u5f53\u79ef\u6781\u8868\u8fbe\uff0c\u7279\u522b\u662f\u5728\u9ad8\u98ce\u9669\u60c5\u5883\u4e0bLLM\u66f4\u5bb9\u6613\u4ea7\u751f\u4e0d\u5207\u5b9e\u9645\u7684\u4e50\u89c2\u56de\u5e94\u3002", "motivation": "\u7814\u7a76\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\u4e2d\u5584\u610f\u4f46\u53ef\u80fd\u9002\u5f97\u5176\u53cd\u7684\u79ef\u6781\u56de\u5e94\u73b0\u8c61\uff0c\u8fd9\u4e9b\u56de\u5e94\u53ef\u80fd\u663e\u5f97\u8f7b\u63cf\u6de1\u5199\u3001\u6700\u5c0f\u5316\u95ee\u9898\u6216\u4e0d\u5207\u5b9e\u9645\u5730\u4e50\u89c2\uff0c\u5f71\u54cd\u5bf9\u8bdd\u6548\u679c\u3002", "method": "\u6536\u96c6Reddit\u771f\u5b9e\u7528\u6237-\u52a9\u624b\u5bf9\u8bdd\uff0c\u6309\u60c5\u611f\u5f3a\u5ea6\u5206\u7c7b\u4e3a\u8f7b\u5ea6\uff08\u5173\u7cfb\u7d27\u5f20\u3001\u4e00\u822c\u5efa\u8bae\uff09\u548c\u91cd\u5ea6\uff08\u60b2\u4f24\u3001\u7126\u8651\uff09\uff0c\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u989d\u5916\u56de\u5e94\uff0c\u5f00\u53d1\u5f31\u76d1\u7763\u591a\u6807\u7b7e\u5206\u7c7b\u5668\u68c0\u6d4b\u4e0d\u534f\u8c03\u79ef\u6781\u56de\u5e94\u7c7b\u578b\u3002", "result": "\u53d1\u73b0LLM\u5728\u9ad8\u98ce\u9669\u60c5\u5883\u4e0b\u66f4\u5bb9\u6613\u901a\u8fc7\u8f7b\u63cf\u6de1\u5199\u548c\u6700\u5c0f\u5316\u8bed\u6c14\u8868\u73b0\u51fa\u4e0d\u5207\u5b9e\u9645\u7684\u79ef\u6781\u6027\uff0c\u5f00\u53d1\u7684\u591a\u6807\u7b7e\u5206\u7c7b\u5668\u5728\u68c0\u6d4b\u4e0d\u540c\u7c7b\u578b\u4e0d\u534f\u8c03\u79ef\u6781\u6027\u65b9\u9762\u8868\u73b0\u6539\u5584\u3002", "conclusion": "\u9700\u8981\u8d85\u8d8a\u751f\u6210\u901a\u7528\u79ef\u6781\u56de\u5e94\uff0c\u7814\u7a76\u534f\u8c03\u7684\u652f\u6301\u63aa\u65bd\u6765\u5e73\u8861\u79ef\u6781\u60c5\u611f\u4e0e\u60c5\u611f\u8ba4\u540c\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u5728\u7ebf\u652f\u6301\u5bf9\u8bdd\u4e2d\u7684\u60c5\u611f\u671f\u671b\u5bf9\u9f50\u63d0\u4f9b\u89c1\u89e3\uff0c\u63a8\u52a8\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u4fe1\u4efb\u4fdd\u62a4\u7684\u5728\u7ebf\u5bf9\u8bdd\u7cfb\u7edf\u53d1\u5c55\u3002"}}
{"id": "2509.10367", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10367", "abs": "https://arxiv.org/abs/2509.10367", "authors": ["Tong Chen", "Raghavendra Selvan"], "title": "A Discrepancy-Based Perspective on Dataset Condensation", "comment": "30 pages, 4 tables, 1 figure", "summary": "Given a dataset of finitely many elements $\\mathcal{T} = \\{\\mathbf{x}_i\\}_{i\n= 1}^N$, the goal of dataset condensation (DC) is to construct a synthetic\ndataset $\\mathcal{S} = \\{\\tilde{\\mathbf{x}}_j\\}_{j = 1}^M$ which is\nsignificantly smaller ($M \\ll N$) such that a model trained from scratch on\n$\\mathcal{S}$ achieves comparable or even superior generalization performance\nto a model trained on $\\mathcal{T}$. Recent advances in DC reveal a close\nconnection to the problem of approximating the data distribution represented by\n$\\mathcal{T}$ with a reduced set of points. In this work, we present a unified\nframework that encompasses existing DC methods and extend the task-specific\nnotion of DC to a more general and formal definition using notions of\ndiscrepancy, which quantify the distance between probability distribution in\ndifferent regimes. Our framework broadens the objective of DC beyond\ngeneralization, accommodating additional objectives such as robustness,\nprivacy, and other desirable properties.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u6570\u636e\u96c6\u538b\u7f29\u4efb\u52a1\u4ece\u4f20\u7edf\u7684\u6cdb\u5316\u6027\u80fd\u6269\u5c55\u5230\u5305\u62ec\u9c81\u68d2\u6027\u3001\u9690\u79c1\u4fdd\u62a4\u7b49\u66f4\u591a\u76ee\u6807\uff0c\u5e76\u4f7f\u7528\u5dee\u5f02\u5ea6\u91cf\u6765\u5f62\u5f0f\u5316\u5b9a\u4e49\u5206\u5e03\u903c\u8fd1\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u538b\u7f29\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6cdb\u5316\u6027\u80fd\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u3002\u4f5c\u8005\u5e0c\u671b\u5efa\u7acb\u4e00\u4e2a\u66f4\u901a\u7528\u7684\u5f62\u5f0f\u5316\u5b9a\u4e49\uff0c\u80fd\u591f\u6db5\u76d6\u591a\u79cd\u76ee\u6807\u800c\u4e0d\u4ec5\u4ec5\u662f\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5dee\u5f02\u5ea6\u91cf\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u4f7f\u7528\u6982\u7387\u5206\u5e03\u95f4\u7684\u8ddd\u79bb\u5ea6\u91cf\u6765\u5f62\u5f0f\u5316\u6570\u636e\u96c6\u538b\u7f29\u95ee\u9898\uff0c\u5c06\u4efb\u52a1\u4ece\u5355\u7eaf\u7684\u6cdb\u5316\u6269\u5c55\u5230\u9c81\u68d2\u6027\u3001\u9690\u79c1\u4fdd\u62a4\u7b49\u591a\u4e2a\u7ef4\u5ea6\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u80fd\u591f\u7edf\u4e00\u73b0\u6709\u7684\u6570\u636e\u96c6\u538b\u7f29\u65b9\u6cd5\uff0c\u5e76\u4e3a\u6269\u5c55DC\u76ee\u6807\u5230\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6570\u636e\u96c6\u538b\u7f29\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u7406\u8bba\u652f\u6491\uff0c\u4f7f\u5176\u80fd\u591f\u9002\u5e94\u5305\u62ec\u9c81\u68d2\u6027\u3001\u9690\u79c1\u4fdd\u62a4\u5728\u5185\u7684\u591a\u79cd\u5b9e\u9645\u9700\u6c42\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.10199", "categories": ["cs.CL", "I.7; I.2; J.4"], "pdf": "https://arxiv.org/pdf/2509.10199", "abs": "https://arxiv.org/abs/2509.10199", "authors": ["Mikl\u00f3s Seb\u0151k", "Viktor Kov\u00e1cs", "Martin B\u00e1n\u00f3czy", "Daniel M\u00f8ller Eriksen", "Nathalie Neptune", "Philippe Roussille"], "title": "Beyond Token Limits: Assessing Language Model Performance on Long Text Classification", "comment": null, "summary": "The most widely used large language models in the social sciences (such as\nBERT, and its derivatives, e.g. RoBERTa) have a limitation on the input text\nlength that they can process to produce predictions. This is a particularly\npressing issue for some classification tasks, where the aim is to handle long\ninput texts. One such area deals with laws and draft laws (bills), which can\nhave a length of multiple hundred pages and, therefore, are not particularly\namenable for processing with models that can only handle e.g. 512 tokens. In\nthis paper, we show results from experiments covering 5 languages with\nXLM-RoBERTa, Longformer, GPT-3.5, GPT-4 models for the multiclass\nclassification task of the Comparative Agendas Project, which has a codebook of\n21 policy topic labels from education to health care. Results show no\nparticular advantage for the Longformer model, pre-trained specifically for the\npurposes of handling long inputs. The comparison between the GPT variants and\nthe best-performing open model yielded an edge for the latter. An analysis of\nclass-level factors points to the importance of support and substance overlaps\nbetween specific categories when it comes to performance on long text inputs.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u591a\u79cd\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u6587\u672c\u5206\u7c7b\u4efb\u52a1\uff08\u7279\u522b\u662f\u6cd5\u5f8b\u6587\u4ef6\uff09\u65f6\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u4e13\u95e8\u4e3a\u957f\u6587\u672c\u8bbe\u8ba1\u7684Longformer\u6a21\u578b\u5e76\u65e0\u660e\u663e\u4f18\u52bf\uff0c\u5f00\u6e90\u6a21\u578b\u8868\u73b0\u4f18\u4e8eGPT\u53d8\u4f53\u3002", "motivation": "\u73b0\u6709\u4e3b\u6d41\u8bed\u8a00\u6a21\u578b\uff08\u5982BERT\u53ca\u5176\u884d\u751f\u6a21\u578b\uff09\u5b58\u5728\u8f93\u5165\u957f\u5ea6\u9650\u5236\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u957f\u8fbe\u6570\u767e\u9875\u7684\u6cd5\u5f8b\u6587\u4ef6\u548c\u6cd5\u6848\u6587\u672c\uff0c\u8fd9\u5728\u793e\u4f1a\u79d1\u5b66\u7684\u957f\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u662f\u4e00\u4e2a\u4e9f\u5f85\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u57285\u79cd\u8bed\u8a00\u4e0a\u5b9e\u9a8c\u6bd4\u8f83\u4e86XLM-RoBERTa\u3001Longformer\u3001GPT-3.5\u548cGPT-4\u6a21\u578b\uff0c\u4f7f\u7528\u6bd4\u8f83\u8bae\u7a0b\u9879\u76ee\u768421\u4e2a\u653f\u7b56\u4e3b\u9898\u6807\u7b7e\u8fdb\u884c\u591a\u7c7b\u5206\u7c7b\u4efb\u52a1\u3002", "result": "\u4e13\u95e8\u4e3a\u957f\u6587\u672c\u9884\u8bad\u7ec3\u7684Longformer\u6a21\u578b\u6ca1\u6709\u663e\u793a\u51fa\u7279\u522b\u4f18\u52bf\uff1b\u5f00\u6e90\u6a21\u578b\u5728\u6bd4\u8f83\u4e2d\u8868\u73b0\u4f18\u4e8eGPT\u53d8\u4f53\uff1b\u7c7b\u522b\u5c42\u9762\u7684\u5206\u6790\u663e\u793a\u7279\u5b9a\u7c7b\u522b\u4e4b\u95f4\u7684\u652f\u6301\u5ea6\u548c\u5185\u5bb9\u91cd\u53e0\u5bf9\u957f\u6587\u672c\u6027\u80fd\u5f88\u91cd\u8981\u3002", "conclusion": "\u5bf9\u4e8e\u957f\u6587\u672c\u5206\u7c7b\u4efb\u52a1\uff0c\u4e13\u95e8\u7684\u957f\u6587\u672c\u6a21\u578b\u4e0d\u4e00\u5b9a\u4f18\u4e8e\u901a\u7528\u6a21\u578b\uff0c\u5f00\u6e90\u6a21\u578b\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u597d\uff0c\u7c7b\u522b\u95f4\u7684\u76f8\u4f3c\u6027\u5bf9\u5206\u7c7b\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\u3002"}}
{"id": "2509.10369", "categories": ["cs.LG", "cs.AI", "eess.SP", "q-bio.TO"], "pdf": "https://arxiv.org/pdf/2509.10369", "abs": "https://arxiv.org/abs/2509.10369", "authors": ["Gul Rukh Khattak", "Konstantinos Patlatzoglou", "Joseph Barker", "Libor Pastika", "Boroumand Zeidaabadi", "Ahmed El-Medany", "Hesham Aggour", "Yixiu Liang", "Antonio H. Ribeiro", "Jeffrey Annis", "Antonio Luiz Pinho Ribeiro", "Junbo Ge", "Daniel B. Kramer", "Jonathan W. Waks", "Evan Brittain", "Nicholas Peters", "Fu Siong Ng", "Arunashis Sau"], "title": "Data distribution impacts the performance and generalisability of contrastive learning-based foundation models of electrocardiograms", "comment": "Currently under review at npj Digital Medicine", "summary": "Contrastive learning is a widely adopted self-supervised pretraining\nstrategy, yet its dependence on cohort composition remains underexplored. We\npresent Contrasting by Patient Augmented Electrocardiograms (CAPE) foundation\nmodel and pretrain on four cohorts (n = 5,203,352), from diverse populations\nacross three continents (North America, South America, Asia). We systematically\nassess how cohort demographics, health status, and population diversity\ninfluence the downstream performance for prediction tasks also including two\nadditional cohorts from another continent (Europe). We find that downstream\nperformance depends on the distributional properties of the pretraining cohort,\nincluding demographics and health status. Moreover, while pretraining with a\nmulti-centre, demographically diverse cohort improves in-distribution accuracy,\nit reduces out-of-distribution (OOD) generalisation of our contrastive approach\nby encoding cohort-specific artifacts. To address this, we propose the\nIn-Distribution Batch (IDB) strategy, which preserves intra-cohort consistency\nduring pretraining and enhances OOD robustness. This work provides important\ninsights for developing clinically fair and generalisable foundation models.", "AI": {"tldr": "\u5bf9\u6bd4\u5b66\u4e60\u5728ECG\u9884\u8bad\u7ec3\u4e2d\u53d7\u961f\u5217\u7ec4\u6210\u5f71\u54cd\uff0c\u591a\u4e2d\u5fc3\u591a\u6837\u5316\u961f\u5217\u63d0\u5347\u5206\u5e03\u5185\u51c6\u786e\u7387\u4f46\u964d\u4f4e\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\uff0c\u63d0\u51faIDB\u7b56\u7565\u589e\u5f3a\u9c81\u68d2\u6027", "motivation": "\u63a2\u7d22\u5bf9\u6bd4\u5b66\u4e60\u5728\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u4e2d\u5bf9\u961f\u5217\u7ec4\u6210\u7684\u4f9d\u8d56\u6027\uff0c\u7279\u522b\u662f\u4eba\u53e3\u7edf\u8ba1\u5b66\u7279\u5f81\u3001\u5065\u5eb7\u72b6\u51b5\u548c\u4eba\u7fa4\u591a\u6837\u6027\u5bf9\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd", "method": "\u63d0\u51faCAPE\u57fa\u7840\u6a21\u578b\uff0c\u5728\u56db\u5927\u6d32\u4e94\u4e2a\u961f\u5217(n=5,203,352)\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7cfb\u7edf\u8bc4\u4f30\u961f\u5217\u7279\u5f81\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51faIn-Distribution Batch (IDB)\u7b56\u7565\u6765\u4fdd\u6301\u961f\u5217\u5185\u4e00\u81f4\u6027", "result": "\u53d1\u73b0\u4e0b\u6e38\u6027\u80fd\u53d6\u51b3\u4e8e\u9884\u8bad\u7ec3\u961f\u5217\u7684\u5206\u5e03\u7279\u6027\uff0c\u591a\u4e2d\u5fc3\u591a\u6837\u5316\u961f\u5217\u63d0\u9ad8\u5206\u5e03\u5185\u51c6\u786e\u7387\u4f46\u7f16\u7801\u4e86\u961f\u5217\u7279\u5f02\u6027\u4f2a\u5f71\uff0c\u964d\u4f4e\u4e86\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\u3002IDB\u7b56\u7565\u6709\u6548\u589e\u5f3a\u4e86OOD\u9c81\u68d2\u6027", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5f00\u53d1\u4e34\u5e8a\u516c\u5e73\u548c\u53ef\u6cdb\u5316\u7684\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u5f3a\u8c03\u4e86\u961f\u5217\u7ec4\u6210\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5173\u952e\u5f71\u54cd"}}
{"id": "2509.10208", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10208", "abs": "https://arxiv.org/abs/2509.10208", "authors": ["Shengqiang Fu"], "title": "SI-FACT: Mitigating Knowledge Conflict via Self-Improving Faithfulness-Aware Contrastive Tuning", "comment": null, "summary": "Large Language Models often generate unfaithful responses in knowledge\nintensive tasks due to knowledge conflict,that is,a preference for relying on\ninternal parametric knowledge rather than the provided context.To address this\nissue,we propose a novel self improving framework,Self Improving Faithfulness\nAware Contrastive Tuning.The framework uses a self instruct mechanism that\nallows the base LLM to automatically generate high quality,structured\ncontrastive learning data,including anchor samples,semantically equivalent\npositive samples,and negative samples simulating unfaithful scenarios.This\napproach significantly reduces the cost of manual\nannotation.Subsequently,contrastive learning is applied to train the\nmodel,enabling it to pull faithful responses closer and push unfaithful\nresponses farther apart in the representation space.Experiments on knowledge\nconflict evaluation benchmarks ECARE KRE and COSE KRE show that the SI FACT\nmodel based on Llama3 8B Instruct improves the Contextual Recall Rate by 6.2%\nover the best baseline method,while significantly reducing dependence on\ninternal memory.The results indicate that SI FACT provides strong effectiveness\nand high data efficiency in enhancing the contextual faithfulness of\nLLMs,offering a practical pathway toward building more proactive and\ntrustworthy language models.", "AI": {"tldr": "\u63d0\u51faSelf Improving Faithfulness Aware Contrastive Tuning\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u6307\u5bfc\u673a\u5236\u81ea\u52a8\u751f\u6210\u5bf9\u6bd4\u5b66\u4e60\u6570\u636e\uff0c\u63d0\u5347LLM\u5728\u77e5\u8bc6\u51b2\u7a81\u4efb\u52a1\u4e2d\u7684\u5fe0\u5b9e\u5ea6", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7ecf\u5e38\u751f\u6210\u4e0d\u5fe0\u5b9e\u7684\u54cd\u5e94\uff0c\u503e\u5411\u4e8e\u4f9d\u8d56\u5185\u90e8\u53c2\u6570\u77e5\u8bc6\u800c\u975e\u63d0\u4f9b\u7684\u4e0a\u4e0b\u6587\uff0c\u5b58\u5728\u77e5\u8bc6\u51b2\u7a81\u95ee\u9898", "method": "\u4f7f\u7528\u81ea\u6307\u5bfc\u673a\u5236\u8ba9\u57fa\u7840LLM\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u7ed3\u6784\u5316\u5bf9\u6bd4\u5b66\u4e60\u6570\u636e\uff08\u951a\u6837\u672c\u3001\u8bed\u4e49\u7b49\u4ef7\u6b63\u6837\u672c\u3001\u6a21\u62df\u4e0d\u5fe0\u5b9e\u573a\u666f\u7684\u8d1f\u6837\u672c\uff09\uff0c\u7136\u540e\u5e94\u7528\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3\u6a21\u578b", "result": "\u5728ECARE KRE\u548cCOSE KRE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u57fa\u4e8eLlama3 8B Instruct\u7684SI FACT\u6a21\u578b\u5c06\u4e0a\u4e0b\u6587\u53ec\u56de\u7387\u63d0\u9ad8\u4e866.2%\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5bf9\u5185\u90e8\u8bb0\u5fc6\u7684\u4f9d\u8d56", "conclusion": "SI FACT\u5728\u589e\u5f3aLLM\u4e0a\u4e0b\u6587\u5fe0\u5b9e\u5ea6\u65b9\u9762\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u6709\u6548\u6027\u548c\u9ad8\u6570\u636e\u6548\u7387\uff0c\u4e3a\u6784\u5efa\u66f4\u4e3b\u52a8\u548c\u53ef\u4fe1\u7684\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u9014\u5f84"}}
{"id": "2509.10384", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.10384", "abs": "https://arxiv.org/abs/2509.10384", "authors": ["Jianxin Zhang", "Clayton Scott"], "title": "Flow Straight and Fast in Hilbert Space: Functional Rectified Flow", "comment": null, "summary": "Many generative models originally developed in finite-dimensional Euclidean\nspace have functional generalizations in infinite-dimensional settings.\nHowever, the extension of rectified flow to infinite-dimensional spaces remains\nunexplored. In this work, we establish a rigorous functional formulation of\nrectified flow in an infinite-dimensional Hilbert space. Our approach builds\nupon the superposition principle for continuity equations in an\ninfinite-dimensional space. We further show that this framework extends\nnaturally to functional flow matching and functional probability flow ODEs,\ninterpreting them as nonlinear generalizations of rectified flow. Notably, our\nextension to functional flow matching removes the restrictive measure-theoretic\nassumptions in the existing theory of \\citet{kerrigan2024functional}.\nFurthermore, we demonstrate experimentally that our method achieves superior\nperformance compared to existing functional generative models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u65e0\u9650\u7ef4\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u6574\u6d41\u6d41\u7684\u4e25\u683c\u51fd\u6570\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u5efa\u7acb\u4e86\u57fa\u4e8e\u8fde\u7eed\u6027\u65b9\u7a0b\u53e0\u52a0\u539f\u7406\u7684\u51fd\u6570\u6574\u6d41\u6d41\u7406\u8bba\uff0c\u5e76\u6269\u5c55\u5230\u51fd\u6570\u6d41\u5339\u914d\u548c\u6982\u7387\u6d41ODE\uff0c\u5b9e\u9a8c\u663e\u793a\u4f18\u4e8e\u73b0\u6709\u51fd\u6570\u751f\u6210\u6a21\u578b\u3002", "motivation": "\u867d\u7136\u8bb8\u591a\u751f\u6210\u6a21\u578b\u5df2\u5728\u6709\u9650\u7ef4\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u53d1\u5c55\u5e76\u63a8\u5e7f\u5230\u65e0\u9650\u7ef4\u8bbe\u7f6e\uff0c\u4f46\u6574\u6d41\u6d41\u5411\u65e0\u9650\u7ef4\u7a7a\u95f4\u7684\u6269\u5c55\u4ecd\u672a\u88ab\u63a2\u7d22\u3002\u73b0\u6709\u51fd\u6570\u6d41\u5339\u914d\u7406\u8bba\u5b58\u5728\u9650\u5236\u6027\u7684\u6d4b\u5ea6\u7406\u8bba\u5047\u8bbe\uff0c\u9700\u8981\u66f4\u4e00\u822c\u7684\u6846\u67b6\u3002", "method": "\u57fa\u4e8e\u65e0\u9650\u7ef4\u7a7a\u95f4\u4e2d\u8fde\u7eed\u6027\u65b9\u7a0b\u7684\u53e0\u52a0\u539f\u7406\uff0c\u5efa\u7acb\u4e86\u51fd\u6570\u6574\u6d41\u6d41\u7684\u4e25\u683c\u6570\u5b66\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u81ea\u7136\u5730\u6269\u5c55\u5230\u51fd\u6570\u6d41\u5339\u914d\u548c\u51fd\u6570\u6982\u7387\u6d41ODE\uff0c\u5c06\u5176\u89e3\u91ca\u4e3a\u6574\u6d41\u6d41\u7684\u975e\u7ebf\u6027\u63a8\u5e7f\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u51fd\u6570\u751f\u6210\u6a21\u578b\u5177\u6709\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u3002\u7406\u8bba\u6846\u67b6\u79fb\u9664\u4e86\u73b0\u6709\u51fd\u6570\u6d41\u5339\u914d\u7406\u8bba\u4e2d\u7684\u9650\u5236\u6027\u6d4b\u5ea6\u7406\u8bba\u5047\u8bbe\u3002", "conclusion": "\u6210\u529f\u5efa\u7acb\u4e86\u65e0\u9650\u7ef4\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u6574\u6d41\u6d41\u7684\u51fd\u6570\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u63d0\u4f9b\u4e86\u66f4\u4e00\u822c\u7684\u51fd\u6570\u751f\u6210\u6a21\u578b\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5c55\u73b0\u4e86\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2509.10377", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10377", "abs": "https://arxiv.org/abs/2509.10377", "authors": ["Yixiao Zhou", "Ziyu Zhao", "Dongzhou Cheng", "zhiliang wu", "Jie Gui", "Yi Yang", "Fei Wu", "Yu Cheng", "Hehe Fan"], "title": "Dropping Experts, Recombining Neurons: Retraining-Free Pruning for Sparse Mixture-of-Experts LLMs", "comment": "Accepted to EMNLP2025", "summary": "Sparse Mixture-of-Experts (SMoE) architectures are widely used in large\nlanguage models (LLMs) due to their computational efficiency. However, though\nonly a few experts are activated for each token, SMoE still requires loading\nall expert parameters, leading to high memory usage and challenges in\ndeployment. Previous work has tried to reduce the overhead by pruning and\nmerging experts, but primarily focused on expert-level operations, leaving\nneuron-level structure underexplored. We propose DERN (Dropping Experts,\nRecombining Neurons), a task-agnostic and retraining-free framework for expert\npruning and reconstruction. We observe that experts are often misaligned and\ncontain semantic conflicts at the neuron level, which poses challenges for\ndirect merging. To solve this, DERN works in three steps: it first prunes\nredundant experts using router statistics; then it decomposes them into\nneuron-level expert segments, assigning each segment to its most compatible\nretained expert; and finally, it merges segments within each retained expert to\nbuild a compact representation. Experiments on Mixtral, Qwen, and DeepSeek SMoE\nmodels show that DERN improves performance by more than 5% on commonsense\nreasoning and MMLU benchmarks under 50% expert sparsity, without extra\ntraining. It also greatly reduces the number of experts and memory usage,\nmaking SMoE LLMs easier to deploy in practice.", "AI": {"tldr": "DERN\u662f\u4e00\u4e2a\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u4efb\u52a1\u65e0\u5173\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u5bb6\u526a\u679d\u548c\u795e\u7ecf\u5143\u91cd\u7ec4\u6765\u51cf\u5c11SMoE\u6a21\u578b\u7684\u5185\u5b58\u4f7f\u7528\uff0c\u572850%\u4e13\u5bb6\u7a00\u758f\u5ea6\u4e0b\u6027\u80fd\u63d0\u5347\u8d85\u8fc75%", "motivation": "SMoE\u67b6\u6784\u867d\u7136\u8ba1\u7b97\u9ad8\u6548\uff0c\u4f46\u4ecd\u9700\u52a0\u8f7d\u6240\u6709\u4e13\u5bb6\u53c2\u6570\uff0c\u5bfc\u81f4\u5185\u5b58\u4f7f\u7528\u9ad8\u548c\u90e8\u7f72\u56f0\u96be\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u4e13\u5bb6\u7ea7\u64cd\u4f5c\uff0c\u5ffd\u7565\u4e86\u795e\u7ecf\u5143\u7ea7\u7ed3\u6784", "method": "\u4e09\u6b65\u6846\u67b6\uff1a1) \u4f7f\u7528\u8def\u7531\u5668\u7edf\u8ba1\u526a\u679d\u5197\u4f59\u4e13\u5bb6\uff1b2) \u5c06\u4e13\u5bb6\u5206\u89e3\u4e3a\u795e\u7ecf\u5143\u7ea7\u7247\u6bb5\u5e76\u5206\u914d\u5230\u6700\u517c\u5bb9\u7684\u4fdd\u7559\u4e13\u5bb6\uff1b3) \u5728\u4fdd\u7559\u4e13\u5bb6\u5185\u5408\u5e76\u7247\u6bb5\u6784\u5efa\u7d27\u51d1\u8868\u793a", "result": "\u5728Mixtral\u3001Qwen\u548cDeepSeek SMoE\u6a21\u578b\u4e0a\uff0c\u5e38\u8bc6\u63a8\u7406\u548cMMLU\u57fa\u51c6\u6d4b\u8bd5\u6027\u80fd\u63d0\u5347\u8d85\u8fc75%\uff0c\u4e13\u5bb6\u6570\u91cf\u548c\u5185\u5b58\u4f7f\u7528\u5927\u5e45\u51cf\u5c11", "conclusion": "DERN\u6709\u6548\u89e3\u51b3\u4e86\u4e13\u5bb6\u95f4\u795e\u7ecf\u5143\u7ea7\u8bed\u4e49\u51b2\u7a81\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u9ad8\u6548\u4e13\u5bb6\u526a\u679d\u548c\u91cd\u5efa\uff0c\u4f7fSMoE LLMs\u66f4\u6613\u4e8e\u5b9e\u9645\u90e8\u7f72"}}
{"id": "2509.10390", "categories": ["cs.LG", "cs.IT", "math.IT", "q-bio.PE"], "pdf": "https://arxiv.org/pdf/2509.10390", "abs": "https://arxiv.org/abs/2509.10390", "authors": ["Quan Nguyen", "Adji Bousso Dieng"], "title": "Vendi Information Gain for Active Learning and its Application to Ecology", "comment": null, "summary": "While monitoring biodiversity through camera traps has become an important\nendeavor for ecological research, identifying species in the captured image\ndata remains a major bottleneck due to limited labeling resources. Active\nlearning -- a machine learning paradigm that selects the most informative data\nto label and train a predictive model -- offers a promising solution, but\ntypically focuses on uncertainty in the individual predictions without\nconsidering uncertainty across the entire dataset. We introduce a new active\nlearning policy, Vendi information gain (VIG), that selects images based on\ntheir impact on dataset-wide prediction uncertainty, capturing both\ninformativeness and diversity. Applied to the Snapshot Serengeti dataset, VIG\nachieves impressive predictive accuracy close to full supervision using less\nthan 10% of the labels. It consistently outperforms standard baselines across\nmetrics and batch sizes, collecting more diverse data in the feature space. VIG\nhas broad applicability beyond ecology, and our results highlight its value for\nbiodiversity monitoring in data-limited environments.", "AI": {"tldr": "\u63d0\u51faVendi\u4fe1\u606f\u589e\u76ca(VIG)\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u8003\u8651\u6570\u636e\u96c6\u5c42\u9762\u7684\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u6765\u9009\u62e9\u6700\u5177\u4fe1\u606f\u91cf\u548c\u591a\u6837\u6027\u7684\u56fe\u50cf\u8fdb\u884c\u6807\u6ce8\uff0c\u5728Snapshot Serengeti\u6570\u636e\u96c6\u4e0a\u4ec5\u7528\u4e0d\u523010%\u7684\u6807\u7b7e\u5c31\u8fbe\u5230\u4e86\u63a5\u8fd1\u5168\u76d1\u7763\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u76f8\u673a\u9677\u9631\u76d1\u6d4b\u751f\u7269\u591a\u6837\u6027\u65f6\uff0c\u7269\u79cd\u8bc6\u522b\u56e0\u6807\u6ce8\u8d44\u6e90\u6709\u9650\u6210\u4e3a\u4e3b\u8981\u74f6\u9888\u3002\u4f20\u7edf\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u53ea\u5173\u6ce8\u5355\u4e2a\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u6574\u4e2a\u6570\u636e\u96c6\u5c42\u9762\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u5f00\u53d1Vendi\u4fe1\u606f\u589e\u76ca(VIG)\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\uff0c\u9009\u62e9\u80fd\u591f\u6700\u5927\u7a0b\u5ea6\u51cf\u5c11\u6570\u636e\u96c6\u6574\u4f53\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684\u56fe\u50cf\u8fdb\u884c\u6807\u6ce8\uff0c\u540c\u65f6\u8003\u8651\u4fe1\u606f\u91cf\u548c\u591a\u6837\u6027\u3002", "result": "\u5728Snapshot Serengeti\u6570\u636e\u96c6\u4e0a\uff0cVIG\u4ec5\u4f7f\u7528\u4e0d\u523010%\u7684\u6807\u7b7e\u5c31\u8fbe\u5230\u4e86\u63a5\u8fd1\u5168\u76d1\u7763\u7684\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5728\u5404\u79cd\u6307\u6807\u548c\u6279\u6b21\u5927\u5c0f\u4e0b\u5747\u4f18\u4e8e\u6807\u51c6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u6536\u96c6\u4e86\u66f4\u591a\u6837\u5316\u7684\u6570\u636e\u3002", "conclusion": "VIG\u65b9\u6cd5\u5728\u6570\u636e\u6709\u9650\u7684\u73af\u5883\u4e2d\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\uff0c\u5bf9\u4e8e\u751f\u7269\u591a\u6837\u6027\u76d1\u6d4b\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u80fd\u591f\u663e\u8457\u51cf\u5c11\u6807\u6ce8\u9700\u6c42\u540c\u65f6\u4fdd\u6301\u9ad8\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2509.10414", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10414", "abs": "https://arxiv.org/abs/2509.10414", "authors": ["Adrian de Wynter"], "title": "Is In-Context Learning Learning?", "comment": "Director's cut", "summary": "In-context learning (ICL) allows some autoregressive models to solve tasks\nvia next-token prediction and without needing further training. This has led to\nclaims about these model's ability to solve (learn) unseen tasks with only a\nfew shots (exemplars) in the prompt. However, deduction does not always imply\nlearning, as ICL does not explicitly encode a given observation. Instead, the\nmodels rely on their prior knowledge and the exemplars given, if any. We argue\nthat, mathematically, ICL does constitute learning, but its full\ncharacterisation requires empirical work. We then carry out a large-scale\nanalysis of ICL ablating out or accounting for memorisation, pretraining,\ndistributional shifts, and prompting style and phrasing. We find that ICL is an\neffective learning paradigm, but limited in its ability to learn and generalise\nto unseen tasks. We note that, in the limit where exemplars become more\nnumerous, accuracy is insensitive to exemplar distribution, model, prompt\nstyle, and the input's linguistic features. Instead, it deduces patterns from\nregularities in the prompt, which leads to distributional sensitivity,\nespecially in prompting styles such as chain-of-thought. Given the varied\naccuracies on formally similar tasks, we conclude that autoregression's ad-hoc\nencoding is not a robust mechanism, and suggests limited all-purpose\ngeneralisability.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\u53d1\u73b0\uff0c\u4e0a\u4e0b\u6587\u5b66\u4e60(ICL)\u786e\u5b9e\u6784\u6210\u4e00\u79cd\u5b66\u4e60\u8303\u5f0f\uff0c\u4f46\u5176\u5b66\u4e60\u80fd\u529b\u548c\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u5bf9\u63d0\u793a\u683c\u5f0f\u548c\u5206\u5e03\u53d8\u5316\u654f\u611f\uff0c\u81ea\u56de\u5f52\u7684\u4e34\u65f6\u7f16\u7801\u673a\u5236\u4e0d\u591f\u9c81\u68d2\u3002", "motivation": "\u9488\u5bf9\u5f53\u524d\u5173\u4e8e\u81ea\u56de\u5f52\u6a21\u578b\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u5426\u771f\u6b63\u5b66\u4e60\u65b0\u4efb\u52a1\u7684\u4e89\u8bae\uff0c\u672c\u6587\u65e8\u5728\u4ece\u6570\u5b66\u548c\u5b9e\u8bc1\u89d2\u5ea6\u7cfb\u7edf\u5206\u6790ICL\u7684\u5b66\u4e60\u672c\u8d28\u548c\u5c40\u9650\u6027\u3002", "method": "\u8fdb\u884c\u5927\u89c4\u6a21ICL\u6d88\u878d\u5b9e\u9a8c\uff0c\u63a7\u5236\u8bb0\u5fc6\u6548\u5e94\u3001\u9884\u8bad\u7ec3\u3001\u5206\u5e03\u504f\u79fb\u3001\u63d0\u793a\u98ce\u683c\u548c\u63aa\u8f9e\u7b49\u56e0\u7d20\uff0c\u5206\u6790\u4e0d\u540c\u6761\u4ef6\u4e0b\u6a21\u578b\u7684\u8868\u73b0\u3002", "result": "ICL\u662f\u4e00\u79cd\u6709\u6548\u7684\u5b66\u4e60\u8303\u5f0f\uff0c\u4f46\u5728\u5b66\u4e60\u672a\u89c1\u4efb\u52a1\u548c\u6cdb\u5316\u65b9\u9762\u80fd\u529b\u6709\u9650\uff1b\u5f53\u793a\u4f8b\u6570\u91cf\u8db3\u591f\u591a\u65f6\uff0c\u51c6\u786e\u7387\u5bf9\u793a\u4f8b\u5206\u5e03\u3001\u6a21\u578b\u3001\u63d0\u793a\u98ce\u683c\u548c\u8bed\u8a00\u7279\u5f81\u4e0d\u654f\u611f\uff1bICL\u4e3b\u8981\u4ece\u63d0\u793a\u4e2d\u7684\u89c4\u5f8b\u6027\u63a8\u65ad\u6a21\u5f0f\uff0c\u5bfc\u81f4\u5206\u5e03\u654f\u611f\u6027\uff0c\u7279\u522b\u662f\u5728\u601d\u7ef4\u94fe\u7b49\u63d0\u793a\u98ce\u683c\u4e2d\u3002", "conclusion": "\u81ea\u56de\u5f52\u6a21\u578b\u7684\u4e34\u65f6\u7f16\u7801\u673a\u5236\u4e0d\u662f\u9c81\u68d2\u7684\u5b66\u4e60\u673a\u5236\uff0c\u8868\u660e\u5176\u901a\u7528\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u66f4\u6df1\u5165\u7684\u7406\u8bba\u548c\u5b9e\u8bc1\u7814\u7a76\u6765\u7406\u89e3ICL\u7684\u672c\u8d28\u3002"}}
{"id": "2509.10396", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10396", "abs": "https://arxiv.org/abs/2509.10396", "authors": ["Siyan Zhao", "Mengchen Liu", "Jing Huang", "Miao Liu", "Chenyu Wang", "Bo Liu", "Yuandong Tian", "Guan Pang", "Sean Bell", "Aditya Grover", "Feiyu Chen"], "title": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models", "comment": "preprint; 21 pages", "summary": "Masked diffusion large language models (dLLMs) are emerging as promising\nalternatives to autoregressive LLMs, offering competitive performance while\nsupporting unique generation capabilities such as inpainting. We explore how\ninpainting can inform RL algorithm design for dLLMs. Aligning LLMs with\nreinforcement learning faces an exploration challenge: sparse reward signals\nand sample waste when models fail to discover correct solutions. While this\ninefficiency affects LLMs broadly, dLLMs offer a distinctive opportunity--their\ninpainting ability can guide exploration. We introduce IGPO (Inpainting Guided\nPolicy Optimization), an RL framework that strategically inserts partial\nground-truth reasoning traces during online sampling. Unlike providing full\nsolutions, inpainting steers exploration toward promising trajectory spaces\nwhile preserving self-generated reasoning, bridging supervised fine-tuning and\nreinforcement learning. We apply IGPO to group-based optimization methods such\nas GRPO, where exploration failures cause zero advantages and gradients. IGPO\nrestores meaningful gradients while improving sample efficiency. We also\npropose supervised fine-tuning on synthetically rewritten concise traces that\nbetter align with dLLM generation patterns. With additional techniques\nincluding entropy-based filtering, our training recipe yields substantial gains\nacross three mathematical benchmarks--GSM8K, Math500, and AMC--achieving new\nstate-of-the-art results for full-attention masked dLLMs.", "AI": {"tldr": "IGPO\u662f\u4e00\u79cd\u9488\u5bf9\u63a9\u7801\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528inpainting\u80fd\u529b\u6307\u5bfc\u63a2\u7d22\uff0c\u901a\u8fc7\u63d2\u5165\u90e8\u5206\u771f\u5b9e\u63a8\u7406\u8f68\u8ff9\u6765\u63d0\u9ad8\u6837\u672c\u6548\u7387\u548c\u6027\u80fd", "motivation": "\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50LLM\u65f6\u7684\u63a2\u7d22\u6311\u6218\uff0c\u5305\u62ec\u7a00\u758f\u5956\u52b1\u4fe1\u53f7\u548c\u6837\u672c\u6d6a\u8d39\u95ee\u9898\uff0c\u5229\u7528dLLM\u72ec\u7279\u7684inpainting\u80fd\u529b\u6765\u5f15\u5bfc\u63a2\u7d22", "method": "\u63d0\u51faIGPO\u6846\u67b6\uff0c\u5728\u5728\u7ebf\u91c7\u6837\u65f6\u7b56\u7565\u6027\u5730\u63d2\u5165\u90e8\u5206\u771f\u5b9e\u63a8\u7406\u8f68\u8ff9\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u71b5\u7684\u8fc7\u6ee4\u7b49\u6280\u672f", "result": "\u5728GSM8K\u3001Math500\u548cAMC\u4e09\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u4e3a\u5168\u6ce8\u610f\u529b\u63a9\u7801dLLM\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7ed3\u679c", "conclusion": "IGPO\u6709\u6548\u89e3\u51b3\u4e86dLLM\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22\u6548\u7387\u95ee\u9898\uff0c\u8bc1\u660e\u4e86inpainting\u80fd\u529b\u5728\u6307\u5bfc\u7b56\u7565\u4f18\u5316\u4e2d\u7684\u4ef7\u503c"}}
{"id": "2509.10417", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10417", "abs": "https://arxiv.org/abs/2509.10417", "authors": ["Christopher Ormerod", "Gitit Kehat"], "title": "Long Context Automated Essay Scoring with Language Models", "comment": "8 pages, 2 figures, 2 tables", "summary": "Transformer-based language models are architecturally constrained to process\ntext of a fixed maximum length. Essays written by higher-grade students\nfrequently exceed the maximum allowed length for many popular open-source\nmodels. A common approach to addressing this issue when using these models for\nAutomated Essay Scoring is to truncate the input text. This raises serious\nvalidity concerns as it undermines the model's ability to fully capture and\nevaluate organizational elements of the scoring rubric, which requires long\ncontexts to assess. In this study, we evaluate several models that incorporate\narchitectural modifications of the standard transformer architecture to\novercome these length limitations using the Kaggle ASAP 2.0 dataset. The models\nconsidered in this study include fine-tuned versions of XLNet, Longformer,\nModernBERT, Mamba, and Llama models.", "AI": {"tldr": "\u8bc4\u4f30\u591a\u79cd\u6539\u8fdb\u7684Transformer\u67b6\u6784\u6a21\u578b\uff08XLNet\u3001Longformer\u3001ModernBERT\u3001Mamba\u3001Llama\uff09\u5728\u957f\u6587\u672c\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u4e2d\u7684\u8868\u73b0\uff0c\u89e3\u51b3\u4f20\u7edf\u6a21\u578b\u56e0\u957f\u5ea6\u9650\u5236\u9700\u8981\u622a\u65ad\u6587\u672c\u800c\u5f71\u54cd\u8bc4\u5206\u6709\u6548\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfTransformer\u6a21\u578b\u6709\u56fa\u5b9a\u6700\u5927\u957f\u5ea6\u9650\u5236\uff0c\u800c\u9ad8\u5e74\u7ea7\u5b66\u751f\u7684\u4f5c\u6587\u7ecf\u5e38\u8d85\u8fc7\u8fd9\u4e2a\u9650\u5236\u3002\u622a\u65ad\u5904\u7406\u4f1a\u4e25\u91cd\u5f71\u54cd\u6a21\u578b\u5bf9\u4f5c\u6587\u7ec4\u7ec7\u7ed3\u6784\u7b49\u8bc4\u5206\u8981\u7d20\u7684\u5b8c\u6574\u8bc4\u4f30\u80fd\u529b\uff0c\u5b58\u5728\u6709\u6548\u6027\u62c5\u5fe7\u3002", "method": "\u4f7f\u7528Kaggle ASAP 2.0\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u591a\u79cd\u6539\u8fdb\u67b6\u6784\u7684\u6a21\u578b\uff1aXLNet\u3001Longformer\u3001ModernBERT\u3001Mamba\u548cLlama\u7684\u5fae\u8c03\u7248\u672c\uff0c\u8fd9\u4e9b\u6a21\u578b\u90fd\u5bf9\u6807\u51c6Transformer\u67b6\u6784\u8fdb\u884c\u4e86\u4fee\u6539\u4ee5\u5904\u7406\u66f4\u957f\u6587\u672c\u3002", "result": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4e0d\u540c\u6539\u8fdb\u6a21\u578b\u5728\u957f\u6587\u672c\u4f5c\u6587\u8bc4\u5206\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u4f46\u5177\u4f53\u7ed3\u679c\u6570\u636e\u672a\u5728\u6458\u8981\u4e2d\u63d0\u4f9b\u3002", "conclusion": "\u9700\u8981\u91c7\u7528\u80fd\u591f\u5904\u7406\u66f4\u957f\u4e0a\u4e0b\u6587\u7684\u6539\u8fdb\u67b6\u6784\u6a21\u578b\u6765\u89e3\u51b3\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u4e2d\u7684\u6587\u672c\u957f\u5ea6\u9650\u5236\u95ee\u9898\uff0c\u4ee5\u4fdd\u6301\u8bc4\u5206\u6807\u51c6\u7684\u5b8c\u6574\u6027\u548c\u8bc4\u4f30\u6709\u6548\u6027\u3002"}}
{"id": "2509.10406", "categories": ["cs.LG", "68W25, 68T50 (primary) 68W40, 68T07 (secondary)", "I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.10406", "abs": "https://arxiv.org/abs/2509.10406", "authors": ["Rupert Mitchell", "Kristian Kersting"], "title": "Multipole Semantic Attention: A Fast Approximation of Softmax Attention for Pretraining", "comment": null, "summary": "We present Multipole Semantic Attention (MuSe), an efficient approximation of\nsoftmax attention that combines semantic clustering with multipole expansions\nfrom computational physics. Our method addresses the quadratic computational\ncomplexity of transformers in the context length by clustering queries and keys\nseparately in their learned representation spaces, enabling a hierarchical\ntwo-stage attention mechanism. Unlike prior clustering approaches that group\nonly keys or use unified clustering, we maintain separate clusterings that\nrespect attention's asymmetric treatment of these spaces. We augment\ncentroid-based (monopole) approximations with dipole corrections that capture\ndirectional variance within clusters, preserving richer information during\ntraining. The method operates as a drop-in replacement for standard attention,\nrequiring only hyperparameter specification without architectural\nmodifications. Our approach achieves $\\mathcal{O}(NCD)$ complexity for acausal\nattention with $C$ clusters and $\\mathcal{O}(NCD \\log N)$ for causal attention.\nOn isolated attention layers, we demonstrate $3\\times$ speedup over CUDNN Flash\nAttention at 8k context length, with relative squared errors below 20%. For\ncausal attention, we develop a hierarchical block decomposition that combines\nexact local computation with efficient long-range approximation. In end-to-end\npretraining of a 30M parameter model on book-length texts with 16k context, we\nachieve 12.2% runtime reduction with only 0.36% loss degradation, establishing\nthe viability of multipole approximations for efficient transformer\npretraining.", "AI": {"tldr": "MuSe\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u6ce8\u610f\u529b\u673a\u5236\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u805a\u7c7b\u548c\u591a\u6781\u5c55\u5f00\u6765\u964d\u4f4eTransformer\u7684\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u89e3\u51b3Transformer\u4e2dsoftmax\u6ce8\u610f\u529b\u673a\u5236\u5728\u5e8f\u5217\u957f\u5ea6\u4e0a\u7684\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u901a\u8fc7\u66f4\u9ad8\u6548\u7684\u8fd1\u4f3c\u65b9\u6cd5\u6765\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "method": "\u91c7\u7528\u8bed\u4e49\u805a\u7c7b\u548c\u591a\u6781\u5c55\u5f00\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff1a1\uff09\u5728\u5b66\u4e60\u7684\u8868\u793a\u7a7a\u95f4\u4e2d\u5206\u522b\u5bf9\u67e5\u8be2\u548c\u952e\u8fdb\u884c\u805a\u7c7b\uff1b2\uff09\u4f7f\u7528\u5c42\u6b21\u5316\u7684\u4e24\u9636\u6bb5\u6ce8\u610f\u529b\u673a\u5236\uff1b3\uff09\u5f15\u5165\u5076\u6781\u6821\u6b63\u6765\u6355\u6349\u7c07\u5185\u65b9\u5411\u65b9\u5dee\uff1b4\uff09\u4f5c\u4e3a\u6807\u51c6\u6ce8\u610f\u529b\u7684\u5373\u63d2\u5373\u7528\u66ff\u4ee3\u65b9\u6848\u3002", "result": "\u57288k\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u6bd4CUDNN Flash Attention\u5feb3\u500d\uff0c\u76f8\u5bf9\u5e73\u65b9\u8bef\u5dee\u4f4e\u4e8e20%\uff1b\u572830M\u53c2\u6570\u6a21\u578b\u7684\u7aef\u5230\u7aef\u9884\u8bad\u7ec3\u4e2d\uff0c16k\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u5b9e\u73b012.2%\u7684\u8fd0\u884c\u65f6\u95f4\u51cf\u5c11\uff0c\u4ec5\u635f\u59310.36%\u7684\u6027\u80fd\u3002", "conclusion": "\u591a\u6781\u8fd1\u4f3c\u65b9\u6cd5\u4e3a\u9ad8\u6548Transformer\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5206\u79bb\u805a\u7c7b\u548c\u5c42\u6b21\u5316\u5757\u5206\u89e3\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002"}}
{"id": "2509.10436", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10436", "abs": "https://arxiv.org/abs/2509.10436", "authors": ["Shadikur Rahman", "Aroosa Hameed", "Gautam Srivastava", "Syed Muhammad Danish"], "title": "RefactorCoderQA: Benchmarking LLMs for Multi-Domain Coding Question Solutions in Cloud and Edge Deployment", "comment": "12 pages, 5 figures, submitted to IEEE Transactions on Services\n  Computing", "summary": "To optimize the reasoning and problem-solving capabilities of Large Language\nModels (LLMs), we propose a novel cloud-edge collaborative architecture that\nenables a structured, multi-agent prompting framework. This framework comprises\nthree specialized components: GuideLLM, a lightweight model deployed at the\nedge to provide methodological guidance; SolverLLM, a more powerful model\nhosted in the cloud responsible for generating code solutions; and JudgeLLM, an\nautomated evaluator for assessing solution correctness and quality. To evaluate\nand demonstrate the effectiveness of this architecture in realistic settings,\nwe introduce RefactorCoderQA, a comprehensive benchmark designed to evaluate\nand enhance the performance of Large Language Models (LLMs) across multi-domain\ncoding tasks. Motivated by the limitations of existing benchmarks,\nRefactorCoderQA systematically covers various technical domains, including\nSoftware Engineering, Data Science, Machine Learning, and Natural Language\nProcessing, using authentic coding challenges from Stack Overflow. Extensive\nexperiments reveal that our fine-tuned model, RefactorCoder-MoE, achieves\nstate-of-the-art performance, significantly outperforming leading open-source\nand commercial baselines with an overall accuracy of 76.84%. Human evaluations\nfurther validate the interpretability, accuracy, and practical relevance of the\ngenerated solutions. In addition, we evaluate system-level metrics, such as\nthroughput and latency, to gain deeper insights into the performance\ncharacteristics and trade-offs of the proposed architecture.", "AI": {"tldr": "\u63d0\u51fa\u4e91\u8fb9\u534f\u4f5c\u7684\u591a\u667a\u80fd\u4f53\u63d0\u793a\u6846\u67b6\uff0c\u5305\u542bGuideLLM\u3001SolverLLM\u548cJudgeLLM\u4e09\u4e2a\u7ec4\u4ef6\uff0c\u5e76\u521b\u5efaRefactorCoderQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5728\u591a\u4e2a\u7f16\u7a0b\u9886\u57df\u8fbe\u523076.84%\u7684SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u548c\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u591a\u9886\u57df\u7f16\u7a0b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u91c7\u7528\u4e91\u8fb9\u534f\u4f5c\u67b6\u6784\uff0c\u5305\u542b\u8fb9\u7f18\u90e8\u7f72\u7684\u8f7b\u91cf\u7ea7GuideLLM\uff08\u63d0\u4f9b\u65b9\u6cd5\u6307\u5bfc\uff09\u3001\u4e91\u7aefSolverLLM\uff08\u751f\u6210\u4ee3\u7801\u89e3\u51b3\u65b9\u6848\uff09\u548cJudgeLLM\uff08\u81ea\u52a8\u8bc4\u4f30\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\uff09\u3002\u521b\u5efaRefactorCoderQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8986\u76d6\u8f6f\u4ef6\u5de5\u7a0b\u3001\u6570\u636e\u79d1\u5b66\u3001\u673a\u5668\u5b66\u4e60\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7b49\u591a\u4e2a\u6280\u672f\u9886\u57df\u3002", "result": "\u5fae\u8c03\u6a21\u578bRefactorCoder-MoE\u8fbe\u523076.84%\u7684\u6574\u4f53\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u9886\u5148\u7684\u5f00\u6e90\u548c\u5546\u4e1a\u57fa\u7ebf\u6a21\u578b\u3002\u4eba\u5de5\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u751f\u6210\u89e3\u51b3\u65b9\u6848\u7684\u53ef\u89e3\u91ca\u6027\u3001\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002\u7cfb\u7edf\u7ea7\u6307\u6807\uff08\u541e\u5410\u91cf\u548c\u5ef6\u8fdf\uff09\u5206\u6790\u63d0\u4f9b\u4e86\u6027\u80fd\u7279\u5f81\u7684\u6df1\u5165\u6d1e\u5bdf\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e91\u8fb9\u534f\u4f5c\u591a\u667a\u80fd\u4f53\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86LLM\u5728\u591a\u9886\u57df\u7f16\u7a0b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0cRefactorCoderQA\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u8bc4\u4f30\u548c\u589e\u5f3aLLM\u80fd\u529b\u63d0\u4f9b\u4e86\u5168\u9762\u5de5\u5177\u3002"}}
{"id": "2509.10419", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10419", "abs": "https://arxiv.org/abs/2509.10419", "authors": ["Francesco Vitale", "Tommaso Zoppi", "Francesco Flammini", "Nicola Mazzocca"], "title": "Run-Time Monitoring of ERTMS/ETCS Control Flow by Process Mining", "comment": "Accepted to the 6th International Conference on Reliability, Safety,\n  and Security of Railway Systems (RSSRail2025)", "summary": "Ensuring the resilience of computer-based railways is increasingly crucial to\naccount for uncertainties and changes due to the growing complexity and\ncriticality of those systems. Although their software relies on strict\nverification and validation processes following well-established best-practices\nand certification standards, anomalies can still occur at run-time due to\nresidual faults, system and environmental modifications that were unknown at\ndesign-time, or other emergent cyber-threat scenarios. This paper explores\nrun-time control-flow anomaly detection using process mining to enhance the\nresilience of ERTMS/ETCS L2 (European Rail Traffic Management System / European\nTrain Control System Level 2). Process mining allows learning the actual\ncontrol flow of the system from its execution traces, thus enabling run-time\nmonitoring through online conformance checking. In addition, anomaly\nlocalization is performed through unsupervised machine learning to link\nrelevant deviations to critical system components. We test our approach on a\nreference ERTMS/ETCS L2 scenario, namely the RBC/RBC Handover, to show its\ncapability to detect and localize anomalies with high accuracy, efficiency, and\nexplainability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u8fc7\u7a0b\u6316\u6398\u548c\u673a\u5668\u5b66\u4e60\u6280\u672f\u6765\u5b9e\u65f6\u68c0\u6d4b\u548c\u5b9a\u4f4d\u94c1\u8def\u63a7\u5236\u7cfb\u7edfERTMS/ETCS L2\u4e2d\u7684\u63a7\u5236\u6d41\u5f02\u5e38\uff0c\u63d0\u9ad8\u7cfb\u7edf\u97e7\u6027\u3002", "motivation": "\u968f\u7740\u94c1\u8def\u7cfb\u7edf\u590d\u6742\u6027\u548c\u5173\u952e\u6027\u589e\u52a0\uff0c\u5c3d\u7ba1\u6709\u4e25\u683c\u7684\u9a8c\u8bc1\u548c\u8ba4\u8bc1\u6d41\u7a0b\uff0c\u8fd0\u884c\u65f6\u4ecd\u53ef\u80fd\u51fa\u73b0\u8bbe\u8ba1\u65f6\u672a\u77e5\u7684\u6545\u969c\u3001\u7cfb\u7edf\u73af\u5883\u53d8\u5316\u6216\u7f51\u7edc\u5a01\u80c1\uff0c\u9700\u8981\u589e\u5f3a\u7cfb\u7edf\u97e7\u6027\u3002", "method": "\u91c7\u7528\u8fc7\u7a0b\u6316\u6398\u6280\u672f\u4ece\u6267\u884c\u8f68\u8ff9\u4e2d\u5b66\u4e60\u7cfb\u7edf\u5b9e\u9645\u63a7\u5236\u6d41\uff0c\u8fdb\u884c\u5728\u7ebf\u4e00\u81f4\u6027\u68c0\u67e5\uff1b\u4f7f\u7528\u65e0\u76d1\u7763\u673a\u5668\u5b66\u4e60\u8fdb\u884c\u5f02\u5e38\u5b9a\u4f4d\uff0c\u5c06\u504f\u5dee\u5173\u8054\u5230\u5173\u952e\u7cfb\u7edf\u7ec4\u4ef6\u3002", "result": "\u5728ERTMS/ETCS L2\u7684RBC/RBC\u4ea4\u63a5\u53c2\u8003\u573a\u666f\u4e2d\u6d4b\u8bd5\uff0c\u65b9\u6cd5\u80fd\u591f\u4ee5\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u6548\u7387\u548c\u9ad8\u53ef\u89e3\u91ca\u6027\u68c0\u6d4b\u548c\u5b9a\u4f4d\u5f02\u5e38\u3002", "conclusion": "\u8fc7\u7a0b\u6316\u6398\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u4e3a\u94c1\u8def\u63a7\u5236\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8fd0\u884c\u65f6\u5f02\u5e38\u68c0\u6d4b\u548c\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\uff0c\u589e\u5f3a\u4e86\u7cfb\u7edf\u5728\u9762\u5bf9\u672a\u77e5\u53d8\u5316\u548c\u5a01\u80c1\u65f6\u7684\u97e7\u6027\u3002"}}
{"id": "2509.10446", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10446", "abs": "https://arxiv.org/abs/2509.10446", "authors": ["Rui Lu", "Zhenyu Hou", "Zihan Wang", "Hanchen Zhang", "Xiao Liu", "Yujiang Li", "Shi Feng", "Jie Tang", "Yuxiao Dong"], "title": "DeepDive: Advancing Deep Search Agents with Knowledge Graphs and Multi-Turn RL", "comment": null, "summary": "Augmenting large language models (LLMs) with browsing tools substantially\nimproves their potential as deep search agents to solve complex, real-world\ntasks. Yet, open LLMs still perform poorly in such settings due to limited\nlong-horizon reasoning capacity with browsing tools and the lack of\nsufficiently difficult supervised data. To address these challenges, we present\nDeepDive to advance deep search agents. First, we propose a strategy to\nautomatically synthesize complex, difficult, and hard-to-find questions from\nopen knowledge graphs. Second, we apply end-to-end multi-turn reinforcement\nlearning (RL) to enhance LLMs' long-horizon reasoning with deep search.\nExperiments show that DeepDive-32B achieves a new open-source competitive\nresult on BrowseComp, outperforming WebSailor, DeepSeek-R1-Browse, and\nSearch-o1. We demonstrate that multi-turn RL training improves deep search\nability and significantly contributes to the performance improvements across\nmultiple benchmarks. We observe that DeepDive enables test-time scaling of tool\ncalls and parallel sampling. All datasets, models, and code are publicly\navailable at https://github.com/THUDM/DeepDive.", "AI": {"tldr": "DeepDive\u662f\u4e00\u4e2a\u901a\u8fc7\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u6df1\u5ea6\u641c\u7d22\u80fd\u529b\u7684\u7cfb\u7edf\uff0c\u4f7f\u7528\u77e5\u8bc6\u56fe\u8c31\u81ea\u52a8\u5408\u6210\u590d\u6742\u95ee\u9898\uff0c\u5728BrowseComp\u57fa\u51c6\u4e0a\u8fbe\u5230\u5f00\u6e90\u6a21\u578b\u6700\u4f73\u6027\u80fd", "motivation": "\u73b0\u6709\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6df1\u5ea6\u641c\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u957f\u65f6\u7a0b\u63a8\u7406\u80fd\u529b\u548c\u7f3a\u4e4f\u8db3\u591f\u96be\u5ea6\u7684\u76d1\u7763\u6570\u636e", "method": "1) \u4ece\u5f00\u653e\u77e5\u8bc6\u56fe\u8c31\u81ea\u52a8\u5408\u6210\u590d\u6742\u96be\u5bfb\u7684\u95ee\u9898 2) \u5e94\u7528\u7aef\u5230\u7aef\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u589e\u5f3aLLMs\u7684\u957f\u65f6\u7a0b\u6df1\u5ea6\u641c\u7d22\u63a8\u7406", "result": "DeepDive-32B\u5728BrowseComp\u57fa\u51c6\u4e0a\u8d85\u8d8aWebSailor\u3001DeepSeek-R1-Browse\u548cSearch-o1\u7b49\u6a21\u578b\uff0c\u8fbe\u5230\u5f00\u6e90\u7ade\u4e89\u6027\u6700\u4f73\u7ed3\u679c", "conclusion": "\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u6df1\u5ea6\u641c\u7d22\u80fd\u529b\uff0c\u652f\u6301\u6d4b\u8bd5\u65f6\u5de5\u5177\u8c03\u7528\u6269\u5c55\u548c\u5e76\u884c\u91c7\u6837\uff0c\u6240\u6709\u8d44\u6e90\u5df2\u5f00\u6e90"}}
{"id": "2509.10439", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.10439", "abs": "https://arxiv.org/abs/2509.10439", "authors": ["Ahmed Khaled", "Satyen Kale", "Arthur Douillard", "Chi Jin", "Rob Fergus", "Manzil Zaheer"], "title": "Understanding Outer Optimizers in Local SGD: Learning Rates, Momentum, and Acceleration", "comment": null, "summary": "Modern machine learning often requires training with large batch size,\ndistributed data, and massively parallel compute hardware (like mobile and\nother edge devices or distributed data centers). Communication becomes a major\nbottleneck in such settings but methods like Local Stochastic Gradient Descent\n(Local SGD) show great promise in reducing this additional communication\noverhead. Local SGD consists of three parts: a local optimization process, an\naggregation mechanism, and an outer optimizer that uses the aggregated updates\nfrom the nodes to produce a new model. While there exists an extensive\nliterature on understanding the impact of hyperparameters in the local\noptimization process, the choice of outer optimizer and its hyperparameters is\nless clear. We study the role of the outer optimizer in Local SGD, and prove\nnew convergence guarantees for the algorithm. In particular, we show that\ntuning the outer learning rate allows us to (a) trade off between optimization\nerror and stochastic gradient noise variance, and (b) make up for ill-tuning of\nthe inner learning rate. Our theory suggests that the outer learning rate\nshould sometimes be set to values greater than $1$. We extend our results to\nsettings where we use momentum in the outer optimizer, and we show a similar\nrole for the momentum-adjusted outer learning rate. We also study acceleration\nin the outer optimizer and show that it improves the convergence rate as a\nfunction of the number of communication rounds, improving upon the convergence\nrate of prior algorithms that apply acceleration locally. Finally, we also\nintroduce a novel data-dependent analysis of Local SGD that yields further\ninsights on outer learning rate tuning. We conduct comprehensive experiments\nwith standard language models and various outer optimizers to validate our\ntheory.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Local SGD\u4e2d\u5916\u90e8\u4f18\u5316\u5668\u7684\u4f5c\u7528\uff0c\u8bc1\u660e\u4e86\u65b0\u7684\u6536\u655b\u4fdd\u8bc1\uff0c\u53d1\u73b0\u8c03\u6574\u5916\u90e8\u5b66\u4e60\u7387\u53ef\u4ee5\u5728\u4f18\u5316\u8bef\u5dee\u548c\u968f\u673a\u68af\u5ea6\u566a\u58f0\u65b9\u5dee\u4e4b\u95f4\u6743\u8861\uff0c\u5e76\u5f25\u8865\u5185\u90e8\u5b66\u4e60\u7387\u7684\u4e0d\u826f\u8c03\u6574\u3002\u7406\u8bba\u8868\u660e\u5916\u90e8\u5b66\u4e60\u7387\u6709\u65f6\u5e94\u5927\u4e8e1\uff0c\u5e76\u6269\u5c55\u5230\u52a8\u91cf\u3001\u52a0\u901f\u7b49\u573a\u666f\u3002", "motivation": "\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u9700\u8981\u5927\u6279\u91cf\u8bad\u7ec3\u548c\u5206\u5e03\u5f0f\u8ba1\u7b97\uff0c\u901a\u4fe1\u6210\u4e3a\u4e3b\u8981\u74f6\u9888\u3002Local SGD\u80fd\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u672c\u5730\u4f18\u5316\u8fc7\u7a0b\u7684\u8d85\u53c2\u6570\uff0c\u5bf9\u5916\u90e8\u4f18\u5316\u5668\u53ca\u5176\u8d85\u53c2\u6570\u7684\u9009\u62e9\u4e0d\u591f\u6e05\u6670\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660eLocal SGD\u7684\u6536\u655b\u4fdd\u8bc1\uff0c\u7814\u7a76\u5916\u90e8\u5b66\u4e60\u7387\u7684\u4f5c\u7528\u673a\u5236\uff0c\u6269\u5c55\u5230\u52a8\u91cf\u4f18\u5316\u5668\u548c\u52a0\u901f\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u6570\u636e\u4f9d\u8d56\u7684\u5206\u6790\u65b9\u6cd5\u3002\u4f7f\u7528\u6807\u51c6\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u7406\u8bba\u8868\u660e\u8c03\u6574\u5916\u90e8\u5b66\u4e60\u7387\u80fd\u591f\u6743\u8861\u4f18\u5316\u8bef\u5dee\u548c\u968f\u673a\u68af\u5ea6\u566a\u58f0\u65b9\u5dee\uff0c\u8865\u507f\u5185\u90e8\u5b66\u4e60\u7387\u7684\u4e0d\u826f\u8c03\u6574\u3002\u5916\u90e8\u5b66\u4e60\u7387\u6709\u65f6\u5e94\u5927\u4e8e1\uff0c\u52a8\u91cf\u8c03\u6574\u548c\u52a0\u901f\u65b9\u6cd5\u80fd\u6539\u5584\u6536\u655b\u901f\u7387\u3002", "conclusion": "\u5916\u90e8\u4f18\u5316\u5668\u5728Local SGD\u4e2d\u626e\u6f14\u5173\u952e\u89d2\u8272\uff0c\u9002\u5f53\u8c03\u6574\u5916\u90e8\u5b66\u4e60\u7387\u80fd\u663e\u8457\u6539\u5584\u7b97\u6cd5\u6027\u80fd\uff0c\u52a8\u91cf\u52a0\u901f\u65b9\u6cd5\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u6536\u655b\u6548\u7387\uff0c\u4e3a\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u91cd\u8981\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2509.10452", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10452", "abs": "https://arxiv.org/abs/2509.10452", "authors": ["Akshat Pandey", "Karun Kumar", "Raphael Tang"], "title": "WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained Speech Recognition Transformers", "comment": "5 pages, 2 figures", "summary": "Pretrained automatic speech recognition (ASR) models such as Whisper perform\nwell but still need domain adaptation to handle unseen vocabulary and parlance.\nIn many real-world settings, collecting speech data is impractical,\nnecessitating text-only adaptation. We propose WhisTLE, a deeply supervised,\ntext-only adaptation method for pretrained encoder-decoder ASR models. WhisTLE\ntrains a variational autoencoder (VAE) to model encoder outputs from text and\nfine-tunes the decoder using the learned text-to-latent encoder, optionally\ncombined with text-to-speech (TTS) adaptation. At inference, the original\nencoder is restored, incurring no extra runtime cost. Across four out-of-domain\ndatasets and four ASR models, WhisTLE with TTS reduces word error rate (WER) by\n12.3% relative to TTS-only adaptation and outperforms all non-WhisTLE baselines\nin 27 of 32 scenarios.", "AI": {"tldr": "WhisTLE\u662f\u4e00\u79cd\u4ec5\u4f7f\u7528\u6587\u672c\u7684\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u9002\u914d\u65b9\u6cd5\uff0c\u901a\u8fc7\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5efa\u6a21\u7f16\u7801\u5668\u8f93\u51fa\u5e76\u5fae\u8c03\u89e3\u7801\u5668\uff0c\u65e0\u9700\u989d\u5916\u8fd0\u884c\u65f6\u6210\u672c\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u964d\u4f4e\u8bcd\u9519\u8bef\u7387", "motivation": "\u9884\u8bad\u7ec3\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u5982Whisper\u5728\u5904\u7406\u672a\u89c1\u8bcd\u6c47\u548c\u65b9\u8a00\u65f6\u9700\u8981\u9886\u57df\u9002\u914d\uff0c\u4f46\u5728\u8bb8\u591a\u5b9e\u9645\u573a\u666f\u4e2d\u6536\u96c6\u8bed\u97f3\u6570\u636e\u4e0d\u5207\u5b9e\u9645\uff0c\u56e0\u6b64\u9700\u8981\u4ec5\u4f7f\u7528\u6587\u672c\u7684\u9002\u914d\u65b9\u6cd5", "method": "\u63d0\u51faWhisTLE\u65b9\u6cd5\uff1a1\uff09\u8bad\u7ec3\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u4ece\u6587\u672c\u5efa\u6a21\u7f16\u7801\u5668\u8f93\u51fa\uff1b2\uff09\u4f7f\u7528\u5b66\u4e60\u7684\u6587\u672c\u5230\u6f5c\u5728\u7f16\u7801\u5668\u5fae\u8c03\u89e3\u7801\u5668\uff1b3\uff09\u53ef\u9009\u7ed3\u5408\u6587\u672c\u5230\u8bed\u97f3\uff08TTS\uff09\u9002\u914d\uff1b4\uff09\u63a8\u7406\u65f6\u6062\u590d\u539f\u59cb\u7f16\u7801\u5668\uff0c\u65e0\u989d\u5916\u8fd0\u884c\u65f6\u6210\u672c", "result": "\u5728\u56db\u4e2a\u57df\u5916\u6570\u636e\u96c6\u548c\u56db\u4e2aASR\u6a21\u578b\u4e0a\uff0cWhisTLE\u7ed3\u5408TTS\u76f8\u6bd4\u4ec5\u4f7f\u7528TTS\u9002\u914d\u76f8\u5bf9\u964d\u4f4e\u8bcd\u9519\u8bef\u738712.3%\uff0c\u572832\u4e2a\u573a\u666f\u4e2d\u768427\u4e2a\u573a\u666f\u4e2d\u4f18\u4e8e\u6240\u6709\u975eWhisTLE\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "WhisTLE\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6587\u672conly\u9002\u914d\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u9884\u8bad\u7ec3ASR\u6a21\u578b\u5728\u672a\u89c1\u9886\u57df\u7684\u6027\u80fd\uff0c\u4e14\u4e0d\u589e\u52a0\u63a8\u7406\u65f6\u7684\u8ba1\u7b97\u6210\u672c"}}
