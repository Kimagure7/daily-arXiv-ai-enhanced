<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 9]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [SteerEval: A Framework for Evaluating Steerability with Natural Language Profiles for Recommendation](https://arxiv.org/abs/2601.21105)
*Joyce Zhou,Weijie Zhou,Doug Turnbull,Thorsten Joachims*

Main category: cs.IR

TL;DR: 本文提出SteerEval评估框架，用于测试基于自然语言的推荐系统是否能够真正响应用户的显式偏好编辑和引导指令，超越了传统仅关注电影类型等简单属性的评估方法。


<details>
  <summary>Details</summary>
Motivation: 虽然自然语言用户档案因其可解释性和可引导性受到关注，但现有评估方法主要关注电影类型等简单属性，无法衡量更丰富、更细致的用户控制能力，这限制了可引导推荐系统的实际应用价值。

Method: 提出了SteerEval评估框架，使用从电影类型到内容警告等多种干预措施来评估推荐系统的可引导性。评估了一系列预训练的自然语言推荐模型，比较了不同档案和推荐干预措施对引导效果的影响。

Result: 评估显示当前自然语言推荐方法在可引导性方面存在局限性，特别是在相对小众的主题上。不同干预措施对引导效果有显著影响，为实际系统设计提供了重要见解。

Conclusion: SteerEval框架填补了可引导推荐系统评估的空白，揭示了当前方法的局限性，并为未来可引导推荐系统的设计提供了实用建议和研究方向。

Abstract: Natural-language user profiles have recently attracted attention not only for improved interpretability, but also for their potential to make recommender systems more steerable. By enabling direct editing, natural-language profiles allow users to explicitly articulate preferences that may be difficult to infer from past behavior. However, it remains unclear whether current natural-language-based recommendation methods can follow such steering commands. While existing steerability evaluations have shown some success for well-recognized item attributes (e.g., movie genres), we argue that these benchmarks fail to capture the richer forms of user control that motivate steerable recommendations. To address this gap, we introduce SteerEval, an evaluation framework designed to measure more nuanced and diverse forms of steerability by using interventions that range from genres to content-warning for movies. We assess the steerability of a family of pretrained natural-language recommenders, examine the potential and limitations of steering on relatively niche topics, and compare how different profile and recommendation interventions impact steering effectiveness. Finally, we offer practical design suggestions informed by our findings and discuss future steps in steerable recommender design.

</details>


### [2] [A2RAG: Adaptive Agentic Graph Retrieval for Cost-Aware and Reliable Reasoning](https://arxiv.org/abs/2601.21162)
*Jiate Liu,Zebin Chen,Shaobo Qiao,Mingchen Ju,Danting Zhang,Bocheng Han,Shuyue Yu,Xin Shu,Jingling Wu,Dong Wen,Xin Cao,Guanfeng Liu,Zhengyi Yang*

Main category: cs.IR

TL;DR: A2RAG是一个自适应智能的Graph-RAG框架，通过自适应控制器验证证据充分性并仅在必要时触发针对性优化，结合智能检索器逐步提升检索力度并将图信号映射回源文本，在混合难度工作负载和提取损失问题上实现高效可靠的多跳问答。


<details>
  <summary>Details</summary>
Motivation: Graph-RAG在多跳问答中面临两个瓶颈：1) 混合难度工作负载中，一刀切的检索方法要么在简单查询上浪费成本，要么在困难的多跳案例上失败；2) 提取损失问题，图抽象会遗漏仅在源文本中的细粒度限定词。

Method: 提出A2RAG框架，包含自适应控制器（验证证据充分性，仅在必要时触发针对性优化）和智能检索器（逐步提升检索力度，将图信号映射回源文本以应对提取损失和不完整图）。

Result: 在HotpotQA和2WikiMultiHopQA上的实验显示，A2RAG在Recall@2上获得+9.9/+11.8的绝对提升，同时将令牌消耗和端到端延迟相对于迭代多跳基线减少约50%。

Conclusion: A2RAG通过自适应控制和智能检索机制，在混合难度工作负载和提取损失场景下实现了成本感知且可靠的多跳问答推理，显著提升了性能同时大幅降低了计算成本。

Abstract: Graph Retrieval-Augmented Generation (Graph-RAG) enhances multihop question answering by organizing corpora into knowledge graphs and routing evidence through relational structure. However, practical deployments face two persistent bottlenecks: (i) mixed-difficulty workloads where one-size-fits-all retrieval either wastes cost on easy queries or fails on hard multihop cases, and (ii) extraction loss, where graph abstraction omits fine-grained qualifiers that remain only in source text. We present A2RAG, an adaptive-and-agentic GraphRAG framework for cost-aware and reliable reasoning. A2RAG couples an adaptive controller that verifies evidence sufficiency and triggers targeted refinement only when necessary, with an agentic retriever that progressively escalates retrieval effort and maps graph signals back to provenance text to remain robust under extraction loss and incomplete graphs. Experiments on HotpotQA and 2WikiMultiHopQA demonstrate that A2RAG achieves +9.9/+11.8 absolute gains in Recall@2, while cutting token consumption and end-to-end latency by about 50% relative to iterative multihop baselines.

</details>


### [3] [Thinking Broad, Acting Fast: Latent Reasoning Distillation from Multi-Perspective Chain-of-Thought for E-Commerce Relevance](https://arxiv.org/abs/2601.21611)
*Baopu Qiu,Hao Chen,Yuanrong Wu,Changtong Zan,Chao Wei,Weiru Zhang,Xiaoyi Zeng*

Main category: cs.IR

TL;DR: 提出MPCoT+LRKD框架，通过多视角思维链增强电商搜索相关性建模，并利用潜在推理知识蒸馏实现高效部署


<details>
  <summary>Details</summary>
Motivation: 现有LLM相关性模型存在两个关键限制：1) 单视角思维链无法捕捉电商相关性的多维度特性；2) 知识蒸馏方法丢弃了推理结构，只将思维链作为临时辅助信号

Method: 提出多视角思维链(MPCoT)生成多样化推理，结合SFT和DPO构建鲁棒推理器；引入潜在推理知识蒸馏(LRKD)，为学生模型配备轻量级推理提取器，实现高效推理能力内部化

Result: 在服务数千万用户的电商搜索广告平台上进行离线和在线A/B测试，方法在商业性能和用户体验方面均显示出显著优势

Conclusion: 提出的框架通过充分利用思维链语义，解决了现有LLM相关性模型的局限性，实现了高效、低延迟的电商搜索相关性建模

Abstract: Effective relevance modeling is crucial for e-commerce search, as it aligns search results with user intent and enhances customer experience. Recent work has leveraged large language models (LLMs) to address the limitations of traditional relevance models, especially for long-tail and ambiguous queries. By incorporating Chain-of-Thought (CoT) reasoning, these approaches improve both accuracy and interpretability through multi-step reasoning. However, two key limitations remain: (1) most existing approaches rely on single-perspective CoT reasoning, which fails to capture the multifaceted nature of e-commerce relevance (e.g., user intent vs. attribute-level matching vs. business-specific rules); and (2) although CoT-enhanced LLM's offer rich reasoning capabilities, their high inference latency necessitates knowledge distillation for real-time deployment, yet current distillation methods discard the CoT rationale structure at inference, using it as a transient auxiliary signal and forfeiting its reasoning utility. To address these challenges, we propose a novel framework that better exploits CoT semantics throughout the optimization pipeline. Specifically, the teacher model leverages Multi-Perspective CoT (MPCoT) to generate diverse rationales and combines Supervised Fine-Tuning (SFT) with Direct Preference Optimization (DPO) to construct a more robust reasoner. For distillation, we introduce Latent Reasoning Knowledge Distillation (LRKD), which endows a student model with a lightweight inference-time latent reasoning extractor, allowing efficient and low-latency internalization of the LLM's sophisticated reasoning capabilities. Evaluated in offline experiments and online A/B tests on an e-commerce search advertising platform serving tens of millions of users daily, our method delivers significant offline gains, showing clear benefits in both commercial performance and user experience.

</details>


### [4] [Influence Guided Sampling for Domain Adaptation of Text Retrievers](https://arxiv.org/abs/2601.21759)
*Meet Doshi,Vishwajeet Kumar,Yulong Li,Jaydeep Sen*

Main category: cs.IR

TL;DR: Inf-DDS：基于强化学习的轻量级自适应训练数据采样框架，通过影响度奖励信号优化多数据集采样策略，显著提升检索性能并降低GPU计算成本


<details>
  <summary>Details</summary>
Motivation: 通用开放域稠密检索系统通常使用多样化的语料库和搜索任务进行训练，但如何最优地采样这些训练数据尚未得到充分研究。传统方法采用均匀采样、按实例数量比例采样或依赖专家监督，这些策略对模型性能有重大影响但缺乏自适应优化。

Method: 提出Inf-DDS框架，使用强化学习自适应调整训练数据集权重。通过影响度奖励信号指导采样策略，迭代优化采样策略以最大化目标开发集上的模型性能。相比基于梯度的采样方法，该方法更轻量级且GPU消耗更低。

Result: 在广泛的文本检索任务上评估显示：1）训练多语言bge-m3模型时获得5.03的绝对NDCG@10提升；2）训练all-MiniLM-L6-v2模型时获得0.94的绝对NDCG@10提升；3）相比现有方法GPU计算成本降低1.5-4倍；4）即使在专家分配权重的基础上仍能进一步改进。

Conclusion: Inf-DDS框架通过强化学习驱动的自适应采样策略，显著提高了稠密检索模型的性能，同时大幅降低了训练计算成本，为多数据集训练提供了有效的优化方案。

Abstract: General-purpose open-domain dense retrieval systems are usually trained with a large, eclectic mix of corpora and search tasks. How should these diverse corpora and tasks be sampled for training? Conventional approaches sample them uniformly, proportional to their instance population sizes, or depend on human-level expert supervision. It is well known that the training data sampling strategy can greatly impact model performance. However, how to find the optimal strategy has not been adequately studied in the context of embedding models. We propose Inf-DDS, a novel reinforcement learning driven sampling framework that adaptively reweighs training datasets guided by influence-based reward signals and is much more lightweight with respect to GPU consumption. Our technique iteratively refines the sampling policy, prioritizing datasets that maximize model performance on a target development set. We evaluate the efficacy of our sampling strategy on a wide range of text retrieval tasks, demonstrating strong improvements in retrieval performance and better adaptation compared to existing gradient-based sampling methods, while also being 1.5x to 4x cheaper in GPU compute. Our sampling strategy achieves a 5.03 absolute NDCG@10 improvement while training a multilingual bge-m3 model and an absolute NDCG@10 improvement of 0.94 while training all-MiniLM-L6-v2, even when starting from expert-assigned weights on a large pool of training datasets.

</details>


### [5] [OneMall: One Model, More Scenarios -- End-to-End Generative Recommender Family at Kuaishou E-Commerce](https://arxiv.org/abs/2601.21770)
*Kun Zhang,Jingming Zhang,Wei Cheng,Yansong Cheng,Jiaqi Zhang,Hao Lu,Xu Zhang,Haixiang Gan,Jiangxia Cao,Tenglong Wang,Ximing Zhang,Boyang Xia,Kuo Cai,Shiyao Wang,Hongjian Dou,Jinkai Yu,Mingxing Wen,Qiang Luo,Dongxu Liang,Chenyi Lei,Jun Wang,Runan Liu,Zhaojie Liu,Ruiming Tang,Tingting Gao,Shaoguo Liu,Yuqing Ding,Hui Kong,Han Li,Guorui Zhou,Wenwu Ou,Kun Gai*

Main category: cs.IR

TL;DR: 快手提出OneMall端到端生成式推荐框架，统一电商多场景分发，通过语义分词器、Transformer架构和强化学习管道，在商品卡、短视频和直播场景均实现显著效果提升。


<details>
  <summary>Details</summary>
Motivation: 电商平台存在多种商品分发场景（商品卡、短视频、直播），传统推荐系统难以统一处理。需要构建端到端的生成式推荐框架，利用大语言模型技术统一优化多场景推荐效果。

Method: 1. 电商语义分词器：捕捉真实世界语义和业务特定商品关系；2. Transformer架构：使用Query-Former压缩长序列、Cross-Attention融合多行为序列、Sparse MoE实现可扩展自回归生成；3. 强化学习管道：连接检索和排序模型，用排序模型作为奖励信号优化端到端策略检索模型。

Result: 在快手电商所有场景均取得显著提升：商品卡GMV提升13.01%，短视频订单提升15.32%，直播订单提升2.78%。已部署服务超过4亿日活用户。

Conclusion: OneMall成功构建了端到端的生成式推荐框架，统一处理电商多场景分发，通过语义理解、Transformer架构和强化学习的结合，在实际部署中验证了其有效性和可扩展性。

Abstract: In the wave of generative recommendation, we present OneMall, an end-to-end generative recommendation framework tailored for e-commerce services at Kuaishou. Our OneMall systematically unifies the e-commerce's multiple item distribution scenarios, such as Product-card, short-video and live-streaming. Specifically, it comprises three key components, aligning the entire model training pipeline to the LLM's pre-training/post-training: (1) E-commerce Semantic Tokenizer: we provide a tokenizer solution that captures both real-world semantics and business-specific item relations across different scenarios; (2) Transformer-based Architecture: we largely utilize Transformer as our model backbone, e.g., employing Query-Former for long sequence compression, Cross-Attention for multi-behavior sequence fusion, and Sparse MoE for scalable auto-regressive generation; (3) Reinforcement Learning Pipeline: we further connect retrieval and ranking models via RL, enabling the ranking model to serve as a reward signal for end-to-end policy retrieval model optimization. Extensive experiments demonstrate that OneMall achieves consistent improvements across all e-commerce scenarios: +13.01\% GMV in product-card, +15.32\% Orders in Short-Video, and +2.78\% Orders in Live-Streaming. OneMall has been deployed, serving over 400 million daily active users at Kuaishou.

</details>


### [6] [The Double-Edged Sword of Knowledge Transfer: Diagnosing and Curing Fairness Pathologies in Cross-Domain Recommendation](https://arxiv.org/abs/2601.21805)
*Yuhan Zhao,Weixin Chen,Li Chen,Weike Pan*

Main category: cs.IR

TL;DR: 跨域推荐中的公平性增强框架：通过自适应整合未标记数据和信息论方法解决跨域差异转移和信息增益不公平问题


<details>
  <summary>Details</summary>
Motivation: 跨域推荐虽然能提升推荐质量，但研究发现它会无意中加剧群体层面的不公平性。本文旨在从理论和实证角度分析这种不公平性产生的原因，并提出解决方案。

Method: 提出跨域公平增强（CDFA）框架，包含两个核心组件：1）通过自适应整合未标记数据来平衡不同群体间训练信号的信息量，缓解跨域差异转移；2）通过信息论方法重新分配跨域信息增益，确保不同群体间利益分配的公平性。

Result: 在多个数据集和基线方法上的广泛实验表明，该框架能显著减少跨域推荐中的不公平性，同时不牺牲整体推荐性能，甚至能提升性能。

Conclusion: 跨域推荐中的公平性问题主要源于跨域差异转移和跨域信息增益的不公平分配。提出的CDFA框架能有效解决这两个挑战，在保持推荐性能的同时显著提升公平性。

Abstract: Cross-domain recommendation (CDR) offers an effective strategy for improving recommendation quality in a target domain by leveraging auxiliary signals from source domains. Nonetheless, emerging evidence shows that CDR can inadvertently heighten group-level unfairness. In this work, we conduct a comprehensive theoretical and empirical analysis to uncover why these fairness issues arise. Specifically, we identify two key challenges: (i) Cross-Domain Disparity Transfer, wherein existing group-level disparities in the source domain are systematically propagated to the target domain; and (ii) Unfairness from Cross-Domain Information Gain, where the benefits derived from cross-domain knowledge are unevenly allocated among distinct groups. To address these two challenges, we propose a Cross-Domain Fairness Augmentation (CDFA) framework composed of two key components. Firstly, it mitigates cross-domain disparity transfer by adaptively integrating unlabeled data to equilibrate the informativeness of training signals across groups. Secondly, it redistributes cross-domain information gains via an information-theoretic approach to ensure equitable benefit allocation across groups. Extensive experiments on multiple datasets and baselines demonstrate that our framework significantly reduces unfairness in CDR without sacrificing overall recommendation performance, while even enhancing it.

</details>


### [7] [LEMUR: Learned Multi-Vector Retrieval](https://arxiv.org/abs/2601.21853)
*Elias Jääsaari,Ville Hyvönen,Teemu Roos*

Main category: cs.IR

TL;DR: LEMUR是一个高效的多向量相似性搜索框架，通过两层问题简化将多向量搜索转化为单向量ANN搜索，比现有方法快一个数量级。


<details>
  <summary>Details</summary>
Motivation: 多向量表示（如ColBERT）在信息检索中比单向量表示质量更高，但检索延迟显著增加，需要设计高效的多向量近似最近邻搜索算法。

Method: LEMUR采用两层问题简化：1）将多向量相似性搜索公式化为可用单隐藏层神经网络解决的监督学习问题；2）将该模型的推理简化为其潜在空间中的单向量相似性搜索，从而可利用现有单向量ANNS方法加速检索。

Result: LEMUR在ColBERTv2嵌入、现代多向量文本模型和多向量视觉文档检索模型的嵌入上评估，比早期多向量相似性搜索方法快一个数量级。

Conclusion: LEMUR提供了一个简单而高效的框架，通过将多向量搜索转化为单向量ANN搜索，显著提升了多向量检索的效率，同时保持了检索质量。

Abstract: Multi-vector representations generated by late interaction models, such as ColBERT, enable superior retrieval quality compared to single-vector representations in information retrieval applications. In multi-vector retrieval systems, both queries and documents are encoded using one embedding for each token, and similarity between queries and documents is measured by the MaxSim similarity measure. However, the improved recall of multi-vector retrieval comes at the expense of significantly increased latency. This necessitates designing efficient approximate nearest neighbor search (ANNS) algorithms for multi-vector search. In this work, we introduce LEMUR, a simple-yet-efficient framework for multi-vector similarity search. LEMUR consists of two consecutive problem reductions: We first formulate multi-vector similarity search as a supervised learning problem that can be solved using a one-hidden-layer neural network. Second, we reduce inference under this model to single-vector similarity search in its latent space, which enables the use of existing single-vector ANNS methods for speeding up retrieval. In addition to performance evaluation on ColBERTv2 embeddings, we evaluate LEMUR on embeddings generated by modern multi-vector text models and multi-vector visual document retrieval models. LEMUR is an order of magnitude faster than earlier multi-vector similarity search methods.

</details>


### [8] [SpecTran: Spectral-Aware Transformer-based Adapter for LLM-Enhanced Sequential Recommendation](https://arxiv.org/abs/2601.21986)
*Yu Cui,Feng Liu,Zhaoxiang Wang,Changwang Zhang,Jun Wang,Can Wang,Jiawei Chen*

Main category: cs.IR

TL;DR: 提出SpecTran方法，通过谱域注意力机制全谱选择聚合信息，解决传统适配器维度坍缩和SVD方法信息丢失问题，在序列推荐中提升性能9.17%


<details>
  <summary>Details</summary>
Motivation: 传统序列推荐模型只学习低维ID嵌入，忽略了文本信息。现有LLM文本嵌入注入方法存在两大问题：适配器方法有维度坍缩问题，信息集中在少数维度；SVD方法过于刚性，只保留主要谱成分而丢弃丰富信息。

Method: 提出SpecTran，一种谱感知的基于Transformer的适配器，在谱域操作，关注全谱以选择和聚合信息成分。使用可学习的谱位置编码注入奇异值线索作为归纳偏置，引导注意力关注重要谱成分并促进嵌入维度的多样性。

Result: 在四个真实世界数据集和三个序列推荐骨干网络上，SpecTran始终优于强基线，平均提升9.17%。

Conclusion: SpecTran通过谱域注意力机制有效解决了现有文本嵌入注入方法的局限性，在序列推荐任务中取得了显著性能提升。

Abstract: Traditional sequential recommendation (SR) models learn low-dimensional item ID embeddings from user-item interactions, often overlooking textual information such as item titles or descriptions. Recent advances in Large Language Models (LLMs) have inspired a surge of research that encodes item textual information with high-dimensional semantic embeddings, and designs transformation methods to inject such embeddings into SR models. These embedding transformation strategies can be categorized into two types, both of which exhibits notable drawbacks: 1) adapter-based methods suffer from pronounced dimension collapse, concentrating information into a few dominant dimensions; 2) SVD-based methods are rigid and manual, considering only a few principal spectral components while discarding rich information in the remaining spectrum.
  To address these limitations, we propose SpecTran, a spectral-aware transformer-based adapter that operates in the spectral domain, attending to the full spectrum to select and aggregates informative components. A learnable spectral-position encoding injects singular-value cues as an inductive bias, guiding attention toward salient spectral components and promoting diversity across embedding dimensions. Across four real-world datasets and three SR backbones, it consistently outperforms strong baselines, achieving an average improvement of 9.17%.

</details>


### [9] [LANCER: LLM Reranking for Nugget Coverage](https://arxiv.org/abs/2601.22008)
*Jia-Huei Ju,François G. Landry,Eugene Yang,Suzan Verberne,Andrew Yates*

Main category: cs.IR

TL;DR: LANCER是一种基于LLM的重排序方法，专门针对长文本检索增强生成任务，通过生成子问题并预测文档回答这些子问题的能力，优化信息覆盖而非仅相关性排序。


<details>
  <summary>Details</summary>
Motivation: 现有的检索方法主要针对相关性排序优化，而长文本RAG任务（如自动报告生成）需要覆盖广泛的相关信息，现有方法在信息覆盖方面存在不足。

Method: LANCER方法：1) 预测需要回答哪些子问题以满足信息需求；2) 预测哪些文档能回答这些子问题；3) 重新排序文档以尽可能覆盖更多信息块。

Result: 实验结果显示LANCER在信息块覆盖指标上优于其他基于LLM的重排序方法，在α-nDCG和信息覆盖方面表现更好。Oracle分析进一步表明子问题生成是关键因素。

Conclusion: LANCER通过专注于信息覆盖而非仅相关性排序，有效提升了长文本RAG任务的检索质量，子问题生成是实现这一改进的核心机制。

Abstract: Unlike short-form retrieval-augmented generation (RAG), such as factoid question answering, long-form RAG requires retrieval to provide documents covering a wide range of relevant information. Automated report generation exemplifies this setting: it requires not only relevant information but also a more elaborate response with comprehensive information. Yet, existing retrieval methods are primarily optimized for relevance ranking rather than information coverage. To address this limitation, we propose LANCER, an LLM-based reranking method for nugget coverage. LANCER predicts what sub-questions should be answered to satisfy an information need, predicts which documents answer these sub-questions, and reranks documents in order to provide a ranked list covering as many information nuggets as possible. Our empirical results show that LANCER enhances the quality of retrieval as measured by nugget coverage metrics, achieving higher $α$-nDCG and information coverage than other LLM-based reranking methods. Our oracle analysis further reveals that sub-question generation plays an essential role.

</details>
