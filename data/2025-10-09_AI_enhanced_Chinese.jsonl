{"id": "2510.06657", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.06657", "abs": "https://arxiv.org/abs/2510.06657", "authors": ["Boyuan Long", "Yueqi Wang", "Hiloni Mehta", "Mick Zomnir", "Omkar Pathak", "Changping Meng", "Ruolin Jia", "Yajun Peng", "Dapeng Hong", "Xia Wu", "Mingyan Gao", "Onkar Dalal", "Ningren Han"], "title": "LLM-Powered Nuanced Video Attribute Annotation for Enhanced Recommendations", "comment": "RecSys 2025 Industry Track", "summary": "This paper presents a case study on deploying Large Language Models (LLMs) as\nan advanced \"annotation\" mechanism to achieve nuanced content understanding\n(e.g., discerning content \"vibe\") at scale within a large-scale industrial\nshort-form video recommendation system. Traditional machine learning\nclassifiers for content understanding face protracted development cycles and a\nlack of deep, nuanced comprehension. The \"LLM-as-annotators\" approach addresses\nthese by significantly shortening development times and enabling the annotation\nof subtle attributes. This work details an end-to-end workflow encompassing:\n(1) iterative definition and robust evaluation of target attributes, refined by\noffline metrics and online A/B testing; (2) scalable offline bulk annotation of\nvideo corpora using LLMs with multimodal features, optimized inference, and\nknowledge distillation for broad application; and (3) integration of these rich\nannotations into the online recommendation serving system, for example, through\npersonalized restrict retrieval. Experimental results demonstrate the efficacy\nof this approach, with LLMs outperforming human raters in offline annotation\nquality for nuanced attributes and yielding significant improvements of user\nparticipation and satisfied consumption in online A/B tests. The study provides\ninsights into designing and scaling production-level LLM pipelines for rich\ncontent evaluation, highlighting the adaptability and benefits of LLM-generated\nnuanced understanding for enhancing content discovery, user satisfaction, and\nthe overall effectiveness of modern recommendation systems.", "AI": {"tldr": "\u4f7f\u7528LLM\u4f5c\u4e3a\u6807\u6ce8\u5de5\u5177\u6765\u589e\u5f3a\u77ed\u89c6\u9891\u63a8\u8350\u7cfb\u7edf\u7684\u5185\u5bb9\u7406\u89e3\u80fd\u529b\uff0c\u663e\u8457\u7f29\u77ed\u5f00\u53d1\u5468\u671f\u5e76\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u5c5e\u6027\u6807\u6ce8\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u5de5\u4f5c\u6d41\u63d0\u5347\u63a8\u8350\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u5f00\u53d1\u5468\u671f\u957f\u4e14\u7f3a\u4e4f\u6df1\u5ea6\u3001\u7ec6\u7c92\u5ea6\u7684\u5185\u5bb9\u7406\u89e3\u80fd\u529b\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5feb\u901f\u90e8\u7f72\u5e76\u7406\u89e3\u5185\u5bb9\u7ec6\u5fae\u7279\u5f81\u7684\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u7aef\u5230\u7aef\u5de5\u4f5c\u6d41\uff1a1) \u8fed\u4ee3\u5b9a\u4e49\u548c\u8bc4\u4f30\u76ee\u6807\u5c5e\u6027\uff1b2) \u4f7f\u7528\u591a\u6a21\u6001\u7279\u5f81\u7684LLM\u8fdb\u884c\u6279\u91cf\u6807\u6ce8\uff0c\u5e76\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u4f18\u5316\uff1b3) \u5c06\u4e30\u5bcc\u6807\u6ce8\u96c6\u6210\u5230\u5728\u7ebf\u63a8\u8350\u7cfb\u7edf\u4e2d\u3002", "result": "LLM\u5728\u7ec6\u7c92\u5ea6\u5c5e\u6027\u6807\u6ce8\u8d28\u91cf\u4e0a\u8d85\u8d8a\u4eba\u5de5\u6807\u6ce8\u8005\uff0c\u5728\u7ebfA/B\u6d4b\u8bd5\u663e\u793a\u7528\u6237\u53c2\u4e0e\u5ea6\u548c\u6ee1\u610f\u5ea6\u663e\u8457\u63d0\u5347\u3002", "conclusion": "LLM\u751f\u6210\u7684\u7ec6\u7c92\u5ea6\u7406\u89e3\u80fd\u591f\u6709\u6548\u589e\u5f3a\u5185\u5bb9\u53d1\u73b0\u3001\u7528\u6237\u6ee1\u610f\u5ea6\u548c\u73b0\u4ee3\u63a8\u8350\u7cfb\u7edf\u7684\u6574\u4f53\u6548\u80fd\uff0c\u5c55\u793a\u4e86\u751f\u4ea7\u7ea7LLM\u7ba1\u9053\u7684\u8bbe\u8ba1\u548c\u6269\u5c55\u6f5c\u529b\u3002"}}
{"id": "2510.06658", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.06658", "abs": "https://arxiv.org/abs/2510.06658", "authors": ["Jiaman He", "Zikang Leng", "Dana McKay", "Damiano Spina", "Johanne R. Trippas"], "title": "Can We Hide Machines in the Crowd? Quantifying Equivalence in LLM-in-the-loop Annotation Tasks", "comment": "Accepted at SIGIR-AP 2025", "summary": "Many evaluations of large language models (LLMs) in text annotation focus\nprimarily on the correctness of the output, typically comparing model-generated\nlabels to human-annotated ``ground truth'' using standard performance metrics.\nIn contrast, our study moves beyond effectiveness alone. We aim to explore how\nlabeling decisions -- by both humans and LLMs -- can be statistically evaluated\nacross individuals. Rather than treating LLMs purely as annotation systems, we\napproach LLMs as an alternative annotation mechanism that may be capable of\nmimicking the subjective judgments made by humans. To assess this, we develop a\nstatistical evaluation method based on Krippendorff's $\\alpha$, paired\nbootstrapping, and the Two One-Sided t-Tests (TOST) equivalence test procedure.\nThis evaluation method tests whether an LLM can blend into a group of human\nannotators without being distinguishable.\n  We apply this approach to two datasets -- MovieLens 100K and PolitiFact --\nand find that the LLM is statistically indistinguishable from a human annotator\nin the former ($p = 0.004$), but not in the latter ($p = 0.155$), highlighting\ntask-dependent differences. It also enables early evaluation on a small sample\nof human data to inform whether LLMs are suitable for large-scale annotation in\na given application.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u7edf\u8ba1\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7528\u4e8e\u6d4b\u8bd5LLM\u662f\u5426\u80fd\u878d\u5165\u4eba\u7c7b\u6807\u6ce8\u8005\u7fa4\u4f53\u800c\u4e0d\u88ab\u533a\u5206\uff0c\u53d1\u73b0LLM\u5728MovieLens\u6570\u636e\u96c6\u4e0a\u4e0e\u4eba\u65e0\u5f02\uff0c\u4f46\u5728PolitiFact\u6570\u636e\u96c6\u4e0a\u53ef\u533a\u5206\u3002", "motivation": "\u8d85\u8d8a\u4f20\u7edf\u4ec5\u5173\u6ce8LLM\u6807\u6ce8\u6b63\u786e\u6027\u7684\u8bc4\u4f30\uff0c\u63a2\u7d22LLM\u662f\u5426\u80fd\u6a21\u62df\u4eba\u7c7b\u4e3b\u89c2\u5224\u65ad\uff0c\u4f5c\u4e3a\u66ff\u4ee3\u6807\u6ce8\u673a\u5236\u3002", "method": "\u57fa\u4e8eKrippendorff's \u03b1\u3001\u914d\u5bf9\u81ea\u52a9\u6cd5\u548cTOST\u7b49\u4ef7\u6027\u68c0\u9a8c\u7684\u7edf\u8ba1\u8bc4\u4f30\u65b9\u6cd5\uff0c\u6d4b\u8bd5LLM\u662f\u5426\u4e0e\u4eba\u7c7b\u6807\u6ce8\u8005\u65e0\u6cd5\u533a\u5206\u3002", "result": "\u5728MovieLens 100K\u6570\u636e\u96c6\u4e0a\uff0cLLM\u4e0e\u4eba\u7c7b\u6807\u6ce8\u8005\u7edf\u8ba1\u4e0a\u65e0\u6cd5\u533a\u5206(p=0.004)\uff1b\u5728PolitiFact\u6570\u636e\u96c6\u4e0a\u53ef\u533a\u5206(p=0.155)\u3002", "conclusion": "LLM\u4f5c\u4e3a\u6807\u6ce8\u5de5\u5177\u7684\u6548\u679c\u5177\u6709\u4efb\u52a1\u4f9d\u8d56\u6027\uff0c\u8be5\u65b9\u6cd5\u53ef\u5728\u5c0f\u6837\u672c\u4e0a\u65e9\u671f\u8bc4\u4f30LLM\u662f\u5426\u9002\u5408\u7279\u5b9a\u5e94\u7528\u7684\u5927\u89c4\u6a21\u6807\u6ce8\u3002"}}
{"id": "2510.06728", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.06728", "abs": "https://arxiv.org/abs/2510.06728", "authors": ["Cile van Marken", "Roxana Petcu"], "title": "Reproducing and Extending Causal Insights Into Term Frequency Computation in Neural Rankers", "comment": "10 pages, 6 figures, submitted to SIGIR-AP", "summary": "Neural ranking models have shown outstanding performance across a variety of\ntasks, such as document retrieval, re-ranking, question answering and\nconversational retrieval. However, the inner decision process of these models\nremains largely unclear, especially as models increase in size. Most\ninterpretability approaches, such as probing, focus on correlational insights\nrather than establishing causal relationships. The paper 'Axiomatic Causal\nInterventions for Reverse Engineering Relevance Computation in Neural Retrieval\nModels' by Chen et al. addresses this gap by introducing a framework for\nactivation patching - a causal interpretability method - in the information\nretrieval domain, offering insights into how neural retrieval models compute\ndocument relevance. The study demonstrates that neural ranking models not only\ncapture term-frequency information, but also that these representations can be\nlocalized to specific components of the model, such as individual attention\nheads or layers. This paper aims to reproduce the findings by Chen et al. and\nto further explore the presence of pre-defined retrieval axioms in neural IR\nmodels. We validate the main claims made by Chen et al., and extend the\nframework to include an additional term-frequency axiom, which states that the\nimpact of increasing query term frequency on document ranking diminishes as the\nfrequency becomes higher. We successfully identify a group of attention heads\nthat encode this axiom and analyze their behavior to give insight into the\ninner decision-making process of neural ranking models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u6fc0\u6d3b\u4fee\u8865\u65b9\u6cd5\u7814\u7a76\u795e\u7ecf\u68c0\u7d22\u6a21\u578b\u7684\u56e0\u679c\u53ef\u89e3\u91ca\u6027\uff0c\u9a8c\u8bc1\u5e76\u6269\u5c55\u4e86Chen\u7b49\u4eba\u7684\u53d1\u73b0\uff0c\u8bc6\u522b\u51fa\u7f16\u7801\u8bcd\u9891\u9012\u51cf\u6548\u5e94\u7684\u6ce8\u610f\u529b\u5934\u3002", "motivation": "\u89e3\u51b3\u795e\u7ecf\u6392\u5e8f\u6a21\u578b\u5185\u90e8\u51b3\u7b56\u8fc7\u7a0b\u4e0d\u900f\u660e\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u968f\u7740\u6a21\u578b\u89c4\u6a21\u589e\u5927\uff0c\u5927\u591a\u6570\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u53ea\u63d0\u4f9b\u76f8\u5173\u6027\u89c1\u89e3\u800c\u975e\u56e0\u679c\u5173\u7cfb\u3002", "method": "\u91c7\u7528\u6fc0\u6d3b\u4fee\u8865\u8fd9\u4e00\u56e0\u679c\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u5728\u4fe1\u606f\u68c0\u7d22\u9886\u57df\u5e94\u7528\u8be5\u6846\u67b6\uff0c\u5e76\u6269\u5c55\u5305\u542b\u989d\u5916\u7684\u8bcd\u9891\u516c\u7406\u3002", "result": "\u6210\u529f\u9a8c\u8bc1\u4e86Chen\u7b49\u4eba\u7684\u4e3b\u8981\u53d1\u73b0\uff0c\u8bc6\u522b\u51fa\u7f16\u7801\u8bcd\u9891\u9012\u51cf\u6548\u5e94\u7684\u7279\u5b9a\u6ce8\u610f\u529b\u5934\u7ec4\uff0c\u63ed\u793a\u4e86\u795e\u7ecf\u6392\u5e8f\u6a21\u578b\u7684\u5185\u90e8\u51b3\u7b56\u673a\u5236\u3002", "conclusion": "\u795e\u7ecf\u6392\u5e8f\u6a21\u578b\u4e0d\u4ec5\u6355\u83b7\u8bcd\u9891\u4fe1\u606f\uff0c\u800c\u4e14\u8fd9\u4e9b\u8868\u793a\u53ef\u4ee5\u5b9a\u4f4d\u5230\u6a21\u578b\u7684\u7279\u5b9a\u7ec4\u4ef6\uff0c\u4e3a\u7406\u89e3\u6a21\u578b\u5185\u90e8\u8ba1\u7b97\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2510.06838", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06838", "abs": "https://arxiv.org/abs/2510.06838", "authors": ["Elena Senger", "Yuri Campbell", "Rob van der Goot", "Barbara Plank"], "title": "Crossing Domains without Labels: Distant Supervision for Term Extraction", "comment": "Accepted at EMNLP Industry Track 2025", "summary": "Automatic Term Extraction (ATE) is a critical component in downstream NLP\ntasks such as document tagging, ontology construction and patent analysis.\nCurrent state-of-the-art methods require expensive human annotation and\nstruggle with domain transfer, limiting their practical deployment. This\nhighlights the need for more robust, scalable solutions and realistic\nevaluation settings. To address this, we introduce a comprehensive benchmark\nspanning seven diverse domains, enabling performance evaluation at both the\ndocument- and corpus-levels. Furthermore, we propose a robust LLM-based model\nthat outperforms both supervised cross-domain encoder models and few-shot\nlearning baselines and performs competitively with its GPT-4o teacher on this\nbenchmark. The first step of our approach is generating psuedo-labels with this\nblack-box LLM on general and scientific domains to ensure generalizability.\nBuilding on this data, we fine-tune the first LLMs for ATE. To further enhance\ndocument-level consistency, oftentimes needed for downstream tasks, we\nintroduce lightweight post-hoc heuristics. Our approach exceeds previous\napproaches on 5/7 domains with an average improvement of 10 percentage points.\nWe release our dataset and fine-tuned models to support future research in this\narea.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u672f\u8bed\u63d0\u53d6\u6a21\u578b\uff0c\u57287\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5e73\u5747\u63d0\u534710\u4e2a\u767e\u5206\u70b9\uff0c\u5e76\u4e0eGPT-4o\u6559\u5e08\u6a21\u578b\u8868\u73b0\u76f8\u5f53\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u81ea\u52a8\u672f\u8bed\u63d0\u53d6\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\uff0c\u4e14\u5728\u9886\u57df\u8fc1\u79fb\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u548c\u66f4\u73b0\u5b9e\u7684\u8bc4\u4f30\u8bbe\u7f6e\u3002", "method": "\u9996\u5148\u4f7f\u7528\u9ed1\u76d2LLM\u5728\u901a\u7528\u548c\u79d1\u5b66\u9886\u57df\u751f\u6210\u4f2a\u6807\u7b7e\u4ee5\u786e\u4fdd\u6cdb\u5316\u6027\uff0c\u7136\u540e\u5fae\u8c03\u9996\u4e2a\u7528\u4e8eATE\u7684LLM\uff0c\u6700\u540e\u5f15\u5165\u8f7b\u91cf\u7ea7\u540e\u5904\u7406\u542f\u53d1\u5f0f\u65b9\u6cd5\u6765\u589e\u5f3a\u6587\u6863\u7ea7\u4e00\u81f4\u6027\u3002", "result": "\u57287\u4e2a\u9886\u57df\u4e2d\u76845\u4e2a\u9886\u57df\u8d85\u8d8a\u4e86\u5148\u524d\u65b9\u6cd5\uff0c\u5e73\u5747\u63d0\u534710\u4e2a\u767e\u5206\u70b9\uff0c\u4e0eGPT-4o\u6559\u5e08\u6a21\u578b\u8868\u73b0\u76f8\u5f53\u3002", "conclusion": "\u63d0\u51fa\u7684LLM-based\u65b9\u6cd5\u5728\u81ea\u52a8\u672f\u8bed\u63d0\u53d6\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u53d1\u5e03\u4e86\u6570\u636e\u96c6\u548c\u5fae\u8c03\u6a21\u578b\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2510.06888", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06888", "abs": "https://arxiv.org/abs/2510.06888", "authors": ["Arkadeep Acharya", "Akash Ghosh", "Pradeepika Verma", "Kitsuchart Pasupa", "Sriparna Saha", "Priti Singh"], "title": "M3Retrieve: Benchmarking Multimodal Retrieval for Medicine", "comment": "EMNLP Mains 2025", "summary": "With the increasing use of RetrievalAugmented Generation (RAG), strong\nretrieval models have become more important than ever. In healthcare,\nmultimodal retrieval models that combine information from both text and images\noffer major advantages for many downstream tasks such as question answering,\ncross-modal retrieval, and multimodal summarization, since medical data often\nincludes both formats. However, there is currently no standard benchmark to\nevaluate how well these models perform in medical settings. To address this\ngap, we introduce M3Retrieve, a Multimodal Medical Retrieval Benchmark.\nM3Retrieve, spans 5 domains,16 medical fields, and 4 distinct tasks, with over\n1.2 Million text documents and 164K multimodal queries, all collected under\napproved licenses. We evaluate leading multimodal retrieval models on this\nbenchmark to explore the challenges specific to different medical specialities\nand to understand their impact on retrieval performance. By releasing\nM3Retrieve, we aim to enable systematic evaluation, foster model innovation,\nand accelerate research toward building more capable and reliable multimodal\nretrieval systems for medical applications. The dataset and the baselines code\nare available in this github page https://github.com/AkashGhosh/M3Retrieve.", "AI": {"tldr": "\u63d0\u51fa\u4e86M3Retrieve\u2014\u2014\u4e00\u4e2a\u591a\u6a21\u6001\u533b\u5b66\u68c0\u7d22\u57fa\u51c6\uff0c\u6db5\u76d65\u4e2a\u9886\u57df\u300116\u4e2a\u533b\u5b66\u4e13\u4e1a\u548c4\u4e2a\u4efb\u52a1\uff0c\u5305\u542b120\u4e07\u6587\u672c\u6587\u6863\u548c16.4\u4e07\u591a\u6a21\u6001\u67e5\u8be2\uff0c\u65e8\u5728\u89e3\u51b3\u533b\u7597\u9886\u57df\u7f3a\u4e4f\u6807\u51c6\u5316\u591a\u6a21\u6001\u68c0\u7d22\u8bc4\u4f30\u57fa\u51c6\u7684\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5f3a\u5927\u7684\u68c0\u7d22\u6a21\u578b\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\u3002\u5728\u533b\u7597\u9886\u57df\uff0c\u7ed3\u5408\u6587\u672c\u548c\u56fe\u50cf\u4fe1\u606f\u7684\u591a\u6a21\u6001\u68c0\u7d22\u6a21\u578b\u5bf9\u95ee\u7b54\u3001\u8de8\u6a21\u6001\u68c0\u7d22\u548c\u591a\u6a21\u6001\u6458\u8981\u7b49\u4efb\u52a1\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u6784\u5efa\u4e86M3Retrieve\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6db5\u76d65\u4e2a\u533b\u5b66\u9886\u57df\u300116\u4e2a\u4e13\u4e1a\u9886\u57df\u548c4\u4e2a\u4e0d\u540c\u4efb\u52a1\uff0c\u6536\u96c6\u4e86\u8d85\u8fc7120\u4e07\u6587\u672c\u6587\u6863\u548c16.4\u4e07\u591a\u6a21\u6001\u67e5\u8be2\uff0c\u6240\u6709\u6570\u636e\u5747\u5728\u6279\u51c6\u8bb8\u53ef\u4e0b\u6536\u96c6\u3002", "result": "\u5728M3Retrieve\u57fa\u51c6\u4e0a\u8bc4\u4f30\u4e86\u9886\u5148\u7684\u591a\u6a21\u6001\u68c0\u7d22\u6a21\u578b\uff0c\u63a2\u7d22\u4e86\u4e0d\u540c\u533b\u5b66\u4e13\u4e1a\u7279\u6709\u7684\u6311\u6218\u53ca\u5176\u5bf9\u68c0\u7d22\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "\u901a\u8fc7\u53d1\u5e03M3Retrieve\uff0c\u65e8\u5728\u5b9e\u73b0\u7cfb\u7edf\u5316\u8bc4\u4f30\uff0c\u4fc3\u8fdb\u6a21\u578b\u521b\u65b0\uff0c\u5e76\u52a0\u901f\u6784\u5efa\u66f4\u5f3a\u5927\u53ef\u9760\u7684\u533b\u7597\u591a\u6a21\u6001\u68c0\u7d22\u7cfb\u7edf\u7684\u7814\u7a76\u3002"}}
{"id": "2510.06924", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.06924", "abs": "https://arxiv.org/abs/2510.06924", "authors": ["Jordan Nelson", "Almas Baimagambetov", "Konstantinos Avgerinakis", "Nikolaos Polatidis"], "title": "Ethical AI prompt recommendations in large language models using collaborative filtering", "comment": "This paper has been accepted to by the International Journal of\n  Parallel, Emergent & Distributed Systems (Taylor and Francis) and has an\n  assigned DOI. We have already chose to make this open access using CC BY. The\n  article is not yet available online on the publisher's website. The DOI is:\n  doi.org/10.1080/17445760.2025.2573086", "summary": "As large language models (LLMs) shape AI development, ensuring ethical prompt\nrecommendations is crucial. LLMs offer innovation but risk bias, fairness\nissues, and accountability concerns. Traditional oversight methods struggle\nwith scalability, necessitating dynamic solutions. This paper proposes using\ncollaborative filtering, a technique from recommendation systems, to enhance\nethical prompt selection. By leveraging user interactions, it promotes ethical\nguidelines while reducing bias. Contributions include a synthetic dataset for\nprompt recommendations and the application of collaborative filtering. The work\nalso tackles challenges in ethical AI, such as bias mitigation, transparency,\nand preventing unethical prompt engineering.", "AI": {"tldr": "\u4f7f\u7528\u534f\u540c\u8fc7\u6ee4\u6280\u672f\u6765\u63d0\u5347\u4f26\u7406\u63d0\u793a\u9009\u62e9\uff0c\u901a\u8fc7\u7528\u6237\u4ea4\u4e92\u4fc3\u8fdb\u4f26\u7406\u6307\u5357\u5e76\u51cf\u5c11\u504f\u89c1\uff0c\u5305\u62ec\u521b\u5efa\u5408\u6210\u6570\u636e\u96c6\u548c\u5e94\u7528\u534f\u540c\u8fc7\u6ee4\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u786e\u4fdd\u4f26\u7406\u63d0\u793a\u63a8\u8350\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u76d1\u7763\u65b9\u6cd5\u96be\u4ee5\u6269\u5c55\uff0c\u9700\u8981\u52a8\u6001\u89e3\u51b3\u65b9\u6848\u6765\u5e94\u5bf9\u504f\u89c1\u3001\u516c\u5e73\u6027\u548c\u95ee\u8d23\u5236\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u534f\u540c\u8fc7\u6ee4\u6280\u672f\uff0c\u901a\u8fc7\u7528\u6237\u4ea4\u4e92\u6765\u589e\u5f3a\u4f26\u7406\u63d0\u793a\u9009\u62e9\uff0c\u5e76\u521b\u5efa\u4e86\u7528\u4e8e\u63d0\u793a\u63a8\u8350\u7684\u5408\u6210\u6570\u636e\u96c6\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4fc3\u8fdb\u4f26\u7406\u6307\u5357\u7684\u5b9e\u65bd\uff0c\u540c\u65f6\u51cf\u5c11\u504f\u89c1\uff0c\u89e3\u51b3\u4e86\u4f26\u7406AI\u4e2d\u7684\u504f\u89c1\u7f13\u89e3\u3001\u900f\u660e\u5ea6\u548c\u9632\u6b62\u4e0d\u9053\u5fb7\u63d0\u793a\u5de5\u7a0b\u7b49\u6311\u6218\u3002", "conclusion": "\u534f\u540c\u8fc7\u6ee4\u6280\u672f\u4e3a\u4f26\u7406\u63d0\u793a\u63a8\u8350\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u52a8\u6001\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u8d1f\u8d23\u4efb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u3002"}}
