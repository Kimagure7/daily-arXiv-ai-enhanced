<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 5]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Resource-Efficient LLM Application for Structured Transformation of Unstructured Financial Contracts](https://arxiv.org/abs/2510.23990)
*Maruf Ahmed Mridul,Oshani Seneviratne*

Main category: cs.IR

TL;DR: 扩展CDMizer框架，使用模板驱动方法将法律合同转换为标准化的CDM格式，并与较小开源LLM集成，在CSA条款提取任务中达到与大型专有模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 将非结构化法律合同转换为机器可读格式对金融工作流自动化至关重要，但复杂文档如信用支持附件(CSA)的CDM转换仍面临挑战。

Method: 扩展CDMizer框架，采用模板驱动解决方案，确保合同到CDM转换过程中的语法正确性和CDM模式遵从性。

Result: 与ISDA基准比较，CDMizer与较小开源LLM集成在准确性和效率方面达到与大型专有模型竞争的性能。

Conclusion: 资源高效的解决方案在自动化法律合同转换方面具有潜力，为资源受限或有严格数据隐私要求的金融机构提供经济有效且可扩展的方法。

Abstract: The transformation of unstructured legal contracts into standardized,
machine-readable formats is essential for automating financial workflows. The
Common Domain Model (CDM) provides a standardized framework for this purpose,
but converting complex legal documents like Credit Support Annexes (CSAs) into
CDM representations remains a significant challenge. In this paper, we present
an extension of the CDMizer framework, a template-driven solution that ensures
syntactic correctness and adherence to the CDM schema during contract-to-CDM
conversion. We apply this extended framework to a real-world task, comparing
its performance with a benchmark developed by the International Swaps and
Derivatives Association (ISDA) for CSA clause extraction. Our results show that
CDMizer, when integrated with a significantly smaller, open-source Large
Language Model (LLM), achieves competitive performance in terms of accuracy and
efficiency against larger, proprietary models. This work underscores the
potential of resource-efficient solutions to automate legal contract
transformation, offering a cost-effective and scalable approach that can meet
the needs of financial institutions with constrained resources or strict data
privacy requirements.

</details>


### [2] [DUET: Dual Model Co-Training for Entire Space CTR Prediction](https://arxiv.org/abs/2510.24369)
*Yutian Xiao,Meng Yuan,Fuzhen Zhuang,Wei Chen,Shukuan Wang,Shanqi Liu,Chao Feng,Wenhui Yu,Xiang Li,Lantao Hu,Han Li,Zhao Zhang*

Main category: cs.IR

TL;DR: DUET是一个集合级别的预排序框架，通过双模型协同训练解决推荐系统中模型表达能力与计算效率的权衡问题，能够在严格延迟约束下实现表达性建模。


<details>
  <summary>Details</summary>
Motivation: 解决大规模推荐系统中预排序阶段面临的模型表达能力与计算效率之间的内在权衡问题。传统双塔架构虽然计算高效但估计能力有限，难以捕捉候选物品间的协同和抑制关系，且会放大样本选择偏差问题。

Method: 提出DUET框架：1）进行集合级别的预测，在单次前向传播中对整个候选子集进行评分，实现候选物品间的信息感知交互；2）采用双模型协同训练机制，通过相互伪标签精炼将监督扩展到未曝光物品，有效缓解样本选择偏差。

Result: 经过广泛的离线实验和在线A/B测试，DUET持续优于最先进的基线方法，并在多个核心业务指标上取得改进。目前已在快手和快手极速版应用中全面部署，为数亿用户提供服务。

Conclusion: DUET成功解决了预排序阶段的表达性建模与计算效率的权衡问题，通过集合级别预测和双模型协同训练机制，在严格计算预算下实现了更好的推荐性能，并有效缓解了样本选择偏差问题。

Abstract: The pre-ranking stage plays a pivotal role in large-scale recommender systems
but faces an intrinsic trade-off between model expressiveness and computational
efficiency. Owing to the massive candidate pool and strict latency constraints,
industry systems often rely on lightweight two-tower architectures, which are
computationally efficient yet limited in estimation capability. As a result,
they struggle to capture the complex synergistic and suppressive relationships
among candidate items, which are essential for producing contextually coherent
and diverse recommendation lists. Moreover, this simplicity further amplifies
the Sample Selection Bias (SSB) problem, as coarse-grained models trained on
biased exposure data must generalize to a much larger candidate space with
distinct distributions.
  To address these issues, we propose \textbf{DUET} (\textbf{DU}al Model
Co-Training for \textbf{E}ntire Space C\textbf{T}R Prediction), a set-wise
pre-ranking framework that achieves expressive modeling under tight
computational budgets. Instead of scoring items independently, DUET performs
set-level prediction over the entire candidate subset in a single forward pass,
enabling information-aware interactions among candidates while amortizing the
computational cost across the set. Moreover, a dual model co-training mechanism
extends supervision to unexposed items via mutual pseudo-label refinement,
effectively mitigating SSB. Validated through extensive offline experiments and
online A/B testing, DUET consistently outperforms state-of-the-art baselines
and achieves improvements across multiple core business metrics. At present,
DUET has been fully deployed in Kuaishou and Kuaishou Lite Apps, serving the
main traffic for hundreds of millions of users.

</details>


### [3] [Metadata-Driven Retrieval-Augmented Generation for Financial Question Answering](https://arxiv.org/abs/2510.24402)
*Michail Dadopoulos,Anestis Ladas,Stratos Moschidis,Ioannis Negkakis*

Main category: cs.IR

TL;DR: 论文提出了一种针对金融文档的多阶段元数据驱动RAG架构，通过LLM生成的元数据和上下文丰富的文档块来提升检索性能，在FinanceBench数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统RAG在处理结构复杂、证据稀疏且交叉引用的金融文件时表现不佳，需要更先进的元数据驱动方法来提升检索精度。

Method: 提出多阶段RAG架构，包括LLM生成的元数据索引管道、预检索过滤、后检索重排序和增强嵌入等技术，特别强调将块元数据直接与文本嵌入的"上下文块"方法。

Result: 结果显示强大的重排序器对精度至关重要，但最大性能提升来自上下文块嵌入。提出的最优架构结合了LLM驱动的预检索优化和上下文嵌入，达到卓越性能。

Conclusion: 研究为构建稳健的元数据感知RAG系统提供了蓝图，展示了在峰值性能和运营效率之间的实用权衡。

Abstract: Retrieval-Augmented Generation (RAG) struggles on long, structured financial
filings where relevant evidence is sparse and cross-referenced. This paper
presents a systematic investigation of advanced metadata-driven
Retrieval-Augmented Generation (RAG) techniques, proposing and evaluating a
novel, multi-stage RAG architecture that leverages LLM-generated metadata. We
introduce a sophisticated indexing pipeline to create contextually rich
document chunks and benchmark a spectrum of enhancements, including
pre-retrieval filtering, post-retrieval reranking, and enriched embeddings,
benchmarked on the FinanceBench dataset. Our results reveal that while a
powerful reranker is essential for precision, the most significant performance
gains come from embedding chunk metadata directly with text ("contextual
chunks"). Our proposed optimal architecture combines LLM-driven pre-retrieval
optimizations with these contextual embeddings to achieve superior performance.
Additionally, we present a custom metadata reranker that offers a compelling,
cost-effective alternative to commercial solutions, highlighting a practical
trade-off between peak performance and operational efficiency. This study
provides a blueprint for building robust, metadata-aware RAG systems for
financial document analysis.

</details>


### [4] [From Time and Place to Preference: LLM-Driven Geo-Temporal Context in Recommendations](https://arxiv.org/abs/2510.24430)
*Yejin Kim,Shaghayegh Agah,Mayur Nankani,Neeraj Sharma,Feifei Peng,Maria Peifer,Sardar Hamidian,H Howie Huang*

Main category: cs.IR

TL;DR: 提出使用LLM从时间戳和粗略位置生成地理时间嵌入的框架，捕捉节假日、季节趋势和本地/全球事件，通过特征融合或辅助损失整合到推荐模型中。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统将时间戳视为数值或周期性值，忽略了现实世界背景如节假日、事件和季节模式。

Method: 使用LLM生成地理时间嵌入，通过直接特征融合与元数据嵌入或语义与地理时间对齐的辅助损失整合到序列模型中。

Result: 在MovieLens、LastFM和生产数据集上证明这些嵌入提供了与完整模型集成结果一致的预测信号。

Conclusion: 研究强调了自适应或混合推荐策略的需求，并发布了上下文丰富的MovieLens数据集以支持未来研究。

Abstract: Most recommender systems treat timestamps as numeric or cyclical values,
overlooking real-world context such as holidays, events, and seasonal patterns.
We propose a scalable framework that uses large language models (LLMs) to
generate geo-temporal embeddings from only a timestamp and coarse location,
capturing holidays, seasonal trends, and local/global events. We then introduce
a geo-temporal embedding informativeness test as a lightweight diagnostic,
demonstrating on MovieLens, LastFM, and a production dataset that these
embeddings provide predictive signal consistent with the outcomes of full model
integrations. Geo-temporal embeddings are incorporated into sequential models
through (1) direct feature fusion with metadata embeddings or (2) an auxiliary
loss that enforces semantic and geo-temporal alignment. Our findings highlight
the need for adaptive or hybrid recommendation strategies, and we release a
context-enriched MovieLens dataset to support future research.

</details>


### [5] [MiniOneRec: An Open-Source Framework for Scaling Generative Recommendation](https://arxiv.org/abs/2510.24431)
*Xiaoyu Kong,Leheng Sheng,Junfei Tan,Yuxin Chen,Jiancan Wu,An Zhang,Xiang Wang,Xiangnan He*

Main category: cs.IR

TL;DR: MiniOneRec是首个完全开源的生成式推荐框架，通过语义ID序列替代传统嵌入表，验证了生成式推荐方法的参数效率，并提出轻量级后训练流程提升排名准确性和候选多样性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的成功引发了对推荐系统能否获得类似扩展效益的兴趣。传统推荐系统在嵌入维度增长时趋于饱和，而生成式范式用紧凑的语义ID序列替代嵌入表，但大多数工业部署仍为专有，需要验证公开基准上的扩展规律和最小后训练方案。

Method: 提供端到端工作流程，包括语义ID构建、监督微调和面向推荐的强化学习。通过残差量化VAE生成语义ID，在Amazon Review数据集上对0.5B到7B参数的Qwen骨干进行后训练，并提出轻量级后训练流程，包括全流程语义ID对齐和带约束解码与混合奖励的强化学习。

Result: 实验显示随着模型规模增加，训练和评估损失呈现一致下降趋势，验证了生成式方法的参数效率。提出的后训练技术显著提升了排名准确性和候选多样性。

Conclusion: 生成式推荐方法具有参数效率优势，提出的轻量级后训练流程能有效提升性能，为开源生成式推荐系统提供了可行方案。

Abstract: The recent success of large language models (LLMs) has renewed interest in
whether recommender systems can achieve similar scaling benefits. Conventional
recommenders, dominated by massive embedding tables, tend to plateau as
embedding dimensions grow. In contrast, the emerging generative paradigm
replaces embeddings with compact Semantic ID (SID) sequences produced by
autoregressive Transformers. Yet most industrial deployments remain
proprietary, leaving two fundamental questions open: (1) Do the expected
scaling laws hold on public benchmarks? (2) What is the minimal post-training
recipe that enables competitive performance?
  We present MiniOneRec, to the best of our knowledge, the first fully
open-source generative recommendation framework, which provides an end-to-end
workflow spanning SID construction, supervised fine-tuning, and
recommendation-oriented reinforcement learning. We generate SIDs via a Residual
Quantized VAE and post-train Qwen backbones ranging from 0.5B to 7B parameters
on the Amazon Review dataset. Our experiments reveal a consistent downward
trend in both training and evaluation losses with increasing model size,
validating the parameter efficiency of the generative approach. To further
enhance performance, we propose a lightweight yet effective post-training
pipeline that (1) enforces full-process SID alignment and (2) applies
reinforcement learning with constrained decoding and hybrid rewards. Together,
these techniques yield significant improvements in both ranking accuracy and
candidate diversity.

</details>
