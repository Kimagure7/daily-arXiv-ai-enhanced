<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 8]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [ORBIT -- Open Recommendation Benchmark for Reproducible Research with Hidden Tests](https://arxiv.org/abs/2510.26095)
*Jingyuan He,Jiongnan Liu,Vishan Vishesh Oberoi,Bolin Wu,Mahima Jagadeesh Patel,Kangrui Mao,Chuning Shi,I-Ta Lee,Arnold Overwijk,Chenyan Xiong*

Main category: cs.IR

TL;DR: ORBIT是一个统一的推荐系统基准测试框架，包含公开数据集和隐藏测试集ClueWeb-Reco，旨在解决现有数据集不真实和评估设置不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统数据集无法捕捉真实用户行为，评估设置不一致导致结论模糊，阻碍了推荐系统的研究和开发。

Method: ORBIT提供标准化评估框架，包含可复现的数据分割和透明设置。引入新的网页推荐任务ClueWeb-Reco，基于8700万高质量网页的浏览序列，作为隐藏测试集评估模型泛化能力。

Result: 在公开基准测试中，推荐系统普遍有所改进，但个体表现差异较大。隐藏测试结果显示现有方法在大规模网页推荐中存在局限性，LLM集成显示出改进潜力。

Conclusion: ORBIT基准测试揭示了现有推荐方法的局限性，特别是在大规模网页推荐场景中，并展示了LLM集成的潜力。该框架为推荐系统的可复现研究提供了标准化平台。

Abstract: Recommender systems are among the most impactful AI applications, interacting
with billions of users every day, guiding them to relevant products, services,
or information tailored to their preferences. However, the research and
development of recommender systems are hindered by existing datasets that fail
to capture realistic user behaviors and inconsistent evaluation settings that
lead to ambiguous conclusions. This paper introduces the Open Recommendation
Benchmark for Reproducible Research with HIdden Tests (ORBIT), a unified
benchmark for consistent and realistic evaluation of recommendation models.
ORBIT offers a standardized evaluation framework of public datasets with
reproducible splits and transparent settings for its public leaderboard.
Additionally, ORBIT introduces a new webpage recommendation task, ClueWeb-Reco,
featuring web browsing sequences from 87 million public, high-quality webpages.
ClueWeb-Reco is a synthetic dataset derived from real, user-consented, and
privacy-guaranteed browsing data. It aligns with modern recommendation
scenarios and is reserved as the hidden test part of our leaderboard to
challenge recommendation models' generalization ability. ORBIT measures 12
representative recommendation models on its public benchmark and introduces a
prompted LLM baseline on the ClueWeb-Reco hidden test. Our benchmark results
reflect general improvements of recommender systems on the public datasets,
with variable individual performances. The results on the hidden test reveal
the limitations of existing approaches in large-scale webpage recommendation
and highlight the potential for improvements with LLM integrations. ORBIT
benchmark, leaderboard, and codebase are available at
https://www.open-reco-bench.ai.

</details>


### [2] [OneTrans: Unified Feature Interaction and Sequence Modeling with One Transformer in Industrial Recommender](https://arxiv.org/abs/2510.26104)
*Zhaoqi Zhang,Haolei Pei,Jun Guo,Tianyu Wang,Yufei Feng,Hui Sun,Shaowei Liu,Aixin Sun*

Main category: cs.IR

TL;DR: OneTrans是一个统一的Transformer骨干网络，同时执行用户行为序列建模和特征交互，通过统一tokenizer和参数共享机制实现高效扩展。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统中，特征交互模块和用户行为序列模块通常分开开发，阻碍了双向信息交换和统一优化扩展。

Method: 使用统一tokenizer将顺序和非顺序属性转换为单一token序列；堆叠的OneTrans块在相似顺序token间共享参数，为非顺序token分配特定参数；通过因果注意力和跨请求KV缓存实现预计算和缓存。

Result: 在工业规模数据集上，OneTrans随参数增加而高效扩展，持续优于强基线，在线A/B测试中实现人均GMV提升5.68%。

Conclusion: OneTrans成功统一了用户行为序列建模和特征交互，实现了高效扩展和性能提升。

Abstract: In recommendation systems, scaling up feature-interaction modules (e.g.,
Wukong, RankMixer) or user-behavior sequence modules (e.g., LONGER) has
achieved notable success. However, these efforts typically proceed on separate
tracks, which not only hinders bidirectional information exchange but also
prevents unified optimization and scaling. In this paper, we propose OneTrans,
a unified Transformer backbone that simultaneously performs user-behavior
sequence modeling and feature interaction. OneTrans employs a unified tokenizer
to convert both sequential and non-sequential attributes into a single token
sequence. The stacked OneTrans blocks share parameters across similar
sequential tokens while assigning token-specific parameters to non-sequential
tokens. Through causal attention and cross-request KV caching, OneTrans enables
precomputation and caching of intermediate representations, significantly
reducing computational costs during both training and inference. Experimental
results on industrial-scale datasets demonstrate that OneTrans scales
efficiently with increasing parameters, consistently outperforms strong
baselines, and yields a 5.68% lift in per-user GMV in online A/B tests.

</details>


### [3] [ReaKase-8B: Legal Case Retrieval via Knowledge and Reasoning Representations with LLMs](https://arxiv.org/abs/2510.26178)
*Yanran Tang,Ruihong Qiu,Xue Li,Zi Huang*

Main category: cs.IR

TL;DR: 提出了ReaKase-8B框架，通过整合法律事实、法律问题、法律关系三元组和法律推理来增强法律案例检索性能


<details>
  <summary>Details</summary>
Motivation: 现有法律案例检索方法主要依赖传统词汇模型和预训练语言模型，但忽略了法律实体间的关系以及法律推理过程，这些信息对区分不同案例至关重要

Method: 设计了上下文法律案例表示学习范式，使用微调的大型语言模型，整合法律事实、法律问题、法律关系三元组和法律推理信息

Result: 在COLIEE 2022和2023基准数据集上的实验表明，该框架显著提高了检索性能，优于基线模型

Conclusion: 将法律推理整合到法律案例检索系统中具有巨大潜力，能够有效提升检索准确性

Abstract: Legal case retrieval (LCR) is a cornerstone of real-world legal decision
making, as it enables practitioners to identify precedents for a given query
case. Existing approaches mainly rely on traditional lexical models and
pretrained language models to encode the texts of legal cases. Yet there are
rich information in the relations among different legal entities as well as the
crucial reasoning process that uncovers how legal facts and legal issues can
lead to judicial decisions. Such relational reasoning process reflects the
distinctive characteristics of each case that can distinguish one from another,
mirroring the real-world judicial process. Naturally, incorporating such
information into the precise case embedding could further enhance the accuracy
of case retrieval. In this paper, a novel ReaKase-8B framework is proposed to
leverage extracted legal facts, legal issues, legal relation triplets and legal
reasoning for effective legal case retrieval. ReaKase-8B designs an in-context
legal case representation learning paradigm with a fine-tuned large language
model. Extensive experiments on two benchmark datasets from COLIEE 2022 and
COLIEE 2023 demonstrate that our knowledge and reasoning augmented embeddings
substantially improve retrieval performance over baseline models, highlighting
the potential of integrating legal reasoning into legal case retrieval systems.
The code has been released on https://github.com/yanran-tang/ReaKase-8B.

</details>


### [4] [DiSE: A diffusion probabilistic model for automatic structure elucidation of organic compounds](https://arxiv.org/abs/2510.26231)
*Haochen Chen,Qi Huang,Anan Wu,Wenhao Zhang,Jianliang Ye,Jianming Wu,Kai Tan,Xin Lu,Xin Xu*

Main category: cs.IR

TL;DR: DiSE是一个基于扩散的端到端生成模型，整合多种光谱模态实现有机化合物的自动结构解析，为自驱动实验室提供关键能力。


<details>
  <summary>Details</summary>
Motivation: 实现自动结构解析对于自驱动实验室至关重要，能够闭合实验反馈循环，为机器学习模型提供可靠的结构信息以进行实时决策和优化。

Method: 开发DiSE模型，整合质谱(MS)、13C和1H化学位移、HSQC和COSY等多种光谱模态，通过数据驱动方法学习光谱间的内在关联。

Result: DiSE在化学多样性数据集上表现出卓越的准确性、强大的泛化能力和对实验数据的鲁棒性，尽管仅在计算光谱上训练。

Conclusion: DiSE代表了向完全自动化结构解析的重大进展，在天然产物研究、药物发现和自驱动实验室中具有广泛潜力。

Abstract: Automatic structure elucidation is essential for self-driving laboratories as
it enables the system to achieve truly autonomous. This capability closes the
experimental feedback loop, ensuring that machine learning models receive
reliable structure information for real-time decision-making and optimization.
Herein, we present DiSE, an end-to-end diffusion-based generative model that
integrates multiple spectroscopic modalities, including MS, 13C and 1H chemical
shifts, HSQC, and COSY, to achieve automated yet accurate structure elucidation
of organic compounds. By learning inherent correlations among spectra through
data-driven approaches, DiSE achieves superior accuracy, strong generalization
across chemically diverse datasets, and robustness to experimental data despite
being trained on calculated spectra. DiSE thus represents a significant advance
toward fully automated structure elucidation, with broad potential in natural
product research, drug discovery, and self-driving laboratories.

</details>


### [5] [Barlow Twins for Sequential Recommendation](https://arxiv.org/abs/2510.26407)
*Ivan Razvorotnev,Marina Munkhoeva,Evgeny Frolov*

Main category: cs.IR

TL;DR: BT-SR是一个新颖的非对比自监督学习框架，将Barlow Twins冗余减少原则集成到基于Transformer的下一项推荐器中，无需负采样或人工扰动即可提高推荐准确性和多样性。


<details>
  <summary>Details</summary>
Motivation: 解决序列推荐中稀疏交互数据、流行度偏差以及准确性与多样性之间的冲突目标问题，同时避免现有对比学习方法的大批量需求、手工增强和负采样等缺点。

Method: 集成Barlow Twins冗余减少原则到Transformer推荐器中，学习对齐相似短期行为用户嵌入，同时保持长期区分性，无需负采样或人工扰动。

Result: 在五个公共基准测试中，BT-SR持续提高下一项预测准确性，显著增强长尾项目覆盖率和推荐校准度，单个超参数可控制准确性-多样性权衡。

Conclusion: BT-SR通过结构敏感对齐有效识别新兴用户意图并减轻噪声历史背景影响，为实践者提供可适应特定应用需求的推荐解决方案。

Abstract: Sequential recommendation models must navigate sparse interaction data
popularity bias and conflicting objectives like accuracy versus diversity While
recent contrastive selfsupervised learning SSL methods offer improved accuracy
they come with tradeoffs large batch requirements reliance on handcrafted
augmentations and negative sampling that can reinforce popularity bias In this
paper we introduce BT-SR a novel noncontrastive SSL framework that integrates
the Barlow Twins redundancyreduction principle into a Transformerbased nextitem
recommender BTSR learns embeddings that align users with similar shortterm
behaviors while preserving longterm distinctionswithout requiring negative
sampling or artificial perturbations This structuresensitive alignment allows
BT-SR to more effectively recognize emerging user intent and mitigate the
influence of noisy historical context Our experiments on five public benchmarks
demonstrate that BTSR consistently improves nextitem prediction accuracy and
significantly enhances longtail item coverage and recommendation calibration
Crucially we show that a single hyperparameter can control the
accuracydiversity tradeoff enabling practitioners to adapt recommendations to
specific application needs

</details>


### [6] [Vectorized Context-Aware Embeddings for GAT-Based Collaborative Filtering](https://arxiv.org/abs/2510.26461)
*Danial Ebrat,Sepideh Ahmadian,Luis Rueda*

Main category: cs.IR

TL;DR: 提出基于图注意力网络的协同过滤框架，结合LLM生成上下文感知嵌入，有效缓解数据稀疏和冷启动问题


<details>
  <summary>Details</summary>
Motivation: 推荐系统常面临数据稀疏和冷启动场景的限制，难以为新用户或不活跃用户提供准确推荐

Method: 使用LLM生成用户简档和统一项目元数据作为图节点特征，在二分图上应用GAT，结合BPR损失函数和余弦相似度项的混合损失

Result: 在MovieLens数据集上相比SOTA基线在Precision、NDCG和MAP指标上持续提升，对交互历史有限的用户表现出鲁棒性

Conclusion: 通过将LLM衍生的上下文理解整合到图架构中，有效缓解稀疏性和冷启动限制，未来将关注准确性、覆盖度、多样性的平衡以及公平性约束

Abstract: Recommender systems often struggle with data sparsity and cold-start
scenarios, limiting their ability to provide accurate suggestions for new or
infrequent users. This paper presents a Graph Attention Network (GAT) based
Collaborative Filtering (CF) framework enhanced with Large Language Model (LLM)
driven context aware embeddings. Specifically, we generate concise textual user
profiles and unify item metadata (titles, genres, overviews) into rich textual
embeddings, injecting these as initial node features in a bipartite user item
graph. To further optimize ranking performance, we introduce a hybrid loss
function that combines Bayesian Personalized Ranking (BPR) with a cosine
similarity term and robust negative sampling, ensuring explicit negative
feedback is distinguished from unobserved data. Experiments on the MovieLens
100k and 1M datasets show consistent improvements over state-of-the-art
baselines in Precision, NDCG, and MAP while demonstrating robustness for users
with limited interaction history. Ablation studies confirm the critical role of
LLM-augmented embeddings and the cosine similarity term in capturing nuanced
semantic relationships. Our approach effectively mitigates sparsity and
cold-start limitations by integrating LLM-derived contextual understanding into
graph-based architectures. Future directions include balancing recommendation
accuracy with coverage and diversity, and introducing fairness-aware
constraints and interpretability features to enhance system performance
further.

</details>


### [7] [WeaveRec: An LLM-Based Cross-Domain Sequential Recommendation Framework with Model Merging](https://arxiv.org/abs/2510.26546)
*Min Hou,Xin Liu,Le Wu,Chenyi He,Hao Liu,Zhi Li,Xin Li,Si Wei*

Main category: cs.IR

TL;DR: 提出WeaveRec方法解决跨域序列推荐中LLM性能下降问题，通过交叉训练和融合LoRA模块实现知识迁移，无需重叠用户或物品，且不增加推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有跨域推荐方法依赖重叠用户/物品，这在现实中很少见。虽然LLM和模型融合技术可以统一多域数据，但简单地组合训练或融合多个域特定LLM往往会导致性能下降。

Method: WeaveRec方法交叉训练多个LoRA模块，以编织方式使用源域和目标域数据，然后通过模型融合将它们合并。该方法可扩展到多源域场景，且不增加推理延迟或内存成本。

Result: 在单源、多源和跨平台跨域推荐场景的广泛实验中，WeaveRec有效缓解了性能下降问题，并在实际推荐任务中始终优于基线方法。

Conclusion: WeaveRec通过创新的交叉训练和模型融合策略，成功解决了LLM在跨域推荐中的性能退化问题，为无需重叠用户/物品的跨域推荐提供了有效解决方案。

Abstract: Cross-Domain Sequential Recommendation (CDSR) seeks to improve user
preference modeling by transferring knowledge from multiple domains. Despite
the progress made in CDSR, most existing methods rely on overlapping users or
items to establish cross-domain correlations-a requirement that rarely holds in
real-world settings. The advent of large language models (LLM) and
model-merging techniques appears to overcome this limitation by unifying
multi-domain data without explicit overlaps. Yet, our empirical study shows
that naively training an LLM on combined domains-or simply merging several
domain-specific LLMs-often degrades performance relative to a model trained
solely on the target domain. To address these challenges, we first
experimentally investigate the cause of suboptimal performance in LLM-based
cross-domain recommendation and model merging. Building on these insights, we
introduce WeaveRec, which cross-trains multiple LoRA modules with source and
target domain data in a weaving fashion, and fuses them via model merging.
WeaveRec can be extended to multi-source domain scenarios and notably does not
introduce additional inference-time cost in terms of latency or memory.
Furthermore, we provide a theoretical guarantee that WeaveRec can reduce the
upper bound of the expected error in the target domain. Extensive experiments
on single-source, multi-source, and cross-platform cross-domain recommendation
scenarios validate that WeaveRec effectively mitigates performance degradation
and consistently outperforms baseline approaches in real-world recommendation
tasks.

</details>


### [8] [ProfOlaf: Semi-Automated Tool for Systematic Literature Reviews](https://arxiv.org/abs/2510.26750)
*Martim Afonso,Nuno Saavedra,Bruno Lourenço,Alexandra Mendes,João Ferreira*

Main category: cs.IR

TL;DR: ProfOlaf是一个半自动化工具，通过结合自动化与人工指导，提升系统综述的效率、质量和可重复性。


<details>
  <summary>Details</summary>
Motivation: 系统综述和映射研究对于综合研究、识别差距和指导未来工作至关重要，但通常劳动密集且耗时。现有工具仅支持特定步骤，大部分过程仍需手动且容易出错。

Method: ProfOlaf支持迭代式滚雪球法进行文献收集，采用人机协同过滤，并利用大语言模型协助分析文章、提取关键主题和回答论文内容相关问题。

Result: 通过将自动化与指导性人工努力相结合，ProfOlaf提高了系统综述的效率、质量和可重复性。

Conclusion: ProfOlaf是一个有效的半自动化工具，能够在保持方法严谨性的同时，显著提升系统综述过程的效率和质量。

Abstract: Systematic reviews and mapping studies are critical for synthesizing
research, identifying gaps, and guiding future work, but they are often
labor-intensive and time-consuming. Existing tools provide partial support for
specific steps, leaving much of the process manual and error-prone. We present
ProfOlaf, a semi-automated tool designed to streamline systematic reviews while
maintaining methodological rigor. ProfOlaf supports iterative snowballing for
article collection with human-in-the-loop filtering and uses large language
models to assist in analyzing articles, extracting key topics, and answering
queries about the content of papers. By combining automation with guided manual
effort, ProfOlaf enhances the efficiency, quality, and reproducibility of
systematic reviews across research fields. A video describing and demonstrating
ProfOlaf is available at: https://youtu.be/4noUXfcmxsE

</details>
