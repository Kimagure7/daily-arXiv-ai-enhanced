<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 30]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Improving E-commerce Search with Category-Aligned Retrieval](https://arxiv.org/abs/2510.21711)
*Rauf Aliev*

Main category: cs.IR

TL;DR: 提出类别对齐检索系统(CARS)，通过查询预测产品类别并提升该类别产品排序，但直接应用类别提升可能损害检索相关性指标。


<details>
  <summary>Details</summary>
Motivation: 解决传统电商搜索系统中用户查询与产品目录之间的语义鸿沟问题。

Method: 使用可训练类别原型从查询嵌入中预测产品类别，然后在该类别内提升产品排序。评估了两种嵌入模型：all-MiniLM-L6-v2和OpenAI text-embedding-ada-002。

Result: 离线评估显示该方法很有效，OpenAI模型将Top-3类别预测准确率从43.8%提升到83.2%。但端到端模拟显示，盲目应用类别提升会降低nDCG@10等检索相关性指标。

Conclusion: 该方法具有价值，但需要置信度感知和自适应集成策略，避免过度约束检索系统。

Abstract: Traditional e-commerce search systems often struggle with the semantic gap
between user queries and product catalogs. In this paper, we propose a
Category-Aligned Retrieval System (CARS) that improves search relevance by
first predicting the product category from a user's query and then boosting
products within that category. We introduce a novel method for creating
"Trainable Category Prototypes" from query embeddings. We evaluate this method
with two models: a lightweight all-MiniLM-L6-v2 and OpenAI's
text-embedding-ada-002. Our offline evaluation shows this method is highly
effective, with the OpenAI model increasing Top-3 category prediction accuracy
from a zero-shot baseline of 43.8% to 83.2% after training. The end-to-end
simulation, however, highlights the limitations of blindly applying category
boosts in a complex retrieval pipeline: while accuracy is high, naive
integration can negatively affect search relevance metrics such as nDCG@10. We
argue that this is partly due to dataset-specific ambiguities (e.g., polysemous
queries in the Amazon ESCI corpus) and partly due to the sensitivity of
retrieval systems to over-constraining filters. Crucially, these results do not
diminish the value of the approach; rather, they emphasize the need for
confidence-aware and adaptive integration strategies.

</details>


### [2] [DecoupleSearch: Decouple Planning and Search via Hierarchical Reward Modeling](https://arxiv.org/abs/2510.21712)
*Hao Sun,Zile Qiao,Bo Wang,Guoxin Chen,Yingyan Hou,Yong Jiang,Pengjun Xie,Fei Huang,Yan Zhang*

Main category: cs.IR

TL;DR: 提出了DecoupleSearch框架，通过双价值模型解耦规划和搜索过程，解决Agentic RAG面临的挑战，包括步骤依赖、中间推理监督缺失和候选空间过大问题。


<details>
  <summary>Details</summary>
Motivation: Agentic RAG在增强LLM灵活性方面面临三个主要挑战：规划和搜索的相互依赖、中间推理步骤缺乏监督、以及规划和搜索的候选空间指数级增长。

Method: 使用双价值模型解耦规划和搜索过程，构建推理树，利用蒙特卡洛树搜索评估每个步骤质量，在推理时采用分层束搜索迭代优化规划和搜索候选。

Result: 在不同参数规模的政策模型上进行广泛实验，证明了该方法的有效性。

Conclusion: DecoupleSearch框架通过解耦规划和搜索，实现了规划推理和搜索基础的独立优化，有效解决了Agentic RAG的关键挑战。

Abstract: Retrieval-Augmented Generation (RAG) systems have emerged as a pivotal
methodology for enhancing Large Language Models (LLMs) through the dynamic
integration of external knowledge. To further improve RAG's flexibility,
Agentic RAG introduces autonomous agents into the workflow. However, Agentic
RAG faces several challenges: (1) the success of each step depends on both
high-quality planning and accurate search, (2) the lack of supervision for
intermediate reasoning steps, and (3) the exponentially large candidate space
for planning and searching. To address these challenges, we propose
DecoupleSearch, a novel framework that decouples planning and search processes
using dual value models, enabling independent optimization of plan reasoning
and search grounding. Our approach constructs a reasoning tree, where each node
represents planning and search steps. We leverage Monte Carlo Tree Search to
assess the quality of each step. During inference, Hierarchical Beam Search
iteratively refines planning and search candidates with dual value models.
Extensive experiments across policy models of varying parameter sizes,
demonstrate the effectiveness of our method.

</details>


### [3] [asLLR: LLM based Leads Ranking in Auto Sales](https://arxiv.org/abs/2510.21713)
*Yin Sun,Yiwen Liu,Junjie Song,Chenyu Zhang,Xinyuan Zhang,Lingjie Liu,Siqi Chen,Yuji Cao*

Main category: cs.IR

TL;DR: 该研究提出asLLR模型，将CTR损失和问答损失集成到仅解码器的大语言模型架构中，用于汽车销售线索排序，在真实销售场景中比传统方法提升约9.5%的销售量。


<details>
  <summary>Details</summary>
Motivation: 传统CTR预测方法难以处理CRM系统中复杂的自然语言交互特征，限制了销售线索排序的效果。大语言模型的出现为改进推荐系统提供了新途径。

Method: 提出asLLR模型，在仅解码器的大语言模型架构中集成CTR损失和问答损失，同时建模表格数据和自然语言特征。

Result: 在包含30万训练样本和4万测试样本的数据集上，asLLR达到AUC 0.8127，比传统CTR方法提升0.0231，在真实A/B测试中提升销售量约9.5%。

Conclusion: asLLR能有效建模商业数据集中的复杂模式，为商业智能和运营决策提供了有价值的工具。

Abstract: In the area of commercial auto sales system, high-quality lead score
sequencing determines the priority of a sale's work and is essential for
optimizing the efficiency of the sales system. Since CRM (Customer Relationship
Management) system contains plenty of textual interaction features between
sales and customers, traditional techniques such as Click Through Rate (CTR)
prediction struggle with processing the complex information inherent in natural
language features, which limits their effectiveness in sales lead ranking.
Bridging this gap is critical for enhancing business intelligence and
decision-making. Recently, the emergence of large language models (LLMs) has
opened new avenues for improving recommendation systems, this study introduces
asLLR (LLM-based Leads Ranking in Auto Sales), which integrates CTR loss and
Question Answering (QA) loss within a decoder-only large language model
architecture. This integration enables the simultaneous modeling of both
tabular and natural language features. To verify the efficacy of asLLR, we
constructed an innovative dataset derived from the customer lead pool of a
prominent new energy vehicle brand, with 300,000 training samples and 40,000
testing samples. Our experimental results demonstrate that asLLR effectively
models intricate patterns in commercial datasets, achieving the AUC of 0.8127,
surpassing traditional CTR estimation methods by 0.0231. Moreover, asLLR
enhances CTR models when used for extracting text features by 0.0058. In
real-world sales scenarios, after rigorous online A/B testing, asLLR increased
the sales volume by about 9.5% compared to the traditional method, providing a
valuable tool for business intelligence and operational decision-making.

</details>


### [4] [Practice on Long Behavior Sequence Modeling in Tencent Advertising](https://arxiv.org/abs/2510.21714)
*Xian Hu,Ming Yue,Zhixiang Feng,Junwei Pan,Junjie Zhai,Ximei Wang,Xinrui Miao,Qian Li,Xun Liu,Shangyu Zhang,Letian Wang,Hua Lu,Zijian Zeng,Chen Cai,Wei Wang,Fei Xiong,Pengfei Xiong,Jintao Zhang,Zhiyuan Wu,Chunhui Zhang,Anan Liu,Jiulong You,Chao Deng,Yuekui Yang,Shudong Huang,Dapeng Liu,Haijie Gu*

Main category: cs.IR

TL;DR: 提出了一种跨域长序列建模方法，解决广告推荐中用户行为稀疏性问题，通过整合跨广告场景和内容域的行为数据构建统一商业行为轨迹，并在两阶段框架中设计了多种技术来应对特征分类差异、字段干扰和目标冲突等挑战。


<details>
  <summary>Details</summary>
Motivation: 广告领域用户行为稀疏，仅使用单一广告域数据难以构建长行为序列，需要整合跨广告场景和内容域的用户行为数据来构建统一商业行为轨迹。

Method: 采用两阶段框架：第一阶段设计分层硬搜索处理复杂特征分类层次和解耦嵌入软搜索缓解注意力机制与特征表示的冲突；第二阶段引入解耦侧信息时序兴趣网络缓解字段冲突、目标解耦位置编码和目标解耦SASRec解决目标干扰、堆叠TIN建模高阶行为相关性。

Result: 在腾讯大规模广告平台部署后，微信视频号整体GMV提升4.22%，微信朋友圈整体GMV提升1.96%。

Conclusion: 跨域长序列建模能有效解决广告推荐中的行为稀疏问题，提出的两阶段框架和各项技术创新在实际部署中取得了显著效果提升。

Abstract: Long-sequence modeling has become an indispensable frontier in recommendation
systems for capturing users' long-term preferences. However, user behaviors
within advertising domains are inherently sparse, posing a significant barrier
to constructing long behavioral sequences using data from a single advertising
domain alone. This motivates us to collect users' behaviors not only across
diverse advertising scenarios, but also beyond the boundaries of the
advertising domain into content domains-thereby constructing unified commercial
behavior trajectories. This cross-domain or cross-scenario integration gives
rise to the following challenges: (1) feature taxonomy gaps between distinct
scenarios and domains, (2) inter-field interference arising from irrelevant
feature field pairs, and (3) target-wise interference in temporal and semantic
patterns when optimizing for different advertising targets. To address these
challenges, we propose several practical approaches within the two-stage
framework for long-sequence modeling. In the first (search) stage, we design a
hierarchical hard search method for handling complex feature taxonomy
hierarchies, alongside a decoupled embedding-based soft search to alleviate
conflicts between attention mechanisms and feature representation. In the
second (sequence modeling) stage, we introduce: (a) Decoupled Side Information
Temporal Interest Networks (TIN) to mitigate inter-field conflicts; (b)
Target-Decoupled Positional Encoding and Target-Decoupled SASRec to address
target-wise interference; and (c) Stacked TIN to model high-order behavioral
correlations. Deployed in production on Tencent's large-scale advertising
platforms, our innovations delivered significant performance gains: an overall
4.22% GMV lift in WeChat Channels and an overall 1.96% GMV increase in WeChat
Moments.

</details>


### [5] [Words to Waves: Emotion-Adaptive Music Recommendation System](https://arxiv.org/abs/2510.21724)
*Apoorva Chavali,Reeve Menezes*

Main category: cs.IR

TL;DR: 提出了一种基于Wide and Deep Learning架构的音乐推荐系统，通过实时情感状态推断来推荐匹配用户当前情绪的音乐。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统往往忽略情感上下文，仅依赖历史收听模式或静态情绪标签，无法捕捉用户的实时情感需求。

Method: 使用基于transformer的嵌入模型从用户文本描述中提取情感维度（效价-唤醒度），结合Wide and Deep架构：深度组件泛化未见情感模式，宽度组件通过交叉特征记忆用户-情感和情感-流派关联。

Result: 实验结果显示，个性化音乐选择对用户情绪产生积极影响，并显著提高了情感相关性。

Conclusion: 该框架能够有效利用实时情感上下文进行音乐推荐，提升用户体验和情感匹配度。

Abstract: Current recommendation systems often tend to overlook emotional context and
rely on historical listening patterns or static mood tags. This paper
introduces a novel music recommendation framework employing a variant of Wide
and Deep Learning architecture that takes in real-time emotional states
inferred directly from natural language as inputs and recommends songs that
closely portray the mood. The system captures emotional contexts from
user-provided textual descriptions by using transformer-based embeddings, which
were finetuned to predict the emotional dimensions of valence-arousal. The deep
component of the architecture utilizes these embeddings to generalize unseen
emotional patterns, while the wide component effectively memorizes user-emotion
and emotion-genre associations through cross-product features. Experimental
results show that personalized music selections positively influence the user's
emotions and lead to a significant improvement in emotional relevance.

</details>


### [6] [From Authors to Reviewers: Leveraging Rankings to Improve Peer Review](https://arxiv.org/abs/2510.21726)
*Weichen Wang,Chengchun Shi*

Main category: cs.IR

TL;DR: 本文提出了一种替代Su等人(2025)的方法，利用审稿人而非作者的排名信息来评估论文质量。通过模拟ICML 2023会议数据，发现结合审稿人和作者的排名信息能获得最准确的论文评估。


<details>
  <summary>Details</summary>
Motivation: 机器学习会议评审质量因投稿量激增而备受关注。Su等人(2025)使用作者排名信息，本文探讨利用审稿人排名信息的替代方法。

Method: 模拟了与ICML 2023会议提交数据相似的评审数据，比较了仅使用作者排名信息、仅使用审稿人排名信息以及结合两者的方法。

Result: (i) 使用审稿人排名信息能显著改善论文质量评估，通常优于仅使用作者排名信息；(ii) 在多数情况下，结合审稿人和作者的排名信息能获得最准确的论文评估。

Conclusion: 结合审稿人和作者的排名信息是评估机器学习会议论文质量的最有效方法，为改进会议评审流程提供了新思路。

Abstract: This paper is a discussion of the 2025 JASA discussion paper by Su et al.
(2025). We would like to congratulate the authors on conducting a comprehensive
and insightful empirical investigation of the 2023 ICML ranking data. The
review quality of machine learning (ML) conferences has become a big concern in
recent years, due to the rapidly growing number of submitted manuscripts. In
this discussion, we propose an approach alternative to Su et al. (2025) that
leverages ranking information from reviewers rather than authors. We simulate
review data that closely mimics the 2023 ICML conference submissions. Our
results show that (i) incorporating ranking information from reviewers can
significantly improve the evaluation of each paper's quality, often
outperforming the use of ranking information from authors alone; and (ii)
combining ranking information from both reviewers and authors yields the most
accurate evaluation of submitted papers in most scenarios.

</details>


### [7] [Your Dense Retriever is Secretly an Expeditious Reasoner](https://arxiv.org/abs/2510.21727)
*Yichi Zhang,Jun Bai,Zhixin Cai,Shuhan Qin,Zhuofan Chen,Jinghua Guan,Wenge Rong*

Main category: cs.IR

TL;DR: AdaQR是一个混合查询重写框架，通过Reasoner Router动态选择快速密集推理或深度LLM推理，在保持检索性能的同时显著降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 密集检索器在处理推理密集型查询时表现不佳，而LLM虽然能重写查询但计算成本高昂，需要平衡效率与准确性。

Method: 提出自适应查询推理框架，包含Reasoner Router动态路由查询，Dense Reasoner在嵌入空间直接进行LLM风格推理。

Result: 在BRIGHT基准测试中，推理成本降低28%，检索性能提升7%。

Conclusion: AdaQR框架成功实现了效率与准确性的可控权衡，为推理密集型查询提供了有效的解决方案。

Abstract: Dense retrievers enhance retrieval by encoding queries and documents into
continuous vectors, but they often struggle with reasoning-intensive queries.
Although Large Language Models (LLMs) can reformulate queries to capture
complex reasoning, applying them universally incurs significant computational
cost. In this work, we propose Adaptive Query Reasoning (AdaQR), a hybrid query
rewriting framework. Within this framework, a Reasoner Router dynamically
directs each query to either fast dense reasoning or deep LLM reasoning. The
dense reasoning is achieved by the Dense Reasoner, which performs LLM-style
reasoning directly in the embedding space, enabling a controllable trade-off
between efficiency and accuracy. Experiments on large-scale retrieval
benchmarks BRIGHT show that AdaQR reduces reasoning cost by 28% while
preserving-or even improving-retrieval performance by 7%.

</details>


### [8] [Modeling Bias Evolution in Fashion Recommender Systems: A System Dynamics Approach](https://arxiv.org/abs/2510.21728)
*Mahsa Goodarzi,M. Abdullah Canbaz*

Main category: cs.IR

TL;DR: 本研究采用动态建模方法分析时尚推荐系统中的偏见机制，发现归纳偏见比用户偏见对系统结果影响更大，现有去偏见策略虽有效但仍需改进。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中的偏见不仅扭曲用户体验，还会强化社会刻板印象，特别是在时尚电商领域，需要深入理解偏见激活和强化的机制。

Method: 使用系统动力学建模和实验模拟，分析偏见的时态演变及其对系统性能的多方面影响。

Result: 归纳偏见对系统结果的影响比用户偏见更显著；当前去偏见策略（数据重平衡和算法正则化）有一定效果但需要进一步改进。

Conclusion: 需要推进去偏见策略，扩展系统边界以纳入用户人口统计和物品多样性等更广泛背景因素，促进时尚推荐系统的包容性和公平性。

Abstract: Bias in recommender systems not only distorts user experience but also
perpetuates and amplifies existing societal stereotypes, particularly in
sectors like fashion e-commerce. This study employs a dynamic modeling approach
to scrutinize the mechanisms of bias activation and reinforcement within
Fashion Recommender Systems (FRS). By leveraging system dynamics modeling and
experimental simulations, we dissect the temporal evolution of bias and its
multifaceted impacts on system performance. Our analysis reveals that inductive
biases exert a more substantial influence on system outcomes than user biases,
suggesting critical areas for intervention. We demonstrate that while current
debiasing strategies, including data rebalancing and algorithmic
regularization, are effective to an extent, they require further enhancement to
comprehensively mitigate biases. This research underscores the necessity for
advancing these strategies and extending system boundaries to incorporate
broader contextual factors such as user demographics and item diversity, aiming
to foster inclusivity and fairness in FRS. The findings advocate for a
proactive approach in recommender system design to counteract bias propagation
and ensure equitable user experiences.

</details>


### [9] [CustomIR: Unsupervised Fine-Tuning of Dense Embeddings for Known Document Corpora](https://arxiv.org/abs/2510.21729)
*Nathan Paull*

Main category: cs.IR

TL;DR: CustomIR是一个无监督框架，通过使用LLM生成的查询-文档对来适应预训练语言嵌入模型到特定领域语料库，无需人工标注即可提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 密集嵌入模型在应用于预训练分布之外的专门语料库时性能会下降，需要一种无需昂贵人工标注的领域适应方法。

Method: 利用大语言模型生成基于目标语料库的多样化查询，并配对LLM验证的困难负例，进行无监督的领域适应训练。

Result: 在企业邮件和消息数据集上的实验表明，CustomIR持续提升检索效果，小模型在Recall@10指标上最高提升2.3个百分点，性能可与更大模型相媲美。

Conclusion: 定向的合成微调为提升领域特定性能提供了一种可扩展且成本效益高的策略，能够实现更便宜的RAG部署。

Abstract: Dense embedding models have become critical for modern information retrieval,
particularly in RAG pipelines, but their performance often degrades when
applied to specialized corpora outside their pre-training distribution. To
address thi we introduce \textbf{CustomIR}, a framework for unsupervised
adaptation of pre-trained language embedding models to domain-specific corpora
using synthetically generated query-document pairs. CustomIR leverages large
language models (LLMs) to create diverse queries grounded in a known target
corpus, paired with LLM-verified hard negatives, eliminating the need for
costly human annotation. Experiments on enterprise email and messaging datasets
show that CustomIR consistently improves retrieval effectiveness with small
models gaining up to 2.3 points in Recall@10. This performance increase allows
these small models to rival the performance of much larger alternatives,
allowing for cheaper RAG deployments. These results highlight that targeted
synthetic fine-tuning offers a scalable and cost-efficient strategy for
increasing domain-specific performance.

</details>


### [10] [TriMat: Context-aware Recommendation by Tri-Matrix Factorization](https://arxiv.org/abs/2510.21730)
*Hao Wang*

Main category: cs.IR

TL;DR: 本文利用三矩阵分解技术将上下文信息融入推荐系统，实验证明该方法能同时提升推荐准确性和公平性。


<details>
  <summary>Details</summary>
Motivation: 上下文感知推荐系统(CARS)长期以来主要停留在理论研究层面，缺乏实际应用进展。随着TikTok等应用的成功，推荐系统已成为Web 3.0的关键技术，但CARS领域仍有许多未解决的问题。

Method: 采用三矩阵分解技术，在矩阵分解框架中融入上下文信息。

Result: 实验结果表明，该方法在准确性和公平性指标上都有显著提升。

Conclusion: 三矩阵分解技术是解决上下文感知推荐系统实际应用问题的有效方法，能够同时优化多个性能指标。

Abstract: Search engine is the symbolic technology of Web 2.0, and many people used to
believe recommender systems is the new frontier of Web 3.0. In the past 10
years, with the advent of TikTok and similar apps, recommender systems has
materialized the vision of the machine learning pioneers. However, many
research topics of the field remain unfixed until today. One such topic is CARS
(Context-aware Recommender Systems) , which is largely a theoretical topic
without much advance in real-world applications. In this paper, we utilize
tri-matrix factorization technique to incorporate contextual information into
our matrix factorization framework, and prove that our technique is effective
in improving both the accuracy and fairness metrics in our experiments.

</details>


### [11] [Augmenting Researchy Questions with Sub-question Judgments](https://arxiv.org/abs/2510.21733)
*Jia-Huei Ju,Eugene Yang,Trevor Adriaanse,Andrew Yates*

Main category: cs.IR

TL;DR: 本研究对Researchy Questions数据集进行了增强，使用Llama3.3 70B模型为每个子问题添加了LLM判断的标签，旨在为训练支持复杂信息需求的检索模型提供资源。


<details>
  <summary>Details</summary>
Motivation: Researchy Questions数据集包含约10万个复杂信息需求的查询，每个查询都有GPT-4生成的子问题，但缺乏子问题与相关文档之间的关联标签。

Method: 使用Llama3.3 70B模型为数据集中的每个子问题生成LLM判断的标签。

Result: 成功为Researchy Questions数据集中的子问题添加了标签，增强了数据集的质量和实用性。

Conclusion: 这些子问题标签将作为训练检索模型的资源，帮助模型更好地支持复杂信息需求。

Abstract: The Researchy Questions dataset provides about 100k question queries with
complex information needs that require retrieving information about several
aspects of a topic. Each query in ResearchyQuestions is associated with
sub-questions that were produced by prompting GPT-4. While ResearchyQuestions
contains labels indicating what documents were clicked after issuing the query,
there are no associations in the dataset between sub-questions and relevant
documents. In this work, we augment the Researchy Questions dataset with
LLM-judged labels for each sub-question using a Llama3.3 70B model. We intend
these sub-question labels to serve as a resource for training retrieval models
that better support complex information needs.

</details>


### [12] [From Factoid Questions to Data Product Requests: Benchmarking Data Product Discovery over Tables and Text](https://arxiv.org/abs/2510.21737)
*Liangliang Zhang,Nandana Mihindukulasooriya,Niharika S. D'Souza,Sola Shirai,Sarthak Dash,Yao Ma,Horst Samulowitz*

Main category: cs.IR

TL;DR: 提出了DPBench，这是第一个面向混合表格-文本语料库的用户需求驱动的数据产品基准测试，通过重新组织现有表格-文本QA数据集来构建连贯的数据产品，并生成专业级别的分析请求。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏专门针对数据产品发现的基准测试，现有数据集主要关注单个表格上的事实性问题回答，而非收集多个数据资产来构建更广泛、连贯的数据产品。

Method: 通过将相关表格和段落聚类成连贯的数据产品，生成跨数据源的专业分析请求，并使用多LLM评估验证基准质量，同时保留完整的溯源信息。

Result: 建立了DPBench基准，通过混合检索方法的基线实验证实了数据产品请求评估的可行性，揭示了当前限制，并为自动数据产品发现研究指明了新机会。

Conclusion: DPBench填补了数据产品发现基准的空白，为自动数据产品发现研究提供了可行的评估框架和新的研究方向。

Abstract: Data products are reusable, self-contained assets designed for specific
business use cases. Automating their discovery and generation is of great
industry interest, as it enables discovery in large data lakes and supports
analytical Data Product Requests (DPRs). Currently, there is no benchmark
established specifically for data product discovery. Existing datasets focus on
answering single factoid questions over individual tables rather than
collecting multiple data assets for broader, coherent products. To address this
gap, we introduce DPBench, the first user-request-driven data product benchmark
over hybrid table-text corpora. Our framework systematically repurposes
existing table-text QA datasets by clustering related tables and passages into
coherent data products, generating professional-level analytical requests that
span both data sources, and validating benchmark quality through multi-LLM
evaluation. DPBench preserves full provenance while producing actionable,
analyst-like data product requests. Baseline experiments with hybrid retrieval
methods establish the feasibility of DPR evaluation, reveal current
limitations, and point to new opportunities for automatic data product
discovery research.
  Code and datasets are available at:
https://anonymous.4open.science/r/data-product-benchmark-BBA7/

</details>


### [13] [DiffGRM: Diffusion-based Generative Recommendation Model](https://arxiv.org/abs/2510.21805)
*Zhao Liu,Yichen Zhu,Yiqing Yang,Guoping Tang,Rui Huang,Qiang Luo,Xiao Lv,Ruiming Tang,Kun Gai,Guorui Zhou*

Main category: cs.IR

TL;DR: DiffGRM使用扩散模型替代自回归模型来解决生成式推荐中语义ID的结构问题，通过并行语义编码、策略一致性噪声和置信度引导并行去噪，在多个数据集上显著提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 传统生成式推荐使用自回归模型预测语义ID，但存在两个结构性问题：1) 数字间一致性被左到右因果性破坏；2) 不同数字语义粒度和可预测性不同，但统一目标函数导致训练不平衡。

Method: 提出DiffGRM模型：1) 并行语义编码解耦数字并平衡信息；2) 策略一致性噪声优先处理不确定数字；3) 置信度引导并行去噪先填充高置信度数字并生成多样化候选。

Result: 在多个数据集上相比强基线模型，NDCG@10指标提升6.9%-15.5%。

Conclusion: 扩散模型能有效解决语义ID的结构问题，通过双向上下文和并行生成显著提升生成式推荐性能。

Abstract: Generative recommendation (GR) is an emerging paradigm that represents each
item via a tokenizer as an n-digit semantic ID (SID) and predicts the next item
by autoregressively generating its SID conditioned on the user's history.
However, two structural properties of SIDs make ARMs ill-suited. First,
intra-item consistency: the n digits jointly specify one item, yet the
left-to-right causality trains each digit only under its prefix and blocks
bidirectional cross-digit evidence, collapsing supervision to a single causal
path. Second, inter-digit heterogeneity: digits differ in semantic granularity
and predictability, while the uniform next-token objective assigns equal weight
to all digits, overtraining easy digits and undertraining hard digits. To
address these two issues, we propose DiffGRM, a diffusion-based GR model that
replaces the autoregressive decoder with a masked discrete diffusion model
(MDM), thereby enabling bidirectional context and any-order parallel generation
of SID digits for recommendation. Specifically, we tailor DiffGRM in three
aspects: (1) tokenization with Parallel Semantic Encoding (PSE) to decouple
digits and balance per-digit information; (2) training with On-policy Coherent
Noising (OCN) that prioritizes uncertain digits via coherent masking to
concentrate supervision on high-value signals; and (3) inference with
Confidence-guided Parallel Denoising (CPD) that fills higher-confidence digits
first and generates diverse Top-K candidates. Experiments show consistent gains
over strong generative and discriminative recommendation baselines on multiple
datasets, improving NDCG@10 by 6.9%-15.5%. Code is available at
https://github.com/liuzhao09/DiffGRM.

</details>


### [14] [Unifying Inductive, Cross-Domain, and Multimodal Learning for Robust and Generalizable Recommendation](https://arxiv.org/abs/2510.21812)
*Chanyoung Chung,Kyeongryul Lee,Sunbin Park,Joyce Jiyoung Whang*

Main category: cs.IR

TL;DR: MICRec是一个统一的推荐系统框架，通过融合归纳建模、多模态指导和跨域迁移来捕捉异构和不完整真实数据中的用户上下文和潜在偏好。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统主要关注单个方面，难以处理日常消费中跨多个领域的复杂推荐场景。需要统一框架来应对异构和不完整数据。

Method: 基于INMO的归纳骨干网络，通过基于模态的聚合来精化表达表示，并利用跨域重叠用户作为锚点来缓解数据稀疏性问题。

Result: MICRec在12个基线方法上表现优异，特别是在训练数据有限的领域中取得了显著提升。

Conclusion: 该框架能够实现稳健且可泛化的推荐，有效解决了异构和稀疏数据环境下的推荐挑战。

Abstract: Recommender systems have long been built upon the modeling of interactions
between users and items, while recent studies have sought to broaden this
paradigm by generalizing to new users and items, incorporating diverse
information sources, and transferring knowledge across domains. Nevertheless,
these efforts have largely focused on individual aspects, hindering their
ability to tackle the complex recommendation scenarios that arise in daily
consumptions across diverse domains. In this paper, we present MICRec, a
unified framework that fuses inductive modeling, multimodal guidance, and
cross-domain transfer to capture user contexts and latent preferences in
heterogeneous and incomplete real-world data. Moving beyond the inductive
backbone of INMO, our model refines expressive representations through
modality-based aggregation and alleviates data sparsity by leveraging
overlapping users as anchors across domains, thereby enabling robust and
generalizable recommendation. Experiments show that MICRec outperforms 12
baselines, with notable gains in domains with limited training data.

</details>


### [15] [Development of an Automated Web Application for Efficient Web Scraping: Design and Implementation](https://arxiv.org/abs/2510.21831)
*Alok Dutta,Nilanjana Roy,Rhythm Sen,Sougata Dutta,Prabhat Das*

Main category: cs.IR

TL;DR: 开发了一个面向非技术用户的自动化网页抓取工具，将复杂的数据抓取过程简化为获取、提取和执行三个阶段，使用Flask框架和MongoDB实现用户管理和数据存储。


<details>
  <summary>Details</summary>
Motivation: 为了让非技术用户也能轻松进行网页数据抓取，解决传统网页抓取工具技术门槛高、操作复杂的问题，实现数据抓取的民主化。

Method: 采用三阶段方法：获取阶段使用requests库获取HTML内容；提取阶段使用BeautifulSoup和正则表达式提取数据；执行阶段将数据转换为CSV等格式。系统基于Flask框架开发，使用MongoDB存储用户数据和抓取历史。

Result: 成功开发了一个用户友好的网页抓取应用，用户只需输入网址和定义提取参数，即可轻松下载结构化数据，无需技术专业知识。

Conclusion: 该工具显著提高了网页抓取的效率，使各种技术水平的用户都能根据需求收集和管理数据，代表了网页抓取工具在可访问性、效率和易用性方面的重大进步。

Abstract: This paper presents the design and implementation of a user-friendly,
automated web application that simplifies and optimizes the web scraping
process for non-technical users. The application breaks down the complex task
of web scraping into three main stages: fetching, extraction, and execution. In
the fetching stage, the application accesses target websites using the HTTP
protocol, leveraging the requests library to retrieve HTML content. The
extraction stage utilizes powerful parsing libraries like BeautifulSoup and
regular expressions to extract relevant data from the HTML. Finally, the
execution stage structures the data into accessible formats, such as CSV,
ensuring the scraped content is organized for easy use. To provide personalized
and secure experiences, the application includes user registration and login
functionalities, supported by MongoDB, which stores user data and scraping
history. Deployed using the Flask framework, the tool offers a scalable, robust
environment for web scraping. Users can easily input website URLs, define data
extraction parameters, and download the data in a simplified format, without
needing technical expertise. This automated tool not only enhances the
efficiency of web scraping but also democratizes access to data extraction by
empowering users of all technical levels to gather and manage data tailored to
their needs. The methodology detailed in this paper represents a significant
advancement in making web scraping tools accessible, efficient, and easy to use
for a broader audience.

</details>


### [16] [Temporal Graph Theoretic Analysis of Geopolitical Dynamics in the U.S. Entity List](https://arxiv.org/abs/2510.21962)
*Yunsen Lei,Kexin Bai,Quan Li,H. Howie Huang*

Main category: cs.IR

TL;DR: 该论文提出了一个新颖的时间图框架，将美国实体清单从静态登记转变为动态的地缘政治战略表示，揭示了出口控制的动态模式。


<details>
  <summary>Details</summary>
Motivation: 美国实体清单已成为最显著的经济治国工具，但其动态机制尚未得到充分探索，需要新的方法来理解其地缘政治战略演变。

Method: 构建了首个基于事件的美国政府外国实体指定数据集，将其建模为时间二分图，并开发了多层次分析方法。

Result: 应用于25年数据，该框架揭示了静态视角无法捕捉的升级、持续性和协调性动态模式。

Conclusion: 时间图分析为出口控制的地缘政治动态提供了系统的计算洞察，展示了其在理解经济治国工具演变中的价值。

Abstract: Export controls have become one of America's most prominent tools of economic
statecraft. They aim to block rival countries' access to sensitive
technologies, safeguard U.S. supply chains, protect national security, and
shape geopolitical competition. Among various instruments, the U.S. Entity List
has emerged as the most salient, yet its dynamics remain underexplored. This
paper introduces a novel temporal graph framework that transforms the Entity
List documents from a static registry of foreign entities of concern into a
dynamic representation of geopolitical strategy. We construct the first
event-based dataset of U.S. government foreign entity designations and model
them as a temporal bipartite graph. Building on this representation, we develop
a multi-level analytical approach that reveals shifting roles, enforcement
strategy, and broader sanction ecosystems. Applied to 25 years of data, the
framework uncovers dynamic patterns of escalation, persistence, and
coordination that static views cannot capture. More broadly, our study
demonstrates how temporal graph analysis offers systematic computational
insights into the geopolitical dynamics of export controls.

</details>


### [17] [Multimodal Item Scoring for Natural Language Recommendation via Gaussian Process Regression with LLM Relevance Judgments](https://arxiv.org/abs/2510.22023)
*Yifan Liu,Qianfeng Wen,Jiazhou Liang,Mark Zhao,Justin Cui,Anton Korikov,Armin Torogh,Junyoung Kim,Scott Sanner*

Main category: cs.IR

TL;DR: 提出GPR-LLM方法，使用高斯过程回归结合LLM相关性判断，改进了自然语言推荐系统中基于密集检索的单模态评分函数问题。


<details>
  <summary>Details</summary>
Motivation: 现有NLRec方法使用密集检索，将用户请求作为唯一相关性标签，导致单模态评分函数仅围绕查询嵌入，这通常是查询相关性的弱代理。需要更好地捕捉复杂NLRec数据中可能出现的多模态相关性评分函数分布。

Method: 使用高斯过程回归(GPR)结合LLM对候选文本子集的相关性判断，特别是采用能够建模多模态相关性评分函数的RBF核。

Result: 在四个NLRec数据集和两个LLM骨干网络上的实验表明，GPR-LLM使用RBF核持续优于简单的单模态核（点积、余弦相似度）以及基线方法（密集检索、交叉编码器和基于LLM的点式相关性评分），性能提升高达65%。

Conclusion: GPR-LLM在最小LLM标注预算内为NLRec提供了一种高效有效的方法。

Abstract: Natural Language Recommendation (NLRec) generates item suggestions based on
the relevance between user-issued NL requests and NL item description passages.
Existing NLRec approaches often use Dense Retrieval (DR) to compute item
relevance scores from aggregation of inner products between user request
embeddings and relevant passage embeddings. However, DR views the request as
the sole relevance label, thus leading to a unimodal scoring function centered
on the query embedding that is often a weak proxy for query relevance. To
better capture the potential multimodal distribution of the relevance scoring
function that may arise from complex NLRec data, we propose GPR-LLM that uses
Gaussian Process Regression (GPR) with LLM relevance judgments for a subset of
candidate passages. Experiments on four NLRec datasets and two LLM backbones
demonstrate that GPR-LLM with an RBF kernel, capable of modeling multimodal
relevance scoring functions, consistently outperforms simpler unimodal kernels
(dot product, cosine similarity), as well as baseline methods including DR,
cross-encoder, and pointwise LLM-based relevance scoring by up to 65%. Overall,
GPR-LLM provides an efficient and effective approach to NLRec within a minimal
LLM labeling budget.

</details>


### [18] [Massive Memorization with Hundreds of Trillions of Parameters for Sequential Transducer Generative Recommenders](https://arxiv.org/abs/2510.22049)
*Zhimin Chen,Chenyu Zhao,Ka Chun Mo,Yunjiang Jiang,Jane H. Lee,Shouwei Chen,Khushhall Chandra Mahajan,Ning Jiang,Kai Ren,Jinhui Li,Wen-Yun Yang*

Main category: cs.IR

TL;DR: VISTA是一个两阶段建模框架，通过将用户历史序列压缩为数百个摘要token，解决了超长用户历史序列在工业推荐系统中的延迟、QPS和GPU成本问题。


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统使用超长用户历史序列（10k到100k项）能提升性能，但带来了延迟、QPS和GPU成本的显著挑战，现有模型未能充分解决这些工业可扩展性问题。

Method: 提出两阶段建模框架：第一阶段将用户历史序列摘要为几百个token；第二阶段候选物品对这些摘要token进行注意力计算。摘要token嵌入可缓存并作为序列特征用于下游训练和推理。

Result: VISTA能够扩展到终身用户历史（高达100万项），同时保持下游训练和推理成本固定，在离线和在线指标上取得显著改进，已在服务数十亿用户的行业领先推荐平台上成功部署。

Conclusion: VISTA通过创新的可扩展性设计，有效解决了工业推荐系统中超长用户历史序列带来的性能与成本平衡问题，实现了对终身用户历史的高效建模。

Abstract: Modern large-scale recommendation systems rely heavily on user interaction
history sequences to enhance the model performance. The advent of large
language models and sequential modeling techniques, particularly
transformer-like architectures, has led to significant advancements recently
(e.g., HSTU, SIM, and TWIN models). While scaling to ultra-long user histories
(10k to 100k items) generally improves model performance, it also creates
significant challenges on latency, queries per second (QPS) and GPU cost in
industry-scale recommendation systems. Existing models do not adequately
address these industrial scalability issues. In this paper, we propose a novel
two-stage modeling framework, namely VIrtual Sequential Target Attention
(VISTA), which decomposes traditional target attention from a candidate item to
user history items into two distinct stages: (1) user history summarization
into a few hundred tokens; followed by (2) candidate item attention to those
tokens. These summarization token embeddings are then cached in storage system
and then utilized as sequence features for downstream model training and
inference. This novel design for scalability enables VISTA to scale to lifelong
user histories (up to one million items) while keeping downstream training and
inference costs fixed, which is essential in industry. Our approach achieves
significant improvements in offline and online metrics and has been
successfully deployed on an industry leading recommendation platform serving
billions of users.

</details>


### [19] [A Benchmark for Open-Domain Numerical Fact-Checking Enhanced by Claim Decomposition](https://arxiv.org/abs/2510.22055)
*V Venktesh,Deepali Prabhu,Avishek Anand*

Main category: cs.IR

TL;DR: 提出了QuanTemp++数据集，包含自然数值声明、开放领域语料库及相关证据，通过模拟人类事实核查员的声明分解过程收集证据，确保无时间泄漏，并分析了不同声明分解范式的检索性能及其对验证流程的影响。


<details>
  <summary>Details</summary>
Motivation: 现有自动事实核查方法未主要关注自然数值声明，且现有基准使用启发式声明分解方法和弱监督网络搜索收集证据，导致证据相关性低、来源嘈杂且存在时间泄漏，缺乏真实检索环境。

Method: 引入QuanTemp++数据集，通过模拟人类事实核查员的声明分解过程收集相关证据，确保无时间泄漏，并分析不同声明分解范式的检索性能。

Result: 构建了包含自然数值声明和相关证据的数据集，展示了不同声明分解方法在检索性能上的差异，并分析了它们对验证流程结果的影响。

Conclusion: QuanTemp++数据集为开发自动化方法提供了更真实的检索环境，有助于提升数值声明事实核查的准确性和可靠性。

Abstract: Fact-checking numerical claims is critical as the presence of numbers provide
mirage of veracity despite being fake potentially causing catastrophic impacts
on society. The prior works in automatic fact verification do not primarily
focus on natural numerical claims. A typical human fact-checker first retrieves
relevant evidence addressing the different numerical aspects of the claim and
then reasons about them to predict the veracity of the claim. Hence, the search
process of a human fact-checker is a crucial skill that forms the foundation of
the verification process. Emulating a real-world setting is essential to aid in
the development of automated methods that encompass such skills. However,
existing benchmarks employ heuristic claim decomposition approaches augmented
with weakly supervised web search to collect evidences for verifying claims.
This sometimes results in less relevant evidences and noisy sources with
temporal leakage rendering a less realistic retrieval setting for claim
verification. Hence, we introduce QuanTemp++: a dataset consisting of natural
numerical claims, an open domain corpus, with the corresponding relevant
evidence for each claim. The evidences are collected through a claim
decomposition process approximately emulating the approach of human
fact-checker and veracity labels ensuring there is no temporal leakage. Given
this dataset, we also characterize the retrieval performance of key claim
decomposition paradigms. Finally, we observe their effect on the outcome of the
verification pipeline and draw insights. The code for data pipeline along with
link to data can be found at https://github.com/VenkteshV/QuanTemp_Plus

</details>


### [20] [Scaling Up Efficient Small Language Models Serving and Deployment for Semantic Job Search](https://arxiv.org/abs/2510.22101)
*Kayhan Behdin,Qingquan Song,Sriram Vasudevan,Jian Sheng,Xiaojing Ma,Z Zhou,Chuanrui Zhu,Guoyao Li,Chanh Nguyen,Sayan Ghosh,Hejian Sang,Ata Fatahi Baarzi,Sundara Raman Ramachandran,Xiaoqing Wang,Qing Lan,Vinay Y S,Qi Guo,Caleb Johnson,Zhipeng Wang,Fedor Borisyuk*

Main category: cs.IR

TL;DR: 提出了在LinkedIn部署小型语言模型(SLM)进行语义搜索的效率和压缩技术，包括模型剪裁减少40%大小和上下文压缩减少10倍输入长度，实现10倍吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)在预测任务中表现优秀但部署成本过高，无法满足行业对延迟和吞吐量的严格要求，需要开发更高效的解决方案。

Method: 使用纯文本解码器的小型语言模型，采用模型压缩技术(如剪裁)和上下文压缩技术，并优化GPU服务基础设施。

Result: 模型大小减少40%，输入上下文长度减少10倍，系统吞吐量提升10倍，同时保持质量要求。

Conclusion: 通过模型压缩、上下文压缩和基础设施优化，可以在保持质量的同时显著提升小型语言模型的部署效率，满足实际应用需求。

Abstract: Large Language Models (LLMs) have demonstrated impressive quality when
applied to predictive tasks such as relevance ranking and semantic search.
However, deployment of such LLMs remains prohibitively expensive for industry
applications with strict latency and throughput requirements. In this work, we
present lessons and efficiency insights from developing a purely text-based
decoder-only Small Language Model (SLM) for a semantic search application at
LinkedIn. Particularly, we discuss model compression techniques such as pruning
that allow us to reduce the model size by up to $40\%$ while maintaining the
accuracy. Additionally, we present context compression techniques that allow us
to reduce the input context length by up to $10$x with minimal loss of
accuracy. Finally, we present practical lessons from optimizing the serving
infrastructure for deploying such a system on GPUs at scale, serving millions
of requests per second. Taken together, this allows us to increase our system's
throughput by $10$x in a real-world deployment, while meeting our quality bar.

</details>


### [21] [Hybrid-Vector Retrieval for Visually Rich Documents: Combining Single-Vector Efficiency and Multi-Vector Accuracy](https://arxiv.org/abs/2510.22215)
*Juyeon Kim,Geon Lee,Dongwon Choi,Taeuk Kim,Kijung Shin*

Main category: cs.IR

TL;DR: HEAVEN是一个两阶段混合向量检索框架，用于视觉丰富文档检索，在保持高准确性的同时大幅提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决视觉丰富文档检索中单向量检索效率高但精度低、多向量检索精度高但计算成本大的权衡问题。

Method: 采用两阶段方法：第一阶段使用单向量方法在视觉汇总页面上进行高效候选检索；第二阶段使用多向量方法重排序，并通过语言重要性过滤查询标记减少冗余计算。

Result: 在四个基准测试中，HEAVEN平均达到多向量模型Recall@1性能的99.87%，同时将每查询计算量减少99.82%。

Conclusion: HEAVEN框架在视觉丰富文档检索中成功实现了效率和准确性的平衡，并引入了ViMDOC基准来评估真实条件下的检索系统。

Abstract: Retrieval over visually rich documents is essential for tasks such as legal
discovery, scientific search, and enterprise knowledge management. Existing
approaches fall into two paradigms: single-vector retrieval, which is efficient
but coarse, and multi-vector retrieval, which is accurate but computationally
expensive. To address this trade-off, we propose HEAVEN, a two-stage
hybrid-vector framework. In the first stage, HEAVEN efficiently retrieves
candidate pages using a single-vector method over Visually-Summarized Pages
(VS-Pages), which assemble representative visual layouts from multiple pages.
In the second stage, it reranks candidates with a multi-vector method while
filtering query tokens by linguistic importance to reduce redundant
computations. To evaluate retrieval systems under realistic conditions, we also
introduce ViMDOC, the first benchmark for visually rich, multi-document, and
long-document retrieval. Across four benchmarks, HEAVEN attains 99.87% of the
Recall@1 performance of multi-vector models on average while reducing per-query
computation by 99.82%, achieving efficiency and accuracy. Our code and datasets
are available at: https://github.com/juyeonnn/HEAVEN

</details>


### [22] [PaperAsk: A Benchmark for Reliability Evaluation of LLMs in Paper Search and Reading](https://arxiv.org/abs/2510.22242)
*Yutao Wu,Xiao Liu,Yunhao Feng,Jiale Ding,Xingjun Ma*

Main category: cs.IR

TL;DR: PaperAsk是一个系统评估LLM在学术任务中可靠性的基准，涵盖引用检索、内容提取、论文发现和声明验证四个关键研究任务。研究发现LLM存在一致的可靠性失败，包括高失败率的引用检索和内容提取，以及低准确率的论文发现。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地作为研究助手使用，其在学术任务中的可靠性尚未得到充分评估。需要系统评估LLM在真实使用条件下的表现。

Method: 通过PaperAsk基准系统评估GPT-4o、GPT-5和Gemini-2.5-Flash在四个研究任务中的表现，使用真实使用条件下的网络接口进行控制实验，并进行人工分析失败原因。

Result: 发现一致的可靠性失败：多引用查询的引用检索失败率48-98%，特定章节内容提取失败率72-91%，主题论文发现的F1分数低于0.32，错过60%以上相关文献。不同LLM有不同失败行为模式。

Conclusion: LLM在学术任务中存在严重可靠性问题，主要由于检索上下文不受控扩展和LLM倾向于优先考虑语义相关文本而非任务指令。开发了轻量级可靠性分类器来识别不可靠输出，PaperAsk为LLM学术助手系统的可靠性评估提供了可复现的诊断框架。

Abstract: Large Language Models (LLMs) increasingly serve as research assistants, yet
their reliability in scholarly tasks remains under-evaluated. In this work, we
introduce PaperAsk, a benchmark that systematically evaluates LLMs across four
key research tasks: citation retrieval, content extraction, paper discovery,
and claim verification. We evaluate GPT-4o, GPT-5, and Gemini-2.5-Flash under
realistic usage conditions-via web interfaces where search operations are
opaque to the user. Through controlled experiments, we find consistent
reliability failures: citation retrieval fails in 48-98% of multi-reference
queries, section-specific content extraction fails in 72-91% of cases, and
topical paper discovery yields F1 scores below 0.32, missing over 60% of
relevant literature. Further human analysis attributes these failures to the
uncontrolled expansion of retrieved context and the tendency of LLMs to
prioritize semantically relevant text over task instructions. Across basic
tasks, the LLMs display distinct failure behaviors: ChatGPT often withholds
responses rather than risk errors, whereas Gemini produces fluent but
fabricated answers. To address these issues, we develop lightweight reliability
classifiers trained on PaperAsk data to identify unreliable outputs. PaperAsk
provides a reproducible and diagnostic framework for advancing the reliability
evaluation of LLM-based scholarly assistance systems.

</details>


### [23] [Tools are under-documented: Simple Document Expansion Boosts Tool Retrieval](https://arxiv.org/abs/2510.22670)
*Xuan Lu,Haohang Huang,Rui Meng,Yaohui Jin,Wenjun Zeng,Xiaoyu Shen*

Main category: cs.IR

TL;DR: 提出了Tool-DE基准和框架，通过结构化字段丰富工具文档以改进工具检索，并开发了Tool-Embed和Tool-Rank两个专用模型，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决工具检索因不完整和异构工具文档而进展缓慢的问题，通过文档扩展提升检索效果。

Method: 设计可扩展的文档扩展流程，利用开源和闭源LLM生成、验证和精炼丰富的工具配置文件，构建大规模语料库，并开发专用的Tool-Embed密集检索器和Tool-Rank重排序器。

Result: 文档扩展显著提高了检索性能，Tool-Embed和Tool-Rank在ToolRet和Tool-DE基准测试中均达到新的最先进结果。

Conclusion: LLM驱动的文档扩展在工具检索中具有前景但仍有局限，Tool-DE及相关模型为未来工具检索研究奠定了基础。

Abstract: Large Language Models (LLMs) have recently demonstrated strong capabilities
in tool use, yet progress in tool retrieval remains hindered by incomplete and
heterogeneous tool documentation. To address this challenge, we introduce
Tool-DE, a new benchmark and framework that systematically enriches tool
documentation with structured fields to enable more effective tool retrieval,
together with two dedicated models, Tool-Embed and Tool-Rank. We design a
scalable document expansion pipeline that leverages both open- and
closed-source LLMs to generate, validate, and refine enriched tool profiles at
low cost, producing large-scale corpora with 50k instances for embedding-based
retrievers and 200k for rerankers. On top of this data, we develop two models
specifically tailored for tool retrieval: Tool-Embed, a dense retriever, and
Tool-Rank, an LLM-based reranker. Extensive experiments on ToolRet and Tool-DE
demonstrate that document expansion substantially improves retrieval
performance, with Tool-Embed and Tool-Rank achieving new state-of-the-art
results on both benchmarks. We further analyze the contribution of individual
fields to retrieval effectiveness, as well as the broader impact of document
expansion on both training and evaluation. Overall, our findings highlight both
the promise and limitations of LLM-driven document expansion, positioning
Tool-DE, along with the proposed Tool-Embed and Tool-Rank, as a foundation for
future research in tool retrieval.

</details>


### [24] [Diversification as Risk Minimization](https://arxiv.org/abs/2510.22681)
*Rikiya Takehi,Fernando Diaz,Tetsuya Sakai*

Main category: cs.IR

TL;DR: 本文提出VRisk框架，将搜索多样性问题重新定义为风险最小化问题，旨在保护少数意图用户免受糟糕体验，并开发了VRisker算法来优化这一目标。


<details>
  <summary>Details</summary>
Motivation: 现有搜索系统在处理模糊查询时，往往偏向热门意图，导致少数意图用户满意度低。传统多样性指标只关注平均相关性，缺乏对最差情况的保障。

Method: 提出VRisk指标来衡量查询中最差意图子集的预期风险，并开发VRisker贪婪重排序算法，该算法具有可证明的近似保证。

Result: 在NTCIR INTENT-2、TREC Web 2012和MovieLens数据集上的实验表明，VRisker将最差意图失败率降低了33%，而平均性能仅下降2%。

Conclusion: 将多样性视为风险最小化问题能有效提升搜索系统的鲁棒性，VRisker算法在保护少数意图用户方面表现出色。

Abstract: Users tend to remember failures of a search session more than its many
successes. This observation has led to work on search robustness, where systems
are penalized if they perform very poorly on some queries. However, this
principle of robustness has been overlooked within a single query. An ambiguous
or underspecified query (e.g., ``jaguar'') can have several user intents, where
popular intents often dominate the ranking, leaving users with minority intents
unsatisfied. Although the diversification literature has long recognized this
issue, existing metrics only model the average relevance across intents and
provide no robustness guarantees. More surprisingly, we show theoretically and
empirically that many well-known diversification algorithms are no more robust
than a naive, non-diversified algorithm. To address this critical gap, we
propose to frame diversification as a risk-minimization problem. We introduce
VRisk, which measures the expected risk faced by the least-served fraction of
intents in a query. Optimizing VRisk produces a robust ranking, reducing the
likelihood of poor user experiences. We then propose VRisker, a fast greedy
re-ranker with provable approximation guarantees. Finally, experiments on NTCIR
INTENT-2, TREC Web 2012, and MovieLens show the vulnerability of existing
methods. VRisker reduces worst-case intent failures by up to 33% with a minimal
2% drop in average performance.

</details>


### [25] [REVISION:Reflective Intent Mining and Online Reasoning Auxiliary for E-commerce Visual Search System Optimization](https://arxiv.org/abs/2510.22739)
*Yiwen Tang,Qiuyu Zhao,Zenghui Sun,Jinsong Lan,Xiaoyong Zhu,Bo Zheng,Kaifu Zhang*

Main category: cs.IR

TL;DR: 提出了REVISION框架来解决淘宝视觉搜索中的用户-系统意图差异问题，通过离线推理挖掘和在线决策执行来优化平台策略，显著降低无点击率。


<details>
  <summary>Details</summary>
Motivation: 淘宝视觉搜索中存在大量无点击请求，表明用户有多样且隐性的意图。这些意图难以挖掘，导致平台策略适应性有限，限制了用户表达多样意图的能力和视觉搜索系统的可扩展性。

Method: REVISION框架：离线阶段构建周期性管道从历史无点击请求中挖掘差异，利用大模型分析隐性意图因素并推理最优建议；在线阶段使用REVISION-R1-3B模型对查询图像和历史产品进行整体分析，生成优化计划并自适应调度搜索管道策略。

Result: 实验结果表明，该方法提高了从大规模搜索日志中挖掘隐性意图的效率，并显著降低了无点击率。

Conclusion: REVISION框架为大模型与传统搜索系统的集成提供了一个简化的范式，实现了信息聚合和用户交互的端到端智能优化。

Abstract: In Taobao e-commerce visual search, user behavior analysis reveals a large
proportion of no-click requests, suggesting diverse and implicit user intents.
These intents are expressed in various forms and are difficult to mine and
discover, thereby leading to the limited adaptability and lag in platform
strategies. This greatly restricts users' ability to express diverse intents
and hinders the scalability of the visual search system. This mismatch between
user implicit intent expression and system response defines the User-SearchSys
Intent Discrepancy. To alleviate the issue, we propose a novel framework
REVISION. This framework integrates offline reasoning mining with online
decision-making and execution, enabling adaptive strategies to solve implicit
user demands. In the offline stage, we construct a periodic pipeline to mine
discrepancies from historical no-click requests. Leveraging large models, we
analyze implicit intent factors and infer optimal suggestions by jointly
reasoning over query and product metadata. These inferred suggestions serve as
actionable insights for refining platform strategies. In the online stage,
REVISION-R1-3B, trained on the curated offline data, performs holistic analysis
over query images and associated historical products to generate optimization
plans and adaptively schedule strategies across the search pipeline. Our
framework offers a streamlined paradigm for integrating large models with
traditional search systems, enabling end-to-end intelligent optimization across
information aggregation and user interaction. Experimental results demonstrate
that our approach improves the efficiency of implicit intent mining from
large-scale search logs and significantly reduces the no-click rate.

</details>


### [26] [Civic Ground Truth in News Recommenders: A Method for Public Value Scoring](https://arxiv.org/abs/2510.22865)
*James Meese,Kyle Herbertson*

Main category: cs.IR

TL;DR: 提出一种通过大规模结构化受众评估将公民价值观嵌入新闻推荐系统的方法，使用公民基础真值方法生成基于价值观的标签。


<details>
  <summary>Details</summary>
Motivation: 探索将规范性目标（如编辑目标和公共服务价值）整合到现有新闻推荐系统中的最佳方式，为更具公民意识的推荐系统奠定基础。

Method: 通过全国代表性调查和自动元数据丰富，使用公民基础真值方法生成可推广到更广泛新闻语料库的基于价值观的标签。

Result: 提出了一种将公民价值观嵌入新闻推荐系统的具体方法框架。

Conclusion: 该方法为构建更具公民意识的新闻推荐系统提供了可行的技术路径，能够更好地整合公共服务价值。

Abstract: Research in news recommendation systems (NRS) continues to explore the best
ways to integrate normative goals such as editorial objectives and public
service values into existing systems. Prior efforts have incorporated expert
input or audience feedback to quantify these values, laying the groundwork for
more civic-minded recommender systems. This paper contributes to that
trajectory, introducing a method for embedding civic values into NRS through
large-scale, structured audience evaluations. The proposed civic ground truth
approach aims to generate value-based labels through a nationally
representative survey that are generalisable across a wider news corpus, using
automated metadata enrichment.

</details>


### [27] [MGFRec: Towards Reinforced Reasoning Recommendation with Multiple Groundings and Feedback](https://arxiv.org/abs/2510.22888)
*Shihao Cai,Chongming Gao,Haoyan Liu,Wentao Shi,Jianshan Sun,Ruiming Tang,Fuli Feng*

Main category: cs.IR

TL;DR: 提出了一种在推理过程中进行多轮接地的方法，将大语言模型的推理与实际物品空间对齐，并通过用户代理提供反馈来更好地理解用户兴趣。


<details>
  <summary>Details</summary>
Motivation: 现有基于推理的推荐方法仅在语言空间进行推理，没有结合实际物品空间，导致过度解释用户兴趣并偏离真实物品。

Method: 在推理过程中执行多轮接地，帮助LLM更好地理解实际物品空间；引入用户代理在每次接地步骤中提供反馈。

Result: 在三个Amazon评论数据集上的综合实验证明了多轮接地和反馈机制的有效性。

Conclusion: 对于推荐任务，在实际物品空间中进行推理而非局限于语言空间至关重要。

Abstract: The powerful reasoning and generative capabilities of large language models
(LLMs) have inspired researchers to apply them to reasoning-based
recommendation tasks, which require in-depth reasoning about user interests and
the generation of recommended items. However, previous reasoning-based
recommendation methods have typically performed inference within the language
space alone, without incorporating the actual item space. This has led to
over-interpreting user interests and deviating from real items. Towards this
research gap, we propose performing multiple rounds of grounding during
inference to help the LLM better understand the actual item space, which could
ensure that its reasoning remains aligned with real items. Furthermore, we
introduce a user agent that provides feedback during each grounding step,
enabling the LLM to better recognize and adapt to user interests. Comprehensive
experiments conducted on three Amazon review datasets demonstrate the
effectiveness of incorporating multiple groundings and feedback. These findings
underscore the critical importance of reasoning within the actual item space,
rather than being confined to the language space, for recommendation tasks.

</details>


### [28] [Improving Product Search Relevance with EAR-MP: A Solution for the CIKM 2025 AnalytiCup](https://arxiv.org/abs/2510.23018)
*JaeEun Lim,Soomin Kim,Jaeyong Seo,Iori Ono,Qimu Ran,Jae-woong Lee*

Main category: cs.IR

TL;DR: 本文提出了一个针对多语言电商搜索的解决方案，通过数据翻译和清洗、DeBERTa模型优化以及任务特定改进，在有限的算力下取得了竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: 解决多语言电商搜索中的语言多样性和用户生成查询噪声问题，构建稳健且资源高效的多语言相关性系统。

Method: 将多语言数据集翻译为英语进行归一化，通过数据清洗和归一化降低噪声，基于DeBERTa-v3-large模型，采用标签平滑、自蒸馏和dropout技术，并为QC任务引入分层token注入，为QI任务引入混合评分机制。

Result: 在CIKM 2025 AnalytiCup中，QC任务F1得分为0.8796，QI任务F1得分为0.8744。

Conclusion: 系统的数据预处理和定制化的训练策略对于构建稳健、资源高效的多语言相关性系统至关重要。

Abstract: Multilingual e-commerce search is challenging due to linguistic diversity and
the noise inherent in user-generated queries. This paper documents the solution
employed by our team (EAR-MP) for the CIKM 2025 AnalytiCup, which addresses two
core tasks: Query-Category (QC) relevance and Query-Item (QI) relevance. Our
approach first normalizes the multilingual dataset by translating all text into
English, then mitigates noise through extensive data cleaning and
normalization. For model training, we build on DeBERTa-v3-large and improve
performance with label smoothing, self-distillation, and dropout. In addition,
we introduce task-specific upgrades, including hierarchical token injection for
QC and a hybrid scoring mechanism for QI. Under constrained compute, our method
achieves competitive results, attaining an F1 score of 0.8796 on QC and 0.8744
on QI. These findings underscore the importance of systematic data
preprocessing and tailored training strategies for building robust,
resource-efficient multilingual relevance systems.

</details>


### [29] [Multi-Stage Field Extraction of Financial Documents with OCR and Compact Vision-Language Models](https://arxiv.org/abs/2510.23066)
*Yichao Jin,Yushuo Wang,Qishuai Zhong,Kent Chiu Jin-Chun,Kenneth Zhu Ke,Donald MacDonald*

Main category: cs.IR

TL;DR: 提出一个多阶段流水线方法，结合传统图像处理、OCR和紧凑型视觉语言模型，用于从扫描的SMB财务文档中提取结构化信息，相比直接使用大型VLM在准确性和效率上有显著提升。


<details>
  <summary>Details</summary>
Motivation: SMB财务文档通常为扫描图像，存在分辨率低、倾斜、噪声背景等问题，且内容异构（包含叙述、表格、图表和多语言内容），直接使用端到端大型视觉语言模型处理成本高、对噪声敏感且速度慢。

Method: 采用多阶段流水线：1) 图像预处理（分割、方向检测、尺寸归一化）；2) 多语言OCR提取页面文本；3) 基于文本分析检索相关页面；4) 在限定范围内使用紧凑型VLM提取结构化财务指标。

Result: 相比直接将整个文档输入大型VLM，该方法在字段级准确率上提高了8.8倍，GPU成本仅为0.7%，端到端服务延迟降低了92.6%。

Conclusion: 紧凑型VLM结合多阶段流水线的方法能够有效处理大规模扫描财务文档，在保持高准确性的同时显著降低计算成本和延迟。

Abstract: Financial documents are essential sources of information for regulators,
auditors, and financial institutions, particularly for assessing the wealth and
compliance of Small and Medium-sized Businesses. However, SMB documents are
often difficult to parse. They are rarely born digital and instead are
distributed as scanned images that are none machine readable. The scans
themselves are low in resolution, affected by skew or rotation, and often
contain noisy backgrounds. These documents also tend to be heterogeneous,
mixing narratives, tables, figures, and multilingual content within the same
report. Such characteristics pose major challenges for automated information
extraction, especially when relying on end to end large Vision Language Models,
which are computationally expensive, sensitive to noise, and slow when applied
to files with hundreds of pages.
  We propose a multistage pipeline that leverages traditional image processing
models and OCR extraction, together with compact VLMs for structured field
extraction of large-scale financial documents. Our approach begins with image
pre-processing, including segmentation, orientation detection, and size
normalization. Multilingual OCR is then applied to recover page-level text.
Upon analyzing the text information, pages are retrieved for coherent sections.
Finally, compact VLMs are operated within these narrowed-down scopes to extract
structured financial indicators.
  Our approach is evaluated using an internal corpus of multi-lingual, scanned
financial documents. The results demonstrate that compact VLMs, together with a
multistage pipeline, achieves 8.8 times higher field level accuracy relative to
directly feeding the whole document into large VLMs, only at 0.7 percent of the
GPU cost and 92.6 percent less end-to-end service latency.

</details>


### [30] [Think before Recommendation: Autonomous Reasoning-enhanced Recommender](https://arxiv.org/abs/2510.23077)
*Xiaoyu Kong,Junguang Jiang,Bin Liu,Ziru Xu,Han Zhu,Jian Xu,Bo Zheng,Jiancan Wu,Xiang Wang*

Main category: cs.IR

TL;DR: RecZero：基于强化学习的推荐范式，通过纯RL训练单一LLM实现自主推理能力，替代传统蒸馏方法，在评分预测任务上显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于蒸馏的方法存在教师模型推荐能力不足、监督成本高且静态、推理能力转移肤浅等问题，需要新的方法来解决这些限制。

Method: 提出RecZero范式：1）"推荐前思考"提示构建，使用结构化推理模板指导模型逐步分析用户兴趣、物品特征和用户-物品兼容性；2）基于规则的奖励建模，采用GRPO计算推理轨迹奖励并优化LLM。还探索了结合监督微调和RL的混合范式RecOne。

Result: 实验结果表明，RecZero和RecOne在多个基准数据集上显著优于现有基线方法。

Conclusion: RL范式在实现自主推理增强的推荐系统方面具有优越性，能够有效解决传统蒸馏方法的局限性。

Abstract: The core task of recommender systems is to learn user preferences from
historical user-item interactions. With the rapid development of large language
models (LLMs), recent research has explored leveraging the reasoning
capabilities of LLMs to enhance rating prediction tasks. However, existing
distillation-based methods suffer from limitations such as the teacher model's
insufficient recommendation capability, costly and static supervision, and
superficial transfer of reasoning ability. To address these issues, this paper
proposes RecZero, a reinforcement learning (RL)-based recommendation paradigm
that abandons the traditional multi-model and multi-stage distillation
approach. Instead, RecZero trains a single LLM through pure RL to autonomously
develop reasoning capabilities for rating prediction. RecZero consists of two
key components: (1) "Think-before-Recommendation" prompt construction, which
employs a structured reasoning template to guide the model in step-wise
analysis of user interests, item features, and user-item compatibility; and (2)
rule-based reward modeling, which adopts group relative policy optimization
(GRPO) to compute rewards for reasoning trajectories and optimize the LLM.
Additionally, the paper explores a hybrid paradigm, RecOne, which combines
supervised fine-tuning with RL, initializing the model with cold-start
reasoning samples and further optimizing it with RL. Experimental results
demonstrate that RecZero and RecOne significantly outperform existing baseline
methods on multiple benchmark datasets, validating the superiority of the RL
paradigm in achieving autonomous reasoning-enhanced recommender systems.

</details>
