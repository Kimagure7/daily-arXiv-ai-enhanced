<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 12]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [LLaTTE: Scaling Laws for Multi-Stage Sequence Modeling in Large-Scale Ads Recommendation](https://arxiv.org/abs/2601.20083)
*Lee Xiong,Zhirong Chen,Rahul Mayuranath,Shangran Qiu,Arda Ozdemir,Lu Li,Yang Hu,Dave Li,Jingtao Ren,Howard Cheng,Fabian Souto Herrera,Ahmed Agiza,Baruch Epshtein,Anuj Aggarwal,Julia Ulziisaikhan,Chao Wang,Dinesh Ramasamy,Parshva Doshi,Sri Reddy,Arnold Overwijk*

Main category: cs.IR

TL;DR: LLaTTE是一个用于广告推荐的可扩展Transformer架构，通过两阶段设计在严格延迟约束下实现持续扩展，在Meta部署后带来4.3%的转化提升


<details>
  <summary>Details</summary>
Motivation: 推荐系统中的序列建模遵循类似LLM的可预测幂律缩放规律，但需要语义特征作为扩展前提，如何在严格延迟约束下实现持续扩展是工业推荐系统的关键挑战

Method: 提出LLaTTE架构：1）利用语义特征作为扩展前提；2）采用两阶段设计，将大型长上下文模型的繁重计算卸载到异步上游用户模型；3）上游改进可预测地传递到下游排序任务

Result: 在Meta部署为最大的用户模型，多阶段框架在Facebook Feed和Reels上带来4.3%的转化提升，且服务开销最小，验证了工业推荐系统中缩放定律的实用性

Conclusion: LLaTTE为工业推荐系统利用缩放定律提供了实用蓝图，证明语义特征是扩展的关键前提，两阶段架构能在延迟约束下实现持续扩展，上游改进可预测地提升下游性能

Abstract: We present LLaTTE (LLM-Style Latent Transformers for Temporal Events), a scalable transformer architecture for production ads recommendation. Through systematic experiments, we demonstrate that sequence modeling in recommendation systems follows predictable power-law scaling similar to LLMs. Crucially, we find that semantic features bend the scaling curve: they are a prerequisite for scaling, enabling the model to effectively utilize the capacity of deeper and longer architectures. To realize the benefits of continued scaling under strict latency constraints, we introduce a two-stage architecture that offloads the heavy computation of large, long-context models to an asynchronous upstream user model. We demonstrate that upstream improvements transfer predictably to downstream ranking tasks. Deployed as the largest user model at Meta, this multi-stage framework drives a 4.3\% conversion uplift on Facebook Feed and Reels with minimal serving overhead, establishing a practical blueprint for harnessing scaling laws in industrial recommender systems.

</details>


### [2] [IMRNNs: An Efficient Method for Interpretable Dense Retrieval via Embedding Modulation](https://arxiv.org/abs/2601.20084)
*Yash Saxena,Ankur Padia,Kalpa Gunaratna,Manas Gaur*

Main category: cs.IR

TL;DR: IMRNNs是一个轻量级框架，通过动态双向调制增强稠密检索器的可解释性，同时提升检索效果


<details>
  <summary>Details</summary>
Motivation: 现有稠密检索器使用静态嵌入，掩盖了查询和文档之间的双向语义关系，而事后重排序方法计算成本高且无法揭示底层语义对齐

Method: 提出IMRNNs框架，使用两个独立适配器：一个基于当前查询调整文档嵌入，另一个利用初始检索文档的语料级反馈优化查询嵌入，实现迭代调制

Result: 在7个基准数据集上，IMRNNs相比最先进基线平均提升6.35% nDCG、7.14%召回率和7.04% MRR，同时增强可解释性

Conclusion: 可解释性驱动的调制既能解释又能增强RAG系统中的检索效果，为黑盒稠密检索器提供了轻量级可解释解决方案

Abstract: Interpretability in black-box dense retrievers remains a central challenge in Retrieval-Augmented Generation (RAG). Understanding how queries and documents semantically interact is critical for diagnosing retrieval behavior and improving model design. However, existing dense retrievers rely on static embeddings for both queries and documents, which obscures this bidirectional relationship. Post-hoc approaches such as re-rankers are computationally expensive, add inference latency, and still fail to reveal the underlying semantic alignment. To address these limitations, we propose Interpretable Modular Retrieval Neural Networks (IMRNNs), a lightweight framework that augments any dense retriever with dynamic, bidirectional modulation at inference time. IMRNNs employ two independent adapters: one conditions document embeddings on the current query, while the other refines the query embedding using corpus-level feedback from initially retrieved documents. This iterative modulation process enables the model to adapt representations dynamically and expose interpretable semantic dependencies between queries and documents. Empirically, IMRNNs not only enhance interpretability but also improve retrieval effectiveness. Across seven benchmark datasets, applying our method to standard dense retrievers yields average gains of +6.35% nDCG, +7.14% recall, and +7.04% MRR over state-of-the-art baselines. These results demonstrate that incorporating interpretability-driven modulation can both explain and enhance retrieval in RAG systems.

</details>


### [3] [Taxonomy of the Retrieval System Framework: Pitfalls and Paradigms](https://arxiv.org/abs/2601.20131)
*Deep Shah,Sanket Badhe,Nehal Kathrotia*

Main category: cs.IR

TL;DR: 论文提出了一个四层框架（表示层、粒度层、编排层、鲁棒性层）来系统化设计嵌入检索系统，帮助实践者在效率与效果之间做出平衡决策。


<details>
  <summary>Details</summary>
Motivation: 设计嵌入检索系统需要在效率和效果之间做出复杂的权衡决策，当前缺乏系统化的设计框架来指导这些决策。

Method: 通过垂直遍历系统设计栈的四层结构：表示层（损失函数和架构选择）、粒度层（文档分割策略）、编排层（超越单向量的检索方法）、鲁棒性层（应对领域泛化等问题的架构缓解措施）。

Result: 提出了一个全面的分类框架，将检索系统的设计选择系统化，帮助实践者优化神经搜索系统的效率-效果边界。

Conclusion: 通过四层框架结构化嵌入检索系统的设计决策，为实践者提供了在效率和效果之间进行优化的系统化方法。

Abstract: Designing an embedding retrieval system requires navigating a complex design space of conflicting trade-offs between efficiency and effectiveness. This work structures these decisions as a vertical traversal of the system design stack. We begin with the Representation Layer by examining how loss functions and architectures, specifically Bi-encoders and Cross-encoders, define semantic relevance and geometric projection. Next, we analyze the Granularity Layer and evaluate how segmentation strategies like Atomic and Hierarchical chunking mitigate information bottlenecks in long-context documents. Moving to the Orchestration Layer, we discuss methods that transcend the single-vector paradigm, including hierarchical retrieval, agentic decomposition, and multi-stage reranking pipelines to resolve capacity limitations. Finally, we address the Robustness Layer by identifying architectural mitigations for domain generalization failures, lexical blind spots, and the silent degradation of retrieval quality due to temporal drift. By categorizing these limitations and design choices, we provide a comprehensive framework for practitioners to optimize the efficiency-effectiveness frontier in modern neural search systems.

</details>


### [4] [MERGE: Next-Generation Item Indexing Paradigm for Large-Scale Streaming Recommendation](https://arxiv.org/abs/2601.20199)
*Jing Yan,Yimeng Bai,Zongyu Liu,Yahui Liu,Junwei Wang,Jingze Huang,Haoda Li,Sihao Ding,Shaohui Ruan,Yang Zhang*

Main category: cs.IR

TL;DR: MERGE是一种新一代项目索引范式，通过自适应构建集群、动态监控集群占用和细到粗的合并形成分层索引结构，解决了传统VQ方法在流式推荐中项目分布高度偏斜和非平稳的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于向量量化(VQ)的项目索引方法在处理流式行业推荐系统中常见的高度偏斜和非平稳项目分布时存在困难，导致分配准确性差、集群占用不平衡和集群分离不足的问题。

Method: MERGE采用自适应从零构建集群、动态监控集群占用、通过细到粗合并形成分层索引结构的范式，而不是传统的VQ方法。

Result: 大量实验表明MERGE在分配准确性、集群均匀性和集群分离方面显著优于现有索引方法，在线A/B测试显示关键业务指标有实质性提升。

Conclusion: MERGE有潜力成为大规模推荐系统的基础索引方法，解决了传统VQ方法在流式推荐环境中的局限性。

Abstract: Item indexing, which maps a large corpus of items into compact discrete representations, is critical for both discriminative and generative recommender systems, yet existing Vector Quantization (VQ)-based approaches struggle with the highly skewed and non-stationary item distributions common in streaming industry recommenders, leading to poor assignment accuracy, imbalanced cluster occupancy, and insufficient cluster separation. To address these challenges, we propose MERGE, a next-generation item indexing paradigm that adaptively constructs clusters from scratch, dynamically monitors cluster occupancy, and forms hierarchical index structures via fine-to-coarse merging. Extensive experiments demonstrate that MERGE significantly improves assignment accuracy, cluster uniformity, and cluster separation compared with existing indexing methods, while online A/B tests show substantial gains in key business metrics, highlighting its potential as a foundational indexing approach for large-scale recommendation.

</details>


### [5] [Towards End-to-End Alignment of User Satisfaction via Questionnaire in Video Recommendation](https://arxiv.org/abs/2601.20215)
*Na Li,Jiaqi Yu,Minzhi Xie,Tiantian He,Xiaoxiao Xu,Zixiu Wang,Lantao Hu,Yongqi Liu,Han Li,Kaiqiao Zhan,Kun Gai*

Main category: cs.IR

TL;DR: 提出EASQ框架，通过问卷反馈实现推荐模型与用户满意度的端到端实时对齐，解决行为信号噪声与问卷信号稀疏的问题。


<details>
  <summary>Details</summary>
Motivation: 短视频推荐系统通常使用点击、观看时长等密集行为信号优化排序模型，但这些信号只是用户满意度的间接代理，存在噪声和偏差。问卷收集的显式满意度反馈是高质量的直接对齐监督，但极其稀疏，容易被丰富的行为数据淹没，难以融入在线推荐模型。

Method: 提出EASQ框架：1) 通过多任务架构和轻量级LoRA模块为稀疏问卷信号构建独立参数路径，多任务设计分离稀疏满意度监督与密集行为信号，LoRA模块以参数隔离方式预注入偏好；2) 采用基于DPO的在线学习优化目标，使主模型输出与稀疏满意度信号实时对齐，实现端到端在线学习。

Result: 大量离线实验和大规模在线A/B测试表明，EASQ在多个场景下持续提升用户满意度指标。已在生产短视频推荐系统中成功部署，带来显著且稳定的业务收益。

Conclusion: EASQ框架有效解决了行为信号噪声与问卷信号稀疏的挑战，实现了推荐模型与用户满意度的实时端到端对齐，在实际生产系统中验证了其有效性。

Abstract: Short-video recommender systems typically optimize ranking models using dense user behavioral signals, such as clicks and watch time. However, these signals are only indirect proxies of user satisfaction and often suffer from noise and bias. Recently, explicit satisfaction feedback collected through questionnaires has emerged as a high-quality direct alignment supervision, but is extremely sparse and easily overwhelmed by abundant behavioral data, making it difficult to incorporate into online recommendation models. To address these challenges, we propose a novel framework which is towards End-to-End Alignment of user Satisfaction via Questionaire, named EASQ, to enable real-time alignment of ranking models with true user satisfaction. Specifically, we first construct an independent parameter pathway for sparse questionnaire signals by combining a multi-task architecture and a lightweight LoRA module. The multi-task design separates sparse satisfaction supervision from dense behavioral signals, preventing the former from being overwhelmed. The LoRA module pre-inject these preferences in a parameter-isolated manner, ensuring stability in the backbone while optimizing user satisfaction. Furthermore, we employ a DPO-based optimization objective tailored for online learning, which aligns the main model outputs with sparse satisfaction signals in real time. This design enables end-to-end online learning, allowing the model to continuously adapt to new questionnaire feedback while maintaining the stability and effectiveness of the backbone. Extensive offline experiments and large-scale online A/B tests demonstrate that EASQ consistently improves user satisfaction metrics across multiple scenarios. EASQ has been successfully deployed in a production short-video recommendation system, delivering significant and stable business gains.

</details>


### [6] [MALLOC: Benchmarking the Memory-aware Long Sequence Compression for Large Sequential Recommendation](https://arxiv.org/abs/2601.20234)
*Qihang Yu,Kairui Fu,Zhaocheng Du,Yuxuan Si,Kaiyuan Li,Weihao Zhao,Zhicheng Zhang,Jieming Zhu,Quanyu Dai,Zhenhua Dong,Shengyu Zhang,Kun Kuang,Fei Wu*

Main category: cs.IR

TL;DR: MALLOC是一个用于内存感知长序列压缩的基准测试框架，旨在解决大规模推荐系统中因用户长序列行为存储带来的巨大内存开销问题。


<details>
  <summary>Details</summary>
Motivation: 随着推荐模型规模的扩大，用户长序列行为依赖导致计算成本急剧增加。现有方法通过预存用户历史状态来减少重复计算，但忽视了由此带来的巨大内存开销，这在拥有数十亿用户、每个用户数千次交互的真实推荐系统中尤为关键。

Method: 提出了MALLOC基准测试框架，对适用于大型序列推荐的内存管理技术进行全面调查和系统分类，将这些技术集成到最先进的推荐器中，构建可复现、易访问的评估平台。

Result: 通过在准确性、效率和复杂性方面的广泛实验，证明了MALLOC在推进大规模推荐系统发展方面的整体可靠性。

Conclusion: MALLOC填补了推荐系统中内存管理技术评估的空白，为大规模推荐系统的内存优化提供了系统性的评估框架和解决方案。

Abstract: The scaling law, which indicates that model performance improves with increasing dataset and model capacity, has fueled a growing trend in expanding recommendation models in both industry and academia. However, the advent of large-scale recommenders also brings significantly higher computational costs, particularly under the long-sequence dependencies inherent in the user intent of recommendation systems. Current approaches often rely on pre-storing the intermediate states of the past behavior for each user, thereby reducing the quadratic re-computation cost for the following requests. Despite their effectiveness, these methods often treat memory merely as a medium for acceleration, without adequately considering the space overhead it introduces. This presents a critical challenge in real-world recommendation systems with billions of users, each of whom might initiate thousands of interactions and require massive memory for state storage. Fortunately, there have been several memory management strategies examined for compression in LLM, while most have not been evaluated on the recommendation task. To mitigate this gap, we introduce MALLOC, a comprehensive benchmark for memory-aware long sequence compression. MALLOC presents a comprehensive investigation and systematic classification of memory management techniques applicable to large sequential recommendations. These techniques are integrated into state-of-the-art recommenders, enabling a reproducible and accessible evaluation platform. Through extensive experiments across accuracy, efficiency, and complexity, we demonstrate the holistic reliability of MALLOC in advancing large-scale recommendation. Code is available at https://anonymous.4open.science/r/MALLOC.

</details>


### [7] [One Word is Enough: Minimal Adversarial Perturbations for Neural Text Ranking](https://arxiv.org/abs/2601.20283)
*Tanmay Karmakar,Sourav Saha,Debapriyo Majumdar,Surjyanee Halder*

Main category: cs.IR

TL;DR: 该论文提出了一种针对神经排序模型的单词语义攻击方法，通过插入或替换单个与查询语义对齐的词（查询中心词），就能显著提升目标文档的排名，平均修改少于2个token即可达到91%的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 神经排序模型虽然检索效果好，但先前研究表明它们容易受到对抗性扰动攻击。本文重新审视这一鲁棒性问题，旨在开发一种最小化的查询感知攻击方法，揭示神经排序模型的脆弱性，为未来防御提供动机。

Method: 提出了一种最小化的查询感知攻击方法，通过插入或替换单个语义对齐的词（查询中心词）来提升目标文档排名。研究了启发式和梯度引导的变体，包括一种白盒方法用于识别有影响力的插入点。在TREC-DL 2019/2020数据集上使用BERT和monoT5重排序器进行实验。

Result: 单词语义攻击在平均修改少于2个token的情况下达到91%的成功率，在与PRADA可比的白盒设置下，以更少的编辑次数实现了竞争性的排名和分数提升。分析发现存在一个"金发姑娘区域"：中等排名的文档最易受攻击。

Conclusion: 该研究揭示了神经排序模型的实际风险，单词语义攻击的高效性表明现有模型存在严重脆弱性。研究结果强调了开发鲁棒神经排序防御机制的必要性，并为未来防御研究提供了诊断指标和分析框架。

Abstract: Neural ranking models (NRMs) achieve strong retrieval effectiveness, yet prior work has shown they are vulnerable to adversarial perturbations. We revisit this robustness question with a minimal, query-aware attack that promotes a target document by inserting or substituting a single, semantically aligned word - the query center. We study heuristic and gradient-guided variants, including a white-box method that identifies influential insertion points. On TREC-DL 2019/2020 with BERT and monoT5 re-rankers, our single-word attacks achieve up to 91% success while modifying fewer than two tokens per document on average, achieving competitive rank and score boosts with far fewer edits under a comparable white-box setup to ensure fair evaluation against PRADA. We also introduce new diagnostic metrics to analyze attack sensitivity beyond aggregate success rates. Our analysis reveals a Goldilocks zone in which mid-ranked documents are most vulnerable. These findings demonstrate practical risks and motivate future defenses for robust neural ranking.

</details>


### [8] [Less is More: Benchmarking LLM Based Recommendation Agents](https://arxiv.org/abs/2601.20316)
*Kargi Chauhan,Mahalakshmi Venkateswarlu*

Main category: cs.IR

TL;DR: LLM推荐系统中，更长的用户购买历史并不一定带来更好的推荐质量，使用5-10个物品的上下文即可达到与50个物品相同的效果，同时能减少88%的推理成本。


<details>
  <summary>Details</summary>
Motivation: 挑战当前LLM推荐系统中"上下文越长越好"的普遍假设，验证更长的用户购买历史是否真的能带来更好的推荐质量。

Method: 使用四个最先进的LLM（GPT-4o-mini、DeepSeek-V3、Qwen2.5-72B、Gemini 2.5 Flash）在REGEN数据集上进行系统基准测试，上下文长度从5到50个物品，采用50个用户的被试内设计。

Result: 令人惊讶的是，增加上下文长度并没有带来显著的质量提升，质量分数在所有条件下保持平稳（0.17-0.23）。使用5-10个物品的上下文可以节省约88%的推理成本，而不牺牲推荐质量。

Conclusion: 挑战了"更多上下文更好"的现有范式，为基于LLM的推荐系统提供了实用的成本效益指南，建议使用较短的上下文来大幅降低推理成本。

Abstract: Large Language Models (LLMs) are increasingly deployed for personalized product recommendations, with practitioners commonly assuming that longer user purchase histories lead to better predictions. We challenge this assumption through a systematic benchmark of four state of the art LLMs GPT-4o-mini, DeepSeek-V3, Qwen2.5-72B, and Gemini 2.5 Flash across context lengths ranging from 5 to 50 items using the REGEN dataset.
  Surprisingly, our experiments with 50 users in a within subject design reveal no significant quality improvement with increased context length. Quality scores remain flat across all conditions (0.17--0.23). Our findings have significant practical implications: practitioners can reduce inference costs by approximately 88\% by using context (5--10 items) instead of longer histories (50 items), without sacrificing recommendation quality. We also analyze latency patterns across providers and find model specific behaviors that inform deployment decisions. This work challenges the existing ``more context is better'' paradigm and provides actionable guidelines for cost effective LLM based recommendation systems.

</details>


### [9] [Eliminating Hallucination in Diffusion-Augmented Interactive Text-to-Image Retrieval](https://arxiv.org/abs/2601.20391)
*Zhuocheng Zhang,Kangheng Liang,Guanxuan Li,Paul Henderson,Richard Mccreadie,Zijun Long*

Main category: cs.IR

TL;DR: DAI-TIR通过扩散模型生成查询图像作为用户意图的额外"视图"，但扩散生成可能引入幻觉视觉线索，降低检索性能。DMCL框架通过语义一致性和扩散感知对比学习，将幻觉线索映射到空空间，提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 扩散增强的交互式文本到图像检索(DAI-TIR)通过扩散模型生成查询图像作为用户意图的额外视图，但扩散生成可能引入与原始查询文本冲突的幻觉视觉线索，这些幻觉线索会显著降低检索性能。

Method: 提出扩散感知多视图对比学习(DMCL)框架，将DAI-TIR建模为查询意图和目标图像表示的联合优化。引入语义一致性和扩散感知对比目标，对齐文本和扩散生成的查询视图，同时抑制幻觉查询信号。编码器作为语义过滤器，将幻觉线索映射到空空间。

Result: 在五个标准基准测试中，DMCL在多轮Hits@10指标上实现了一致改进，最高达到7.37%的提升，超越了之前的微调和零样本基线。注意力可视化和几何嵌入空间分析证实了过滤行为。

Conclusion: DMCL是一个通用且鲁棒的DAI-TIR训练框架，能够有效处理扩散生成中的幻觉问题，提高检索性能，更好地表示用户意图。

Abstract: Diffusion-Augmented Interactive Text-to-Image Retrieval (DAI-TIR) is a promising paradigm that improves retrieval performance by generating query images via diffusion models and using them as additional ``views'' of the user's intent. However, these generative views can be incorrect because diffusion generation may introduce hallucinated visual cues that conflict with the original query text. Indeed, we empirically demonstrate that these hallucinated cues can substantially degrade DAI-TIR performance. To address this, we propose Diffusion-aware Multi-view Contrastive Learning (DMCL), a hallucination-robust training framework that casts DAI-TIR as joint optimization over representations of query intent and the target image. DMCL introduces semantic-consistency and diffusion-aware contrastive objectives to align textual and diffusion-generated query views while suppressing hallucinated query signals. This yields an encoder that acts as a semantic filter, effectively mapping hallucinated cues into a null space, improving robustness to spurious cues and better representing the user's intent. Attention visualization and geometric embedding-space analyses corroborate this filtering behavior. Across five standard benchmarks, DMCL delivers consistent improvements in multi-round Hits@10, reaching as high as 7.37\% over prior fine-tuned and zero-shot baselines, which indicates it is a general and robust training framework for DAI-TIR.

</details>


### [10] [When Vision Meets Texts in Listwise Reranking](https://arxiv.org/abs/2601.20623)
*Hongyi Cai*

Main category: cs.IR

TL;DR: Rank-Nexus：一个轻量级多模态图像-文本文档重排序器，通过渐进式跨模态训练策略解决模态鸿沟问题，在文本和图像重排序基准上取得优异性能


<details>
  <summary>Details</summary>
Motivation: 当前信息检索中视觉和文本信息整合存在挑战：模态鸿沟、对齐数据稀缺；现有方法依赖大型模型（7B-32B参数）和基于推理的蒸馏，计算开销大且主要关注文本模态

Method: 提出Rank-Nexus多模态图像-文本文档重排序器，采用列表式定性重排序；引入渐进式跨模态训练策略：先分别训练文本分支（利用丰富文本重排序数据蒸馏）和图像分支（从MLLM标注构建蒸馏对），然后蒸馏联合图像-文本重排序数据集

Result: 在文本重排序基准（TREC、BEIR）和挑战性图像重排序基准（INQUIRE、MMDocIR）上取得优异性能，仅使用轻量级2B预训练视觉语言模型

Conclusion: Rank-Nexus的高效设计确保了在多样化多模态场景中的强泛化能力，无需过多参数或推理开销，为多模态重排序提供了轻量级解决方案

Abstract: Recent advancements in information retrieval have highlighted the potential of integrating visual and textual information, yet effective reranking for image-text documents remains challenging due to the modality gap and scarcity of aligned datasets. Meanwhile, existing approaches often rely on large models (7B to 32B parameters) with reasoning-based distillation, incurring unnecessary computational overhead while primarily focusing on textual modalities. In this paper, we propose Rank-Nexus, a multimodal image-text document reranker that performs listwise qualitative reranking on retrieved lists incorporating both images and texts. To bridge the modality gap, we introduce a progressive cross-modal training strategy. We first train modalities separately: leveraging abundant text reranking data, we distill knowledge into the text branch. For images, where data is scarce, we construct distilled pairs from multimodal large language model (MLLM) captions on image retrieval benchmarks. Subsequently, we distill a joint image-text reranking dataset. Rank-Nexus achieves outstanding performance on text reranking benchmarks (TREC, BEIR) and the challenging image reranking benchmark (INQUIRE, MMDocIR), using only a lightweight 2B pretrained visual-language model. This efficient design ensures strong generalization across diverse multimodal scenarios without excessive parameters or reasoning overhead.

</details>


### [11] [Overview of the TREC 2025 Tip-of-the-Tongue track](https://arxiv.org/abs/2601.20671)
*Jaime Arguello,Fernando Diaz,Maik Fröebe,To Eun Kim,Bhaskar Mitra*

Main category: cs.IR

TL;DR: TREC 2025 ToT 赛道扩展至通用领域，整合了来自MS-ToT数据集、人工主题开发和LLM合成查询生成的多源测试查询，共有9个团队提交了32次运行结果。


<details>
  <summary>Details</summary>
Motivation: 舌尖现象（ToT）已知项检索涉及重新找到用户无法可靠回忆标识符的项目。ToT信息请求通常冗长且包含多种复杂现象，这对现有信息检索系统提出了特别挑战。

Method: 将TREC ToT赛道从特定领域扩展到通用领域，整合了三种不同来源的测试查询：MS-ToT数据集、人工主题开发和基于LLM的合成查询生成。

Result: 共有9个团队（包括赛道协调者）提交了32次运行结果，展示了不同方法在ToT检索任务上的表现。

Conclusion: 通过整合多源测试查询和扩展至通用领域，TREC 2025 ToT赛道为评估和改进ToT检索系统提供了更全面和多样化的基准。

Abstract: Tip-of-the-tongue (ToT) known-item retrieval involves re-finding an item for which the searcher does not reliably recall an identifier. ToT information requests (or queries) are verbose and tend to include several complex phenomena, making them especially difficult for existing information retrieval systems. The TREC 2025 ToT track focused on a single ad-hoc retrieval task. This year, we extended the track to general domain and incorporated different sets of test queries from diverse sources, namely from the MS-ToT dataset, manual topic development, and LLM-based synthetic query generation. This year, 9 groups (including the track coordinators) submitted 32 runs.

</details>


### [12] [MedViz: An Agent-based, Visual-guided Research Assistant for Navigating Biomedical Literature](https://arxiv.org/abs/2601.20709)
*Huan He,Xueqing Peng,Yutong Xie,Qijia Liu,Chia-Hsuan Chang,Lingfei Qian,Brian Ondov,Qiaozhu Mei,Hua Xu*

Main category: cs.IR

TL;DR: MedViz是一个结合AI代理与交互式可视化的生物医学文献分析系统，通过语义地图和智能代理功能支持大规模文献探索与知识发现。


<details>
  <summary>Details</summary>
Motivation: 生物医学研究者面临数百万跨领域文献的导航挑战，传统搜索引擎仅提供排名文本列表，缺乏全局探索和深度分析支持。虽然生成式AI和大型语言模型在摘要、提取和问答任务中表现出潜力，但其对话式实现与文献搜索工作流程集成不足。

Method: MedViz整合多个AI代理与交互式可视化，结合数百万文章的语义地图，提供查询、摘要和假设生成等代理驱动功能，支持研究者迭代式精炼问题、识别趋势和发现隐藏联系。

Result: MedViz将生物医学文献搜索转变为动态的探索性过程，通过桥接智能代理与交互式可视化，加速知识发现。

Conclusion: MedViz系统通过整合AI代理与可视化技术，解决了传统文献搜索的局限性，为生物医学研究者提供了更有效的知识探索工具。

Abstract: Biomedical researchers face increasing challenges in navigating millions of publications in diverse domains. Traditional search engines typically return articles as ranked text lists, offering little support for global exploration or in-depth analysis. Although recent advances in generative AI and large language models have shown promise in tasks such as summarization, extraction, and question answering, their dialog-based implementations are poorly integrated with literature search workflows. To address this gap, we introduce MedViz, a visual analytics system that integrates multiple AI agents with interactive visualization to support the exploration of the large-scale biomedical literature. MedViz combines a semantic map of millions of articles with agent-driven functions for querying, summarizing, and hypothesis generation, allowing researchers to iteratively refine questions, identify trends, and uncover hidden connections. By bridging intelligent agents with interactive visualization, MedViz transforms biomedical literature search into a dynamic, exploratory process that accelerates knowledge discovery.

</details>
