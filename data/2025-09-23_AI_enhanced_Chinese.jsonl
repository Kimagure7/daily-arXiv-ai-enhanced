{"id": "2509.16369", "categories": ["cs.IR", "cs.AI", "cs.CE", "H.4; H.5; H.3.3"], "pdf": "https://arxiv.org/pdf/2509.16369", "abs": "https://arxiv.org/abs/2509.16369", "authors": ["Akshay Govind Srinivasan", "Ryan Jacob George", "Jayden Koshy Joe", "Hrushikesh Kant", "Harshith M R", "Sachin Sundar", "Sudharshan Suresh", "Rahul Vimalkanth", "Vijayavallabh"], "title": "Enhancing Financial RAG with Agentic AI and Multi-HyDE: A Novel Approach to Knowledge Retrieval and Hallucination Reduction", "comment": "14 Pages, 8 Tables, 2 Figures. Accepted and to be published in the\n  proceedings of FinNLP, Empirical Methods in Natural Language Processing 2025", "summary": "Accurate and reliable knowledge retrieval is vital for financial\nquestion-answering, where continually updated data sources and complex,\nhigh-stakes contexts demand precision. Traditional retrieval systems rely on a\nsingle database and retriever, but financial applications require more\nsophisticated approaches to handle intricate regulatory filings, market\nanalyses, and extensive multi-year reports. We introduce a framework for\nfinancial Retrieval Augmented Generation (RAG) that leverages agentic AI and\nthe Multi-HyDE system, an approach that generates multiple, nonequivalent\nqueries to boost the effectiveness and coverage of retrieval from large,\nstructured financial corpora. Our pipeline is optimized for token efficiency\nand multi-step financial reasoning, and we demonstrate that their combination\nimproves accuracy by 11.2% and reduces hallucinations by 15%. Our method is\nevaluated on standard financial QA benchmarks, showing that integrating\ndomain-specific retrieval mechanisms such as Multi-HyDE with robust toolsets,\nincluding keyword and table-based retrieval, significantly enhances both the\naccuracy and reliability of answers. This research not only delivers a modular,\nadaptable retrieval framework for finance but also highlights the importance of\nstructured agent workflows and multi-perspective retrieval for trustworthy\ndeployment of AI in high-stakes financial applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u91d1\u878dRAG\u6846\u67b6\uff0c\u7ed3\u5408\u667a\u80fd\u4f53AI\u548cMulti-HyDE\u7cfb\u7edf\uff0c\u901a\u8fc7\u751f\u6210\u591a\u4e2a\u975e\u7b49\u4ef7\u67e5\u8be2\u63d0\u5347\u91d1\u878d\u77e5\u8bc6\u68c0\u7d22\u6548\u679c\uff0c\u5728\u6807\u51c6\u91d1\u878dQA\u57fa\u51c6\u4e0a\u51c6\u786e\u7387\u63d0\u534711.2%\uff0c\u5e7b\u89c9\u51cf\u5c1115%\u3002", "motivation": "\u91d1\u878d\u95ee\u7b54\u9700\u8981\u7cbe\u786e\u53ef\u9760\u7684\u77e5\u8bc6\u68c0\u7d22\uff0c\u4f20\u7edf\u5355\u4e00\u6570\u636e\u5e93\u548c\u68c0\u7d22\u5668\u65e0\u6cd5\u5904\u7406\u590d\u6742\u7684\u91d1\u878d\u76d1\u7ba1\u6587\u4ef6\u3001\u5e02\u573a\u5206\u6790\u548c\u591a\u5e74\u62a5\u544a\u7b49\u7ed3\u6784\u5316\u91d1\u878d\u8bed\u6599\u5e93\u3002", "method": "\u91c7\u7528\u667a\u80fd\u4f53AI\u548cMulti-HyDE\u7cfb\u7edf\uff0c\u751f\u6210\u591a\u4e2a\u975e\u7b49\u4ef7\u67e5\u8be2\u6765\u589e\u5f3a\u68c0\u7d22\u8986\u76d6\u9762\u548c\u6548\u679c\uff0c\u4f18\u5316token\u6548\u7387\u548c\u591a\u6b65\u9aa4\u91d1\u878d\u63a8\u7406\u6d41\u7a0b\uff0c\u7ed3\u5408\u5173\u952e\u8bcd\u548c\u57fa\u4e8e\u8868\u683c\u7684\u68c0\u7d22\u5de5\u5177\u3002", "result": "\u5728\u6807\u51c6\u91d1\u878dQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u51c6\u786e\u7387\u63d0\u9ad811.2%\uff0c\u5e7b\u89c9\u51cf\u5c1115%\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7b54\u6848\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u53ef\u9002\u5e94\u7684\u91d1\u878d\u68c0\u7d22\u6846\u67b6\uff0c\u8fd8\u5f3a\u8c03\u4e86\u7ed3\u6784\u5316\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u548c\u591a\u89c6\u89d2\u68c0\u7d22\u5bf9\u4e8e\u5728\u9ad8\u98ce\u9669\u91d1\u878d\u5e94\u7528\u4e2d\u53ef\u4fe1\u90e8\u7f72AI\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.16411", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.16411", "abs": "https://arxiv.org/abs/2509.16411", "authors": ["Chong You", "Rajesh Jayaram", "Ananda Theertha Suresh", "Robin Nittka", "Felix Yu", "Sanjiv Kumar"], "title": "Hierarchical Retrieval: The Geometry and a Pretrain-Finetune Recipe", "comment": "NeurIPS 2025", "summary": "Dual encoder (DE) models, where a pair of matching query and document are\nembedded into similar vector representations, are widely used in information\nretrieval due to their simplicity and scalability. However, the Euclidean\ngeometry of the embedding space limits the expressive power of DEs, which may\ncompromise their quality. This paper investigates such limitations in the\ncontext of hierarchical retrieval (HR), where the document set has a\nhierarchical structure and the matching documents for a query are all of its\nancestors. We first prove that DEs are feasible for HR as long as the embedding\ndimension is linear in the depth of the hierarchy and logarithmic in the number\nof documents. Then we study the problem of learning such embeddings in a\nstandard retrieval setup where DEs are trained on samples of matching query and\ndocument pairs. Our experiments reveal a lost-in-the-long-distance phenomenon,\nwhere retrieval accuracy degrades for documents further away in the hierarchy.\nTo address this, we introduce a pretrain-finetune recipe that significantly\nimproves long-distance retrieval without sacrificing performance on closer\ndocuments. We experiment on a realistic hierarchy from WordNet for retrieving\ndocuments at various levels of abstraction, and show that pretrain-finetune\nboosts the recall on long-distance pairs from 19% to 76%. Finally, we\ndemonstrate that our method improves retrieval of relevant products on a\nshopping queries dataset.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u53cc\u7f16\u7801\u5668\u6a21\u578b\u5728\u5c42\u6b21\u68c0\u7d22\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u9884\u8bad\u7ec3-\u5fae\u8c03\u65b9\u6cd5\u89e3\u51b3\u957f\u8ddd\u79bb\u68c0\u7d22\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u68c0\u7d22\u6548\u679c\u3002", "motivation": "\u53cc\u7f16\u7801\u5668\u6a21\u578b\u5728\u4fe1\u606f\u68c0\u7d22\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u6b27\u51e0\u91cc\u5f97\u51e0\u4f55\u5d4c\u5165\u7a7a\u95f4\u9650\u5236\u4e86\u8868\u8fbe\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5c42\u6b21\u68c0\u7d22\u573a\u666f\u4e2d\uff0c\u5339\u914d\u6587\u6863\u662f\u67e5\u8be2\u7684\u6240\u6709\u7956\u5148\u8282\u70b9\u65f6\uff0c\u5b58\u5728\u957f\u8ddd\u79bb\u68c0\u7d22\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u9996\u5148\u8bc1\u660e\u53cc\u7f16\u7801\u5668\u5728\u5c42\u6b21\u68c0\u7d22\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u7136\u540e\u63d0\u51fa\u9884\u8bad\u7ec3-\u5fae\u8c03\u65b9\u6cd5\uff1a\u5148\u9884\u8bad\u7ec3\u6a21\u578b\u7406\u89e3\u5c42\u6b21\u7ed3\u6784\uff0c\u518d\u5fae\u8c03\u4f18\u5316\u68c0\u7d22\u6027\u80fd\uff0c\u7279\u522b\u9488\u5bf9\u957f\u8ddd\u79bb\u6587\u6863\u5bf9\u3002", "result": "\u5728WordNet\u5c42\u6b21\u7ed3\u6784\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u9884\u8bad\u7ec3-\u5fae\u8c03\u65b9\u6cd5\u5c06\u957f\u8ddd\u79bb\u6587\u6863\u5bf9\u7684\u53ec\u56de\u7387\u4ece19%\u63d0\u5347\u523076%\uff0c\u540c\u65f6\u5728\u8d2d\u7269\u67e5\u8be2\u6570\u636e\u96c6\u4e0a\u4e5f\u6539\u5584\u4e86\u76f8\u5173\u4ea7\u54c1\u68c0\u7d22\u6548\u679c\u3002", "conclusion": "\u9884\u8bad\u7ec3-\u5fae\u8c03\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u53cc\u7f16\u7801\u5668\u5728\u5c42\u6b21\u68c0\u7d22\u4e2d\u7684\u957f\u8ddd\u79bb\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u8d28\u91cf\uff0c\u4e3a\u5c42\u6b21\u7ed3\u6784\u6570\u636e\u68c0\u7d22\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.16442", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16442", "abs": "https://arxiv.org/abs/2509.16442", "authors": ["Pranjal A. Chitale", "Bishal Santra", "Yashoteja Prabhu", "Amit Sharma"], "title": "Evaluating the Effectiveness and Scalability of LLM-Based Data Augmentation for Retrieval", "comment": "EMNLP 2025 (MAIN Conference)", "summary": "Compact dual-encoder models are widely used for retrieval owing to their\nefficiency and scalability. However, such models often underperform compared to\ntheir Large Language Model (LLM)-based retrieval counterparts, likely due to\ntheir limited world knowledge. While LLM-based data augmentation has been\nproposed as a strategy to bridge this performance gap, there is insufficient\nunderstanding of its effectiveness and scalability to real-world retrieval\nproblems. Existing research does not systematically explore key factors such as\nthe optimal augmentation scale, the necessity of using large augmentation\nmodels, and whether diverse augmentations improve generalization, particularly\nin out-of-distribution (OOD) settings. This work presents a comprehensive study\nof the effectiveness of LLM augmentation for retrieval, comprising over 100\ndistinct experimental settings of retrieval models, augmentation models and\naugmentation strategies. We find that, while augmentation enhances retrieval\nperformance, its benefits diminish beyond a certain augmentation scale, even\nwith diverse augmentation strategies. Surprisingly, we observe that\naugmentation with smaller LLMs can achieve performance competitive with larger\naugmentation models. Moreover, we examine how augmentation effectiveness varies\nwith retrieval model pre-training, revealing that augmentation provides the\nmost benefit to models which are not well pre-trained. Our insights pave the\nway for more judicious and efficient augmentation strategies, thus enabling\ninformed decisions and maximizing retrieval performance while being more\ncost-effective. Code and augmented datasets accompanying this work are publicly\navailable at https://aka.ms/DAGR.", "AI": {"tldr": "\u672c\u7814\u7a76\u5bf9LLM\u6570\u636e\u589e\u5f3a\u5728\u68c0\u7d22\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u5206\u6790\uff0c\u53d1\u73b0\u589e\u5f3a\u6548\u679c\u5b58\u5728\u6536\u76ca\u9012\u51cf\u73b0\u8c61\uff0c\u5c0f\u6a21\u578b\u589e\u5f3a\u53ef\u8fbe\u5230\u4e0e\u5927\u6a21\u578b\u76f8\u5f53\u7684\u6548\u679c\uff0c\u4e14\u589e\u5f3a\u5bf9\u9884\u8bad\u7ec3\u4e0d\u8db3\u7684\u6a21\u578b\u5e2e\u52a9\u6700\u5927\u3002", "motivation": "\u89e3\u51b3\u7d27\u51d1\u53cc\u7f16\u7801\u5668\u6a21\u578b\u56e0\u77e5\u8bc6\u6709\u9650\u800c\u6027\u80fd\u4e0d\u5982LLM\u68c0\u7d22\u6a21\u578b\u7684\u95ee\u9898\uff0c\u63a2\u7d22LLM\u6570\u636e\u589e\u5f3a\u5728\u5b9e\u9645\u68c0\u7d22\u95ee\u9898\u4e2d\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u901a\u8fc7\u8d85\u8fc7100\u79cd\u4e0d\u540c\u7684\u5b9e\u9a8c\u8bbe\u7f6e\uff0c\u7cfb\u7edf\u7814\u7a76\u68c0\u7d22\u6a21\u578b\u3001\u589e\u5f3a\u6a21\u578b\u548c\u589e\u5f3a\u7b56\u7565\u7b49\u5173\u952e\u56e0\u7d20\uff0c\u5305\u62ec\u589e\u5f3a\u89c4\u6a21\u3001\u6a21\u578b\u5927\u5c0f\u548c\u591a\u6837\u6027\u7b49\u7ef4\u5ea6\u3002", "result": "\u6570\u636e\u589e\u5f3a\u80fd\u63d0\u5347\u68c0\u7d22\u6027\u80fd\u4f46\u5b58\u5728\u6536\u76ca\u9012\u51cf\uff1b\u5c0fLLM\u589e\u5f3a\u6548\u679c\u53ef\u4e0e\u5927\u6a21\u578b\u7ade\u4e89\uff1b\u589e\u5f3a\u5bf9\u9884\u8bad\u7ec3\u4e0d\u8db3\u7684\u6a21\u578b\u5e2e\u52a9\u6700\u5927\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5236\u5b9a\u66f4\u660e\u667a\u548c\u9ad8\u6548\u7684\u589e\u5f3a\u7b56\u7565\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u53ef\u5728\u4fdd\u8bc1\u68c0\u7d22\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u6210\u672c\u6548\u76ca\u6700\u5927\u5316\u3002"}}
{"id": "2509.16446", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16446", "abs": "https://arxiv.org/abs/2509.16446", "authors": ["Ruohan Zhang", "Jiacheng Li", "Julian McAuley", "Yupeng Hou"], "title": "Purely Semantic Indexing for LLM-based Generative Recommendation and Retrieval", "comment": null, "summary": "Semantic identifiers (IDs) have proven effective in adapting large language\nmodels for generative recommendation and retrieval. However, existing methods\noften suffer from semantic ID conflicts, where semantically similar documents\n(or items) are assigned identical IDs. A common strategy to avoid conflicts is\nto append a non-semantic token to distinguish them, which introduces randomness\nand expands the search space, therefore hurting performance. In this paper, we\npropose purely semantic indexing to generate unique, semantic-preserving IDs\nwithout appending non-semantic tokens. We enable unique ID assignment by\nrelaxing the strict nearest-centroid selection and introduce two model-agnostic\nalgorithms: exhaustive candidate matching (ECM) and recursive residual\nsearching (RRS). Extensive experiments on sequential recommendation, product\nsearch, and document retrieval tasks demonstrate that our methods improve both\noverall and cold-start performance, highlighting the effectiveness of ensuring\nID uniqueness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7eaf\u8bed\u4e49\u7d22\u5f15\u65b9\u6cd5\uff0c\u901a\u8fc7\u653e\u677e\u4e25\u683c\u7684\u6700\u8fd1\u8d28\u5fc3\u9009\u62e9\u6765\u751f\u6210\u552f\u4e00\u4e14\u4fdd\u6301\u8bed\u4e49\u7684ID\uff0c\u907f\u514d\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u8bed\u4e49ID\u51b2\u7a81\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u8bed\u4e49ID\u65b9\u6cd5\u5b58\u5728\u8bed\u4e49\u51b2\u7a81\u95ee\u9898\uff0c\u5373\u8bed\u4e49\u76f8\u4f3c\u7684\u6587\u6863\u88ab\u5206\u914d\u76f8\u540c\u7684ID\u3002\u5e38\u89c1\u7684\u907f\u514d\u51b2\u7a81\u7b56\u7565\u662f\u6dfb\u52a0\u975e\u8bed\u4e49\u6807\u8bb0\uff0c\u4f46\u8fd9\u4f1a\u5f15\u5165\u968f\u673a\u6027\u5e76\u6269\u5927\u641c\u7d22\u7a7a\u95f4\uff0c\u4ece\u800c\u635f\u5bb3\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u6a21\u578b\u65e0\u5173\u7684\u7b97\u6cd5\uff1a\u7a77\u4e3e\u5019\u9009\u5339\u914d\uff08ECM\uff09\u548c\u9012\u5f52\u6b8b\u5dee\u641c\u7d22\uff08RRS\uff09\uff0c\u901a\u8fc7\u653e\u677e\u4e25\u683c\u7684\u6700\u8fd1\u8d28\u5fc3\u9009\u62e9\u6765\u5b9e\u73b0\u552f\u4e00ID\u5206\u914d\u3002", "result": "\u5728\u987a\u5e8f\u63a8\u8350\u3001\u4ea7\u54c1\u641c\u7d22\u548c\u6587\u6863\u68c0\u7d22\u4efb\u52a1\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6574\u4f53\u6027\u80fd\u548c\u51b7\u542f\u52a8\u6027\u80fd\u3002", "conclusion": "\u786e\u4fddID\u552f\u4e00\u6027\u7684\u6709\u6548\u6027\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u7eaf\u8bed\u4e49\u7d22\u5f15\u65b9\u6cd5\u5728\u907f\u514d\u8bed\u4e49\u51b2\u7a81\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u8bed\u4e49\u4fe1\u606f\u3002"}}
{"id": "2509.16539", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16539", "abs": "https://arxiv.org/abs/2509.16539", "authors": ["Pushpa Devi", "Ayush Agrawal", "Ashutosh Dubey", "C. Ravindranath Chowdary"], "title": "Long document summarization using page specific target text alignment and distilling page importance", "comment": "8 pages, 2 figures", "summary": "The rapid growth of textual data across news, legal, medical, and scientific\ndomains is becoming a challenge for efficiently accessing and understanding\nlarge volumes of content. It is increasingly complex for users to consume and\nextract meaningful information efficiently. Thus, raising the need for\nsummarization. Unlike short document summarization, long document abstractive\nsummarization is resource-intensive, and very little literature is present in\nthis direction. BART is a widely used efficient sequence-to-sequence\n(seq-to-seq) model. However, when it comes to summarizing long documents, the\nlength of the context window limits its capabilities. We proposed a model\ncalled PTS (Page-specific Target-text alignment Summarization) that extends the\nseq-to-seq method for abstractive summarization by dividing the source document\ninto several pages. PTS aligns each page with the relevant part of the target\nsummary for better supervision. Partial summaries are generated for each page\nof the document. We proposed another model called PTSPI (Page-specific\nTarget-text alignment Summarization with Page Importance), an extension to PTS\nwhere an additional layer is placed before merging the partial summaries into\nthe final summary. This layer provides dynamic page weightage and explicit\nsupervision to focus on the most informative pages. We performed experiments on\nthe benchmark dataset and found that PTSPI outperformed the SOTA by 6.32\\% in\nROUGE-1 and 8.08\\% in ROUGE-2 scores.", "AI": {"tldr": "\u63d0\u51fa\u4e86PTS\u548cPTSPI\u6a21\u578b\u6765\u89e3\u51b3\u957f\u6587\u6863\u6458\u8981\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u9875\u5bf9\u9f50\u548c\u52a8\u6001\u6743\u91cd\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u6458\u8981\u8d28\u91cf\uff0c\u5728ROUGE\u6307\u6807\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u6587\u672c\u6570\u636e\u5728\u65b0\u95fb\u3001\u6cd5\u5f8b\u3001\u533b\u7597\u7b49\u9886\u57df\u7684\u5feb\u901f\u589e\u957f\uff0c\u7528\u6237\u9700\u8981\u9ad8\u6548\u8bbf\u95ee\u548c\u7406\u89e3\u5927\u91cf\u5185\u5bb9\u3002\u957f\u6587\u6863\u6458\u8981\u76f8\u6bd4\u77ed\u6587\u6863\u6458\u8981\u8d44\u6e90\u6d88\u8017\u5927\u4e14\u7814\u7a76\u8f83\u5c11\uff0c\u73b0\u6709BART\u6a21\u578b\u53d7\u9650\u4e8e\u4e0a\u4e0b\u6587\u7a97\u53e3\u957f\u5ea6\u3002", "method": "PTS\u6a21\u578b\u5c06\u6e90\u6587\u6863\u5206\u9875\uff0c\u6bcf\u9875\u4e0e\u76ee\u6807\u6458\u8981\u7684\u76f8\u5173\u90e8\u5206\u5bf9\u9f50\u751f\u6210\u90e8\u5206\u6458\u8981\u3002PTSPI\u5728PTS\u57fa\u7840\u4e0a\u589e\u52a0\u9875\u9762\u91cd\u8981\u6027\u5c42\uff0c\u63d0\u4f9b\u52a8\u6001\u6743\u91cd\u548c\u663e\u5f0f\u76d1\u7763\uff0c\u91cd\u70b9\u5173\u6ce8\u4fe1\u606f\u91cf\u6700\u5927\u7684\u9875\u9762\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cPTSPI\u5728ROUGE-1\u548cROUGE-2\u5206\u6570\u4e0a\u5206\u522b\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u63d0\u5347\u4e866.32%\u548c8.08%\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u9875\u5bf9\u9f50\u65b9\u6cd5\u548c\u52a8\u6001\u6743\u91cd\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u957f\u6587\u6863\u6458\u8981\u7684\u6311\u6218\uff0cPTSPI\u6a21\u578b\u5728\u6458\u8981\u8d28\u91cf\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2509.16621", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16621", "abs": "https://arxiv.org/abs/2509.16621", "authors": ["Hiun Kim", "Tae Kwan Lee", "Taeryun Won"], "title": "The Role of Vocabularies in Learning Sparse Representations for Ranking", "comment": null, "summary": "Learned Sparse Retrieval (LSR) such as SPLADE has growing interest for\neffective semantic 1st stage matching while enjoying the efficiency of inverted\nindices. A recent work on learning SPLADE models with expanded vocabularies\n(ESPLADE) was proposed to represent queries and documents into a sparse space\nof custom vocabulary which have different levels of vocabularic granularity.\nWithin this effort, however, there have not been many studies on the role of\nvocabulary in SPLADE models and their relationship to retrieval efficiency and\neffectiveness.\n  To study this, we construct BERT models with 100K-sized output vocabularies,\none initialized with the ESPLADE pretraining method and one initialized\nrandomly. After finetune on real-world search click logs, we applied logit\nscore-based queries and documents pruning to max size for further balancing\nefficiency. The experimental result in our evaluation set shows that, when\npruning is applied, the two models are effective compared to the 32K-sized\nnormal SPLADE model in the computational budget under the BM25. And the ESPLADE\nmodels are more effective than the random vocab model, while having a similar\nretrieval cost.\n  The result indicates that the size and pretrained weight of output\nvocabularies play the role of configuring the representational specification\nfor queries, documents, and their interactions in the retrieval engine, beyond\ntheir original meaning and purposes in NLP. These findings can provide a new\nroom for improvement for LSR by identifying the importance of representational\nspecification from vocabulary configuration for efficient and effective\nretrieval.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86SPLADE\u6a21\u578b\u4e2d\u8bcd\u6c47\u8868\u5927\u5c0f\u548c\u9884\u8bad\u7ec3\u6743\u91cd\u5bf9\u68c0\u7d22\u6548\u7387\u548c\u6548\u679c\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u6784\u5efa100K\u8bcd\u6c47\u8868\u7684BERT\u6a21\u578b\u5e76\u5e94\u7528\u526a\u679d\u6280\u672f\uff0c\u53d1\u73b0\u6269\u5927\u8bcd\u6c47\u8868\u53ef\u5728\u4fdd\u6301\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u68c0\u7d22\u6548\u679c\u3002", "motivation": "\u76ee\u524d\u5bf9SPLADE\u6a21\u578b\u4e2d\u8bcd\u6c47\u8868\u4f5c\u7528\u53ca\u5176\u4e0e\u68c0\u7d22\u6548\u7387\u6548\u679c\u5173\u7cfb\u7684\u7814\u7a76\u8f83\u5c11\uff0c\u9700\u8981\u6df1\u5165\u63a2\u8ba8\u8bcd\u6c47\u8868\u914d\u7f6e\u5bf9\u68c0\u7d22\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u6784\u5efa\u4e86\u4e24\u4e2a100K\u8bcd\u6c47\u8868\u7684BERT\u6a21\u578b\uff08\u4e00\u4e2a\u4f7f\u7528ESPLADE\u9884\u8bad\u7ec3\uff0c\u4e00\u4e2a\u968f\u673a\u521d\u59cb\u5316\uff09\uff0c\u5728\u771f\u5b9e\u641c\u7d22\u70b9\u51fb\u65e5\u5fd7\u4e0a\u5fae\u8c03\uff0c\u5e76\u5e94\u7528\u57fa\u4e8elogit\u5206\u6570\u7684\u67e5\u8be2\u548c\u6587\u6863\u526a\u679d\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u5e94\u7528\u526a\u679d\u540e\uff0c\u4e24\u4e2a100K\u8bcd\u6c47\u8868\u6a21\u578b\u5728\u8ba1\u7b97\u9884\u7b97\u4e0b\u6bd432K\u8bcd\u6c47\u8868\u7684\u666e\u901aSPLADE\u6a21\u578b\u66f4\u6709\u6548\uff0c\u4e14ESPLADE\u6a21\u578b\u6bd4\u968f\u673a\u8bcd\u6c47\u8868\u6a21\u578b\u6548\u679c\u66f4\u597d\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u4f3c\u7684\u68c0\u7d22\u6210\u672c\u3002", "conclusion": "\u8bcd\u6c47\u8868\u5927\u5c0f\u548c\u9884\u8bad\u7ec3\u6743\u91cd\u5728\u68c0\u7d22\u5f15\u64ce\u4e2d\u914d\u7f6e\u4e86\u67e5\u8be2\u3001\u6587\u6863\u53ca\u5176\u4ea4\u4e92\u7684\u8868\u5f81\u89c4\u8303\uff0c\u8d85\u8d8a\u4e86\u5176\u5728NLP\u4e2d\u7684\u539f\u59cb\u610f\u4e49\uff0c\u4e3aLSR\u7684\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2509.16780", "categories": ["cs.IR", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.16780", "abs": "https://arxiv.org/abs/2509.16780", "authors": ["Eason Chen", "Chuangji Li", "Shizhuo Li", "Conrad Borchers", "Zimo Xiao", "Chloe Qianhui Zhao", "Jionghao Lin", "Kenneth R. Koedinger"], "title": "Comparing RAG and GraphRAG for Page-Level Retrieval Question Answering on Math Textbook", "comment": null, "summary": "Technology-enhanced learning environments often help students retrieve\nrelevant learning content for questions arising during self-paced study. Large\nlanguage models (LLMs) have emerged as novel aids for information retrieval\nduring learning. While LLMs are effective for general-purpose\nquestion-answering, they typically lack alignment with the domain knowledge of\nspecific course materials such as textbooks and slides. We investigate\nRetrieval-Augmented Generation (RAG) and GraphRAG, a knowledge graph-enhanced\nRAG approach, for page-level question answering in an undergraduate mathematics\ntextbook. While RAG has been effective for retrieving discrete, contextually\nrelevant passages, GraphRAG may excel in modeling interconnected concepts and\nhierarchical knowledge structures. We curate a dataset of 477 question-answer\npairs, each tied to a distinct textbook page. We then compare the standard\nembedding-based RAG methods to GraphRAG for evaluating both retrieval\naccuracy-whether the correct page is retrieved-and generated answer quality via\nF1 scores. Our findings show that embedding-based RAG achieves higher retrieval\naccuracy and better F1 scores compared to GraphRAG, which tends to retrieve\nexcessive and sometimes irrelevant content due to its entity-based structure.\nWe also explored re-ranking the retrieved pages with LLM and observed mixed\nresults, including performance drop and hallucinations when dealing with larger\ncontext windows. Overall, this study highlights both the promises and\nchallenges of page-level retrieval systems in educational contexts, emphasizing\nthe need for more refined retrieval methods to build reliable AI tutoring\nsolutions in providing reference page numbers.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u57fa\u4e8e\u5d4c\u5165\u7684RAG\u548cGraphRAG\u5728\u6570\u5b66\u6559\u79d1\u4e66\u9875\u9762\u7ea7\u95ee\u7b54\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6807\u51c6RAG\u5728\u68c0\u7d22\u51c6\u786e\u6027\u548c\u7b54\u6848\u8d28\u91cf\u65b9\u9762\u4f18\u4e8eGraphRAG\uff0c\u540e\u8005\u7531\u4e8e\u57fa\u4e8e\u5b9e\u4f53\u7684\u7ed3\u6784\u5bb9\u6613\u68c0\u7d22\u8fc7\u591a\u65e0\u5173\u5185\u5bb9\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u80fd\u6709\u6548\u56de\u7b54\u4e00\u822c\u6027\u95ee\u9898\uff0c\u4f46\u7f3a\u4e4f\u4e0e\u7279\u5b9a\u8bfe\u7a0b\u6750\u6599\uff08\u5982\u6559\u79d1\u4e66\u548c\u5e7b\u706f\u7247\uff09\u7684\u9886\u57df\u77e5\u8bc6\u5bf9\u9f50\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u6539\u8fdb\u68c0\u7d22\u65b9\u6cd5\uff0c\u4e3a\u6559\u80b2\u73af\u5883\u6784\u5efa\u53ef\u9760\u7684AI\u8f85\u5bfc\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528477\u4e2a\u95ee\u9898-\u7b54\u6848\u5bf9\u7684\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u95ee\u9898\u5bf9\u5e94\u6559\u79d1\u4e66\u7684\u7279\u5b9a\u9875\u9762\u3002\u6bd4\u8f83\u57fa\u4e8e\u5d4c\u5165\u7684RAG\u65b9\u6cd5\u548cGraphRAG\u65b9\u6cd5\uff0c\u8bc4\u4f30\u68c0\u7d22\u51c6\u786e\u6027\u548c\u751f\u6210\u7b54\u6848\u8d28\u91cf\uff08\u4f7f\u7528F1\u5206\u6570\uff09\u3002\u8fd8\u63a2\u7d22\u4e86\u7528LLM\u91cd\u65b0\u6392\u5e8f\u68c0\u7d22\u9875\u9762\u7684\u65b9\u6cd5\u3002", "result": "\u57fa\u4e8e\u5d4c\u5165\u7684RAG\u5728\u68c0\u7d22\u51c6\u786e\u6027\u548cF1\u5206\u6570\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002GraphRAG\u7531\u4e8e\u57fa\u4e8e\u5b9e\u4f53\u7684\u7ed3\u6784\u5bb9\u6613\u68c0\u7d22\u8fc7\u591a\u65e0\u5173\u5185\u5bb9\u3002\u4f7f\u7528LLM\u91cd\u65b0\u6392\u5e8f\u68c0\u7d22\u9875\u9762\u65f6\u6548\u679c\u4e0d\u4e00\uff0c\u5728\u5904\u7406\u8f83\u5927\u4e0a\u4e0b\u6587\u7a97\u53e3\u65f6\u4f1a\u51fa\u73b0\u6027\u80fd\u4e0b\u964d\u548c\u5e7b\u89c9\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u9875\u9762\u7ea7\u68c0\u7d22\u7cfb\u7edf\u5728\u6559\u80b2\u73af\u5883\u4e2d\u7684\u524d\u666f\u548c\u6311\u6218\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u68c0\u7d22\u65b9\u6cd5\u6765\u6784\u5efa\u53ef\u9760\u7684AI\u8f85\u5bfc\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u63d0\u4f9b\u53c2\u8003\u9875\u7801\u65b9\u9762\u3002"}}
{"id": "2509.16895", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.16895", "abs": "https://arxiv.org/abs/2509.16895", "authors": ["Xinye Wanyan", "Danula Hettiachchi", "Chenglong Ma", "Ziqi Xu", "Jeffrey Chan"], "title": "Temporal-Aware User Behaviour Simulation with Large Language Models for Recommender Systems", "comment": null, "summary": "Large Language Models (LLMs) demonstrate human-like capabilities in language\nunderstanding, reasoning, and generation, driving interest in using LLM-based\nagents to simulate human feedback in recommender systems. However, most\nexisting approaches rely on static user profiling, neglecting the temporal and\ndynamic nature of user interests. This limitation stems from a disconnect\nbetween language modelling and behaviour modelling, which constrains the\ncapacity of agents to represent sequential patterns. To address this challenge,\nwe propose a Dynamic Temporal-aware Agent-based simulator for Recommender\nSystems, DyTA4Rec, which enables agents to model and utilise evolving user\nbehaviour based on historical interactions. DyTA4Rec features a dynamic updater\nfor real-time profile refinement, temporal-enhanced prompting for sequential\ncontext, and self-adaptive aggregation for coherent feedback. Experimental\nresults at group and individual levels show that DyTA4Rec significantly\nimproves the alignment between simulated and actual user behaviour by modelling\ndynamic characteristics and enhancing temporal awareness in LLM-based agents.", "AI": {"tldr": "DyTA4Rec\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u52a8\u6001\u65f6\u5e8f\u611f\u77e5\u4ee3\u7406\u6a21\u62df\u5668\uff0c\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\uff0c\u901a\u8fc7\u5efa\u6a21\u7528\u6237\u884c\u4e3a\u7684\u52a8\u6001\u6f14\u5316\u6765\u6539\u8fdb\u6a21\u62df\u53cd\u9988\u4e0e\u771f\u5b9e\u7528\u6237\u884c\u4e3a\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u63a8\u8350\u7cfb\u7edf\u4ee3\u7406\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u7528\u6237\u753b\u50cf\uff0c\u5ffd\u89c6\u4e86\u7528\u6237\u5174\u8da3\u7684\u65f6\u5e8f\u6027\u548c\u52a8\u6001\u6027\uff0c\u9650\u5236\u4e86\u4ee3\u7406\u6355\u6349\u5e8f\u5217\u6a21\u5f0f\u7684\u80fd\u529b\u3002", "method": "DyTA4Rec\u5305\u542b\u52a8\u6001\u66f4\u65b0\u5668\u7528\u4e8e\u5b9e\u65f6\u753b\u50cf\u4f18\u5316\u3001\u65f6\u5e8f\u589e\u5f3a\u63d0\u793a\u7528\u4e8e\u5e8f\u5217\u4e0a\u4e0b\u6587\u5efa\u6a21\u3001\u81ea\u9002\u5e94\u805a\u5408\u7528\u4e8e\u4e00\u81f4\u6027\u53cd\u9988\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDyTA4Rec\u5728\u7fa4\u4f53\u548c\u4e2a\u4f53\u5c42\u9762\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u62df\u884c\u4e3a\u4e0e\u771f\u5b9e\u7528\u6237\u884c\u4e3a\u7684\u4e00\u81f4\u6027\uff0c\u901a\u8fc7\u5efa\u6a21\u52a8\u6001\u7279\u6027\u548c\u589e\u5f3a\u65f6\u5e8f\u611f\u77e5\u80fd\u529b\u3002", "conclusion": "DyTA4Rec\u901a\u8fc7\u5f15\u5165\u52a8\u6001\u65f6\u5e8f\u5efa\u6a21\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u4ee3\u7406\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u5bf9\u7528\u6237\u884c\u4e3a\u52a8\u6001\u6027\u5efa\u6a21\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u62df\u6548\u679c\u3002"}}
{"id": "2509.16931", "categories": ["cs.IR", "cs.AI", "cs.LG", "I.2.0; I.5.0; I.7.0"], "pdf": "https://arxiv.org/pdf/2509.16931", "abs": "https://arxiv.org/abs/2509.16931", "authors": ["Yutong Li", "Yu Zhu", "Yichen Qiao", "Ziyu Guan", "Lv Shao", "Tong Liu", "Bo Zheng"], "title": "Equip Pre-ranking with Target Attention by Residual Quantization", "comment": "5 pages, 2 figures, submitted to WSDM 2026 Short Paper Track", "summary": "The pre-ranking stage in industrial recommendation systems faces a\nfundamental conflict between efficiency and effectiveness. While powerful\nmodels like Target Attention (TA) excel at capturing complex feature\ninteractions in the ranking stage, their high computational cost makes them\ninfeasible for pre-ranking, which often relies on simplistic vector-product\nmodels. This disparity creates a significant performance bottleneck for the\nentire system. To bridge this gap, we propose TARQ, a novel pre-ranking\nframework. Inspired by generative models, TARQ's key innovation is to equip\npre-ranking with an architecture approximate to TA by Residual Quantization.\nThis allows us to bring the modeling power of TA into the latency-critical\npre-ranking stage for the first time, establishing a new state-of-the-art\ntrade-off between accuracy and efficiency. Extensive offline experiments and\nlarge-scale online A/B tests at Taobao demonstrate TARQ's significant\nimprovements in ranking performance. Consequently, our model has been fully\ndeployed in production, serving tens of millions of daily active users and\nyielding substantial business improvements.", "AI": {"tldr": "TARQ\u662f\u4e00\u4e2a\u65b0\u7684\u9884\u6392\u5e8f\u6846\u67b6\uff0c\u901a\u8fc7\u6b8b\u5dee\u91cf\u5316\u6280\u672f\u5c06\u76ee\u6807\u6ce8\u610f\u529b\u6a21\u578b\u7684\u5f3a\u5927\u5efa\u6a21\u80fd\u529b\u5f15\u5165\u5230\u5ef6\u8fdf\u654f\u611f\u7684\u9884\u6392\u5e8f\u9636\u6bb5\uff0c\u89e3\u51b3\u4e86\u6548\u7387\u4e0e\u6548\u679c\u4e4b\u95f4\u7684\u51b2\u7a81\u3002", "motivation": "\u5de5\u4e1a\u63a8\u8350\u7cfb\u7edf\u4e2d\u9884\u6392\u5e8f\u9636\u6bb5\u9762\u4e34\u6548\u7387\u4e0e\u6548\u679c\u7684\u51b2\u7a81\uff0c\u5f3a\u5927\u7684\u76ee\u6807\u6ce8\u610f\u529b\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u65e0\u6cd5\u7528\u4e8e\u9884\u6392\u5e8f\uff0c\u800c\u7b80\u5355\u7684\u5411\u91cf\u79ef\u6a21\u578b\u6027\u80fd\u6709\u9650\uff0c\u8fd9\u6210\u4e3a\u6574\u4e2a\u7cfb\u7edf\u7684\u6027\u80fd\u74f6\u9888\u3002", "method": "TARQ\u6846\u67b6\u501f\u9274\u751f\u6210\u6a21\u578b\u601d\u60f3\uff0c\u901a\u8fc7\u6b8b\u5dee\u91cf\u5316\u6280\u672f\u6784\u5efa\u8fd1\u4f3c\u76ee\u6807\u6ce8\u610f\u529b\u7684\u67b6\u6784\uff0c\u5c06TA\u7684\u5efa\u6a21\u80fd\u529b\u9996\u6b21\u5f15\u5165\u5230\u5ef6\u8fdf\u5173\u952e\u7684\u9884\u6392\u5e8f\u9636\u6bb5\u3002", "result": "\u5728\u6dd8\u5b9d\u8fdb\u884c\u7684\u5927\u89c4\u6a21\u79bb\u7ebf\u548c\u5728\u7ebfA/B\u6d4b\u8bd5\u8868\u660e\uff0cTARQ\u5728\u6392\u5e8f\u6027\u80fd\u4e0a\u6709\u663e\u8457\u63d0\u5347\uff0c\u5df2\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u5168\u9762\u90e8\u7f72\uff0c\u670d\u52a1\u6570\u5343\u4e07\u65e5\u6d3b\u7528\u6237\u5e76\u5e26\u6765\u663e\u8457\u4e1a\u52a1\u6539\u8fdb\u3002", "conclusion": "TARQ\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6743\u8861\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u9884\u6392\u5e8f\u9636\u6bb5\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2509.17265", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.17265", "abs": "https://arxiv.org/abs/2509.17265", "authors": ["David Liu", "Erik Weis", "Moritz Laber", "Tina Eliassi-Rad", "Brennan Klein"], "title": "Identifying and Upweighting Power-Niche Users to Mitigate Popularity Bias in Recommendations", "comment": null, "summary": "Recommender systems have been shown to exhibit popularity bias by\nover-recommending popular items and under-recommending relevant niche items. We\nseek to understand interactions with niche items in benchmark recommendation\ndatasets as a step toward mitigating popularity bias. We find that, compared to\nmainstream users, niche-preferring users exhibit a longer-tailed activity-level\ndistribution, indicating the existence of users who both prefer niche items and\nexhibit high activity levels. We partition users along two axes: (1) activity\nlevel (\"power\" vs. \"light\") and (2) item-popularity preference (\"mainstream\"\nvs. \"niche\"), and show that in several benchmark datasets, the number of\npower-niche users (high activity and niche preference) is statistically\nsignificantly larger than expected under a null configuration model. Motivated\nby this observation, we propose a framework for reweighting the Bayesian\nPersonalized Ranking (BPR) loss that simultaneously reweights based on user\nactivity level and item popularity. Our method introduces two interpretable\nparameters: one controlling the significance of user activity level, and the\nother of item popularity. Experiments on benchmark datasets show that\nupweighting power-niche users reduces popularity bias and can increase overall\nperformance. In contrast to previous work that only considers user activity\nlevel or item popularity in isolation, our results suggest that considering\ntheir interaction leads to Pareto-dominant performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u6d41\u884c\u5ea6\u504f\u5dee\u95ee\u9898\uff0c\u53d1\u73b0\u504f\u597d\u5c0f\u4f17\u7269\u54c1\u7684\u7528\u6237\u5177\u6709\u957f\u5c3e\u6d3b\u52a8\u5206\u5e03\uff0c\u63d0\u51fa\u4e86\u540c\u65f6\u8003\u8651\u7528\u6237\u6d3b\u8dc3\u5ea6\u548c\u7269\u54c1\u6d41\u884c\u5ea6\u7684BPR\u635f\u5931\u91cd\u52a0\u6743\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u6d41\u884c\u5ea6\u504f\u5dee\u5e76\u63d0\u5347\u6574\u4f53\u6027\u80fd\u3002", "motivation": "\u63a8\u8350\u7cfb\u7edf\u5b58\u5728\u6d41\u884c\u5ea6\u504f\u5dee\uff0c\u8fc7\u5ea6\u63a8\u8350\u70ed\u95e8\u7269\u54c1\u800c\u5ffd\u89c6\u76f8\u5173\u7684\u5c0f\u4f17\u7269\u54c1\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u7406\u89e3\u57fa\u51c6\u63a8\u8350\u6570\u636e\u96c6\u4e2d\u7528\u6237\u4e0e\u5c0f\u4f17\u7269\u54c1\u7684\u4ea4\u4e92\uff0c\u6765\u7f13\u89e3\u8fd9\u79cd\u504f\u5dee\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u7528\u6237\u6d3b\u8dc3\u5ea6\u548c\u7269\u54c1\u6d41\u884c\u5ea6\u7684BPR\u635f\u5931\u91cd\u52a0\u6743\u6846\u67b6\uff0c\u5f15\u5165\u4e24\u4e2a\u53ef\u89e3\u91ca\u53c2\u6570\u5206\u522b\u63a7\u5236\u7528\u6237\u6d3b\u8dc3\u5ea6\u548c\u7269\u54c1\u6d41\u884c\u5ea6\u7684\u91cd\u8981\u6027\u3002\u5c06\u7528\u6237\u6309\u6d3b\u8dc3\u5ea6\uff08\u9ad8\u6d3b\u8dc3vs\u4f4e\u6d3b\u8dc3\uff09\u548c\u7269\u54c1\u504f\u597d\uff08\u4e3b\u6d41vs\u5c0f\u4f17\uff09\u8fdb\u884c\u5212\u5206\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u9ad8\u6d3b\u8dc3-\u5c0f\u4f17\u504f\u597d\u7684\u7528\u6237\u6570\u91cf\u663e\u8457\u591a\u4e8e\u9884\u671f\u3002\u901a\u8fc7\u63d0\u5347\u8fd9\u7c7b\u7528\u6237\u7684\u6743\u91cd\uff0c\u80fd\u591f\u51cf\u5c11\u6d41\u884c\u5ea6\u504f\u5dee\u5e76\u63d0\u9ad8\u6574\u4f53\u63a8\u8350\u6027\u80fd\u3002", "conclusion": "\u4e0e\u4ee5\u5f80\u5355\u72ec\u8003\u8651\u7528\u6237\u6d3b\u8dc3\u5ea6\u6216\u7269\u54c1\u6d41\u884c\u5ea6\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u540c\u65f6\u8003\u8651\u4e24\u8005\u7684\u4ea4\u4e92\u4f5c\u7528\u80fd\u591f\u83b7\u5f97\u5e15\u7d2f\u6258\u4f18\u52bf\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2509.17359", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.17359", "abs": "https://arxiv.org/abs/2509.17359", "authors": ["Tianyuan Li", "Lei Wang", "Ahtamjan Ahmat", "Yating Yang", "Bo Ma", "Rui Dong", "Bangju Han"], "title": "MLLM-Driven Semantic Identifier Generation for Generative Cross-Modal Retrieval", "comment": null, "summary": "Generative cross-modal retrieval, which treats retrieval as a generation\ntask, has emerged as a promising direction with the rise of Multimodal Large\nLanguage Models (MLLMs). In this setting, the model responds to a text query by\ngenerating an identifier corresponding to the target image. However, existing\nmethods typically rely on manually crafted string IDs, clustering-based labels,\nor atomic identifiers requiring vocabulary expansion, all of which face\nchallenges in semantic alignment or scalability.To address these limitations,\nwe propose a vocabulary-efficient identifier generation framework that prompts\nMLLMs to generate Structured Semantic Identifiers from image-caption pairs.\nThese identifiers are composed of concept-level tokens such as objects and\nactions, naturally aligning with the model's generation space without modifying\nthe tokenizer. Additionally, we introduce a Rationale-Guided Supervision\nStrategy, prompting the model to produce a one-sentence explanation alongside\neach identifier serves as an auxiliary supervision signal that improves\nsemantic grounding and reduces hallucinations during training.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bcd\u6c47\u9ad8\u6548\u6807\u8bc6\u7b26\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8bed\u4e49\u6807\u8bc6\u7b26\u548c\u7406\u6027\u5f15\u5bfc\u76d1\u7763\u7b56\u7565\u6539\u8fdb\u751f\u6210\u5f0f\u8de8\u6a21\u6001\u68c0\u7d22", "motivation": "\u73b0\u6709\u751f\u6210\u5f0f\u8de8\u6a21\u6001\u68c0\u7d22\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u5b57\u7b26\u4e32ID\u3001\u805a\u7c7b\u6807\u7b7e\u6216\u9700\u8981\u8bcd\u6c47\u6269\u5c55\u7684\u539f\u5b50\u6807\u8bc6\u7b26\uff0c\u5728\u8bed\u4e49\u5bf9\u9f50\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218", "method": "\u4f7f\u7528\u7ed3\u6784\u5316\u8bed\u4e49\u6807\u8bc6\u7b26\uff08\u5305\u542b\u5bf9\u8c61\u3001\u52a8\u4f5c\u7b49\u6982\u5ff5\u7ea7\u6807\u8bb0\uff09\u548c\u7406\u6027\u5f15\u5bfc\u76d1\u7763\u7b56\u7565\uff08\u751f\u6210\u4e00\u53e5\u8bdd\u89e3\u91ca\u4f5c\u4e3a\u8f85\u52a9\u76d1\u7763\u4fe1\u53f7\uff09", "result": "\u8be5\u65b9\u6cd5\u81ea\u7136\u5bf9\u9f50\u6a21\u578b\u7684\u751f\u6210\u7a7a\u95f4\uff0c\u65e0\u9700\u4fee\u6539\u5206\u8bcd\u5668\uff0c\u63d0\u9ad8\u4e86\u8bed\u4e49\u57fa\u7840\u5e76\u51cf\u5c11\u4e86\u8bad\u7ec3\u4e2d\u7684\u5e7b\u89c9", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u751f\u6210\u5f0f\u8de8\u6a21\u6001\u68c0\u7d22\u4e2d\u7684\u8bed\u4e49\u5bf9\u9f50\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898"}}
{"id": "2509.17361", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17361", "abs": "https://arxiv.org/abs/2509.17361", "authors": ["Ruihan Luo", "Xuanjing Chen", "Ziyang Ding"], "title": "SeqUDA-Rec: Sequential User Behavior Enhanced Recommendation via Global Unsupervised Data Augmentation for Personalized Content Marketing", "comment": null, "summary": "Personalized content marketing has become a crucial strategy for digital\nplatforms, aiming to deliver tailored advertisements and recommendations that\nmatch user preferences. Traditional recommendation systems often suffer from\ntwo limitations: (1) reliance on limited supervised signals derived from\nexplicit user feedback, and (2) vulnerability to noisy or unintentional\ninteractions. To address these challenges, we propose SeqUDA-Rec, a novel deep\nlearning framework that integrates user behavior sequences with global\nunsupervised data augmentation to enhance recommendation accuracy and\nrobustness. Our approach first constructs a Global User-Item Interaction Graph\n(GUIG) from all user behavior sequences, capturing both local and global item\nassociations. Then, a graph contrastive learning module is applied to generate\nrobust embeddings, while a sequential Transformer-based encoder models users'\nevolving preferences. To further enhance diversity and counteract sparse\nsupervised labels, we employ a GAN-based augmentation strategy, generating\nplausible interaction patterns and supplementing training data. Extensive\nexperiments on two real-world marketing datasets (Amazon Ads and TikTok Ad\nClicks) demonstrate that SeqUDA-Rec significantly outperforms state-of-the-art\nbaselines such as SASRec, BERT4Rec, and GCL4SR. Our model achieves a 6.7%\nimprovement in NDCG@10 and 11.3% improvement in HR@10, proving its\neffectiveness in personalized advertising and intelligent content\nrecommendation.", "AI": {"tldr": "SeqUDA-Rec\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u7528\u6237\u884c\u4e3a\u5e8f\u5217\u548c\u5168\u5c40\u65e0\u76d1\u7763\u6570\u636e\u589e\u5f3a\u6765\u89e3\u51b3\u4f20\u7edf\u63a8\u8350\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u63a8\u8350\u7cfb\u7edf\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a(1)\u4f9d\u8d56\u6709\u9650\u7684\u663e\u5f0f\u7528\u6237\u53cd\u9988\u76d1\u7763\u4fe1\u53f7\uff0c(2)\u5bb9\u6613\u53d7\u5230\u566a\u58f0\u6216\u65e0\u610f\u4ea4\u4e92\u7684\u5f71\u54cd\u3002\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u4e2a\u6027\u5316\u63a8\u8350\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u5168\u5c40\u7528\u6237-\u7269\u54c1\u4ea4\u4e92\u56fe(GUIG)\uff0c\u5e94\u7528\u56fe\u5bf9\u6bd4\u5b66\u4e60\u751f\u6210\u9c81\u68d2\u5d4c\u5165\uff0c\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u987a\u5e8f\u7f16\u7801\u5668\u5efa\u6a21\u7528\u6237\u504f\u597d\u6f14\u5316\uff0c\u5e76\u91c7\u7528GAN\u589e\u5f3a\u7b56\u7565\u751f\u6210\u53ef\u4fe1\u4ea4\u4e92\u6a21\u5f0f\u8865\u5145\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u8425\u9500\u6570\u636e\u96c6(Amazon Ads\u548cTikTok Ad Clicks)\u4e0a\uff0cSeqUDA-Rec\u663e\u8457\u4f18\u4e8eSASRec\u3001BERT4Rec\u548cGCL4SR\u7b49\u6700\u5148\u8fdb\u57fa\u7ebf\u6a21\u578b\uff0cNDCG@10\u63d0\u53476.7%\uff0cHR@10\u63d0\u534711.3%\u3002", "conclusion": "SeqUDA-Rec\u5728\u4e2a\u6027\u5316\u5e7f\u544a\u548c\u667a\u80fd\u5185\u5bb9\u63a8\u8350\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\uff0c\u4e3a\u89e3\u51b3\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u76d1\u7763\u4fe1\u53f7\u7a00\u758f\u6027\u548c\u566a\u58f0\u4ea4\u4e92\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17440", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.17440", "abs": "https://arxiv.org/abs/2509.17440", "authors": ["J\u00fcri Keller", "Maik Fr\u00f6be", "Gijs Hendriksen", "Daria Alexander", "Martin Potthast", "Philipp Schaer"], "title": "Simplified Longitudinal Retrieval Experiments: A Case Study on Query Expansion and Document Boosting", "comment": "Best of labs paper for LongEval at CLEF 2024", "summary": "The longitudinal evaluation of retrieval systems aims to capture how\ninformation needs and documents evolve over time. However, classical\nCranfield-style retrieval evaluations only consist of a static set of queries\nand documents and thereby miss time as an evaluation dimension. Therefore,\nlongitudinal evaluations need to complement retrieval toolkits with custom\nlogic. This custom logic increases the complexity of research software, which\nmight reduce the reproducibility and extensibility of experiments. Based on our\nsubmissions to the 2024 edition of LongEval, we propose a custom extension of\nir_datasets for longitudinal retrieval experiments. This extension allows for\ndeclaratively, instead of imperatively, describing important aspects of\nlongitudinal retrieval experiments, e.g., which queries, documents, and/or\nrelevance feedback are available at which point in time. We reimplement our\nsubmissions to LongEval 2024 against our new ir_datasets extension, and find\nthat the declarative access can reduce the complexity of the code.", "AI": {"tldr": "\u63d0\u51fa\u4e86ir_datasets\u7684\u81ea\u5b9a\u4e49\u6269\u5c55\uff0c\u7528\u4e8e\u58f0\u660e\u5f0f\u5730\u63cf\u8ff0\u7eb5\u5411\u68c0\u7d22\u5b9e\u9a8c\u7684\u65f6\u95f4\u7ef4\u5ea6\uff0c\u4ee5\u964d\u4f4e\u4ee3\u7801\u590d\u6742\u6027\u5e76\u63d0\u9ad8\u53ef\u91cd\u590d\u6027\u3002", "motivation": "\u4f20\u7edf\u7684Cranfield\u5f0f\u68c0\u7d22\u8bc4\u4f30\u53ea\u5305\u542b\u9759\u6001\u7684\u67e5\u8be2\u548c\u6587\u6863\u96c6\uff0c\u7f3a\u4e4f\u65f6\u95f4\u7ef4\u5ea6\uff0c\u800c\u7eb5\u5411\u8bc4\u4f30\u9700\u8981\u81ea\u5b9a\u4e49\u903b\u8f91\uff0c\u589e\u52a0\u4e86\u7814\u7a76\u8f6f\u4ef6\u7684\u590d\u6742\u6027\uff0c\u964d\u4f4e\u4e86\u5b9e\u9a8c\u7684\u53ef\u91cd\u590d\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u57fa\u4e8e\u5728LongEval 2024\u7684\u63d0\u4ea4\uff0c\u5f00\u53d1\u4e86ir_datasets\u7684\u81ea\u5b9a\u4e49\u6269\u5c55\uff0c\u5141\u8bb8\u58f0\u660e\u5f0f\u5730\u63cf\u8ff0\u7eb5\u5411\u68c0\u7d22\u5b9e\u9a8c\u7684\u91cd\u8981\u65b9\u9762\uff0c\u5982\u67e5\u8be2\u3001\u6587\u6863\u548c\u76f8\u5173\u6027\u53cd\u9988\u5728\u65f6\u95f4\u4e0a\u7684\u53ef\u7528\u6027\u3002", "result": "\u91cd\u65b0\u5b9e\u73b0\u4e86LongEval 2024\u7684\u63d0\u4ea4\uff0c\u53d1\u73b0\u58f0\u660e\u5f0f\u8bbf\u95ee\u53ef\u4ee5\u964d\u4f4e\u4ee3\u7801\u7684\u590d\u6742\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684ir_datasets\u6269\u5c55\u80fd\u591f\u6709\u6548\u7b80\u5316\u7eb5\u5411\u68c0\u7d22\u5b9e\u9a8c\u7684\u5b9e\u73b0\uff0c\u63d0\u9ad8\u7814\u7a76\u7684\u53ef\u91cd\u590d\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2509.17442", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.17442", "abs": "https://arxiv.org/abs/2509.17442", "authors": ["Hideaki Joko", "Shakiba Amirshahi", "Charles L. A. Clarke", "Faegheh Hasibi"], "title": "WildClaims: Information Access Conversations in the Wild(Chat)", "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has transformed\nconversational systems into practical tools used by millions. However, the\nnature and necessity of information retrieval in real-world conversations\nremain largely unexplored, as research has focused predominantly on\ntraditional, explicit information access conversations. The central question\nis: What do real-world information access conversations look like? To this end,\nwe first conduct an observational study on the WildChat dataset, large-scale\nuser-ChatGPT conversations, finding that users' access to information occurs\nimplicitly as check-worthy factual assertions made by the system, even when the\nconversation's primary intent is non-informational, such as creative writing.\nTo enable the systematic study of this phenomenon, we release the WildClaims\ndataset, a novel resource consisting of 121,905 extracted factual claims from\n7,587 utterances in 3,000 WildChat conversations, each annotated for\ncheck-worthiness. Our preliminary analysis of this resource reveals that\nconservatively 18% to 51% of conversations contain check-worthy assertions,\ndepending on the methods employed, and less conservatively, as many as 76% may\ncontain such assertions. This high prevalence underscores the importance of\nmoving beyond the traditional understanding of explicit information access, to\naddress the implicit information access that arises in real-world user-system\nconversations.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u5206\u6790WildChat\u6570\u636e\u96c6\u53d1\u73b0\uff0c\u73b0\u5b9e\u5bf9\u8bdd\u4e2d\u4fe1\u606f\u68c0\u7d22\u5f80\u5f80\u4ee5\u7cfb\u7edf\u505a\u51fa\u7684\u503c\u5f97\u9a8c\u8bc1\u7684\u4e8b\u5b9e\u65ad\u8a00\u5f62\u5f0f\u9690\u5f0f\u53d1\u751f\uff0c\u800c\u975e\u4f20\u7edf\u663e\u5f0f\u4fe1\u606f\u8bbf\u95ee\u3002\u4f5c\u8005\u53d1\u5e03\u4e86WildClaims\u6570\u636e\u96c6\u6765\u7cfb\u7edf\u7814\u7a76\u8fd9\u4e00\u73b0\u8c61\u3002", "motivation": "\u63a2\u7d22\u73b0\u5b9e\u4e16\u754c\u5bf9\u8bdd\u4e2d\u4fe1\u606f\u68c0\u7d22\u7684\u672c\u8d28\u548c\u5fc5\u8981\u6027\uff0c\u56e0\u4e3a\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4f20\u7edf\u7684\u663e\u5f0f\u4fe1\u606f\u8bbf\u95ee\u5bf9\u8bdd\uff0c\u800c\u771f\u5b9e\u5bf9\u8bdd\u4e2d\u7684\u4fe1\u606f\u8bbf\u95ee\u6a21\u5f0f\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002", "method": "\u5728WildChat\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u89c2\u5bdf\u6027\u7814\u7a76\uff0c\u6784\u5efa\u5e76\u53d1\u5e03WildClaims\u6570\u636e\u96c6\uff08\u5305\u542b121,905\u4e2a\u4ece3,000\u4e2a\u5bf9\u8bdd\u4e2d\u63d0\u53d6\u7684\u4e8b\u5b9e\u65ad\u8a00\uff0c\u6bcf\u4e2a\u90fd\u6807\u6ce8\u4e86\u68c0\u67e5\u4ef7\u503c\uff09\u3002", "result": "\u4fdd\u5b88\u4f30\u8ba118%-51%\u7684\u5bf9\u8bdd\u5305\u542b\u503c\u5f97\u9a8c\u8bc1\u7684\u65ad\u8a00\uff0c\u975e\u4fdd\u5b88\u4f30\u8ba1\u53ef\u8fbe76%\uff0c\u8868\u660e\u9690\u5f0f\u4fe1\u606f\u8bbf\u95ee\u5728\u73b0\u5b9e\u5bf9\u8bdd\u4e2d\u666e\u904d\u5b58\u5728\u3002", "conclusion": "\u9700\u8981\u8d85\u8d8a\u4f20\u7edf\u7684\u663e\u5f0f\u4fe1\u606f\u8bbf\u95ee\u7406\u89e3\uff0c\u5173\u6ce8\u73b0\u5b9e\u7528\u6237-\u7cfb\u7edf\u5bf9\u8bdd\u4e2d\u51fa\u73b0\u7684\u9690\u5f0f\u4fe1\u606f\u8bbf\u95ee\u95ee\u9898\u3002"}}
{"id": "2509.17469", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.17469", "abs": "https://arxiv.org/abs/2509.17469", "authors": ["Matteo Cancellieri", "Alaa El-Ebshihy", "Tobias Fink", "Maik Fr\u00f6be", "Petra Galu\u0161\u010d\u00e1kov\u00e1", "Gabriela Gonzalez-Saez", "Lorraine Goeuriot", "David Iommi", "J\u00fcri Keller", "Petr Knoth", "Philippe Mulhem", "Florina Piroi", "David Pride", "Philipp Schaer"], "title": "LongEval at CLEF 2025: Longitudinal Evaluation of IR Systems on Web and Scientific Data", "comment": null, "summary": "The LongEval lab focuses on the evaluation of information retrieval systems\nover time. Two datasets are provided that capture evolving search scenarios\nwith changing documents, queries, and relevance assessments. Systems are\nassessed from a temporal perspective-that is, evaluating retrieval\neffectiveness as the data they operate on changes. In its third edition,\nLongEval featured two retrieval tasks: one in the area of ad-hoc web retrieval,\nand another focusing on scientific article retrieval. We present an overview of\nthis year's tasks and datasets, as well as the participating systems. A total\nof 19 teams submitted their approaches, which we evaluated using nDCG and a\nvariety of measures that quantify changes in retrieval effectiveness over time.", "AI": {"tldr": "LongEval\u5b9e\u9a8c\u5ba4\u4e13\u6ce8\u4e8e\u8bc4\u4f30\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u968f\u65f6\u95f4\u53d8\u5316\u7684\u6027\u80fd\uff0c\u63d0\u4f9b\u4e24\u4e2a\u6570\u636e\u96c6\u6765\u6355\u6349\u6587\u6863\u3001\u67e5\u8be2\u548c\u76f8\u5173\u6027\u8bc4\u4f30\u4e0d\u65ad\u53d8\u5316\u7684\u641c\u7d22\u573a\u666f\u3002\u5728\u7b2c\u4e09\u5c4a\u4e2d\uff0c\u5305\u542b\u7f51\u9875\u68c0\u7d22\u548c\u79d1\u5b66\u6587\u732e\u68c0\u7d22\u4e24\u4e2a\u4efb\u52a1\uff0c19\u4e2a\u56e2\u961f\u63d0\u4ea4\u4e86\u65b9\u6cd5\uff0c\u4f7f\u7528nDCG\u548c\u591a\u79cd\u8861\u91cf\u68c0\u7d22\u6548\u679c\u968f\u65f6\u95f4\u53d8\u5316\u7684\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002", "motivation": "\u8bc4\u4f30\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u5728\u6570\u636e\u968f\u65f6\u95f4\u53d8\u5316\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u4e2d\u641c\u7d22\u573a\u666f\u52a8\u6001\u6f14\u53d8\u7684\u6311\u6218\u3002", "method": "\u63d0\u4f9b\u4e24\u4e2a\u5305\u542b\u52a8\u6001\u53d8\u5316\u6587\u6863\u3001\u67e5\u8be2\u548c\u76f8\u5173\u6027\u8bc4\u4f30\u7684\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u7f51\u9875\u68c0\u7d22\u548c\u79d1\u5b66\u6587\u732e\u68c0\u7d22\u4e24\u4e2a\u4efb\u52a1\uff0c\u4f7f\u7528nDCG\u548c\u4e13\u95e8\u7684\u65f6\u95f4\u53d8\u5316\u8bc4\u4f30\u6307\u6807\u6765\u91cf\u5316\u7cfb\u7edf\u6027\u80fd\u3002", "result": "19\u4e2a\u56e2\u961f\u53c2\u4e0e\u4e86\u8bc4\u4f30\uff0c\u63d0\u4ea4\u4e86\u5404\u81ea\u7684\u68c0\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u79cd\u65f6\u95f4\u611f\u77e5\u7684\u8bc4\u4f30\u6307\u6807\u5bf9\u7cfb\u7edf\u6027\u80fd\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\u3002", "conclusion": "LongEval\u5b9e\u9a8c\u5ba4\u6210\u529f\u5efa\u7acb\u4e86\u8bc4\u4f30\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u65f6\u95f4\u52a8\u6001\u6027\u80fd\u7684\u6846\u67b6\uff0c\u4e3a\u7814\u7a76\u68c0\u7d22\u7cfb\u7edf\u5728\u6570\u636e\u6f14\u53d8\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\u3002"}}
{"id": "2509.17619", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.17619", "abs": "https://arxiv.org/abs/2509.17619", "authors": ["Zhefan Wang", "Ning Geng", "Zhiqiang Guo", "Weizhi Ma", "Min Zhang"], "title": "Human vs. Agent in Task-Oriented Conversations", "comment": "SIGIR-AP 2025", "summary": "Task-oriented conversational systems are essential for efficiently addressing\ndiverse user needs, yet their development requires substantial amounts of\nhigh-quality conversational data that is challenging and costly to obtain.\nWhile large language models (LLMs) have demonstrated potential in generating\nsynthetic conversations, the extent to which these agent-generated interactions\ncan effectively substitute real human conversations remains unclear. This work\npresents the first systematic comparison between LLM-simulated users and human\nusers in personalized task-oriented conversations. We propose a comprehensive\nanalytical framework encompassing three key aspects (conversation strategy,\ninteraction style, and conversation evaluation) and ten distinct dimensions for\nevaluating user behaviors, and collect parallel conversational datasets from\nboth human users and LLM agent users across four representative scenarios under\nidentical conditions. Our analysis reveals significant behavioral differences\nbetween the two user types in problem-solving approaches, question broadness,\nuser engagement, context dependency, feedback polarity and promise, language\nstyle, and hallucination awareness. We found consistency in the agent users and\nhuman users across the depth-first or breadth-first dimensions, as well as the\nusefulness dimensions. These findings provide critical insights for advancing\nLLM-based user simulation. Our multi-dimensional taxonomy constructed a\ngeneralizable framework for analyzing user behavior patterns, offering insights\nfrom LLM agent users and human users. By this work, we provide perspectives on\nrethinking how to use user simulation in conversational systems in the future.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u6bd4\u8f83\u4e86LLM\u6a21\u62df\u7528\u6237\u4e0e\u771f\u5b9e\u7528\u6237\u5728\u4e2a\u6027\u5316\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u4e2d\u7684\u884c\u4e3a\u5dee\u5f02\uff0c\u63d0\u51fa\u4e86\u5305\u542b10\u4e2a\u7ef4\u5ea6\u7684\u5206\u6790\u6846\u67b6\uff0c\u53d1\u73b0\u4e24\u8005\u5728\u591a\u4e2a\u5173\u952e\u884c\u4e3a\u7ef4\u5ea6\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u9700\u8981\u5927\u91cf\u9ad8\u8d28\u91cf\u5bf9\u8bdd\u6570\u636e\uff0c\u4f46\u83b7\u53d6\u6210\u672c\u9ad8\u6602\u3002\u867d\u7136LLM\u6709\u751f\u6210\u5408\u6210\u5bf9\u8bdd\u7684\u6f5c\u529b\uff0c\u4f46LLM\u6a21\u62df\u7528\u6237\u80fd\u5426\u6709\u6548\u66ff\u4ee3\u771f\u5b9e\u4eba\u7c7b\u5bf9\u8bdd\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u63d0\u51fa\u4e86\u5305\u542b\u5bf9\u8bdd\u7b56\u7565\u3001\u4ea4\u4e92\u98ce\u683c\u548c\u5bf9\u8bdd\u8bc4\u4f30\u4e09\u4e2a\u5173\u952e\u65b9\u9762\u517110\u4e2a\u7ef4\u5ea6\u7684\u7efc\u5408\u5206\u6790\u6846\u67b6\uff0c\u5728\u56db\u4e2a\u4ee3\u8868\u6027\u573a\u666f\u4e0b\u6536\u96c6\u4e86\u4eba\u7c7b\u7528\u6237\u548cLLM\u4ee3\u7406\u7528\u6237\u5728\u76f8\u540c\u6761\u4ef6\u4e0b\u7684\u5e73\u884c\u5bf9\u8bdd\u6570\u636e\u96c6\u3002", "result": "\u5206\u6790\u53d1\u73b0\u4e24\u7c7b\u7528\u6237\u5728\u95ee\u9898\u89e3\u51b3\u65b9\u5f0f\u3001\u95ee\u9898\u5e7f\u5ea6\u3001\u7528\u6237\u53c2\u4e0e\u5ea6\u3001\u4e0a\u4e0b\u6587\u4f9d\u8d56\u3001\u53cd\u9988\u6781\u6027\u3001\u8bed\u8a00\u98ce\u683c\u548c\u5e7b\u89c9\u610f\u8bc6\u7b49\u65b9\u9762\u5b58\u5728\u663e\u8457\u884c\u4e3a\u5dee\u5f02\uff0c\u4f46\u5728\u6df1\u5ea6\u4f18\u5148/\u5e7f\u5ea6\u4f18\u5148\u7ef4\u5ea6\u548c\u6709\u7528\u6027\u7ef4\u5ea6\u4e0a\u8868\u73b0\u4e00\u81f4\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u63a8\u8fdb\u57fa\u4e8eLLM\u7684\u7528\u6237\u6a21\u62df\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\uff0c\u6784\u5efa\u7684\u591a\u7ef4\u5ea6\u5206\u7c7b\u6cd5\u4e3a\u5206\u6790\u7528\u6237\u884c\u4e3a\u6a21\u5f0f\u63d0\u4f9b\u4e86\u53ef\u63a8\u5e7f\u7684\u6846\u67b6\uff0c\u4e3a\u672a\u6765\u5982\u4f55\u5728\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u4f7f\u7528\u7528\u6237\u6a21\u62df\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8003\u89c6\u89d2\u3002"}}
{"id": "2509.17749", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.17749", "abs": "https://arxiv.org/abs/2509.17749", "authors": ["Changjiang Zhou", "Ruqing Zhang", "Jiafeng Guo", "Yu-An Liu", "Fan Zhang", "Ganyuan Luo", "Xueqi Cheng"], "title": "A Generative Framework for Personalized Sticker Retrieval", "comment": "Findings of EMNLP2025", "summary": "Formulating information retrieval as a variant of generative modeling,\nspecifically using autoregressive models to generate relevant identifiers for a\ngiven query, has recently attracted considerable attention. However, its\napplication to personalized sticker retrieval remains largely unexplored and\npresents unique challenges: existing relevance-based generative retrieval\nmethods typically lack personalization, leading to a mismatch between diverse\nuser expectations and the retrieved results. To address this gap, we propose\nPEARL, a novel generative framework for personalized sticker retrieval, and\nmake two key contributions: (i) To encode user-specific sticker preferences, we\ndesign a representation learning model to learn discriminative user\nrepresentations. It is trained on three prediction tasks that leverage personal\ninformation and click history; and (ii) To generate stickers aligned with a\nuser's query intent, we propose a novel intent-aware learning objective that\nprioritizes stickers associated with higher-ranked intents. Empirical results\nfrom both offline evaluations and online tests demonstrate that PEARL\nsignificantly outperforms state-of-the-art methods.", "AI": {"tldr": "PEARL\u662f\u4e00\u4e2a\u7528\u4e8e\u4e2a\u6027\u5316\u8d34\u7eb8\u68c0\u7d22\u7684\u751f\u6210\u5f0f\u6846\u67b6\uff0c\u901a\u8fc7\u7528\u6237\u8868\u793a\u5b66\u4e60\u548c\u610f\u56fe\u611f\u77e5\u5b66\u4e60\u76ee\u6807\u6765\u89e3\u51b3\u73b0\u6709\u751f\u6210\u5f0f\u68c0\u7d22\u65b9\u6cd5\u7f3a\u4e4f\u4e2a\u6027\u5316\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u76f8\u5173\u6027\u7684\u751f\u6210\u5f0f\u68c0\u7d22\u65b9\u6cd5\u901a\u5e38\u7f3a\u4e4f\u4e2a\u6027\u5316\uff0c\u5bfc\u81f4\u68c0\u7d22\u7ed3\u679c\u4e0e\u591a\u6837\u5316\u7684\u7528\u6237\u671f\u671b\u4e0d\u5339\u914d\uff0c\u7279\u522b\u662f\u5728\u4e2a\u6027\u5316\u8d34\u7eb8\u68c0\u7d22\u9886\u57df\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u8bbe\u8ba1\u4e86\u8868\u793a\u5b66\u4e60\u6a21\u578b\u6765\u5b66\u4e60\u533a\u5206\u6027\u7528\u6237\u8868\u793a\uff08\u57fa\u4e8e\u4e2a\u4eba\u4fe1\u606f\u548c\u70b9\u51fb\u5386\u53f2\u7684\u4e09\u4e2a\u9884\u6d4b\u4efb\u52a1\uff09\uff0c\u5e76\u63d0\u51fa\u610f\u56fe\u611f\u77e5\u5b66\u4e60\u76ee\u6807\u6765\u751f\u6210\u4e0e\u7528\u6237\u67e5\u8be2\u610f\u56fe\u4e00\u81f4\u7684\u8d34\u7eb8\u3002", "result": "\u79bb\u7ebf\u548c\u5728\u7ebf\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0cPEARL\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "PEARL\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u4e2a\u6027\u5316\u8d34\u7eb8\u68c0\u7d22\u7684\u6311\u6218\uff0c\u901a\u8fc7\u7528\u6237\u504f\u597d\u7f16\u7801\u548c\u610f\u56fe\u5bf9\u9f50\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u68c0\u7d22\u6027\u80fd\u3002"}}
{"id": "2509.17918", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17918", "abs": "https://arxiv.org/abs/2509.17918", "authors": ["Yuanrong Wang", "Yingpeng Du"], "title": "Shilling Recommender Systems by Generating Side-feature-aware Fake User Profiles", "comment": null, "summary": "Recommender systems (RS) greatly influence users' consumption decisions,\nmaking them attractive targets for malicious shilling attacks that inject fake\nuser profiles to manipulate recommendations. Existing shilling methods can\ngenerate effective and stealthy fake profiles when training data only contain\nrating matrix, but they lack comprehensive solutions for scenarios where side\nfeatures are present and utilized by the recommender. To address this gap, we\nextend the Leg-UP framework by enhancing the generator architecture to\nincorporate side features, enabling the generation of side-feature-aware fake\nuser profiles. Experiments on benchmarks show that our method achieves strong\nattack performance while maintaining stealthiness.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86Leg-UP\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u751f\u6210\u5668\u67b6\u6784\u6765\u6574\u5408\u4fa7\u7279\u5f81\uff0c\u751f\u6210\u80fd\u591f\u611f\u77e5\u4fa7\u7279\u5f81\u7684\u865a\u5047\u7528\u6237\u914d\u7f6e\u6587\u4ef6\uff0c\u4ee5\u5e94\u5bf9\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u6076\u610f\u653b\u51fb\u3002", "motivation": "\u73b0\u6709\u7684\u653b\u51fb\u65b9\u6cd5\u5728\u4ec5\u5305\u542b\u8bc4\u5206\u77e9\u9635\u7684\u8bad\u7ec3\u6570\u636e\u4e0b\u80fd\u751f\u6210\u6709\u6548\u4e14\u9690\u853d\u7684\u865a\u5047\u914d\u7f6e\u6587\u4ef6\uff0c\u4f46\u5728\u4fa7\u7279\u5f81\u5b58\u5728\u5e76\u88ab\u63a8\u8350\u7cfb\u7edf\u5229\u7528\u7684\u573a\u666f\u4e0b\u7f3a\u4e4f\u5168\u9762\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6269\u5c55Leg-UP\u6846\u67b6\uff0c\u589e\u5f3a\u751f\u6210\u5668\u67b6\u6784\u4ee5\u6574\u5408\u4fa7\u7279\u5f81\uff0c\u751f\u6210\u4fa7\u7279\u5f81\u611f\u77e5\u7684\u865a\u5047\u7528\u6237\u914d\u7f6e\u6587\u4ef6\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9690\u853d\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u653b\u51fb\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u4fa7\u7279\u5f81\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u6076\u610f\u653b\u51fb\uff0c\u63d0\u5347\u653b\u51fb\u6548\u679c\u5e76\u4fdd\u6301\u9690\u853d\u6027\u3002"}}
{"id": "2509.18054", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18054", "abs": "https://arxiv.org/abs/2509.18054", "authors": ["Nikhil N S", "Amol Dilip Joshi", "Bilal Muhammed", "Soban Babu"], "title": "A Knowledge Graph-based Retrieval-Augmented Generation Framework for Algorithm Selection in the Facility Layout Problem", "comment": "10 pages, 5 figures", "summary": "Selecting a solution algorithm for the Facility Layout Problem (FLP), an\nNP-hard optimization problem with a multiobjective trade-off, is a complex task\nthat requires deep expert knowledge. The performance of a given algorithm\ndepends on specific problem characteristics such as its scale, objectives, and\nconstraints. This creates a need for a data-driven recommendation method to\nguide algorithm selection in automated design systems. This paper introduces a\nnew recommendation method to make such expertise accessible, based on a\nKnowledge Graph-based Retrieval-Augmented Generation (KG RAG) framework. To\naddress this, a domain-specific knowledge graph is constructed from published\nliterature. The method then employs a multi-faceted retrieval mechanism to\ngather relevant evidence from this knowledge graph using three distinct\napproaches, which include a precise graph-based search, flexible vector-based\nsearch, and high-level cluster-based search. The retrieved evidence is utilized\nby a Large Language Model (LLM) to generate algorithm recommendations with\ndata-driven reasoning. The proposed KG-RAG method is compared against a\ncommercial LLM chatbot with access to the knowledge base as a table, across a\nseries of diverse, real-world FLP test cases. Based on recommendation accuracy\nand reasoning capability, the proposed method performed significantly better\nthan the commercial LLM chatbot.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u68c0\u7d22\u589e\u5f3a\u751f\u6210(KG-RAG)\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bbe\u65bd\u5e03\u5c40\u95ee\u9898(FLP)\u7684\u7b97\u6cd5\u63a8\u8350\uff0c\u76f8\u6bd4\u5546\u4e1aLLM\u804a\u5929\u673a\u5668\u4eba\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u51c6\u786e\u6027\u548c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u8bbe\u65bd\u5e03\u5c40\u95ee\u9898\u662f\u4e00\u4e2aNP\u96be\u7684\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u7b97\u6cd5\u9009\u62e9\u9700\u8981\u6df1\u539a\u4e13\u5bb6\u77e5\u8bc6\u4e14\u6027\u80fd\u4f9d\u8d56\u5177\u4f53\u95ee\u9898\u7279\u5f81\uff0c\u9700\u8981\u6570\u636e\u9a71\u52a8\u7684\u63a8\u8350\u65b9\u6cd5\u6765\u6307\u5bfc\u81ea\u52a8\u5316\u8bbe\u8ba1\u7cfb\u7edf\u4e2d\u7684\u7b97\u6cd5\u9009\u62e9\u3002", "method": "\u6784\u5efa\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u56fe\u8c31\uff0c\u91c7\u7528\u591a\u7ef4\u5ea6\u68c0\u7d22\u673a\u5236\uff08\u56fe\u641c\u7d22\u3001\u5411\u91cf\u641c\u7d22\u3001\u805a\u7c7b\u641c\u7d22\uff09\u4ece\u77e5\u8bc6\u56fe\u8c31\u4e2d\u6536\u96c6\u76f8\u5173\u8bc1\u636e\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u57fa\u4e8e\u6570\u636e\u7684\u7b97\u6cd5\u63a8\u8350\u3002", "result": "\u5728\u591a\u6837\u5316\u771f\u5b9eFLP\u6d4b\u8bd5\u6848\u4f8b\u4e2d\uff0cKG-RAG\u65b9\u6cd5\u5728\u63a8\u8350\u51c6\u786e\u6027\u548c\u63a8\u7406\u80fd\u529b\u4e0a\u663e\u8457\u4f18\u4e8e\u5546\u4e1aLLM\u804a\u5929\u673a\u5668\u4eba\u3002", "conclusion": "KG-RAG\u6846\u67b6\u80fd\u591f\u6709\u6548\u5229\u7528\u9886\u57df\u77e5\u8bc6\u4e3a\u590d\u6742\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u6570\u636e\u9a71\u52a8\u7684\u7b97\u6cd5\u63a8\u8350\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.18091", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18091", "abs": "https://arxiv.org/abs/2509.18091", "authors": ["Sunhao Dai", "Jiakai Tang", "Jiahua Wu", "Kun Wang", "Yuxuan Zhu", "Bingjun Chen", "Bangyang Hong", "Yu Zhao", "Cong Fu", "Kangle Wu", "Yabo Ni", "Anxiang Zeng", "Wenjie Wang", "Xu Chen", "Jun Xu", "See-Kiong Ng"], "title": "OnePiece: Bringing Context Engineering and Reasoning to Industrial Cascade Ranking System", "comment": "OnePiece Technical Report; Applied in Shopee", "summary": "Despite the growing interest in replicating the scaled success of large\nlanguage models (LLMs) in industrial search and recommender systems, most\nexisting industrial efforts remain limited to transplanting Transformer\narchitectures, which bring only incremental improvements over strong Deep\nLearning Recommendation Models (DLRMs). From a first principle perspective, the\nbreakthroughs of LLMs stem not only from their architectures but also from two\ncomplementary mechanisms: context engineering, which enriches raw input queries\nwith contextual cues to better elicit model capabilities, and multi-step\nreasoning, which iteratively refines model outputs through intermediate\nreasoning paths. However, these two mechanisms and their potential to unlock\nsubstantial improvements remain largely underexplored in industrial ranking\nsystems.\n  In this paper, we propose OnePiece, a unified framework that seamlessly\nintegrates LLM-style context engineering and reasoning into both retrieval and\nranking models of industrial cascaded pipelines. OnePiece is built on a pure\nTransformer backbone and further introduces three key innovations: (1)\nstructured context engineering, which augments interaction history with\npreference and scenario signals and unifies them into a structured tokenized\ninput sequence for both retrieval and ranking; (2) block-wise latent reasoning,\nwhich equips the model with multi-step refinement of representations and scales\nreasoning bandwidth via block size; (3) progressive multi-task training, which\nleverages user feedback chains to effectively supervise reasoning steps during\ntraining. OnePiece has been deployed in the main personalized search scenario\nof Shopee and achieves consistent online gains across different key business\nmetrics, including over $+2\\%$ GMV/UU and a $+2.90\\%$ increase in advertising\nrevenue.", "AI": {"tldr": "OnePiece\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u5c06LLM\u98ce\u683c\u7684\u4e0a\u4e0b\u6587\u5de5\u7a0b\u548c\u63a8\u7406\u673a\u5236\u96c6\u6210\u5230\u5de5\u4e1a\u7ea7\u68c0\u7d22\u548c\u6392\u5e8f\u6a21\u578b\u4e2d\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\u5de5\u7a0b\u3001\u5757\u7ea7\u9690\u5f0f\u63a8\u7406\u548c\u6e10\u8fdb\u5f0f\u591a\u4efb\u52a1\u8bad\u7ec3\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u5de5\u4e1a\u63a8\u8350\u7cfb\u7edf\u4e3b\u8981\u5c40\u9650\u4e8e\u79fb\u690dTransformer\u67b6\u6784\uff0c\u800cLLM\u7684\u6210\u529f\u4e0d\u4ec5\u6765\u81ea\u67b6\u6784\uff0c\u8fd8\u6765\u81ea\u4e0a\u4e0b\u6587\u5de5\u7a0b\u548c\u591a\u6b65\u63a8\u7406\u673a\u5236\u3002\u8fd9\u4e9b\u673a\u5236\u5728\u5de5\u4e1a\u6392\u5e8f\u7cfb\u7edf\u4e2d\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u57fa\u4e8e\u7eafTransformer\u67b6\u6784\uff0c\u63d0\u51fa\u4e09\u4e2a\u521b\u65b0\uff1a\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\u5de5\u7a0b\uff08\u589e\u5f3a\u4ea4\u4e92\u5386\u53f2\uff09\u3001\u5757\u7ea7\u9690\u5f0f\u63a8\u7406\uff08\u591a\u6b65\u8868\u793a\u7ec6\u5316\uff09\u3001\u6e10\u8fdb\u5f0f\u591a\u4efb\u52a1\u8bad\u7ec3\uff08\u5229\u7528\u7528\u6237\u53cd\u9988\u94fe\u76d1\u7763\u63a8\u7406\u6b65\u9aa4\uff09\u3002", "result": "\u5728Shopee\u4e3b\u8981\u4e2a\u6027\u5316\u641c\u7d22\u573a\u666f\u90e8\u7f72\uff0c\u5b9e\u73b0\u4e00\u81f4\u5728\u7ebf\u6536\u76ca\uff1aGMV/UU\u63d0\u5347\u8d85\u8fc72%\uff0c\u5e7f\u544a\u6536\u5165\u589e\u52a02.90%\u3002", "conclusion": "OnePiece\u6846\u67b6\u6210\u529f\u5c06LLM\u7684\u4e0a\u4e0b\u6587\u5de5\u7a0b\u548c\u63a8\u7406\u673a\u5236\u5f15\u5165\u5de5\u4e1a\u63a8\u8350\u7cfb\u7edf\uff0c\u8bc1\u660e\u4e86\u8fd9\u4e9b\u673a\u5236\u5bf9\u63d0\u5347\u6392\u5e8f\u6027\u80fd\u7684\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2509.18095", "categories": ["cs.IR", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18095", "abs": "https://arxiv.org/abs/2509.18095", "authors": ["Zilin Xiao", "Qi Ma", "Mengting Gu", "Chun-cheng Jason Chen", "Xintao Chen", "Vicente Ordonez", "Vijai Mohan"], "title": "MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction", "comment": null, "summary": "Universal multimodal embedding models have achieved great success in\ncapturing semantic relevance between queries and candidates. However, current\nmethods either condense queries and candidates into a single vector,\npotentially limiting the expressiveness for fine-grained information, or\nproduce too many vectors that are prohibitively expensive for multi-vector\nretrieval. In this work, we introduce MetaEmbed, a new framework for multimodal\nretrieval that rethinks how multimodal embeddings are constructed and\ninteracted with at scale. During training, a fixed number of learnable Meta\nTokens are appended to the input sequence. At test-time, their last-layer\ncontextualized representations serve as compact yet expressive multi-vector\nembeddings. Through the proposed Matryoshka Multi-Vector Retrieval training,\nMetaEmbed learns to organize information by granularity across multiple\nvectors. As a result, we enable test-time scaling in multimodal retrieval,\nwhere users can balance retrieval quality against efficiency demands by\nselecting the number of tokens used for indexing and retrieval interactions.\nExtensive evaluations on the Massive Multimodal Embedding Benchmark (MMEB) and\nthe Visual Document Retrieval Benchmark (ViDoRe) confirm that MetaEmbed\nachieves state-of-the-art retrieval performance while scaling robustly to\nmodels with 32B parameters.", "AI": {"tldr": "MetaEmbed\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u6a21\u6001\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u53ef\u5b66\u4e60\u7684Meta Tokens\u6765\u751f\u6210\u7d27\u51d1\u4f46\u8868\u8fbe\u529b\u5f3a\u7684\u591a\u5411\u91cf\u5d4c\u5165\uff0c\u652f\u6301\u6d4b\u8bd5\u65f6\u6839\u636e\u6548\u7387\u9700\u6c42\u8c03\u6574\u68c0\u7d22\u8d28\u91cf", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5d4c\u5165\u65b9\u6cd5\u8981\u4e48\u5c06\u67e5\u8be2\u548c\u5019\u9009\u5bf9\u8c61\u538b\u7f29\u4e3a\u5355\u4e2a\u5411\u91cf\uff08\u9650\u5236\u7ec6\u7c92\u5ea6\u4fe1\u606f\u8868\u8fbe\uff09\uff0c\u8981\u4e48\u751f\u6210\u8fc7\u591a\u5411\u91cf\uff08\u591a\u5411\u91cf\u68c0\u7d22\u6210\u672c\u8fc7\u9ad8\uff09\uff0c\u9700\u8981\u4e00\u79cd\u5e73\u8861\u8868\u8fbe\u529b\u548c\u6548\u7387\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u5728\u8bad\u7ec3\u65f6\u5411\u8f93\u5165\u5e8f\u5217\u6dfb\u52a0\u56fa\u5b9a\u6570\u91cf\u7684\u53ef\u5b66\u4e60Meta Tokens\uff0c\u6d4b\u8bd5\u65f6\u4f7f\u7528\u5176\u6700\u540e\u4e00\u5c42\u7684\u4e0a\u4e0b\u6587\u8868\u793a\u4f5c\u4e3a\u591a\u5411\u91cf\u5d4c\u5165\u3002\u901a\u8fc7Matryoshka\u591a\u5411\u91cf\u68c0\u7d22\u8bad\u7ec3\uff0c\u5b66\u4e60\u6309\u7c92\u5ea6\u7ec4\u7ec7\u4fe1\u606f", "result": "\u5728MMEB\u548cViDoRe\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u68c0\u7d22\u6027\u80fd\uff0c\u80fd\u591f\u7a33\u5065\u6269\u5c55\u5230320\u4ebf\u53c2\u6570\u6a21\u578b", "conclusion": "MetaEmbed\u6846\u67b6\u5b9e\u73b0\u4e86\u6d4b\u8bd5\u65f6\u591a\u6a21\u6001\u68c0\u7d22\u7684\u53ef\u6269\u5c55\u6027\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u9009\u62e9\u7528\u4e8e\u7d22\u5f15\u548c\u68c0\u7d22\u4ea4\u4e92\u7684token\u6570\u91cf\u6765\u5e73\u8861\u68c0\u7d22\u8d28\u91cf\u548c\u6548\u7387\u9700\u6c42"}}
