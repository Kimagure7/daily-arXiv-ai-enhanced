<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 5]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [E-GEO: A Testbed for Generative Engine Optimization in E-Commerce](https://arxiv.org/abs/2511.20867)
*Puneet S. Bagga,Vivek F. Farias,Tamar Korkotashvili,Tianyi Peng,Yuhang Wu*

Main category: cs.IR

TL;DR: 提出了E-GEO基准，这是首个专门为电商生成引擎优化设计的基准，包含7000多个真实的多句消费者产品查询，并评估了15种常见的重写启发式方法，开发了一种轻量级迭代提示优化算法。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的兴起，生成引擎正在成为传统搜索的强大替代品，重塑检索任务。在电商领域，对话式购物代理现在引导消费者找到相关产品。这种转变产生了对生成引擎优化的需求，但目前的做法是临时的，其影响尤其是电商领域的影响仍知之甚少。

Method: 引入E-GEO基准，包含7000多个真实的多句消费者产品查询与相关商品列表配对。评估15种常见的重写启发式方法，并将生成引擎优化制定为一个可处理的优化问题，开发了一种轻量级的迭代提示优化算法。

Result: 优化的提示揭示了一个稳定、领域无关的模式，表明存在一个"普遍有效"的生成引擎优化策略。轻量级迭代提示优化算法显著优于基线方法。

Conclusion: E-GEO基准填补了电商生成引擎优化研究的空白，提出的优化算法和发现的通用模式为生成引擎优化提供了理论基础和实践指导。

Abstract: With the rise of large language models (LLMs), generative engines are becoming powerful alternatives to traditional search, reshaping retrieval tasks. In e-commerce, for instance, conversational shopping agents now guide consumers to relevant products. This shift has created the need for generative engine optimization (GEO)--improving content visibility and relevance for generative engines. Yet despite its growing importance, current GEO practices are ad hoc, and their impacts remain poorly understood, especially in e-commerce. We address this gap by introducing E-GEO, the first benchmark built specifically for e-commerce GEO. E-GEO contains over 7,000 realistic, multi-sentence consumer product queries paired with relevant listings, capturing rich intent, constraints, preferences, and shopping contexts that existing datasets largely miss. Using this benchmark, we conduct the first large-scale empirical study of e-commerce GEO, evaluating 15 common rewriting heuristics and comparing their empirical performance. To move beyond heuristics, we further formulate GEO as a tractable optimization problem and develop a lightweight iterative prompt-optimization algorithm that can significantly outperform these baselines. Surprisingly, the optimized prompts reveal a stable, domain-agnostic pattern--suggesting the existence of a "universally effective" GEO strategy. Our data and code are publicly available at https://github.com/psbagga17/E-GEO.

</details>


### [2] [Generating Querying Code from Text for Multi-Modal Electronic Health Record](https://arxiv.org/abs/2511.20904)
*Mengliang ZHang*

Main category: cs.IR

TL;DR: 提出了TQGen数据集和TQGen-EHRQuery框架，用于解决电子健康记录中复杂表格关系和专业术语导致的查询困难问题。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录包含大量结构化和非结构化数据，但复杂的表格关系和专业术语限制了查询准确性，增加了临床医生的工作负担。

Method: 构建TQGen数据集，提出TQGen-EHRQuery框架，包含医疗知识模块和问题模板匹配模块，引入工具集概念封装文本处理模块。

Result: 通过大量实验验证了数据集和工作流程的有效性，展示了在EHR系统中增强信息查询的潜力。

Conclusion: 该研究为电子健康记录中的自然语言查询生成提供了有效的解决方案，能够提高临床信息查询的效率和准确性。

Abstract: Electronic health records (EHR) contain extensive structured and unstructured data, including tabular information and free-text clinical notes. Querying relevant patient information often requires complex database operations, increasing the workload for clinicians. However, complex table relationships and professional terminology in EHRs limit the query accuracy. In this work, we construct a publicly available dataset, TQGen, that integrates both \textbf{T}ables and clinical \textbf{T}ext for natural language-to-query \textbf{Gen}eration. To address the challenges posed by complex medical terminology and diverse types of questions in EHRs, we propose TQGen-EHRQuery, a framework comprising a medical knowledge module and a questions template matching module. For processing medical text, we introduced the concept of a toolset, which encapsulates the text processing module as a callable tool, thereby improving processing efficiency and flexibility. We conducted extensive experiments to assess the effectiveness of our dataset and workflow, demonstrating their potential to enhance information querying in EHR systems.

</details>


### [3] [Beyond Patch Aggregation: 3-Pass Pyramid Indexing for Vision-Enhanced Document Retrieval](https://arxiv.org/abs/2511.21121)
*Anup Roy,Rishabh Gyanendra Upadhyay,Animesh Rameshbhai Panara,Robin Mills*

Main category: cs.IR

TL;DR: VisionRAG是一个免OCR、模型无关的多模态检索系统，通过金字塔索引框架直接处理文档图像，保留布局和空间线索，实现高效检索和问答。


<details>
  <summary>Details</summary>
Motivation: 传统基于OCR的文档检索流程维护成本高、对布局变化敏感且容易丢失空间线索。现有的视觉优先检索方法虽然性能好，但存在内存开销大和部署复杂的问题。

Method: 采用三阶段金字塔索引框架，直接索引文档图像，使用全局页面摘要、章节标题、视觉热点和事实级线索构建语义向量，通过互逆排名融合进行检索排名。

Result: 在金融文档基准测试中，FinanceBench上达到0.8051准确率（@10），TAT DQA上达到0.9629召回率（@100），每页仅存储17-27个向量。

Conclusion: 基于摘要引导的免OCR多模态检索是传统文本提取流程的实用且可扩展的替代方案。

Abstract: Document centric RAG pipelines usually begin with OCR, followed by brittle heuristics for chunking, table parsing, and layout reconstruction. These text first workflows are costly to maintain, sensitive to small layout shifts, and often lose the spatial cues that contain the answer. Vision first retrieval has emerged as a strong alternative. By operating directly on page images, systems like ColPali and ColQwen preserve structure and reduce pipeline complexity while achieving strong benchmark performance. However, these late interaction models tie retrieval to a specific vision backbone and require storing hundreds of patch embeddings per page, creating high memory overhead and complicating large scale deployment.
  We introduce VisionRAG, a multimodal retrieval system that is OCR free and model agnostic. VisionRAG indexes documents directly as images, preserving layout, tables, and spatial cues, and builds semantic vectors without committing to a specific extraction. Our three pass pyramid indexing framework creates vectors using global page summaries, section headers, visual hotspots, and fact level cues. These summaries act as lightweight retrieval surrogates. At query time, VisionRAG retrieves the most relevant pages using the pyramid index, then forwards the raw page image encoded as base64 to a multimodal LLM for final question answering. During retrieval, reciprocal rank fusion integrates signals across the pyramid to produce robust ranking.
  VisionRAG stores only 17 to 27 vectors per page, matching the efficiency of patch based methods while staying flexible across multimodal encoders. On financial document benchmarks, it achieves 0.8051 accuracy at 10 on FinanceBench and 0.9629 recall at 100 on TAT DQA. These results show that OCR free, summary guided multimodal retrieval is a practical and scalable alternative to traditional text extraction pipelines.

</details>


### [4] [FITRep: Attention-Guided Item Representation via MLLMs](https://arxiv.org/abs/2511.21389)
*Guoxiao Zhang,Ao Li,Tan Qu,Qianlong Xie,Xingxing Wang*

Main category: cs.IR

TL;DR: FITRep是一个基于注意力引导的白盒物品表示框架，通过概念层次信息提取、结构保持降维和FAISS聚类，解决多模态物品去重中的局部结构坍塌问题。


<details>
  <summary>Details</summary>
Motivation: 在线平台存在视觉和文本相似的近重复物品，导致用户体验下降。现有多模态大语言模型方法将表示视为黑盒，忽略了结构关系，造成局部结构坍塌问题。

Method: 提出FITRep框架：1) CHIE使用MLLMs提取层次化语义概念；2) SPDR采用自适应UMAP方法进行高效信息压缩；3) FBC基于FAISS聚类为每个物品分配唯一聚类ID。

Result: 在美团广告系统部署后，在线A/B测试显示点击率提升3.60%，千次展示成本提升4.25%，证明了方法的有效性和实际影响。

Conclusion: FITRep成功解决了多模态物品去重中的局部结构坍塌问题，在真实场景中显著提升了广告效果。

Abstract: Online platforms usually suffer from user experience degradation due to near-duplicate items with similar visuals and text. While Multimodal Large Language Models (MLLMs) enable multimodal embedding, existing methods treat representations as black boxes, ignoring structural relationships (e.g., primary vs. auxiliary elements), leading to local structural collapse problem. To address this, inspired by Feature Integration Theory (FIT), we propose FITRep, the first attention-guided, white-box item representation framework for fine-grained item deduplication. FITRep consists of: (1) Concept Hierarchical Information Extraction (CHIE), using MLLMs to extract hierarchical semantic concepts; (2) Structure-Preserving Dimensionality Reduction (SPDR), an adaptive UMAP-based method for efficient information compression; and (3) FAISS-Based Clustering (FBC), a FAISS-based clustering that assigns each item a unique cluster id using FAISS. Deployed on Meituan's advertising system, FITRep achieves +3.60% CTR and +4.25% CPM gains in online A/B tests, demonstrating both effectiveness and real-world impact.

</details>


### [5] [RIA: A Ranking-Infused Approach for Optimized listwise CTR Prediction](https://arxiv.org/abs/2511.21394)
*Guoxiao Zhang,Tan Qu,Ao Li,DongLin Ni,Qianlong Xie,Xingxing Wang*

Main category: cs.IR

TL;DR: RIA是一个统一的端到端重排序框架，通过共享排序和重排序的表示，结合点式和列表式评估，在保持低延迟的同时提升推荐质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法将排序和重排序解耦，导致列表式评估模型在严格延迟约束下存在组合稀疏性和有限表示能力的问题。

Method: 提出四个关键组件：UCDT用于细粒度用户-物品-上下文建模，CUHT用于位置敏感偏好学习，LMH捕获层次化物品依赖关系，EC模块在推理时平衡效率与效果。

Result: 在公开和工业数据集上超越最先进模型，AUC和LogLoss显著提升。在美团广告系统在线A/B测试中，CTR提升1.69%，CPM提升4.54%。

Conclusion: RIA通过统一架构有效解决了排序与重排序解耦的问题，实现了效果与效率的平衡，在工业场景中验证了其价值。

Abstract: Reranking improves recommendation quality by modeling item interactions. However, existing methods often decouple ranking and reranking, leading to weak listwise evaluation models that suffer from combinatorial sparsity and limited representational power under strict latency constraints. In this paper, we propose RIA (Ranking-Infused Architecture), a unified, end-to-end framework that seamlessly integrates pointwise and listwise evaluation. RIA introduces four key components: (1) the User and Candidate DualTransformer (UCDT) for fine-grained user-item-context modeling; (2) the Context-aware User History and Target (CUHT) module for position-sensitive preference learning; (3) the Listwise Multi-HSTU (LMH) module to capture hierarchical item dependencies; and (4) the Embedding Cache (EC) module to bridge efficiency and effectiveness during inference. By sharing representations across ranking and reranking, RIA enables rich contextual knowledge transfer while maintaining low latency. Extensive experiments show that RIA outperforms state-of-the-art models on both public and industrial datasets, achieving significant gains in AUC and LogLoss. Deployed in Meituan advertising system, RIA yields a +1.69% improvement in Click-Through Rate (CTR) and a +4.54% increase in Cost Per Mille (CPM) in online A/B tests.

</details>
