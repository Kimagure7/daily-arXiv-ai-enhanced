<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 52]
- [cs.IR](#cs.IR) [Total: 16]
- [cs.LG](#cs.LG) [Total: 51]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Structured Information Matters: Explainable ICD Coding with Patient-Level Knowledge Graphs](https://arxiv.org/abs/2509.09699)
*Mingyang Li,Viktor Schlegel,Tingting Mu,Warren Del-Pinto,Goran Nenadic*

Main category: cs.CL

TL;DR: 本文提出使用知识图谱结构化表示临床文档，用于自动化ICD编码任务，在减少文本量的同时保留关键信息，显著提升了编码性能


<details>
  <summary>Details</summary>
Motivation: 临床文档到标准化词汇的映射对于临床研究和患者护理至关重要，但手动编码耗时且难以扩展。现有方法主要关注输出编码表示，而输入文档的外部知识表示研究不足

Method: 构建文档级知识图谱来结构化表示输入文档，提供患者状况的全面结构化视图。将知识图谱集成到最先进的PLM-ICD架构中进行ICD-9编码

Result: 知识图谱仅使用原文本23%的内容就保留了90%的信息，在流行基准测试中Macro-F1分数提升高达3.20%，同时提高了训练效率

Conclusion: 知识图谱方法通过不同类型的实体和关系有效提升了ICD编码性能，相比纯文本基线具有更好的可解释性潜力

Abstract: Mapping clinical documents to standardised clinical vocabularies is an
important task, as it provides structured data for information retrieval and
analysis, which is essential to clinical research, hospital administration and
improving patient care. However, manual coding is both difficult and
time-consuming, making it impractical at scale. Automated coding can
potentially alleviate this burden, improving the availability and accuracy of
structured clinical data. The task is difficult to automate, as it requires
mapping to high-dimensional and long-tailed target spaces, such as the
International Classification of Diseases (ICD). While external knowledge
sources have been readily utilised to enhance output code representation, the
use of external resources for representing the input documents has been
underexplored. In this work, we compute a structured representation of the
input documents, making use of document-level knowledge graphs (KGs) that
provide a comprehensive structured view of a patient's condition. The resulting
knowledge graph efficiently represents the patient-centred input documents with
23\% of the original text while retaining 90\% of the information. We assess
the effectiveness of this graph for automated ICD-9 coding by integrating it
into the state-of-the-art ICD coding architecture PLM-ICD. Our experiments
yield improved Macro-F1 scores by up to 3.20\% on popular benchmarks, while
improving training efficiency. We attribute this improvement to different types
of entities and relationships in the KG, and demonstrate the improved
explainability potential of the approach over the text-only baseline.

</details>


### [2] [Cross-Layer Attention Probing for Fine-Grained Hallucination Detection](https://arxiv.org/abs/2509.09700)
*Malavika Suresh,Rahaf Aljundi,Ikechukwu Nkisi-Orji,Nirmalie Wiratunga*

Main category: cs.CL

TL;DR: 提出Cross-Layer Attention Probing (CLAP)方法，通过处理LLM整个残差流的激活作为联合序列，显著改进了幻觉检测效果，支持细粒度检测和检测后缓解策略。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在各种应用中的大规模采用，其生成不准确文本（幻觉）的倾向引发了可靠性担忧，需要有效的检测和缓解方法。

Method: 提出CLAP激活探测技术，将LLM整个残差流的激活作为联合序列进行处理，用于幻觉检测。在五个LLM和三个任务上进行实证评估。

Result: CLAP相比基线方法在贪婪解码和高温采样响应中都改进了幻觉检测，支持细粒度检测不同采样响应中的幻觉和非幻觉，并能在分布外保持高可靠性。

Conclusion: CLAP提供了一种有效的幻觉检测方法，支持检测后缓解策略，相比直接缓解方法能更好地减少幻觉并提高LLM可靠性。

Abstract: With the large-scale adoption of Large Language Models (LLMs) in various
applications, there is a growing reliability concern due to their tendency to
generate inaccurate text, i.e. hallucinations. In this work, we propose
Cross-Layer Attention Probing (CLAP), a novel activation probing technique for
hallucination detection, which processes the LLM activations across the entire
residual stream as a joint sequence. Our empirical evaluations using five LLMs
and three tasks show that CLAP improves hallucination detection compared to
baselines on both greedy decoded responses as well as responses sampled at
higher temperatures, thus enabling fine-grained detection, i.e. the ability to
disambiguate hallucinations and non-hallucinations among different sampled
responses to a given prompt. This allows us to propose a detect-then-mitigate
strategy using CLAP to reduce hallucinations and improve LLM reliability
compared to direct mitigation approaches. Finally, we show that CLAP maintains
high reliability even when applied out-of-distribution.

</details>


### [3] [Optimal Multi-Task Learning at Regularization Horizon for Speech Translation Task](https://arxiv.org/abs/2509.09701)
*JungHo Jung,Junhyun Lee*

Main category: cs.CL

TL;DR: 该论文从正则化角度研究端到端语音翻译中的多任务学习，通过分析跨模态一致性正则化、同模态R-drop和机器翻译损失系数三种正则化源，提出了在高维空间中寻找最优正则化轮廓的方法，在MuST-C数据集上取得了接近最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 端到端语音到文本翻译面临配对语音-文本数据稀缺的问题，需要利用机器翻译任务的双语文本数据通过多任务学习来克服这一局限性。

Method: 从正则化视角重新表述多任务学习，探索序列在模态内和跨模态的正则化方法，包括一致性正则化（不同模态）、R-drop（相同模态）以及机器翻译损失系数的正则化作用。

Result: 实验表明，通过在高维空间中寻找最优正则化轮廓（称为正则化地平线）来调整超参数，在MuST-C数据集上实现了接近最先进的性能。

Conclusion: 多任务学习中的三种正则化源（跨模态一致性、同模态R-drop和MT损失系数）共同构成了有效的正则化机制，通过优化这些正则化参数的组合可以显著提升语音翻译性能。

Abstract: End-to-end speech-to-text translation typically suffers from the scarcity of
paired speech-text data. One way to overcome this shortcoming is to utilize the
bitext data from the Machine Translation (MT) task and perform Multi-Task
Learning (MTL). In this paper, we formulate MTL from a regularization
perspective and explore how sequences can be regularized within and across
modalities. By thoroughly investigating the effect of consistency
regularization (different modality) and R-drop (same modality), we show how
they respectively contribute to the total regularization. We also demonstrate
that the coefficient of MT loss serves as another source of regularization in
the MTL setting. With these three sources of regularization, we introduce the
optimal regularization contour in the high-dimensional space, called the
regularization horizon. Experiments show that tuning the hyperparameters within
the regularization horizon achieves near state-of-the-art performance on the
MuST-C dataset.

</details>


### [4] [Creativity Benchmark: A benchmark for marketing creativity for LLM models](https://arxiv.org/abs/2509.09702)
*Ninad Bhat,Kieran Browne,Pip Bingemann*

Main category: cs.CL

TL;DR: Creativity Benchmark是一个评估大语言模型在营销创意领域表现的框架，包含100个品牌和3种提示类型，通过678名专业创意人员的11,012次匿名比较显示模型表现紧密聚集，没有单一模型在所有品牌或提示类型中占优。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型评估缺乏针对营销创意领域的专业基准，需要建立基于人类专家偏好的评估框架来衡量模型在品牌约束创意任务中的表现。

Method: 使用100个品牌（12个类别）和三种提示类型（洞察、想法、疯狂想法），通过678名专业创意人员进行11,012次匿名成对比较，采用Bradley-Terry模型分析，并计算余弦距离来评估模型多样性和对提示重构的敏感性。

Result: 模型表现紧密聚集（Δθ≈0.45），头对头获胜概率为61%；最高评分模型仅比最低评分模型高61%的胜率；LLM作为评判者与人类排名相关性弱且不一致；传统创造力测试仅部分适用于品牌约束任务。

Conclusion: 自动化评判无法替代人类专家评估，需要采用多样性感知的工作流程，强调人类专家评估在营销创意领域的重要性。

Abstract: We introduce Creativity Benchmark, an evaluation framework for large language
models (LLMs) in marketing creativity. The benchmark covers 100 brands (12
categories) and three prompt types (Insights, Ideas, Wild Ideas). Human
pairwise preferences from 678 practising creatives over 11,012 anonymised
comparisons, analysed with Bradley-Terry models, show tightly clustered
performance with no model dominating across brands or prompt types: the
top-bottom spread is $\Delta\theta \approx 0.45$, which implies a head-to-head
win probability of $0.61$; the highest-rated model beats the lowest only about
$61\%$ of the time. We also analyse model diversity using cosine distances to
capture intra- and inter-model variation and sensitivity to prompt reframing.
Comparing three LLM-as-judge setups with human rankings reveals weak,
inconsistent correlations and judge-specific biases, underscoring that
automated judges cannot substitute for human evaluation. Conventional
creativity tests also transfer only partially to brand-constrained tasks.
Overall, the results highlight the need for expert human evaluation and
diversity-aware workflows.

</details>


### [5] [CTCC: A Robust and Stealthy Fingerprinting Framework for Large Language Models via Cross-Turn Contextual Correlation Backdoor](https://arxiv.org/abs/2509.09703)
*Zhenhua Xu,Xixiang Zhao,Xubin Yue,Shengwei Tian,Changting Lin,Meng Han*

Main category: cs.CL

TL;DR: CTCC是一种新颖的基于规则的LLM指纹框架，通过多轮对话中的上下文关联来嵌入所有权验证痕迹，解决了现有方法在隐蔽性、鲁棒性和通用性方面的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)的广泛部署加剧了知识产权保护担忧，现有指纹方法存在可检测性、易受攻击和通用性差的问题，需要一种更可靠的解决方案。

Method: 提出CTCC框架，采用规则驱动的方法，在多轮对话中编码上下文关联（如反事实关系），而不是依赖词级或单轮触发，支持黑盒访问下的指纹验证。

Result: 在多种LLM架构上的广泛实验表明，CTCC相比先前工作始终实现更强的隐蔽性和鲁棒性，有效减少误报和指纹泄露风险。

Conclusion: CTCC为现实世界LLM部署场景中的所有权验证提供了一个可靠且实用的解决方案，支持在部分触发暴露情况下基于共享语义规则的持续构建。

Abstract: The widespread deployment of large language models (LLMs) has intensified
concerns around intellectual property (IP) protection, as model theft and
unauthorized redistribution become increasingly feasible. To address this,
model fingerprinting aims to embed verifiable ownership traces into LLMs.
However, existing methods face inherent trade-offs between stealthness,
robustness, and generalizability, being either detectable via distributional
shifts, vulnerable to adversarial modifications, or easily invalidated once the
fingerprint is revealed. In this work, we introduce CTCC, a novel rule-driven
fingerprinting framework that encodes contextual correlations across multiple
dialogue turns, such as counterfactual, rather than relying on token-level or
single-turn triggers. CTCC enables fingerprint verification under black-box
access while mitigating false positives and fingerprint leakage, supporting
continuous construction under a shared semantic rule even if partial triggers
are exposed. Extensive experiments across multiple LLM architectures
demonstrate that CTCC consistently achieves stronger stealth and robustness
than prior work. Our findings position CTCC as a reliable and practical
solution for ownership verification in real-world LLM deployment scenarios. Our
code and data are publicly available at <https://github.com/Xuzhenhua55/CTCC>.

</details>


### [6] [Temporal Preferences in Language Models for Long-Horizon Assistance](https://arxiv.org/abs/2509.09704)
*Ali Mazyaki,Mohammad Naghizadeh,Samaneh Ranjkhah Zonouzaghi,Hossein Setareh*

Main category: cs.CL

TL;DR: 研究语言模型在跨期选择中是否表现出未来导向与现在导向的偏好，以及这些偏好是否可以被系统性操纵。通过引入MTO指标来衡量模型时间偏好的可操纵性。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型的时间偏好特征，了解其是否能够像人类一样在跨期决策中表现出稳定的时间导向，以及这些偏好是否容易被外部提示所影响。

Method: 使用改编的人类实验协议，在时间权衡任务中评估多个语言模型，并与人类决策者进行基准比较。引入MTO（时间导向可操纵性）指标来量化模型在面向未来和面向现在提示下的偏好变化。

Result: 推理导向模型（如DeepSeek-Reasoner和grok-3-mini）在面向未来的提示下倾向于选择延迟选项，但在跨身份或地理位置的个性化决策方面表现有限。能够正确推理时间导向的模型会为自身作为AI决策者内化未来导向。

Conclusion: 研究为AI助手的设计提供了重要启示，需要与异质性的长期目标对齐，并提出了个性化情境校准和社会意识部署的研究议程。

Abstract: We study whether language models (LMs) exhibit future- versus
present-oriented preferences in intertemporal choice and whether those
preferences can be systematically manipulated. Using adapted human experimental
protocols, we evaluate multiple LMs on time-tradeoff tasks and benchmark them
against a sample of human decision makers. We introduce an operational metric,
the Manipulability of Time Orientation (MTO), defined as the change in an LM's
revealed time preference between future- and present-oriented prompts. In our
tests, reasoning-focused models (e.g., DeepSeek-Reasoner and grok-3-mini)
choose later options under future-oriented prompts but only partially
personalize decisions across identities or geographies. Moreover, models that
correctly reason about time orientation internalize a future orientation for
themselves as AI decision makers. We discuss design implications for AI
assistants that should align with heterogeneous, long-horizon goals and outline
a research agenda on personalized contextual calibration and socially aware
deployment.

</details>


### [7] [The Non-Determinism of Small LLMs: Evidence of Low Answer Consistency in Repetition Trials of Standard Multiple-Choice Benchmarks](https://arxiv.org/abs/2509.09705)
*Claudio Pinhanez,Paulo Cavalin,Cassia Sanctos,Marcelo Grave,Yago Primerano*

Main category: cs.CL

TL;DR: 本研究探讨了小型LLM（2B-8B参数）在重复回答相同问题时的稳定性，分析了不同温度设置、模型大小、微调状态等因素对答案一致性的影响，并提出了新的分析工具。


<details>
  <summary>Details</summary>
Motivation: 随着小型LLM的广泛应用，了解其在重复回答相同问题时的稳定性变得重要，这关系到模型在实际应用中的可靠性和可信度。

Method: 使用开源LLM对MMLU-Redux和MedQA基准测试中的问题进行10次重复回答，研究不同推理温度、模型大小（小型vs中型50B-80B）、微调状态等参数对答案一致性的影响。

Result: 小型模型在低推理温度下，能够一致回答的问题比例通常在50%-80%之间，一致性答案的准确性与总体准确性有合理相关性。中型模型显示出更高的答案一致性水平。

Conclusion: 模型大小和推理温度显著影响答案一致性，中型模型比小型模型表现更好，一致性答案的准确性可以作为模型整体性能的可靠指标。

Abstract: This work explores the consistency of small LLMs (2B-8B parameters) in
answering multiple times the same question. We present a study on known,
open-source LLMs responding to 10 repetitions of questions from the
multiple-choice benchmarks MMLU-Redux and MedQA, considering different
inference temperatures, small vs. medium models (50B-80B), finetuned vs. base
models, and other parameters. We also look into the effects of requiring
multi-trial answer consistency on accuracy and the trade-offs involved in
deciding which model best provides both of them. To support those studies, we
propose some new analytical and graphical tools. Results show that the number
of questions which can be answered consistently vary considerably among models
but are typically in the 50%-80% range for small models at low inference
temperatures. Also, accuracy among consistent answers seems to reasonably
correlate with overall accuracy. Results for medium-sized models seem to
indicate much higher levels of answer consistency.

</details>


### [8] [Beyond I'm Sorry, I Can't: Dissecting Large Language Model Refusal](https://arxiv.org/abs/2509.09708)
*Nirmalendu Prakash,Yeo Wei Jie,Amir Abdullah,Ranjan Satapathy,Erik Cambria,Roy Ka Wei Lee*

Main category: cs.CL

TL;DR: 使用稀疏自编码器分析指令调优大语言模型的拒绝行为机制，通过特征消融实现越狱，揭示安全行为的因果机制和冗余特征


<details>
  <summary>Details</summary>
Motivation: 理解指令调优大语言模型对有害提示拒绝行为的内在机制，目前对此行为的内部原因了解不足

Method: 在Gemma-2-2B-IT和LLaMA-3.1-8B-IT模型上训练稀疏自编码器，通过三阶段搜索流程：拒绝方向识别、贪婪过滤和交互发现，寻找导致拒绝行为的关键特征

Result: 成功识别出导致拒绝行为的关键特征集，通过特征消融实现模型从拒绝到合规的转变，发现存在冗余特征机制

Conclusion: 该方法为通过操作可解释潜在空间进行细粒度审计和针对性干预安全行为提供了潜力

Abstract: Refusal on harmful prompts is a key safety behaviour in instruction-tuned
large language models (LLMs), yet the internal causes of this behaviour remain
poorly understood. We study two public instruction-tuned models, Gemma-2-2B-IT
and LLaMA-3.1-8B-IT, using sparse autoencoders (SAEs) trained on
residual-stream activations. Given a harmful prompt, we search the SAE latent
space for feature sets whose ablation flips the model from refusal to
compliance, demonstrating causal influence and creating a jailbreak. Our search
proceeds in three stages: (1) Refusal Direction: find a refusal-mediating
direction and collect SAE features near that direction; (2) Greedy Filtering:
prune to a minimal set; and (3) Interaction Discovery: fit a factorization
machine (FM) that captures nonlinear interactions among the remaining active
features and the minimal set. This pipeline yields a broad set of
jailbreak-critical features, offering insight into the mechanistic basis of
refusal. Moreover, we find evidence of redundant features that remain dormant
unless earlier features are suppressed. Our findings highlight the potential
for fine-grained auditing and targeted intervention in safety behaviours by
manipulating the interpretable latent space.

</details>


### [9] [Assisting Research Proposal Writing with Large Language Models: Evaluation and Refinement](https://arxiv.org/abs/2509.09709)
*Jing Ren,Weiqi Wang*

Main category: cs.CL

TL;DR: 提出两个评估指标（内容质量和参考文献有效性）和基于分数的迭代提示方法，用于定量评估ChatGPT学术写作能力并提升其研究提案写作质量


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在学术写作中存在的错误或伪造参考文献等伦理问题，以及当前内容质量评估依赖主观人工判断、缺乏客观性的问题

Method: 提出内容质量和参考文献有效性两个关键评估指标，并基于这些指标的分数设计迭代提示方法

Result: 实验表明提出的指标为ChatGPT写作性能评估提供了客观定量框架，迭代提示显著提升内容质量并减少参考文献错误和伪造

Conclusion: 该方法有效解决了学术环境下LLMs写作的关键伦理挑战，提供了可靠的定量评估和改进机制

Abstract: Large language models (LLMs) like ChatGPT are increasingly used in academic
writing, yet issues such as incorrect or fabricated references raise ethical
concerns. Moreover, current content quality evaluations often rely on
subjective human judgment, which is labor-intensive and lacks objectivity,
potentially compromising the consistency and reliability. In this study, to
provide a quantitative evaluation and enhance research proposal writing
capabilities of LLMs, we propose two key evaluation metrics--content quality
and reference validity--and an iterative prompting method based on the scores
derived from these two metrics. Our extensive experiments show that the
proposed metrics provide an objective, quantitative framework for assessing
ChatGPT's writing performance. Additionally, iterative prompting significantly
enhances content quality while reducing reference inaccuracies and
fabrications, addressing critical ethical challenges in academic contexts.

</details>


### [10] [Generating Individual Travel Diaries Using Large Language Models Informed by Census and Land-Use Data](https://arxiv.org/abs/2509.09710)
*Sepehr Golrokh Amin,Devin Rhoads,Fatemeh Fakhrmoosavi,Nicholas E. Lownes,John N. Ivan*

Main category: cs.CL

TL;DR: 本研究提出了一种使用大型语言模型生成个体出行日记的方法，通过开源数据生成虚拟人物并合成出行记录，验证显示LLM在出行目的确定方面表现优异，整体真实性与传统方法相当。


<details>
  <summary>Details</summary>
Motivation: 传统基于大量专有家庭出行调查的方法成本高昂且数据获取困难，需要开发一种基于开源数据的替代方案来生成真实的个体出行日记。

Method: 使用美国社区调查和智能位置数据库的开源数据随机生成虚拟人物，通过直接提示LLM合成出行日记，并采用包含四个指标（出行次数、时间间隔、出行目的、交通方式）的综合真实度评分体系进行验证。

Result: LLM生成的日记整体真实度与传统方法相当（0.485 vs 0.455），在确定出行目的方面表现更优且一致性更好，传统方法在出行次数和活动时长估计方面略胜一筹。聚合验证显示LLM具有更好的统计代表性（0.612 vs 0.435）。

Conclusion: LLM方法在零样本条件下可行，为未来合成出行日记评估系统建立了可量化的真实度度量标准，展示了在交通建模中的应用潜力。

Abstract: This study introduces a Large Language Model (LLM) scheme for generating
individual travel diaries in agent-based transportation models. While
traditional approaches rely on large quantities of proprietary household travel
surveys, the method presented in this study generates personas stochastically
from open-source American Community Survey (ACS) and Smart Location Database
(SLD) data, then synthesizes diaries through direct prompting. This study
features a novel one-to-cohort realism score: a composite of four metrics (Trip
Count Score, Interval Score, Purpose Score, and Mode Score) validated against
the Connecticut Statewide Transportation Study (CSTS) diaries, matched across
demographic variables. The validation utilizes Jensen-Shannon Divergence to
measure distributional similarities between generated and real diaries. When
compared to diaries generated with classical methods (Negative Binomial for
trip generation; Multinomial Logit for mode/purpose) calibrated on the
validation set, LLM-generated diaries achieve comparable overall realism (LLM
mean: 0.485 vs. 0.455). The LLM excels in determining trip purpose and
demonstrates greater consistency (narrower realism score distribution), while
classical models lead in numerical estimates of trip count and activity
duration. Aggregate validation confirms the LLM's statistical
representativeness (LLM mean: 0.612 vs. 0.435), demonstrating LLM's zero-shot
viability and establishing a quantifiable metric of diary realism for future
synthetic diary evaluation systems.

</details>


### [11] [Psychiatry-Bench: A Multi-Task Benchmark for LLMs in Psychiatry](https://arxiv.org/abs/2509.09711)
*Aya E. Fouda,Abdelrahamn A. Hassan,Radwa J. Hanafy,Mohammed E. Fouda*

Main category: cs.CL

TL;DR: PsychiatryBench是一个基于权威精神病学教科书和案例集的专业基准测试，包含11个问答任务和5300多个专家标注项目，用于评估LLM在精神病学实践中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有评估资源主要依赖小型临床访谈语料库、社交媒体帖子或合成对话，临床有效性有限，无法捕捉精神病学推理的复杂性，需要更专业的评估基准。

Method: 基于权威专家验证的精神病学教科书和案例集构建基准测试，包含诊断推理、治疗计划等11个任务类型，使用传统指标和"LLM-as-judge"相似性评分框架评估多种前沿LLM。

Result: 结果显示在临床一致性和安全性方面存在显著差距，特别是在多轮随访和管理任务中，表明需要专门的模型调优和更强大的评估范式。

Conclusion: PsychiatryBench为高风险心理健康应用中LLM性能的基准测试和改进提供了一个模块化、可扩展的平台。

Abstract: Large language models (LLMs) hold great promise in enhancing psychiatric
practice, from improving diagnostic accuracy to streamlining clinical
documentation and therapeutic support. However, existing evaluation resources
heavily rely on small clinical interview corpora, social media posts, or
synthetic dialogues, which limits their clinical validity and fails to capture
the full complexity of psychiatric reasoning. In this work, we introduce
PsychiatryBench, a rigorously curated benchmark grounded exclusively in
authoritative, expert-validated psychiatric textbooks and casebooks.
PsychiatryBench comprises eleven distinct question-answering tasks ranging from
diagnostic reasoning and treatment planning to longitudinal follow-up,
management planning, clinical approach, sequential case analysis, and
multiple-choice/extended matching formats totaling over 5,300 expert-annotated
items. We evaluate a diverse set of frontier LLMs (including Google Gemini,
DeepSeek, LLaMA 3, and QWQ-32) alongside leading open-source medical models
(e.g., OpenBiloLLM, MedGemma) using both conventional metrics and an
"LLM-as-judge" similarity scoring framework. Our results reveal substantial
gaps in clinical consistency and safety, particularly in multi-turn follow-up
and management tasks, underscoring the need for specialized model tuning and
more robust evaluation paradigms. PsychiatryBench offers a modular, extensible
platform for benchmarking and improving LLM performance in high-stakes mental
health applications.

</details>


### [12] [The Thinking Therapist: Training Large Language Models to Deliver Acceptance and Commitment Therapy using Supervised Fine-Tuning and Odds Ratio Policy Optimization](https://arxiv.org/abs/2509.09712)
*Talha Tahir*

Main category: cs.CL

TL;DR: 本研究比较了SFT和ORPO两种训练方法对小型LLM进行ACT治疗能力的影响，发现ORPO训练模型在治疗保真度和共情方面显著优于SFT和基础模型，而COT推理仅对SFT模型有显著帮助。


<details>
  <summary>Details</summary>
Motivation: 研究第三代认知行为疗法ACT在小型开源大语言模型中的应用效果，探索不同训练方法和显式推理对模型治疗能力的影响。

Method: 使用Mistral-Large生成的50组合成ACT转录本，对Llama-3.2-3b-Instruct进行两种训练方法（SFT和ORPO），每种方法都包含有无COT推理步骤的变体，通过模拟治疗会话评估模型性能。

Result: ORPO训练模型在ACT保真度(χ²=185.15, p<.001)和治疗共情(χ²=140.37, p<.001)方面显著优于SFT和基础模型；COT仅对SFT模型有显著改善(平均提高2.68分，p<.001)。

Conclusion: 偏好对齐策略优化能有效培养小型LLM的ACT能力，显式推理的效用高度依赖于底层训练范式，ORPO通过学习治疗过程而非模仿内容获得优势。

Abstract: Acceptance and Commitment Therapy (ACT) is a third-wave cognitive behavioral
therapy with emerging evidence of efficacy in several psychiatric conditions.
This study investigates the impact of post-training methodology and explicit
reasoning on the ability of a small open-weight large language model (LLM) to
deliver ACT. Using 50 sets of synthetic ACT transcripts generated by
Mistral-Large, we trained Llama-3.2-3b-Instruct with two distinct approaches,
supervised fine-tuning (SFT) and odds ratio policy optimization (ORPO), each
with and without an explicit chain-of-thought (COT) reasoning step. Performance
was evaluated by comparing these four post-trained variants against the base
Instruct model. These models were benchmarked in simulated therapy sessions,
with performance quantitatively assessed on the ACT Fidelity Measure (ACT-FM)
and the Therapist Empathy Scale (TES) by an LLM judge that had been fine-tuned
on human evaluations. Our findings demonstrate that the ORPO-trained models
significantly outperformed both their SFT and Instruct counterparts on ACT
fidelity ($\chi^2(5) = 185.15, p < .001$) and therapeutic empathy ($\chi^2(5) =
140.37, p < .001$). The effect of COT was conditional as it provided a
significant benefit to SFT models, improving ACT-FM scores by an average of
2.68 points ($p < .001$), while offering no discernible advantage to the
superior ORPO or instruct-tuned variants. We posit that the superiority of ORPO
stems from its ability to learn the therapeutic `process' over imitating
`content,' a key aspect of ACT, while COT acts as a necessary scaffold for
models trained only via imitation. This study establishes that
preference-aligned policy optimization can effectively instill ACT competencies
in small LLMs, and that the utility of explicit reasoning is highly dependent
on the underlying training paradigm.

</details>


### [13] [HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented Generation for Multi-hop Question Answering](https://arxiv.org/abs/2509.09713)
*Duolin Sun,Dan Yang,Yue Shen,Yihan Jiao,Zhehao Tan,Jie Feng,Lianzhen Zhong,Jian Wang,Peng Wei,Jinjie Gu*

Main category: cs.CL

TL;DR: HANRAG是一个基于启发式的新框架，通过查询路由、子查询分解和噪声过滤来解决多跳查询中的迭代检索浪费和噪声积累问题，在单跳和多跳问答任务中表现优异


<details>
  <summary>Details</summary>
Motivation: 当前RAG方法在处理多跳查询时面临挑战：过度依赖迭代检索浪费步骤，原始复杂查询检索可能无法捕获相关子查询内容，导致噪声积累问题

Method: 提出HANRAG框架，使用强大的揭示器进行查询路由、将查询分解为子查询、并从检索文档中过滤噪声，增强系统适应性和抗噪能力

Result: 在多个基准测试中与领先行业方法比较，结果显示该框架在单跳和多跳问答任务中都获得了优越性能

Conclusion: HANRAG框架通过有效的查询处理和噪声管理，显著提升了RAG系统处理多样化查询的能力，特别是在复杂多跳查询场景下

Abstract: The Retrieval-Augmented Generation (RAG) approach enhances question-answering
systems and dialogue generation tasks by integrating information retrieval (IR)
technologies with large language models (LLMs). This strategy, which retrieves
information from external knowledge bases to bolster the response capabilities
of generative models, has achieved certain successes. However, current RAG
methods still face numerous challenges when dealing with multi-hop queries. For
instance, some approaches overly rely on iterative retrieval, wasting too many
retrieval steps on compound queries. Additionally, using the original complex
query for retrieval may fail to capture content relevant to specific
sub-queries, resulting in noisy retrieved content. If the noise is not managed,
it can lead to the problem of noise accumulation. To address these issues, we
introduce HANRAG, a novel heuristic-based framework designed to efficiently
tackle problems of varying complexity. Driven by a powerful revelator, HANRAG
routes queries, decomposes them into sub-queries, and filters noise from
retrieved documents. This enhances the system's adaptability and noise
resistance, making it highly capable of handling diverse queries. We compare
the proposed framework against other leading industry methods across various
benchmarks. The results demonstrate that our framework obtains superior
performance in both single-hop and multi-hop question-answering tasks.

</details>


### [14] [Towards Reliable and Interpretable Document Question Answering via VLMs](https://arxiv.org/abs/2509.10129)
*Alessio Chen,Simone Giovannini,Andrea Gemelli,Fabio Coppini,Simone Marinai*

Main category: cs.CL

TL;DR: DocExplainerV0是一个即插即用的边界框预测模块，将答案生成与空间定位解耦，用于提升视觉语言模型在文档中的答案定位能力


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在文档理解方面表现出色，但准确在文档中定位答案仍然是一个主要挑战，这限制了模型的可解释性和实际应用

Method: 引入DocExplainerV0模块，该模块与现有VLM解耦，可以应用于包括专有系统在内的各种视觉语言模型，无需微调即可实现空间定位

Result: 系统评估揭示了文本准确性与空间定位之间的差距，显示正确答案往往缺乏可靠的空间定位，建立了未来研究的基准

Conclusion: 该框架突出了现有模型的局限性，为开发更具可解释性和鲁棒性的文档信息提取视觉语言模型奠定了基础

Abstract: Vision-Language Models (VLMs) have shown strong capabilities in document
understanding, particularly in identifying and extracting textual information
from complex documents. Despite this, accurately localizing answers within
documents remains a major challenge, limiting both interpretability and
real-world applicability. To address this, we introduce
\textit{DocExplainerV0}, a plug-and-play bounding-box prediction module that
decouples answer generation from spatial localization. This design makes it
applicable to existing VLMs, including proprietary systems where fine-tuning is
not feasible. Through systematic evaluation, we provide quantitative insights
into the gap between textual accuracy and spatial grounding, showing that
correct answers often lack reliable localization. Our standardized framework
highlights these shortcomings and establishes a benchmark for future research
toward more interpretable and robust document information extraction VLMs.

</details>


### [15] [How Small Transformation Expose the Weakness of Semantic Similarity Measures](https://arxiv.org/abs/2509.09714)
*Serge Lionel Nikiema,Albérick Euraste Djire,Abdoul Aziz Bonkoungou,Micheline Bénédicte Moumoula,Jordan Samhi,Abdoul Kader Kabore,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.CL

TL;DR: 本研究系统评估了18种语义相似度测量方法，发现常用指标存在严重问题，某些方法错误地将语义相反内容识别为高度相似，而LLM方法在区分语义差异方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在代码搜索、API推荐等软件工程应用中广泛用于语义相似度评估，需要验证这些方法是否真正理解语义关系还是仅识别表面模式。

Method: 研究测试了18种不同方法（基于词、嵌入技术、LLM系统和结构感知算法），通过系统化测试框架对文本和代码应用受控变化来评估各方法处理不同类型语义关系的能力。

Result: 结果显示常用指标存在显著问题：某些嵌入方法错误识别语义相反内容为相似（高达99.9%），某些基于transformer的方法偶尔将相反含义评为比同义词更相似。嵌入方法性能差常源于距离计算方式，从欧几里得距离切换到余弦相似性使结果提升24-66%。LLM方法在区分语义差异方面表现更好，对真正不同含义给出低相似度分数（0.00-0.29），而嵌入方法错误地为不相似内容分配高分（0.82-0.99）。

Conclusion: 当前常用的语义相似度测量方法存在严重缺陷，LLM方法在语义理解方面表现优于传统嵌入方法，但所有方法都需要进一步改进以准确捕捉语义关系。

Abstract: This research examines how well different methods measure semantic
similarity, which is important for various software engineering applications
such as code search, API recommendations, automated code reviews, and
refactoring tools. While large language models are increasingly used for these
similarity assessments, questions remain about whether they truly understand
semantic relationships or merely recognize surface patterns.
  The study tested 18 different similarity measurement approaches, including
word-based methods, embedding techniques, LLM-based systems, and
structure-aware algorithms. The researchers created a systematic testing
framework that applies controlled changes to text and code to evaluate how well
each method handles different types of semantic relationships.
  The results revealed significant issues with commonly used metrics. Some
embedding-based methods incorrectly identified semantic opposites as similar up
to 99.9 percent of the time, while certain transformer-based approaches
occasionally rated opposite meanings as more similar than synonymous ones. The
study found that embedding methods' poor performance often stemmed from how
they calculate distances; switching from Euclidean distance to cosine
similarity improved results by 24 to 66 percent. LLM-based approaches performed
better at distinguishing semantic differences, producing low similarity scores
(0.00 to 0.29) for genuinely different meanings, compared to embedding methods
that incorrectly assigned high scores (0.82 to 0.99) to dissimilar content.

</details>


### [16] [Investigating Symbolic Triggers of Hallucination in Gemma Models Across HaluEval and TruthfulQA](https://arxiv.org/abs/2509.09715)
*Naveen Lamba,Sanju Tiwari,Manas Gaur*

Main category: cs.CL

TL;DR: 本研究识别并表征了导致大语言模型产生幻觉的关键符号属性，发现即使模型规模增大，符号元素（如修饰语和命名实体）仍然是导致幻觉的主要因素。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型的幻觉问题已被广泛研究，但导致模型内在易产生幻觉的属性尚未被识别和研究。本研究旨在识别这些关键属性，并定位模型内部机制的脆弱点。

Method: 使用HaluEval和TruthfulQA两个数据集，将现有的问答格式转换为多种其他格式，以确定符号属性是导致幻觉的原因。测试了Gemma-2系列不同规模的模型（2B、9B、27B）。

Result: Gemma-2-2B的平均幻觉率为79.0%，随着模型规模增大，幻觉率下降至73.6%（9B）和63.9%（27B）。但修饰语（84.76%-94.98%）和命名实体（83.87%-93.96%）在所有模型和数据集上仍然保持很高的幻觉率。

Conclusion: 符号元素持续混淆大语言模型，这表明无论模型规模如何，这些模型在处理此类输入时存在根本性弱点，符号属性是导致幻觉的内在脆弱性因素。

Abstract: Hallucination in Large Language Models (LLMs) is a well studied problem.
However, the properties that make LLM intrinsically vulnerable to
hallucinations have not been identified and studied. This research identifies
and characterizes the key properties, allowing us to pinpoint vulnerabilities
within the model's internal mechanisms. To solidify on these properties, we
utilized two established datasets, HaluEval and TruthfulQA and convert their
existing format of question answering into various other formats to narrow down
these properties as the reason for the hallucinations. Our findings reveal that
hallucination percentages across symbolic properties are notably high for
Gemma-2-2B, averaging 79.0% across tasks and datasets. With increased model
scale, hallucination drops to 73.6% for Gemma-2-9B and 63.9% for Gemma-2-27B,
reflecting a 15 percentage point reduction overall. Although the hallucination
rate decreases as the model size increases, a substantial amount of
hallucination caused by symbolic properties still persists. This is especially
evident for modifiers (ranging from 84.76% to 94.98%) and named entities
(ranging from 83.87% to 93.96%) across all Gemma models and both datasets.
These findings indicate that symbolic elements continue to confuse the models,
pointing to a fundamental weakness in how these LLMs process such
inputs--regardless of their scale.

</details>


### [17] [ALIGNS: Unlocking nomological networks in psychological measurement through a large language model](https://arxiv.org/abs/2509.09723)
*Kai R. Larsen,Sen Yan,Roland Müller,Lan Sang,Mikko Rönkkö,Ravi Starzl,Donald Edmondson*

Main category: cs.CL

TL;DR: ALIGNS是一个基于大语言模型的系统，用于生成包含55万+指标的综合nomological网络，解决心理学测量中构建理论网络的挑战，提供大规模nomological分析来补充传统验证方法。


<details>
  <summary>Details</summary>
Motivation: 心理测量对许多学科至关重要，但构建nomological网络（概念和测量关系的理论图谱）仍然是70年来的挑战，这导致临床试验可能无法检测治疗效果，公共政策可能针对错误的结果。

Method: 开发ALIGNS系统，使用经过验证的问卷测量训练大语言模型，生成三个综合nomological网络，包含心理学、医学、社会政策等领域的55万+指标。

Result: 1) NIH PROMIS焦虑和抑郁工具收敛为单一情绪困扰维度；2) 儿童气质测量识别出四个现有框架未捕捉的潜在维度，并质疑一个现有维度；3) 心理测量学专家评估显示系统具有重要性、可访问性和适用性。

Conclusion: ALIGNS是首个应用大语言模型解决测量验证基础问题的系统，可免费使用，为传统验证方法提供了大规模nomological分析的补充。

Abstract: Psychological measurement is critical to many disciplines. Despite advances
in measurement, building nomological networks, theoretical maps of how concepts
and measures relate to establish validity, remains a challenge 70 years after
Cronbach and Meehl proposed them as fundamental to validation. This limitation
has practical consequences: clinical trials may fail to detect treatment
effects, and public policy may target the wrong outcomes. We introduce Analysis
of Latent Indicators to Generate Nomological Structures (ALIGNS), a large
language model-based system trained with validated questionnaire measures.
ALIGNS provides three comprehensive nomological networks containing over
550,000 indicators across psychology, medicine, social policy, and other
fields. This represents the first application of large language models to solve
a foundational problem in measurement validation. We report classification
accuracy tests used to develop the model, as well as three evaluations. In the
first evaluation, the widely used NIH PROMIS anxiety and depression instruments
are shown to converge into a single dimension of emotional distress. The second
evaluation examines child temperament measures and identifies four potential
dimensions not captured by current frameworks, and questions one existing
dimension. The third evaluation, an applicability check, engages expert
psychometricians who assess the system's importance, accessibility, and
suitability. ALIGNS is freely available at nomologicalnetwork.org,
complementing traditional validation methods with large-scale nomological
analysis.

</details>


### [18] [DiTTO-LLM: Framework for Discovering Topic-based Technology Opportunities via Large Language Model](https://arxiv.org/abs/2509.09724)
*Wonyoung Kim,Sujeong Seo,Juhyun Lee*

Main category: cs.CL

TL;DR: 该论文提出了一个基于技术间时间关系的框架，用于识别新兴技术机会。该框架利用专利数据、文本挖掘和大型语言模型来分析技术主题的演变，并通过AI专利数据集验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 技术机会是推动技术进步、产业发展和创新的关键信息。为了有效识别新兴技术机会，需要建立系统性的分析框架来捕捉技术间的时序关系和演变模式。

Method: 提出基于专利数据的分析框架：1）从专利数据集中提取文本；2）将文本主题映射以发现技术间关系；3）通过追踪主题随时间变化来识别技术机会；4）利用大型语言模型提取主题，并使用提示工程支持机会发现。

Result: 使用美国专利商标局提供的AI专利数据集进行评估，实验结果表明人工智能技术正在向便于日常访问的形式演变，验证了该框架识别未来技术机会的潜力。

Conclusion: 所提出的框架能够有效识别新兴技术机会，展示了结合专利数据分析、文本挖掘和大型语言模型在技术机会发现方面的应用价值，为技术发展和创新提供了重要支持。

Abstract: Technology opportunities are critical information that serve as a foundation
for advancements in technology, industry, and innovation. This paper proposes a
framework based on the temporal relationships between technologies to identify
emerging technology opportunities. The proposed framework begins by extracting
text from a patent dataset, followed by mapping text-based topics to discover
inter-technology relationships. Technology opportunities are then identified by
tracking changes in these topics over time. To enhance efficiency, the
framework leverages a large language model to extract topics and employs a
prompt for a chat-based language model to support the discovery of technology
opportunities. The framework was evaluated using an artificial intelligence
patent dataset provided by the United States Patent and Trademark Office. The
experimental results suggest that artificial intelligence technology is
evolving into forms that facilitate everyday accessibility. This approach
demonstrates the potential of the proposed framework to identify future
technology opportunities.

</details>


### [19] [BIBERT-Pipe on Biomedical Nested Named Entity Linking at BioASQ 2025](https://arxiv.org/abs/2509.09725)
*Chunyu Li,Xindi Zheng,Siqi Liu*

Main category: cs.CL

TL;DR: 提出了一个轻量级的两阶段实体链接系统BIBERT-Pipe，用于处理生物医学文本中的多语言嵌套命名实体链接任务，在BioNNE 2025评测中排名第三


<details>
  <summary>Details</summary>
Motivation: 现有的生物医学实体链接基准主要针对英语和平坦提及，缺乏对多语言和嵌套提及的现实场景研究

Method: 采用两阶段检索-排序架构：检索阶段使用原始预训练模型，排序阶段进行领域特定微调；使用可学习的[Ms]/[Me]标签包装提及；通过三种数据源自动扩展训练语料

Result: 在BioNNE 2025多语言赛道中排名第三，证明了这些最小但原则性修改的有效性和竞争力

Conclusion: 该方法通过保持原始EL模型完整，仅修改三个任务对齐组件，成功解决了多语言生物医学嵌套实体链接问题，代码已开源

Abstract: Entity linking (EL) for biomedical text is typically benchmarked on
English-only corpora with flat mentions, leaving the more realistic scenario of
nested and multilingual mentions largely unexplored. We present our system for
the BioNNE 2025 Multilingual Biomedical Nested Named Entity Linking shared task
(English & Russian), closing this gap with a lightweight pipeline that keeps
the original EL model intact and modifies only three task-aligned components:
Two-stage retrieval-ranking. We leverage the same base encoder model in both
stages: the retrieval stage uses the original pre-trained model, while the
ranking stage applies domain-specific fine-tuning. Boundary cues. In the
ranking stage, we wrap each mention with learnable [Ms] / [Me] tags, providing
the encoder with an explicit, language-agnostic span before robustness to
overlap and nesting. Dataset augmentation. We also automatically expand the
ranking training corpus with three complementary data sources, enhancing
coverage without extra manual annotation. On the BioNNE 2025 leaderboard, our
two stage system, bilingual bert (BIBERT-Pipe), ranks third in the multilingual
track, demonstrating the effectiveness and competitiveness of these minimal yet
principled modifications. Code are publicly available at
https://github.com/Kaggle-Competitions-Code/BioNNE-L.

</details>


### [20] [Natural Language Translation of Formal Proofs through Informalization of Proof Steps and Recursive Summarization along Proof Structure](https://arxiv.org/abs/2509.09726)
*Seiji Hattori,Takuya Matsuzaki,Makoto Fujiwara*

Main category: cs.CL

TL;DR: 提出了一种利用LLM的非正式化和摘要能力将机器可验证的形式证明翻译成自然语言的方法，并在教科书证明和Lean证明库上验证了其可读性和准确性


<details>
  <summary>Details</summary>
Motivation: 为了将形式化的机器可验证证明转换为更易理解的自然语言形式，提高形式证明的可读性和可访问性

Method: 利用大型语言模型(LLMs)的非正式化(形式语言证明步骤的言语化)和摘要能力，对形式证明数据进行自然语言翻译

Result: 在本科教科书的形式证明数据上应用该方法，生成的自然语言证明与原始自然语言证明相比质量良好；在Lean证明助手的现有形式证明库上应用，能够输出高可读性和准确性的自然语言证明

Conclusion: 该方法能够有效将形式证明转换为高质量的自然语言证明，提高了形式证明的可理解性和实用性

Abstract: This paper proposes a natural language translation method for
machine-verifiable formal proofs that leverages the informalization
(verbalization of formal language proof steps) and summarization capabilities
of LLMs. For evaluation, it was applied to formal proof data created in
accordance with natural language proofs taken from an undergraduate-level
textbook, and the quality of the generated natural language proofs was analyzed
in comparison with the original natural language proofs. Furthermore, we will
demonstrate that this method can output highly readable and accurate natural
language proofs by applying it to existing formal proof library of the Lean
proof assistant.

</details>


### [21] [A Role-Aware Multi-Agent Framework for Financial Education Question Answering with LLMs](https://arxiv.org/abs/2509.09727)
*Andy Zhu,Yingjun Du*

Main category: cs.CL

TL;DR: 提出了一种基于多代理框架的金融问答系统，通过角色提示和检索增强生成技术，显著提升了金融领域问答的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在金融问答中难以处理复杂的多步定量推理和专业术语，需要专门的方法来提升金融问题解决能力。

Method: 使用多代理框架，包括基础生成器、证据检索器和专家评审器，结合检索增强生成技术从6本金融教科书中获取上下文证据。

Result: 相比零样本思维链基线，批判性精炼使答案准确率提高了6.6-8.3%，Gemini-2.0-Flash表现最佳，GPT-4o-mini达到与专业金融模型相当的性能。

Conclusion: 该方法为金融问答提供了一种经济高效的解决方案，并为多代理金融LLM系统的进一步研究提供了见解。

Abstract: Question answering (QA) plays a central role in financial education, yet
existing large language model (LLM) approaches often fail to capture the
nuanced and specialized reasoning required for financial problem-solving. The
financial domain demands multistep quantitative reasoning, familiarity with
domain-specific terminology, and comprehension of real-world scenarios. We
present a multi-agent framework that leverages role-based prompting to enhance
performance on domain-specific QA. Our framework comprises a Base Generator, an
Evidence Retriever, and an Expert Reviewer agent that work in a single-pass
iteration to produce a refined answer. We evaluated our framework on a set of
3,532 expert-designed finance education questions from Study.com, an online
learning platform. We leverage retrieval-augmented generation (RAG) for
contextual evidence from 6 finance textbooks and prompting strategies for a
domain-expert reviewer. Our experiments indicate that critique-based refinement
improves answer accuracy by 6.6-8.3% over zero-shot Chain-of-Thought baselines,
with the highest performance from Gemini-2.0-Flash. Furthermore, our method
enables GPT-4o-mini to achieve performance comparable to the finance-tuned
FinGPT-mt_Llama3-8B_LoRA. Our results show a cost-effective approach to
enhancing financial QA and offer insights for further research in multi-agent
financial LLM systems.

</details>


### [22] [A meta-analysis on the performance of machine-learning based language models for sentiment analysis](https://arxiv.org/abs/2509.09728)
*Elena Rohde,Jonas Klingwort,Christian Borgs*

Main category: cs.CL

TL;DR: 该论文通过荟萃分析评估了机器学习在Twitter情感分析中的性能，发现平均准确率为0.80，并指出整体准确率指标存在误导性，需要标准化报告规范。


<details>
  <summary>Details</summary>
Motivation: 评估机器学习在Twitter情感分析中的平均性能表现，分析研究间的异质性，并探讨研究特征如何影响模型性能，以促进更可靠的跨研究比较。

Method: 采用PRISMA指南进行文献检索，从20项研究中筛选出195个试验，使用双反正弦变换和三水平随机效应模型分析整体准确率等性能指标。

Result: AIC优化模型的平均整体准确率为0.80 [0.76, 0.84]，发现整体准确率因对类别不平衡和情感类别数量的敏感性而经常产生误导。

Conclusion: 需要规范模型性能报告标准，包括报告独立测试集的混淆矩阵，以实现跨研究的可靠比较，但目前这种做法远未普及。

Abstract: This paper presents a meta-analysis evaluating ML performance in sentiment
analysis for Twitter data. The study aims to estimate the average performance,
assess heterogeneity between and within studies, and analyze how study
characteristics influence model performance. Using PRISMA guidelines, we
searched academic databases and selected 195 trials from 20 studies with 12
study features. Overall accuracy, the most reported performance metric, was
analyzed using double arcsine transformation and a three-level random effects
model. The average overall accuracy of the AIC-optimized model was 0.80 [0.76,
0.84]. This paper provides two key insights: 1) Overall accuracy is widely used
but often misleading due to its sensitivity to class imbalance and the number
of sentiment classes, highlighting the need for normalization. 2) Standardized
reporting of model performance, including reporting confusion matrices for
independent test sets, is essential for reliable comparisons of ML classifiers
across studies, which seems far from common practice.

</details>


### [23] [MultimodalHugs: Enabling Sign Language Processing in Hugging Face](https://arxiv.org/abs/2509.09729)
*Gerard Sant,Zifan Jiang,Carlos Escolano,Amit Moryossef,Mathias Müller,Rico Sennrich,Sarah Ebling*

Main category: cs.CL

TL;DR: MultimodalHugs是一个基于Hugging Face构建的多模态框架，专门解决手语处理研究中的可复现性和灵活性不足问题，支持姿态估计数据和像素数据等多种模态。


<details>
  <summary>Details</summary>
Motivation: 手语处理研究面临复杂临时代码、低可复现性和不公平比较的问题，现有工具如Hugging Face不够灵活，无法无缝集成手语实验。

Method: 在Hugging Face基础上构建MultimodalHugs框架，增加抽象层以支持更多样化的数据模态和任务，继承Hugging Face生态系统的优势。

Result: 框架能够容纳多种模态数据，包括手语姿态估计数据和文本字符像素数据，通过定量实验验证了其适用性。

Conclusion: MultimodalHugs为手语处理研究提供了更灵活、可复现的实验框架，同时也适用于其他不符合Hugging Face标准模板的多模态用例。

Abstract: In recent years, sign language processing (SLP) has gained importance in the
general field of Natural Language Processing. However, compared to research on
spoken languages, SLP research is hindered by complex ad-hoc code,
inadvertently leading to low reproducibility and unfair comparisons. Existing
tools that are built for fast and reproducible experimentation, such as Hugging
Face, are not flexible enough to seamlessly integrate sign language
experiments. This view is confirmed by a survey we conducted among SLP
researchers.
  To address these challenges, we introduce MultimodalHugs, a framework built
on top of Hugging Face that enables more diverse data modalities and tasks,
while inheriting the well-known advantages of the Hugging Face ecosystem. Even
though sign languages are our primary focus, MultimodalHugs adds a layer of
abstraction that makes it more widely applicable to other use cases that do not
fit one of the standard templates of Hugging Face. We provide quantitative
experiments to illustrate how MultimodalHugs can accommodate diverse modalities
such as pose estimation data for sign languages, or pixel data for text
characters.

</details>


### [24] [Benchmarking Vision-Language Models on Chinese Ancient Documents: From OCR to Knowledge Reasoning](https://arxiv.org/abs/2509.09731)
*Haiyang Yu,Yuchuan Wu,Fan Shi,Lei Liao,Jinghui Lu,Xiaodong Ge,Han Wang,Minghan Zhuo,Xuecheng Wu,Xiang Fei,Hao Feng,Guozhi Tang,An-Lan Wang,Hanshen Zhu,Yangfan He,Quanhuan Liang,Liyuan Meng,Chao Feng,Can Huang,Jingqun Tang,Bin Li*

Main category: cs.CL

TL;DR: AncientDoc是首个针对中文古籍文档的基准测试，包含5个任务，涵盖14种文档类型、100多本书籍和约3000页内容，用于评估视觉语言模型在OCR到知识推理方面的能力。


<details>
  <summary>Details</summary>
Motivation: 中文古籍作为中华历史文化的重要载体，在数字化和理解方面面临挑战，现有基准主要关注英文印刷文本或简体中文，缺乏对中文古籍文档的评估标准。

Method: 构建AncientDoc基准，包含页面级OCR、白话翻译、推理问答、知识问答和语言变体问答五个任务，使用多指标评估主流视觉语言模型，并辅以人工对齐的大语言模型进行评分。

Result: 创建了包含14种文档类型、100多本书籍、约3000页内容的综合基准测试集。

Conclusion: AncientDoc填补了中文古籍文档评估的空白，为视觉语言模型在复杂古籍处理方面的能力提供了系统性的评估框架。

Abstract: Chinese ancient documents, invaluable carriers of millennia of Chinese
history and culture, hold rich knowledge across diverse fields but face
challenges in digitization and understanding, i.e., traditional methods only
scan images, while current Vision-Language Models (VLMs) struggle with their
visual and linguistic complexity. Existing document benchmarks focus on English
printed texts or simplified Chinese, leaving a gap for evaluating VLMs on
ancient Chinese documents. To address this, we present AncientDoc, the first
benchmark for Chinese ancient documents, designed to assess VLMs from OCR to
knowledge reasoning. AncientDoc includes five tasks (page-level OCR, vernacular
translation, reasoning-based QA, knowledge-based QA, linguistic variant QA) and
covers 14 document types, over 100 books, and about 3,000 pages. Based on
AncientDoc, we evaluate mainstream VLMs using multiple metrics, supplemented by
a human-aligned large language model for scoring.

</details>


### [25] [MCP-AgentBench: Evaluating Real-World Language Agent Performance with MCP-Mediated Tools](https://arxiv.org/abs/2509.09734)
*Zikang Guo,Benfeng Xu,Chiwei Zhu,Wentao Hong,Xiaorui Wang,Zhendong Mao*

Main category: cs.CL

TL;DR: MCP-AgentBench是一个专门针对MCP协议的综合基准测试，包含33个服务器、188个工具和600个查询，用于评估语言代理在工具交互中的真实性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试无法准确评估在MCP新范式下的真实代理性能，导致对其实际操作价值的扭曲认知和无法可靠区分能力差异。

Method: 建立了包含33个操作服务器和188个不同工具的MCP测试床；开发了600个系统设计的查询，分布在6个不同复杂度的交互类别中；引入了MCP-Eval这种以结果为导向的新型评估方法。

Result: 通过对领先语言代理的广泛实证评估，提供了基础性见解，展示了不同代理在MCP环境下的性能差异。

Conclusion: MCP-AgentBench为研究社区提供了一个标准化和可靠的框架，用于构建、验证和推进能够充分利用MCP变革性优势的代理，从而加速真正能力和互操作性AI系统的发展。

Abstract: The Model Context Protocol (MCP) is rapidly emerging as a pivotal open
standard, designed to enhance agent-tool integration and interoperability, and
is positioned to unlock a new era of powerful, interconnected, and genuinely
utilitarian agentic AI. However, despite MCP's growing adoption, existing
benchmarks often fail to capture real-world agent performance within this new
paradigm, leading to a distorted perception of their true operational value and
an inability to reliably differentiate proficiencies. To bridge this critical
evaluation gap, we introduce MCP-AgentBench -- a comprehensive benchmark
specifically engineered to rigorously assess language agent capabilities in
MCP-mediated tool interactions. Core contributions of MCP-AgentBench include:
the establishment of a robust MCP testbed comprising 33 operational servers
with 188 distinct tools; the development of a benchmark featuring 600
systematically designed queries distributed across 6 distinct categories of
varying interaction complexity; and the introduction of MCP-Eval, a novel
outcome-oriented evaluation methodology prioritizing real-world task success.
Through extensive empirical evaluation of leading language agents, we provide
foundational insights. MCP-AgentBench aims to equip the research community with
a standardized and reliable framework to build, validate, and advance agents
capable of fully leveraging MCP's transformative benefits, thereby accelerating
progress toward truly capable and interoperable AI systems.

</details>


### [26] [Discrimination by LLMs: Cross-lingual Bias Assessment and Mitigation in Decision-Making and Summarisation](https://arxiv.org/abs/2509.09735)
*Willem Huijzer,Jieying Chen*

Main category: cs.CL

TL;DR: 该研究分析了GPT-3.5和GPT-4o在决策和摘要任务中存在的背景、性别和年龄偏见，发现决策任务中存在显著偏见，而摘要任务偏见较小。研究还评估了跨语言偏见传播和提示指令缓解策略的效果。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在各领域的快速应用，人们对其可能加剧社会不平等和信息偏见的担忧日益增加，需要系统评估模型偏见及其缓解方法。

Method: 使用Tamkin等人(2023)数据集的荷兰语翻译版本，创建了151,200个决策任务提示和176,400个摘要任务提示，测试不同人口统计变量、指令、显著度水平和语言在GPT-3.5和GPT-4o上的表现。

Result: 两个模型在决策任务中都存在显著偏见，偏向女性、年轻年龄和特定背景（如非裔美国人）。摘要任务偏见较小，但GPT-3.5在英语中显示出显著的年龄相关差异。跨语言分析显示英荷偏见模式大体相似。最有效的缓解指令使最有利和最不利人口统计群体间的差距平均减少27%。

Conclusion: 研究强调需要谨慎采用大语言模型并进行特定情境的偏见测试，同时需要持续开发有效的缓解策略以确保AI的负责任部署。GPT-4o相比GPT-3.5在所有英语提示中都显示出偏见减少，表明新模型在基于提示的缓解方面具有特定潜力。

Abstract: The rapid integration of Large Language Models (LLMs) into various domains
raises concerns about societal inequalities and information bias. This study
examines biases in LLMs related to background, gender, and age, with a focus on
their impact on decision-making and summarization tasks. Additionally, the
research examines the cross-lingual propagation of these biases and evaluates
the effectiveness of prompt-instructed mitigation strategies. Using an adapted
version of the dataset by Tamkin et al. (2023) translated into Dutch, we
created 151,200 unique prompts for the decision task and 176,400 for the
summarisation task. Various demographic variables, instructions, salience
levels, and languages were tested on GPT-3.5 and GPT-4o. Our analysis revealed
that both models were significantly biased during decision-making, favouring
female gender, younger ages, and certain backgrounds such as the
African-American background. In contrast, the summarisation task showed minimal
evidence of bias, though significant age-related differences emerged for
GPT-3.5 in English. Cross-lingual analysis showed that bias patterns were
broadly similar between English and Dutch, though notable differences were
observed across specific demographic categories. The newly proposed mitigation
instructions, while unable to eliminate biases completely, demonstrated
potential in reducing them. The most effective instruction achieved a 27\% mean
reduction in the gap between the most and least favorable demographics.
Notably, contrary to GPT-3.5, GPT-4o displayed reduced biases for all prompts
in English, indicating the specific potential for prompt-based mitigation
within newer models. This research underscores the importance of cautious
adoption of LLMs and context-specific bias testing, highlighting the need for
continued development of effective mitigation strategies to ensure responsible
deployment of AI.

</details>


### [27] [HEFT: A Coarse-to-Fine Hierarchy for Enhancing the Efficiency and Accuracy of Language Model Reasoning](https://arxiv.org/abs/2509.09801)
*Brennen Hill*

Main category: cs.CL

TL;DR: HEFT是一种分层高效微调策略，结合了LoRA（权重空间）和ReFT（表示空间）两种PEFT方法，在BoolQ基准测试中仅用3个epoch就达到了85.17%的准确率，超越了单独使用LoRA（20个epoch，85.05%）或ReFT（20个epoch，83.36%）的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在专门推理任务上的适配受计算资源限制，虽然参数高效微调（PEFT）方法提供了解决方案，但不同方法在权重空间或表示空间中操作，本文研究这些范式的协同组合是否能解锁更好的性能和效率。

Method: 提出HEFT分层适配策略：首先使用LoRA在权重空间进行广泛的基础适配，然后使用ReFT对内部激活进行精确的外科式精炼，形成从粗到细的微调方式。

Result: 在Llama-2-7B模型和BoolQ基准测试中，HEFT仅用3个epoch就达到85.17%的准确率，优于LoRA-only（20个epoch，85.05%）和ReFT-only（20个epoch，83.36%）方法。

Conclusion: PEFT方法的精心组合是一种强大的算法创新，为提升语言模型推理能力提供了更高效有效的路径，以更少的计算预算获得更优结果，为适应大规模模型的复杂认知任务提供了原则性方法。

Abstract: The adaptation of large language models (LLMs) to specialized reasoning tasks
is fundamentally constrained by computational resources. Parameter-Efficient
Fine-Tuning (PEFT) methods have emerged as a powerful solution, yet the
landscape of these techniques is diverse, with distinct methods operating in
either the model's weight space or its representation space. This paper
investigates the hypothesis that a synergistic combination of these paradigms
can unlock superior performance and efficiency. We introduce HEFT (Hierarchical
Efficient Fine-Tuning), a novel hierarchical adaptation strategy that composes
two distinct PEFT methods in a coarse-to-fine manner: first, a broad,
foundational adaptation in the weight space using Low-Rank Adaptation (LoRA),
followed by a precise, surgical refinement of internal activations using
Representation Fine-Tuning (ReFT). We evaluate this approach by fine-tuning a
Llama-2-7B model on the BoolQ benchmark, a challenging dataset for inferential
reasoning. Our results reveal a profound synergistic effect. A model fine-tuned
for only three epochs with our HEFT strategy achieves an accuracy of 85.17\%,
exceeding the performance of models trained for 20 epochs with either LoRA-only
(85.05\%) or ReFT-only (83.36\%) methodologies. This work demonstrates that the
thoughtful composition of PEFT methods is a potent algorithmic innovation,
offering a more efficient and effective path toward advancing the reasoning
capabilities of language models. By achieving superior results with a fraction
of the computational budget, our findings present a principled approach to
overcoming the obstacles inherent in adapting large-scale models for complex
cognitive tasks.

</details>


### [28] [Pragmatic Frames Evoked by Gestures: A FrameNet Brasil Approach to Multimodality in Turn Organization](https://arxiv.org/abs/2509.09804)
*Helen de Andrade Abreu,Tiago Timponi Torrent,Ely Edison da Silva Matos*

Main category: cs.CL

TL;DR: 本文提出了一个通过语言和交互手势之间的相关性来建模多模态对话轮次组织的框架，基于语用框架如何被概念化和唤起的分析。


<details>
  <summary>Details</summary>
Motivation: 填补对话轮次组织中特定策略（尤其是手势）在机器学习可用数据集中的编码空白，研究面对面对话中手势在传递、获取和保持对话轮次中的作用。

Method: 开发了注释方法，在Frame2多模态数据集（已标注语义框架）基础上增加语用框架标注，分析巴西电视剧中的10个片段，观察非实验室环境下的交互手势使用。

Result: 确认面对面对话中使用手势作为传递、获取和保持对话轮次的工具，发现了之前未记录的手势变体，证明语用框架标注有助于更深入理解人类认知和语言。

Conclusion: 手势使用源于语用框架的概念化，涉及心理空间、概念整合和概念隐喻，语用框架标注为理解人类认知和语言提供了新的视角。

Abstract: This paper proposes a framework for modeling multimodal conversational turn
organization via the proposition of correlations between language and
interactive gestures, based on analysis as to how pragmatic frames are
conceptualized and evoked by communicators. As a means to provide evidence for
the analysis, we developed an annotation methodology to enrich a multimodal
dataset (annotated for semantic frames) with pragmatic frames modeling
conversational turn organization. Although conversational turn organization has
been studied by researchers from diverse fields, the specific strategies,
especially gestures used by communicators, had not yet been encoded in a
dataset that can be used for machine learning. To fill this gap, we enriched
the Frame2 dataset with annotations of gestures used for turn organization. The
Frame2 dataset features 10 episodes from the Brazilian TV series Pedro Pelo
Mundo annotated for semantic frames evoked in both video and text. This dataset
allowed us to closely observe how communicators use interactive gestures
outside a laboratory, in settings, to our knowledge, not previously recorded in
related literature. Our results have confirmed that communicators involved in
face-to-face conversation make use of gestures as a tool for passing, taking
and keeping conversational turns, and also revealed variations of some gestures
that had not been documented before. We propose that the use of these gestures
arises from the conceptualization of pragmatic frames, involving mental spaces,
blending and conceptual metaphors. In addition, our data demonstrate that the
annotation of pragmatic frames contributes to a deeper understanding of human
cognition and language.

</details>


### [29] [Topic-Guided Reinforcement Learning with LLMs for Enhancing Multi-Document Summarization](https://arxiv.org/abs/2509.09852)
*Chuyuan Li,Austin Xu,Shafiq Joty,Giuseppe Carenini*

Main category: cs.CL

TL;DR: 提出基于主题引导的强化学习方法改进多文档摘要的内容选择，通过主题奖励机制在GRPO框架中提升摘要与源文档的主题对齐度


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在单文档摘要中表现优异，但在多文档摘要中仍有改进空间，需要更好地整合多源信息并保持主题相关性

Method: 使用主题标签明确提示模型，在GRPO框架中引入新颖的主题奖励机制来衡量生成摘要与源文档的主题对齐程度

Result: 在Multi-News和Multi-XScience数据集上的实验结果表明，该方法 consistently优于强基线模型

Conclusion: 利用主题线索在多文档摘要中具有显著效果，主题引导的强化学习方法能有效提升内容选择质量

Abstract: A key challenge in Multi-Document Summarization (MDS) is effectively
integrating information from multiple sources while maintaining coherence and
topical relevance. While Large Language Models have shown impressive results in
single-document summarization, their performance on MDS still leaves room for
improvement. In this paper, we propose a topic-guided reinforcement learning
approach to improve content selection in MDS. We first show that explicitly
prompting models with topic labels enhances the informativeness of the
generated summaries. Building on this insight, we propose a novel topic reward
within the Group Relative Policy Optimization (GRPO) framework to measure topic
alignment between the generated summary and source documents. Experimental
results on the Multi-News and Multi-XScience datasets demonstrate that our
method consistently outperforms strong baselines, highlighting the
effectiveness of leveraging topical cues in MDS.

</details>


### [30] [Emulating Public Opinion: A Proof-of-Concept of AI-Generated Synthetic Survey Responses for the Chilean Case](https://arxiv.org/abs/2509.09871)
*Bastián González-Bustamante,Nando Verelst,Carla Cisternas*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型(LLM)生成合成调查回复的可靠性，通过与智利概率抽样调查的真实人类回复对比，发现LLM在信任类问题上表现优异(F1分数>0.90)，但在捕捉公众意见的细微差别方面仍存在挑战。


<details>
  <summary>Details</summary>
Motivation: LLM为调查研究提供了使用合成受访者模拟人类答案的新方法，可能减少测量和代表性误差，但其恢复聚合项目分布的程度以及可能复制训练数据中的社会刻板印象和偏见的问题仍需验证。

Method: 通过128个提示-模型-问题三元组生成189,696个合成配置文件，使用准确性、精确度、召回率和F1分数等性能指标进行元分析，测试关键社会人口统计维度上的偏见，评估了OpenAI的GPT系列、o系列推理模型以及Llama和Qwen检查点。

Result: 1) 合成回复在信任项目上表现优异(F1分数和准确性>0.90)；2) GPT-4o、GPT-4o-mini和Llama 4 Maverick在此任务上表现相当；3) 45-59岁受访者的合成-人类对齐度最高。LLM合成样本能够近似概率样本的回复，但存在显著的项目级异质性。

Conclusion: 基于LLM的合成样本可以近似概率样本的回复，但要完全捕捉公众意见的细微差别仍然具有挑战性，需要仔细校准和额外的分布测试来确保算法保真度和减少误差。

Abstract: Large Language Models (LLMs) offer promising avenues for methodological and
applied innovations in survey research by using synthetic respondents to
emulate human answers and behaviour, potentially mitigating measurement and
representation errors. However, the extent to which LLMs recover aggregate item
distributions remains uncertain and downstream applications risk reproducing
social stereotypes and biases inherited from training data. We evaluate the
reliability of LLM-generated synthetic survey responses against ground-truth
human responses from a Chilean public opinion probabilistic survey.
Specifically, we benchmark 128 prompt-model-question triplets, generating
189,696 synthetic profiles, and pool performance metrics (i.e., accuracy,
precision, recall, and F1-score) in a meta-analysis across 128
question-subsample pairs to test for biases along key sociodemographic
dimensions. The evaluation spans OpenAI's GPT family and o-series reasoning
models, as well as Llama and Qwen checkpoints. Three results stand out. First,
synthetic responses achieve excellent performance on trust items (F1-score and
accuracy > 0.90). Second, GPT-4o, GPT-4o-mini and Llama 4 Maverick perform
comparably on this task. Third, synthetic-human alignment is highest among
respondents aged 45-59. Overall, LLM-based synthetic samples approximate
responses from a probabilistic sample, though with substantial item-level
heterogeneity. Capturing the full nuance of public opinion remains challenging
and requires careful calibration and additional distributional tests to ensure
algorithmic fidelity and reduce errors.

</details>


### [31] [Large Language Models Meet Legal Artificial Intelligence: A Survey](https://arxiv.org/abs/2509.09969)
*Zhitian Hou,Zihan Ye,Nanli Zeng,Tianyong Hao,Kun Zeng*

Main category: cs.CL

TL;DR: 本文对法律AI领域的大语言模型进行了系统性综述，涵盖了16个法律LLM系列、47个LLM框架、15个基准测试和29个数据集，并分析了挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 推动法律人工智能领域的发展，为大语言模型在法律领域的应用提供系统性的指导和资源支持，帮助初学者入门并促进未来研究。

Method: 通过全面综述和分析现有法律LLM模型、框架、基准测试和数据集，系统梳理法律AI领域的技术现状和发展趋势。

Result: 构建了一个完整的法律AI资源体系，包括模型、框架、评估工具和数据资源，为研究者和实践者提供了全面的技术参考。

Conclusion: 大语言模型在法律AI领域具有巨大潜力，但仍面临诸多挑战，需要进一步研究和发展，本文为这一领域提供了系统性的基础和支持。

Abstract: Large Language Models (LLMs) have significantly advanced the development of
Legal Artificial Intelligence (Legal AI) in recent years, enhancing the
efficiency and accuracy of legal tasks. To advance research and applications of
LLM-based approaches in legal domain, this paper provides a comprehensive
review of 16 legal LLMs series and 47 LLM-based frameworks for legal tasks, and
also gather 15 benchmarks and 29 datasets to evaluate different legal
capabilities. Additionally, we analyse the challenges and discuss future
directions for LLM-based approaches in the legal domain. We hope this paper
provides a systematic introduction for beginners and encourages future research
in this field. Resources are available at
https://github.com/ZhitianHou/LLMs4LegalAI.

</details>


### [32] [CMHG: A Dataset and Benchmark for Headline Generation of Minority Languages in China](https://arxiv.org/abs/2509.09990)
*Guixian Xu,Zeli Su,Ziyin Zhang,Jianing Liu,XU Han,Ting Zhang,Yushuang Dong*

Main category: cs.CL

TL;DR: 该论文针对中国少数民族语言（藏语、维吾尔语、蒙古语）标题生成任务缺乏标注数据的问题，提出了CMHG数据集，包含20万条标注数据，并建立了高质量测试集作为基准。


<details>
  <summary>Details</summary>
Motivation: 中国少数民族语言因其独特的书写系统与国际标准不符，导致相关语料库严重缺乏，特别是在标题生成等监督任务中。这种数据稀缺性阻碍了相关NLP技术的发展。

Method: 构建了Chinese Minority Headline Generation (CMHG)数据集，包含10万条藏语、5万条维吾尔语和5万条蒙古语数据，专门用于标题生成任务。同时创建了由母语者标注的高质量测试集作为基准。

Result: 成功建立了包含20万条标注数据的少数民族语言标题生成数据集，为藏语、维吾尔语和蒙古语提供了首个专门的标题生成基准测试集。

Conclusion: CMHG数据集将成为推动中国少数民族语言标题生成研究的重要资源，有助于相关基准测试的发展，填补了该领域的数据空白。

Abstract: Minority languages in China, such as Tibetan, Uyghur, and Traditional
Mongolian, face significant challenges due to their unique writing systems,
which differ from international standards. This discrepancy has led to a severe
lack of relevant corpora, particularly for supervised tasks like headline
generation. To address this gap, we introduce a novel dataset, Chinese Minority
Headline Generation (CMHG), which includes 100,000 entries for Tibetan, and
50,000 entries each for Uyghur and Mongolian, specifically curated for headline
generation tasks. Additionally, we propose a high-quality test set annotated by
native speakers, designed to serve as a benchmark for future research in this
domain. We hope this dataset will become a valuable resource for advancing
headline generation in Chinese minority languages and contribute to the
development of related benchmarks.

</details>


### [33] [Unsupervised Hallucination Detection by Inspecting Reasoning Processes](https://arxiv.org/abs/2509.10004)
*Ponhvoan Srey,Xiaobao Wu,Anh Tuan Luu*

Main category: cs.CL

TL;DR: IRIS是一个无监督幻觉检测框架，利用LLM内部表征来识别生成内容中的事实错误，无需标注数据，计算成本低且适用于实时检测


<details>
  <summary>Details</summary>
Motivation: 现有无监督方法依赖与事实正确性无关的代理信号，导致检测偏向表面特征，限制了跨数据集和场景的泛化能力

Method: 通过提示LLM仔细验证陈述的真实性，获取其情境化嵌入作为特征，并将响应不确定性作为真实性的软伪标签

Result: 实验结果表明IRIS consistently outperforms existing unsupervised methods，在少量训练数据下也能良好工作

Conclusion: IRIS是一个完全无监督、计算成本低的方法，能够有效检测LLM生成的幻觉内容，适用于实时应用场景

Abstract: Unsupervised hallucination detection aims to identify hallucinated content
generated by large language models (LLMs) without relying on labeled data.
While unsupervised methods have gained popularity by eliminating
labor-intensive human annotations, they frequently rely on proxy signals
unrelated to factual correctness. This misalignment biases detection probes
toward superficial or non-truth-related aspects, limiting generalizability
across datasets and scenarios. To overcome these limitations, we propose IRIS,
an unsupervised hallucination detection framework, leveraging internal
representations intrinsic to factual correctness. IRIS prompts the LLM to
carefully verify the truthfulness of a given statement, and obtain its
contextualized embedding as informative features for training. Meanwhile, the
uncertainty of each response is considered a soft pseudolabel for truthfulness.
Experimental results demonstrate that IRIS consistently outperforms existing
unsupervised methods. Our approach is fully unsupervised, computationally low
cost, and works well even with few training data, making it suitable for
real-time detection.

</details>


### [34] [Multi-Intent Recognition in Dialogue Understanding: A Comparison Between Smaller Open-Source LLMs](https://arxiv.org/abs/2509.10010)
*Adnan Ahmad,Philine Kowol,Stefan Hillmann,Sebastian Möller*

Main category: cs.CL

TL;DR: 本文对开源大语言模型在多标签意图分类任务上的表现进行了全面分析，在MultiWOZ 2.1数据集上比较了LLama2-7B、Mistral-7B和Yi-6B模型，发现Mistral-7B在少样本设置中表现最佳，但监督学习的BERT模型仍优于生成式LLM。


<details>
  <summary>Details</summary>
Motivation: 研究开源大语言模型在消费级硬件上处理多标签意图分类任务的能力，为任务导向对话系统的自然语言理解提供实用框架。

Method: 使用MultiWOZ 2.1数据集，在少样本设置下（提示中包含20个示例）测试三个开源LLM，并与基于BERT的监督学习基线模型进行比较，评估多个性能指标。

Result: Mistral-7B-v0.1在14个意图类别中的11个上F分数表现最佳，加权平均F分数为0.50，但在整体性能上仍不及监督学习的BERT分类器。

Conclusion: 开源LLM在多标签意图分类任务上展现出潜力，Mistral-7B是少样本设置中的最佳选择，但传统监督学习方法在当前仍具有性能优势。

Abstract: In this paper, we provide an extensive analysis of multi-label intent
classification using Large Language Models (LLMs) that are open-source,
publicly available, and can be run in consumer hardware. We use the MultiWOZ
2.1 dataset, a benchmark in the dialogue system domain, to investigate the
efficacy of three popular open-source pre-trained LLMs, namely LLama2-7B-hf,
Mistral-7B-v0.1, and Yi-6B. We perform the classification task in a few-shot
setup, giving 20 examples in the prompt with some instructions. Our approach
focuses on the differences in performance of these models across several
performance metrics by methodically assessing these models on multi-label
intent classification tasks. Additionally, we compare the performance of the
instruction-based fine-tuning approach with supervised learning using the
smaller transformer model BertForSequenceClassification as a baseline. To
evaluate the performance of the models, we use evaluation metrics like
accuracy, precision, and recall as well as micro, macro, and weighted F1 score.
We also report the inference time, VRAM requirements, etc. The Mistral-7B-v0.1
outperforms two other generative models on 11 intent classes out of 14 in terms
of F-Score, with a weighted average of 0.50. It also has relatively lower
Humming Loss and higher Jaccard Similarity, making it the winning model in the
few-shot setting. We find BERT based supervised classifier having superior
performance compared to the best performing few-shot generative LLM. The study
provides a framework for small open-source LLMs in detecting complex
multi-intent dialogues, enhancing the Natural Language Understanding aspect of
task-oriented chatbots.

</details>


### [35] [Linguistic trajectories of bipolar disorder on social media](https://arxiv.org/abs/2509.10035)
*Laurin Plank,Armin Zlomuzica*

Main category: cs.CL

TL;DR: 该研究通过社交媒体语言分析，开发了一种确定双相情感障碍诊断时间的方法，追踪了诊断前后长达24年的语言变化轨迹，发现了与情绪障碍、共病等相关的语言特征，并观察到诊断后二十年间具有12个月周期性的情绪相关语言变化。


<details>
  <summary>Details</summary>
Motivation: 语言是情感障碍的重要标志物，但临床评估规模有限。社交媒体语言分析因其高时间分辨率和纵向范围而受到关注，需要开发方法来利用社交媒体数据进行大规模心理健康监测。

Method: 引入确定用户诊断时间的方法，分析双相情感障碍患者从诊断前3年到诊断后21年的语言轨迹，与单相抑郁症患者和非受影响用户进行对比研究。

Result: 发现双相情感障碍诊断伴随普遍的语言改变，反映情绪障碍、精神共病、物质滥用等特征；观察到诊断后二十年间反复出现的情绪相关语言变化，具有明显的12个月周期性；趋势证据表明女性用户的周期性变化更明显。

Conclusion: 研究结果提供了双相情感障碍急性和慢性阶段语言改变的证据，验证并扩展了利用社交媒体进行可扩展心理健康监测的最新努力。

Abstract: Language provides valuable markers of affective disorders such as bipolar
disorder (BD), yet clinical assessments remain limited in scale. In response,
analyses of social media (SM) language have gained prominence due to their high
temporal resolution and longitudinal scope. Here, we introduce a method to
determine the timing of users' diagnoses and apply it to study language
trajectories from 3 years before to 21 years after BD diagnosis - contrasted
with uses reporting unipolar depression (UD) and non-affected users (HC). We
show that BD diagnosis is accompanied by pervasive linguistic alterations
reflecting mood disturbance, psychiatric comorbidity, substance abuse,
hospitalization, medical comorbidities, unusual thought content, and
disorganized thought. We further observe recurring mood-related language
changes across two decades after the diagnosis, with a pronounced 12-month
periodicity suggestive of seasonal mood episodes. Finally, trend-level evidence
suggests an increased periodicity in users estimated to be female. In sum, our
findings provide evidence for language alterations in the acute and chronic
phase of BD. This validates and extends recent efforts leveraging SM for
scalable monitoring of mental health.

</details>


### [36] [!MSA at BAREC Shared Task 2025: Ensembling Arabic Transformers for Readability Assessment](https://arxiv.org/abs/2509.10040)
*Mohamed Basem,Mohamed Younes,Seif Ahmed,Abdelrahman Moustafa*

Main category: cs.CL

TL;DR: MSA团队在BAREC 2025阿拉伯语细粒度可读性评估任务中获胜，使用四个Transformer模型的置信度加权集成方法，通过数据增强和后处理技术，在六个赛道中均获得第一名。


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯语可读性评估中的类别不平衡和数据稀缺问题，提升细粒度可读性预测的准确性。

Method: 使用AraBERTv2、AraELECTRA、MARBERT和CAMeLBERT四个模型，采用不同损失函数进行微调；应用加权训练、高级预处理、SAMER语料库重新标注和Gemini 2.5 Flash生成约10,000个稀有级别样本的数据增强；通过针对性后处理校正预测分布偏差。

Result: 在句子级别达到87.5%的二次加权Kappa（QWK），在文档级别达到87.4%的QWK，后处理带来6.3%的QWK增益。

Conclusion: 模型和损失函数的多样性、置信度信息融合以及智能数据增强对于鲁棒的阿拉伯语可读性预测具有强大效果。

Abstract: We present MSAs winning system for the BAREC 2025 Shared Task on fine-grained
Arabic readability assessment, achieving first place in six of six tracks. Our
approach is a confidence-weighted ensemble of four complementary transformer
models (AraBERTv2, AraELECTRA, MARBERT, and CAMeLBERT) each fine-tuned with
distinct loss functions to capture diverse readability signals. To tackle
severe class imbalance and data scarcity, we applied weighted training,
advanced preprocessing, SAMER corpus relabeling with our strongest model, and
synthetic data generation via Gemini 2.5 Flash, adding about 10,000 rare-level
samples. A targeted post-processing step corrected prediction distribution
skew, delivering a 6.3 percent Quadratic Weighted Kappa (QWK) gain. Our system
reached 87.5 percent QWK at the sentence level and 87.4 percent at the document
level, demonstrating the power of model and loss diversity, confidence-informed
fusion, and intelligent augmentation for robust Arabic readability prediction.

</details>


### [37] [Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models](https://arxiv.org/abs/2509.10078)
*Dongmin Choi,Woojung Song,Jongwook Han,Eun-Ju Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 本文通过对比分析发现，传统心理测量问卷在测量大语言模型时存在生态效度不足、结果偏差、稳定性问题，建议避免使用


<details>
  <summary>Details</summary>
Motivation: 当前研究中常用传统心理测量问卷（如BFI、PVQ）来测量大语言模型的人格特质，但这些人类设计的问卷是否具有生态效度（ecological validity）存在疑问，需要比较分析传统问卷与生态有效问卷的差异

Method: 进行综合性的对比分析，比较传统心理测量问卷与生态有效问卷在测量大语言模型时的结果差异

Result: 发现传统问卷：1）产生与生态有效问卷显著不同的模型人格交差，偏离用户查询上下文中表达的心理特征；2）问项不够导致测量不稳定；3）造成模型拥有稳定构建的误导印象；4）对人设提示的模型产生夸张的人格描述

Conclusion: 建议谨慎使用传统心理测量问卷来测量大语言模型，这些问卷在生态效度、稳定性和准确性方面存在显著问题

Abstract: Researchers have applied established psychometric questionnaires (e.g., BFI,
PVQ) to measure the personality traits and values reflected in the responses of
Large Language Models (LLMs). However, concerns have been raised about applying
these human-designed questionnaires to LLMs. One such concern is their lack of
ecological validity--the extent to which survey questions adequately reflect
and resemble real-world contexts in which LLMs generate texts in response to
user queries. However, it remains unclear how established questionnaires and
ecologically valid questionnaires differ in their outcomes, and what insights
these differences may provide. In this paper, we conduct a comprehensive
comparative analysis of the two types of questionnaires. Our analysis reveals
that established questionnaires (1) yield substantially different profiles of
LLMs from ecologically valid ones, deviating from the psychological
characteristics expressed in the context of user queries, (2) suffer from
insufficient items for stable measurement, (3) create misleading impressions
that LLMs possess stable constructs, and (4) yield exaggerated profiles for
persona-prompted LLMs. Overall, our work cautions against the use of
established psychological questionnaires for LLMs. Our code will be released
upon publication.

</details>


### [38] [Querying Climate Knowledge: Semantic Retrieval for Scientific Discovery](https://arxiv.org/abs/2509.10087)
*Mustapha Adamu,Qi Zhang,Huitong Pan,Longin Jan Latecki,Eduard C. Dragut*

Main category: cs.CL

TL;DR: 构建了一个面向气候科学领域的知识图谱，支持语义查询和LLM集成，提升气候研究的信息检索效率


<details>
  <summary>Details</summary>
Motivation: 气候科学文献日益复杂和庞大，研究人员难以跨模型、数据集、区域和变量找到相关信息，需要更有效的知识访问方式

Method: 从气候出版物和科学文本构建领域特定知识图谱，支持Cypher查询语言进行结构化语义查询，并与大型语言模型集成构建RAG系统

Result: 知识图谱能够回答精确的连接问题，如特定区域验证的模型、与特定遥相关模式常用的数据集等，提高了信息发现的准确性

Conclusion: 该知识图谱超越了传统构建工作，展示了其在气候研究人员、模型开发者等依赖准确科学信息的实际应用价值，提升了透明度和可靠性

Abstract: The growing complexity and volume of climate science literature make it
increasingly difficult for researchers to find relevant information across
models, datasets, regions, and variables. This paper introduces a
domain-specific Knowledge Graph (KG) built from climate publications and
broader scientific texts, aimed at improving how climate knowledge is accessed
and used. Unlike keyword based search, our KG supports structured, semantic
queries that help researchers discover precise connections such as which models
have been validated in specific regions or which datasets are commonly used
with certain teleconnection patterns. We demonstrate how the KG answers such
questions using Cypher queries, and outline its integration with large language
models in RAG systems to improve transparency and reliability in
climate-related question answering. This work moves beyond KG construction to
show its real world value for climate researchers, model developers, and others
who rely on accurate, contextual scientific information.

</details>


### [39] [Arabic Large Language Models for Medical Text Generation](https://arxiv.org/abs/2509.10095)
*Abdulrahman Allam,Seif Ahmed,Ali Hamdi,Ammar Mohammed*

Main category: cs.CL

TL;DR: 本研究通过微调大型语言模型（如Mistral-7B、LLaMA-2-7B和GPT-2）来生成阿拉伯语医疗文本，旨在解决医院管理系统中的实时医疗建议问题，特别是在非正规输入和 underrepresented 语言方面。微调后的Mistral-7B模型在BERT评分中表现最佳，平均精确率、召回率和F1分数分别为68.5%、69.08%和68.5%。


<details>
  <summary>Details</summary>
Motivation: 现有的医院管理系统往往无法提供准确、实时的医疗建议，尤其是在处理非正规输入和 underrepresented 语言（如阿拉伯语方言）时存在局限。全球医疗系统面临 overcrowding、资源有限和紧急医疗可用性差等挑战，需要更高效的解决方案。

Method: 研究收集了来自社交媒体平台的真实医患对话数据集，涵盖多种阿拉伯语方言。对数据集进行清理和预处理后，微调了Mistral-7B-Instruct-v0.2、LLaMA-2-7B和GPT-2 Medium等先进生成模型，以优化医疗文本生成能力。

Result: 评估结果显示，微调后的Mistral-7B模型在BERT评分中表现最优，平均精确率68.5%、召回率69.08%、F1分数68.5%。比较基准测试和定性评估验证了系统能够对非正规输入产生连贯且相关的医疗回复。

Conclusion: 该研究展示了生成式人工智能在医院管理系统中的潜力，为全球医疗挑战（特别是在语言和文化多样化的环境中）提供了可扩展和适应性强的解决方案。基于LLM的方法能够提供准确的医疗建议、诊断、药物推荐和治疗计划。

Abstract: Efficient hospital management systems (HMS) are critical worldwide to address
challenges such as overcrowding, limited resources, and poor availability of
urgent health care. Existing methods often lack the ability to provide
accurate, real-time medical advice, particularly for irregular inputs and
underrepresented languages. To overcome these limitations, this study proposes
an approach that fine-tunes large language models (LLMs) for Arabic medical
text generation. The system is designed to assist patients by providing
accurate medical advice, diagnoses, drug recommendations, and treatment plans
based on user input. The research methodology required the collection of a
unique dataset from social media platforms, capturing real-world medical
conversations between patients and doctors. The dataset, which includes patient
complaints together with medical advice, was properly cleaned and preprocessed
to account for multiple Arabic dialects. Fine-tuning state-of-the-art
generative models, such as Mistral-7B-Instruct-v0.2, LLaMA-2-7B, and GPT-2
Medium, optimized the system's ability to generate reliable medical text.
Results from evaluations indicate that the fine-tuned Mistral-7B model
outperformed the other models, achieving average BERT (Bidirectional Encoder
Representations from Transformers) Score values in precision, recall, and
F1-scores of 68.5\%, 69.08\%, and 68.5\%, respectively. Comparative
benchmarking and qualitative assessments validate the system's ability to
produce coherent and relevant medical replies to informal input. This study
highlights the potential of generative artificial intelligence (AI) in
advancing HMS, offering a scalable and adaptable solution for global healthcare
challenges, especially in linguistically and culturally diverse environments.

</details>


### [40] [Scaling Arabic Medical Chatbots Using Synthetic Data: Enhancing Generative AI with Synthetic Patient Records](https://arxiv.org/abs/2509.10108)
*Abdulrahman Allam,Seif Ahmed,Ali Hamdi,Khaled Shaban*

Main category: cs.CL

TL;DR: 本研究提出了一种可扩展的合成数据增强策略，使用ChatGPT-4o和Gemini 2.5 Pro生成了80,000个阿拉伯语医学问答对，将训练语料扩展到100,000条记录，显著提升了阿拉伯医疗聊天机器人的性能。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语医疗聊天机器人的发展受到大规模高质量标注数据集稀缺的限制，先前基于20,000条社交媒体医患交互数据训练的模型在可扩展性和泛化能力方面存在局限。

Method: 使用ChatGPT-4o和Gemini 2.5 Pro生成80,000个上下文相关且医学连贯的合成问答对，经过语义过滤和人工验证后整合到训练流程中。对包括Mistral-7B和AraGPT2在内的五个LLM进行微调，并使用BERTScore指标和专家定性评估进行性能评估。

Result: ChatGPT-4o生成的数据在所有模型中 consistently 获得更高的F1分数和更少的幻觉现象。合成数据增强显著提升了模型性能。

Conclusion: 合成数据增强是提升低资源医学NLP领域特定语言模型的有效实用解决方案，为构建更具包容性、可扩展性和准确性的阿拉伯语医疗聊天机器人系统铺平了道路。

Abstract: The development of medical chatbots in Arabic is significantly constrained by
the scarcity of large-scale, high-quality annotated datasets. While prior
efforts compiled a dataset of 20,000 Arabic patient-doctor interactions from
social media to fine-tune large language models (LLMs), model scalability and
generalization remained limited. In this study, we propose a scalable synthetic
data augmentation strategy to expand the training corpus to 100,000 records.
Using advanced generative AI systems ChatGPT-4o and Gemini 2.5 Pro we generated
80,000 contextually relevant and medically coherent synthetic question-answer
pairs grounded in the structure of the original dataset. These synthetic
samples were semantically filtered, manually validated, and integrated into the
training pipeline. We fine-tuned five LLMs, including Mistral-7B and AraGPT2,
and evaluated their performance using BERTScore metrics and expert-driven
qualitative assessments. To further analyze the effectiveness of synthetic
sources, we conducted an ablation study comparing ChatGPT-4o and
Gemini-generated data independently. The results showed that ChatGPT-4o data
consistently led to higher F1-scores and fewer hallucinations across all
models. Overall, our findings demonstrate the viability of synthetic
augmentation as a practical solution for enhancing domain-specific language
models in-low resource medical NLP, paving the way for more inclusive,
scalable, and accurate Arabic healthcare chatbot systems.

</details>


### [41] [Prominence-aware automatic speech recognition for conversational speech](https://arxiv.org/abs/2509.10116)
*Julian Linke,Barbara Schuppler*

Main category: cs.CL

TL;DR: 本文研究了结合重音检测和语音识别的显著性感知自动语音识别系统，在奥地利德语对话数据上实现了85.53%的重音检测准确率，但未显著提升ASR性能。


<details>
  <summary>Details</summary>
Motivation: 研究如何将韵律重音信息整合到自动语音识别系统中，以提升语音理解的准确性，并为语言学研究和基于韵律的对话系统提供支持。

Method: 通过微调wav2vec2模型开发词级重音检测器，自动标注大型语料库的韵律重音，然后训练同时转录单词及其重音水平的显著性感知ASR系统。

Result: 集成重音信息后ASR性能与基线系统相当，但在识别正确的语句中重音检测准确率达到85.53%。

Conclusion: 基于transformer的模型能有效编码韵律信息，为韵律增强的ASR系统提供了新的贡献，具有语言学研究和韵律感知对话系统的应用潜力。

Abstract: This paper investigates prominence-aware automatic speech recognition (ASR)
by combining prominence detection and speech recognition for conversational
Austrian German. First, prominence detectors were developed by fine-tuning
wav2vec2 models to classify word-level prominence. The detector was then used
to automatically annotate prosodic prominence in a large corpus. Based on those
annotations, we trained novel prominence-aware ASR systems that simultaneously
transcribe words and their prominence levels. The integration of prominence
information did not change performance compared to our baseline ASR system,
while reaching a prominence detection accuracy of 85.53% for utterances where
the recognized word sequence was correct. This paper shows that
transformer-based models can effectively encode prosodic information and
represents a novel contribution to prosody-enhanced ASR, with potential
applications for linguistic research and prosody-informed dialogue systems.

</details>


### [42] [Population-Aligned Persona Generation for LLM-based Social Simulation](https://arxiv.org/abs/2509.10127)
*Zhengyu Hu,Zheyuan Xiao,Max Xiong,Yuxuan Lei,Tianfu Wang,Jianxun Lian,Kaize Ding,Ziang Xiao,Nicholas Jing Yuan,Xing Xie*

Main category: cs.CL

TL;DR: 本文提出了一个系统框架，用于生成高质量、与人口分布对齐的AI角色集，以解决大语言模型社会模拟中的人口偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型社会模拟研究主要关注智能体框架和模拟环境设计，往往忽视了角色生成的复杂性以及非代表性角色集可能引入的偏见。需要构建能够真实反映现实世界人口多样性和分布的角色集合。

Method: 1）利用大语言模型从长期社交媒体数据生成叙事角色；2）通过严格质量评估过滤低质量档案；3）应用重要性采样实现与参考心理测量分布（如大五人格特质）的全局对齐；4）引入任务特定模块将全局对齐的角色集适配到目标子群体。

Result: 大量实验表明，该方法显著减少了人口层面的偏差，能够为广泛的研究和政策应用提供准确、灵活的社会模拟。

Conclusion: 该框架为LLM驱动的社会模拟提供了高质量、人口对齐的角色集生成方法，解决了现有研究中角色集代表性不足的问题，具有重要的研究和应用价值。

Abstract: Recent advances in large language models (LLMs) have enabled human-like
social simulations at unprecedented scale and fidelity, offering new
opportunities for computational social science. A key challenge, however, is
the construction of persona sets that authentically represent the diversity and
distribution of real-world populations. Most existing LLM-based social
simulation studies focus primarily on designing agentic frameworks and
simulation environments, often overlooking the complexities of persona
generation and the potential biases introduced by unrepresentative persona
sets. In this paper, we propose a systematic framework for synthesizing
high-quality, population-aligned persona sets for LLM-driven social simulation.
Our approach begins by leveraging LLMs to generate narrative personas from
long-term social media data, followed by rigorous quality assessment to filter
out low-fidelity profiles. We then apply importance sampling to achieve global
alignment with reference psychometric distributions, such as the Big Five
personality traits. To address the needs of specific simulation contexts, we
further introduce a task-specific module that adapts the globally aligned
persona set to targeted subpopulations. Extensive experiments demonstrate that
our method significantly reduces population-level bias and enables accurate,
flexible social simulation for a wide range of research and policy
applications.

</details>


### [43] [Benchmark of stylistic variation in LLM-generated texts](https://arxiv.org/abs/2509.10179)
*Jiří Milička,Anna Marklová,Václav Cvrček*

Main category: cs.CL

TL;DR: 本研究使用Biber的多维分析法比较人类写作与大型语言模型生成文本的语域差异，创建了AI-Brown和AI-Koditex语料库，分析了16个前沿模型在不同设置下的表现，并建立了可解释的模型评估基准。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型生成文本与人类写作在语域特征上的系统性差异，特别是在英语和捷克语等训练数据较少的语言中，为模型评估提供可解释的维度基准。

Method: 应用Biber的多维分析法(MDA)，使用AI-Brown语料库（对应BE-21布朗家族语料库）和AI-Koditex捷克语料库，分析16个前沿LLM在不同设置和提示下的文本生成特征。

Result: 研究发现LLM在多个语域维度上与人类写作存在显著系统性差异，特别是基础模型与指令调优模型之间的表现差异明显，为模型比较提供了可量化的评估维度。

Conclusion: 研究成功建立了基于多维分析的LLM评估基准，能够系统性地比较不同模型在语域特征上的表现，为理解LLM文本生成能力提供了新的分析框架。

Abstract: This study investigates the register variation in texts written by humans and
comparable texts produced by large language models (LLMs). Biber's
multidimensional analysis (MDA) is applied to a sample of human-written texts
and AI-created texts generated to be their counterparts to find the dimensions
of variation in which LLMs differ most significantly and most systematically
from humans. As textual material, a new LLM-generated corpus AI-Brown is used,
which is comparable to BE-21 (a Brown family corpus representing contemporary
British English). Since all languages except English are underrepresented in
the training data of frontier LLMs, similar analysis is replicated on Czech
using AI-Koditex corpus and Czech multidimensional model. Examined were 16
frontier models in various settings and prompts, with emphasis placed on the
difference between base models and instruction-tuned models. Based on this, a
benchmark is created through which models can be compared with each other and
ranked in interpretable dimensions.

</details>


### [44] [Incongruent Positivity: When Miscalibrated Positivity Undermines Online Supportive Conversations](https://arxiv.org/abs/2509.10184)
*Leen Almajed,Abeer ALdayel*

Main category: cs.CL

TL;DR: 论文研究情感支持对话中不协调的积极回应问题，分析人类和LLM生成回应中的不恰当积极表达，特别是在高风险情境下LLM更容易产生不切实际的乐观回应。


<details>
  <summary>Details</summary>
Motivation: 研究情感支持对话中善意但可能适得其反的积极回应现象，这些回应可能显得轻描淡写、最小化问题或不切实际地乐观，影响对话效果。

Method: 收集Reddit真实用户-助手对话，按情感强度分类为轻度（关系紧张、一般建议）和重度（悲伤、焦虑），使用大语言模型生成额外回应，开发弱监督多标签分类器检测不协调积极回应类型。

Result: 发现LLM在高风险情境下更容易通过轻描淡写和最小化语气表现出不切实际的积极性，开发的多标签分类器在检测不同类型不协调积极性方面表现改善。

Conclusion: 需要超越生成通用积极回应，研究协调的支持措施来平衡积极情感与情感认同，为大语言模型与在线支持对话中的情感期望对齐提供见解，推动上下文感知和信任保护的在线对话系统发展。

Abstract: In emotionally supportive conversations, well-intended positivity can
sometimes misfire, leading to responses that feel dismissive, minimizing, or
unrealistically optimistic. We examine this phenomenon of incongruent
positivity as miscalibrated expressions of positive support in both human and
LLM generated responses. To this end, we collected real user-assistant
dialogues from Reddit across a range of emotional intensities and generated
additional responses using large language models for the same context. We
categorize these conversations by intensity into two levels: Mild, which covers
relationship tension and general advice, and Severe, which covers grief and
anxiety conversations. This level of categorization enables a comparative
analysis of how supportive responses vary across lower and higher stakes
contexts. Our analysis reveals that LLMs are more prone to unrealistic
positivity through dismissive and minimizing tone, particularly in high-stakes
contexts. To further study the underlying dimensions of this phenomenon, we
finetune LLMs on datasets with strong and weak emotional reactions. Moreover,
we developed a weakly supervised multilabel classifier ensemble (DeBERTa and
MentalBERT) that shows improved detection of incongruent positivity types
across two sorts of concerns (Mild and Severe). Our findings shed light on the
need to move beyond merely generating generic positive responses and instead
study the congruent support measures to balance positive affect with emotional
acknowledgment. This approach offers insights into aligning large language
models with affective expectations in the online supportive dialogue, paving
the way toward context-aware and trust preserving online conversation systems.

</details>


### [45] [Beyond Token Limits: Assessing Language Model Performance on Long Text Classification](https://arxiv.org/abs/2509.10199)
*Miklós Sebők,Viktor Kovács,Martin Bánóczy,Daniel Møller Eriksen,Nathalie Neptune,Philippe Roussille*

Main category: cs.CL

TL;DR: 本文比较了多种语言模型在处理长文本分类任务（特别是法律文件）时的性能，发现专门为长文本设计的Longformer模型并无明显优势，开源模型表现优于GPT变体。


<details>
  <summary>Details</summary>
Motivation: 现有主流语言模型（如BERT及其衍生模型）存在输入长度限制，无法有效处理长达数百页的法律文件和法案文本，这在社会科学的长文本分类任务中是一个亟待解决的问题。

Method: 在5种语言上实验比较了XLM-RoBERTa、Longformer、GPT-3.5和GPT-4模型，使用比较议程项目的21个政策主题标签进行多类分类任务。

Result: 专门为长文本预训练的Longformer模型没有显示出特别优势；开源模型在比较中表现优于GPT变体；类别层面的分析显示特定类别之间的支持度和内容重叠对长文本性能很重要。

Conclusion: 对于长文本分类任务，专门的长文本模型不一定优于通用模型，开源模型在某些情况下表现更好，类别间的相似性对分类性能有重要影响。

Abstract: The most widely used large language models in the social sciences (such as
BERT, and its derivatives, e.g. RoBERTa) have a limitation on the input text
length that they can process to produce predictions. This is a particularly
pressing issue for some classification tasks, where the aim is to handle long
input texts. One such area deals with laws and draft laws (bills), which can
have a length of multiple hundred pages and, therefore, are not particularly
amenable for processing with models that can only handle e.g. 512 tokens. In
this paper, we show results from experiments covering 5 languages with
XLM-RoBERTa, Longformer, GPT-3.5, GPT-4 models for the multiclass
classification task of the Comparative Agendas Project, which has a codebook of
21 policy topic labels from education to health care. Results show no
particular advantage for the Longformer model, pre-trained specifically for the
purposes of handling long inputs. The comparison between the GPT variants and
the best-performing open model yielded an edge for the latter. An analysis of
class-level factors points to the importance of support and substance overlaps
between specific categories when it comes to performance on long text inputs.

</details>


### [46] [SI-FACT: Mitigating Knowledge Conflict via Self-Improving Faithfulness-Aware Contrastive Tuning](https://arxiv.org/abs/2509.10208)
*Shengqiang Fu*

Main category: cs.CL

TL;DR: 提出Self Improving Faithfulness Aware Contrastive Tuning框架，通过自指导机制自动生成对比学习数据，提升LLM在知识冲突任务中的忠实度


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在知识密集型任务中经常生成不忠实的响应，倾向于依赖内部参数知识而非提供的上下文，存在知识冲突问题

Method: 使用自指导机制让基础LLM自动生成高质量结构化对比学习数据（锚样本、语义等价正样本、模拟不忠实场景的负样本），然后应用对比学习训练模型

Result: 在ECARE KRE和COSE KRE基准测试中，基于Llama3 8B Instruct的SI FACT模型将上下文召回率提高了6.2%，显著减少了对内部记忆的依赖

Conclusion: SI FACT在增强LLM上下文忠实度方面提供了强大的有效性和高数据效率，为构建更主动和可信的语言模型提供了实用途径

Abstract: Large Language Models often generate unfaithful responses in knowledge
intensive tasks due to knowledge conflict,that is,a preference for relying on
internal parametric knowledge rather than the provided context.To address this
issue,we propose a novel self improving framework,Self Improving Faithfulness
Aware Contrastive Tuning.The framework uses a self instruct mechanism that
allows the base LLM to automatically generate high quality,structured
contrastive learning data,including anchor samples,semantically equivalent
positive samples,and negative samples simulating unfaithful scenarios.This
approach significantly reduces the cost of manual
annotation.Subsequently,contrastive learning is applied to train the
model,enabling it to pull faithful responses closer and push unfaithful
responses farther apart in the representation space.Experiments on knowledge
conflict evaluation benchmarks ECARE KRE and COSE KRE show that the SI FACT
model based on Llama3 8B Instruct improves the Contextual Recall Rate by 6.2%
over the best baseline method,while significantly reducing dependence on
internal memory.The results indicate that SI FACT provides strong effectiveness
and high data efficiency in enhancing the contextual faithfulness of
LLMs,offering a practical pathway toward building more proactive and
trustworthy language models.

</details>


### [47] [Dropping Experts, Recombining Neurons: Retraining-Free Pruning for Sparse Mixture-of-Experts LLMs](https://arxiv.org/abs/2509.10377)
*Yixiao Zhou,Ziyu Zhao,Dongzhou Cheng,zhiliang wu,Jie Gui,Yi Yang,Fei Wu,Yu Cheng,Hehe Fan*

Main category: cs.CL

TL;DR: DERN是一个无需重新训练的任务无关框架，通过专家剪枝和神经元重组来减少SMoE模型的内存使用，在50%专家稀疏度下性能提升超过5%


<details>
  <summary>Details</summary>
Motivation: SMoE架构虽然计算高效，但仍需加载所有专家参数，导致内存使用高和部署困难。现有方法主要关注专家级操作，忽略了神经元级结构

Method: 三步框架：1) 使用路由器统计剪枝冗余专家；2) 将专家分解为神经元级片段并分配到最兼容的保留专家；3) 在保留专家内合并片段构建紧凑表示

Result: 在Mixtral、Qwen和DeepSeek SMoE模型上，常识推理和MMLU基准测试性能提升超过5%，专家数量和内存使用大幅减少

Conclusion: DERN有效解决了专家间神经元级语义冲突问题，实现了无需重新训练的高效专家剪枝和重建，使SMoE LLMs更易于实际部署

Abstract: Sparse Mixture-of-Experts (SMoE) architectures are widely used in large
language models (LLMs) due to their computational efficiency. However, though
only a few experts are activated for each token, SMoE still requires loading
all expert parameters, leading to high memory usage and challenges in
deployment. Previous work has tried to reduce the overhead by pruning and
merging experts, but primarily focused on expert-level operations, leaving
neuron-level structure underexplored. We propose DERN (Dropping Experts,
Recombining Neurons), a task-agnostic and retraining-free framework for expert
pruning and reconstruction. We observe that experts are often misaligned and
contain semantic conflicts at the neuron level, which poses challenges for
direct merging. To solve this, DERN works in three steps: it first prunes
redundant experts using router statistics; then it decomposes them into
neuron-level expert segments, assigning each segment to its most compatible
retained expert; and finally, it merges segments within each retained expert to
build a compact representation. Experiments on Mixtral, Qwen, and DeepSeek SMoE
models show that DERN improves performance by more than 5% on commonsense
reasoning and MMLU benchmarks under 50% expert sparsity, without extra
training. It also greatly reduces the number of experts and memory usage,
making SMoE LLMs easier to deploy in practice.

</details>


### [48] [Is In-Context Learning Learning?](https://arxiv.org/abs/2509.10414)
*Adrian de Wynter*

Main category: cs.CL

TL;DR: 本文通过大规模实证分析发现，上下文学习(ICL)确实构成一种学习范式，但其学习能力和泛化能力有限，对提示格式和分布变化敏感，自回归的临时编码机制不够鲁棒。


<details>
  <summary>Details</summary>
Motivation: 针对当前关于自回归模型通过上下文学习能否真正学习新任务的争议，本文旨在从数学和实证角度系统分析ICL的学习本质和局限性。

Method: 进行大规模ICL消融实验，控制记忆效应、预训练、分布偏移、提示风格和措辞等因素，分析不同条件下模型的表现。

Result: ICL是一种有效的学习范式，但在学习未见任务和泛化方面能力有限；当示例数量足够多时，准确率对示例分布、模型、提示风格和语言特征不敏感；ICL主要从提示中的规律性推断模式，导致分布敏感性，特别是在思维链等提示风格中。

Conclusion: 自回归模型的临时编码机制不是鲁棒的学习机制，表明其通用泛化能力有限，需要更深入的理论和实证研究来理解ICL的本质。

Abstract: In-context learning (ICL) allows some autoregressive models to solve tasks
via next-token prediction and without needing further training. This has led to
claims about these model's ability to solve (learn) unseen tasks with only a
few shots (exemplars) in the prompt. However, deduction does not always imply
learning, as ICL does not explicitly encode a given observation. Instead, the
models rely on their prior knowledge and the exemplars given, if any. We argue
that, mathematically, ICL does constitute learning, but its full
characterisation requires empirical work. We then carry out a large-scale
analysis of ICL ablating out or accounting for memorisation, pretraining,
distributional shifts, and prompting style and phrasing. We find that ICL is an
effective learning paradigm, but limited in its ability to learn and generalise
to unseen tasks. We note that, in the limit where exemplars become more
numerous, accuracy is insensitive to exemplar distribution, model, prompt
style, and the input's linguistic features. Instead, it deduces patterns from
regularities in the prompt, which leads to distributional sensitivity,
especially in prompting styles such as chain-of-thought. Given the varied
accuracies on formally similar tasks, we conclude that autoregression's ad-hoc
encoding is not a robust mechanism, and suggests limited all-purpose
generalisability.

</details>


### [49] [Long Context Automated Essay Scoring with Language Models](https://arxiv.org/abs/2509.10417)
*Christopher Ormerod,Gitit Kehat*

Main category: cs.CL

TL;DR: 评估多种改进的Transformer架构模型（XLNet、Longformer、ModernBERT、Mamba、Llama）在长文本自动作文评分中的表现，解决传统模型因长度限制需要截断文本而影响评分有效性的问题。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer模型有固定最大长度限制，而高年级学生的作文经常超过这个限制。截断处理会严重影响模型对作文组织结构等评分要素的完整评估能力，存在有效性担忧。

Method: 使用Kaggle ASAP 2.0数据集，评估多种改进架构的模型：XLNet、Longformer、ModernBERT、Mamba和Llama的微调版本，这些模型都对标准Transformer架构进行了修改以处理更长文本。

Result: 研究比较了不同改进模型在长文本作文评分任务上的表现，但具体结果数据未在摘要中提供。

Conclusion: 需要采用能够处理更长上下文的改进架构模型来解决自动作文评分中的文本长度限制问题，以保持评分标准的完整性和评估有效性。

Abstract: Transformer-based language models are architecturally constrained to process
text of a fixed maximum length. Essays written by higher-grade students
frequently exceed the maximum allowed length for many popular open-source
models. A common approach to addressing this issue when using these models for
Automated Essay Scoring is to truncate the input text. This raises serious
validity concerns as it undermines the model's ability to fully capture and
evaluate organizational elements of the scoring rubric, which requires long
contexts to assess. In this study, we evaluate several models that incorporate
architectural modifications of the standard transformer architecture to
overcome these length limitations using the Kaggle ASAP 2.0 dataset. The models
considered in this study include fine-tuned versions of XLNet, Longformer,
ModernBERT, Mamba, and Llama models.

</details>


### [50] [RefactorCoderQA: Benchmarking LLMs for Multi-Domain Coding Question Solutions in Cloud and Edge Deployment](https://arxiv.org/abs/2509.10436)
*Shadikur Rahman,Aroosa Hameed,Gautam Srivastava,Syed Muhammad Danish*

Main category: cs.CL

TL;DR: 提出云边协作的多智能体提示框架，包含GuideLLM、SolverLLM和JudgeLLM三个组件，并创建RefactorCoderQA基准测试，在多个编程领域达到76.84%的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在局限性，需要优化大语言模型的推理和问题解决能力，特别是在多领域编程任务中的表现。

Method: 采用云边协作架构，包含边缘部署的轻量级GuideLLM（提供方法指导）、云端SolverLLM（生成代码解决方案）和JudgeLLM（自动评估解决方案质量）。创建RefactorCoderQA基准测试，覆盖软件工程、数据科学、机器学习和自然语言处理等多个技术领域。

Result: 微调模型RefactorCoder-MoE达到76.84%的整体准确率，显著优于领先的开源和商业基线模型。人工评估验证了生成解决方案的可解释性、准确性和实用性。系统级指标（吞吐量和延迟）分析提供了性能特征的深入洞察。

Conclusion: 提出的云边协作多智能体框架有效提升了LLM在多领域编程任务中的性能，RefactorCoderQA基准测试为评估和增强LLM能力提供了全面工具。

Abstract: To optimize the reasoning and problem-solving capabilities of Large Language
Models (LLMs), we propose a novel cloud-edge collaborative architecture that
enables a structured, multi-agent prompting framework. This framework comprises
three specialized components: GuideLLM, a lightweight model deployed at the
edge to provide methodological guidance; SolverLLM, a more powerful model
hosted in the cloud responsible for generating code solutions; and JudgeLLM, an
automated evaluator for assessing solution correctness and quality. To evaluate
and demonstrate the effectiveness of this architecture in realistic settings,
we introduce RefactorCoderQA, a comprehensive benchmark designed to evaluate
and enhance the performance of Large Language Models (LLMs) across multi-domain
coding tasks. Motivated by the limitations of existing benchmarks,
RefactorCoderQA systematically covers various technical domains, including
Software Engineering, Data Science, Machine Learning, and Natural Language
Processing, using authentic coding challenges from Stack Overflow. Extensive
experiments reveal that our fine-tuned model, RefactorCoder-MoE, achieves
state-of-the-art performance, significantly outperforming leading open-source
and commercial baselines with an overall accuracy of 76.84%. Human evaluations
further validate the interpretability, accuracy, and practical relevance of the
generated solutions. In addition, we evaluate system-level metrics, such as
throughput and latency, to gain deeper insights into the performance
characteristics and trade-offs of the proposed architecture.

</details>


### [51] [DeepDive: Advancing Deep Search Agents with Knowledge Graphs and Multi-Turn RL](https://arxiv.org/abs/2509.10446)
*Rui Lu,Zhenyu Hou,Zihan Wang,Hanchen Zhang,Xiao Liu,Yujiang Li,Shi Feng,Jie Tang,Yuxiao Dong*

Main category: cs.CL

TL;DR: DeepDive是一个通过多轮强化学习增强大语言模型深度搜索能力的系统，使用知识图谱自动合成复杂问题，在BrowseComp基准上达到开源模型最佳性能


<details>
  <summary>Details</summary>
Motivation: 现有开源大语言模型在深度搜索任务中表现不佳，主要受限于长时程推理能力和缺乏足够难度的监督数据

Method: 1) 从开放知识图谱自动合成复杂难寻的问题 2) 应用端到端多轮强化学习增强LLMs的长时程深度搜索推理

Result: DeepDive-32B在BrowseComp基准上超越WebSailor、DeepSeek-R1-Browse和Search-o1等模型，达到开源竞争性最佳结果

Conclusion: 多轮强化学习训练显著提升深度搜索能力，支持测试时工具调用扩展和并行采样，所有资源已开源

Abstract: Augmenting large language models (LLMs) with browsing tools substantially
improves their potential as deep search agents to solve complex, real-world
tasks. Yet, open LLMs still perform poorly in such settings due to limited
long-horizon reasoning capacity with browsing tools and the lack of
sufficiently difficult supervised data. To address these challenges, we present
DeepDive to advance deep search agents. First, we propose a strategy to
automatically synthesize complex, difficult, and hard-to-find questions from
open knowledge graphs. Second, we apply end-to-end multi-turn reinforcement
learning (RL) to enhance LLMs' long-horizon reasoning with deep search.
Experiments show that DeepDive-32B achieves a new open-source competitive
result on BrowseComp, outperforming WebSailor, DeepSeek-R1-Browse, and
Search-o1. We demonstrate that multi-turn RL training improves deep search
ability and significantly contributes to the performance improvements across
multiple benchmarks. We observe that DeepDive enables test-time scaling of tool
calls and parallel sampling. All datasets, models, and code are publicly
available at https://github.com/THUDM/DeepDive.

</details>


### [52] [WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained Speech Recognition Transformers](https://arxiv.org/abs/2509.10452)
*Akshat Pandey,Karun Kumar,Raphael Tang*

Main category: cs.CL

TL;DR: WhisTLE是一种仅使用文本的语音识别模型适配方法，通过变分自编码器建模编码器输出并微调解码器，无需额外运行时成本，在多个数据集上显著降低词错误率


<details>
  <summary>Details</summary>
Motivation: 预训练语音识别模型如Whisper在处理未见词汇和方言时需要领域适配，但在许多实际场景中收集语音数据不切实际，因此需要仅使用文本的适配方法

Method: 提出WhisTLE方法：1）训练变分自编码器（VAE）从文本建模编码器输出；2）使用学习的文本到潜在编码器微调解码器；3）可选结合文本到语音（TTS）适配；4）推理时恢复原始编码器，无额外运行时成本

Result: 在四个域外数据集和四个ASR模型上，WhisTLE结合TTS相比仅使用TTS适配相对降低词错误率12.3%，在32个场景中的27个场景中优于所有非WhisTLE基线方法

Conclusion: WhisTLE提供了一种有效的文本only适配方法，能够显著提升预训练ASR模型在未见领域的性能，且不增加推理时的计算成本

Abstract: Pretrained automatic speech recognition (ASR) models such as Whisper perform
well but still need domain adaptation to handle unseen vocabulary and parlance.
In many real-world settings, collecting speech data is impractical,
necessitating text-only adaptation. We propose WhisTLE, a deeply supervised,
text-only adaptation method for pretrained encoder-decoder ASR models. WhisTLE
trains a variational autoencoder (VAE) to model encoder outputs from text and
fine-tunes the decoder using the learned text-to-latent encoder, optionally
combined with text-to-speech (TTS) adaptation. At inference, the original
encoder is restored, incurring no extra runtime cost. Across four out-of-domain
datasets and four ASR models, WhisTLE with TTS reduces word error rate (WER) by
12.3% relative to TTS-only adaptation and outperforms all non-WhisTLE baselines
in 27 of 32 scenarios.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [53] [DB3 Team's Solution For Meta KDD Cup' 25](https://arxiv.org/abs/2509.09681)
*Yikuan Xia,Jiazun Chen,Yirui Zhan,Suifeng Zhao,Weipeng Jiang,Chaorui Zhang,Wei Han,Bo Bai,Jun Gao*

Main category: cs.IR

TL;DR: 本文介绍了db3团队在KDD Cup'25 Meta CRAG-MM挑战赛中的获胜解决方案，该方案通过多模态检索管道和统一LLM调优方法，在三个任务中均取得优异成绩并赢得总冠军。


<details>
  <summary>Details</summary>
Motivation: 解决CRAG-MM挑战赛中的多模态、多轮问答基准问题，特别是处理图像索引知识图谱、网络来源和多轮对话的复杂检索需求，以及控制LLM幻觉问题。

Method: 开发了包含(1)针对不同任务的领域特定检索管道，处理图像索引知识图谱、网络来源和多轮对话；(2)使用SFT、DPO和RL进行高级拒绝训练的统一的LLM调优方法。

Result: 在Task 1中获得第2名，Task 2中获得第2名，Task 3中获得第1名，通过优越的第一人称视角处理赢得了自我中心查询方面的总冠军。

Conclusion: 该综合框架成功整合了专门化检索管道和先进的拒绝训练技术，在多模态多轮问答任务中表现出色，特别是在处理第一人称视角挑战方面具有卓越能力。

Abstract: This paper presents the db3 team's winning solution for the Meta CRAG-MM
Challenge 2025 at KDD Cup'25. Addressing the challenge's unique multi-modal,
multi-turn question answering benchmark (CRAG-MM), we developed a comprehensive
framework that integrates tailored retrieval pipelines for different tasks with
a unified LLM-tuning approach for hallucination control. Our solution features
(1) domain-specific retrieval pipelines handling image-indexed knowledge
graphs, web sources, and multi-turn conversations; and (2) advanced refusal
training using SFT, DPO, and RL. The system achieved 2nd place in Task 1, 2nd
place in Task 2, and 1st place in Task 3, securing the grand prize for
excellence in ego-centric queries through superior handling of first-person
perspective challenges.

</details>


### [54] [Faster and Memory-Efficient Training of Sequential Recommendation Models for Large Catalogs](https://arxiv.org/abs/2509.09682)
*Maxim Zhelnin,Dmitry Redko,Volkov Daniil,Anna Volodkevich,Petr Sokerin,Valeriy Shevchenko,Egor Shvetsov,Alexey Vasilev,Darya Denisova,Ruslan Izmailov,Alexey Zaytsev*

Main category: cs.IR

TL;DR: 提出了CCE-方法，一种GPU高效的交叉熵损失实现，可加速训练2倍并减少10倍以上内存消耗，使推荐系统能够使用更多负样本和更大批次提升模型性能


<details>
  <summary>Details</summary>
Motivation: 传统基于transformer的序列推荐模型训练时由于交叉熵损失导致内存消耗与商品目录大小、批次大小和序列长度成正比，工业GPU内存限制阻碍了使用更多负样本和更大批次来提升模型性能

Method: 开发CCE-方法，通过GPU高效实现交叉熵损失与负采样，使用Triton内核优化实现，减少显式内存需求

Result: 训练加速达2倍，内存消耗减少10倍以上，在大型商品目录数据集上相比原始PyTorch实现获得更好的准确性

Conclusion: CCE-方法有效解决了推荐系统训练中的内存瓶颈问题，证明了同时扩展负样本数量和批次大小比只最大化其中一个能获得更好的结果，并发布了高效实现的Triton内核

Abstract: Sequential recommendations (SR) with transformer-based architectures are
widely adopted in real-world applications, where SR models require frequent
retraining to adapt to ever-changing user preferences. However, training
transformer-based SR models often encounters a high computational cost
associated with scoring extensive item catalogs, often exceeding thousands of
items. This occurs mainly due to the use of cross-entropy loss, where peak
memory scales proportionally to catalog size, batch size, and sequence length.
Recognizing this, practitioners in the field of recommendation systems
typically address memory consumption by integrating the cross-entropy (CE) loss
with negative sampling, thereby reducing the explicit memory demands of the
final layer. However, a small number of negative samples would degrade model
performance, and as we demonstrate in our work, increasing the number of
negative samples and the batch size further improves the model's performance,
but rapidly starts to exceed industrial GPUs' size (~40Gb).
  In this work, we introduce the CCE- method, which offers a GPU-efficient
implementation of the CE loss with negative sampling. Our method accelerates
training by up to two times while reducing memory consumption by more than 10
times. Leveraging the memory savings afforded by using CCE- for model training,
it becomes feasible to enhance its accuracy on datasets with a large item
catalog compared to those trained with original PyTorch-implemented loss
functions. Finally, we perform an analysis of key memory-related
hyperparameters and highlight the necessity of a delicate balance among these
factors. We demonstrate that scaling both the number of negative samples and
batch size leads to better results rather than maximizing only one of them. To
facilitate further adoption of CCE-, we release a Triton kernel that
efficiently implements the proposed method.

</details>


### [55] [Forecasting Clicks in Digital Advertising: Multimodal Inputs and Interpretable Outputs](https://arxiv.org/abs/2509.09683)
*Briti Gangopadhyay,Zhao Wang,Shingo Takamatsu*

Main category: cs.IR

TL;DR: 提出了一种结合点击数据和文本日志的多模态预测框架，使用强化学习提升文本信息理解和模态融合效果，在准确性和推理质量上优于基线方法


<details>
  <summary>Details</summary>
Motivation: 传统时间序列模型仅依赖数值数据，忽略了关键词更新等文本元素中的丰富上下文信息，影响了点击量预测的准确性

Method: 多模态预测框架，结合点击数据和广告活动的文本日志，使用强化学习来改进文本信息理解和模态融合，同时生成人类可解释的解释

Result: 在大规模行业数据集上的实验表明，该方法在准确性和推理质量方面均优于基线模型

Conclusion: 多模态方法能够有效利用文本上下文信息，显著提升点击量预测性能，并为预测结果提供可解释性

Abstract: Forecasting click volume is a key task in digital advertising, influencing
both revenue and campaign strategy. Traditional time series models rely solely
on numerical data, often overlooking rich contextual information embedded in
textual elements, such as keyword updates. We present a multimodal forecasting
framework that combines click data with textual logs from real-world ad
campaigns and generates human-interpretable explanations alongside numeric
predictions. Reinforcement learning is used to improve comprehension of textual
information and enhance fusion of modalities. Experiments on a large-scale
industry dataset show that our method outperforms baselines in both accuracy
and reasoning quality.

</details>


### [56] [Text-to-SQL Oriented to the Process Mining Domain: A PT-EN Dataset for Query Translation](https://arxiv.org/abs/2509.09684)
*Bruno Yui Yamate,Thais Rodrigues Neubauer,Marcelo Fantinato,Sarajane Marques Peres*

Main category: cs.IR

TL;DR: text-2-SQL-4-PM是一个面向流程挖掘领域的双语文本到SQL基准数据集，包含1655个自然语言查询和205个SQL语句，用于评估文本到SQL转换系统。


<details>
  <summary>Details</summary>
Motivation: 为流程挖掘领域开发专门的文本到SQL数据集，解决该领域特有的专业词汇和单表关系结构挑战，使非SQL专家也能通过自然语言查询数据库。

Method: 通过专家手动整理、专业翻译和详细标注流程构建数据集，包括人类生成的释义查询，并使用GPT-3.5 Turbo进行基线研究验证可行性。

Result: 数据集支持文本到SQL实现的评估，展示了在流程挖掘领域的应用可行性，并为语义解析等自然语言处理任务提供更广泛的适用性。

Conclusion: text-2-SQL-4-PM数据集为流程挖掘领域的文本到SQL任务提供了有价值的基准资源，促进了自然语言查询数据库技术的发展和应用。

Abstract: This paper introduces text-2-SQL-4-PM, a bilingual (Portuguese-English)
benchmark dataset designed for the text-to-SQL task in the process mining
domain. Text-to-SQL conversion facilitates natural language querying of
databases, increasing accessibility for users without SQL expertise and
productivity for those that are experts. The text-2-SQL-4-PM dataset is
customized to address the unique challenges of process mining, including
specialized vocabularies and single-table relational structures derived from
event logs. The dataset comprises 1,655 natural language utterances, including
human-generated paraphrases, 205 SQL statements, and ten qualifiers. Methods
include manual curation by experts, professional translations, and a detailed
annotation process to enable nuanced analyses of task complexity. Additionally,
a baseline study using GPT-3.5 Turbo demonstrates the feasibility and utility
of the dataset for text-to-SQL applications. The results show that
text-2-SQL-4-PM supports evaluation of text-to-SQL implementations, offering
broader applicability for semantic parsing and other natural language
processing tasks.

</details>


### [57] [TalkPlayData 2: An Agentic Synthetic Data Pipeline for Multimodal Conversational Music Recommendation](https://arxiv.org/abs/2509.09685)
*Keunwoo Choi,Seungheon Doh,Juhan Nam*

Main category: cs.IR

TL;DR: TalkPlayData 2是一个通过多智能体LLM管道生成的合成数据集，用于多模态对话式音乐推荐，包含音频和图像的多模态能力，并通过实验验证了其在训练生成式音乐推荐模型方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决多模态对话式音乐推荐系统训练数据的稀缺问题，通过合成数据生成方法来创建高质量的多模态对话数据集。

Method: 使用多角色LLM智能体（Listener和Recsys LLM）进行对话生成，每个对话基于微调的对话目标，智能体具备音频和图像的多模态处理能力。

Result: 在LLM-as-a-judge和主观评估实验中，TalkPlayData 2在训练生成式音乐推荐模型的多个相关方面达到了预期目标。

Conclusion: TalkPlayData 2成功创建了一个有效的多模态对话音乐推荐合成数据集，并开源了数据集和生成代码，为相关研究提供了重要资源。

Abstract: We present TalkPlayData 2, a synthetic dataset for multimodal conversational
music recommendation generated by an agentic data pipeline. In TalkPlayData 2
pipeline, multiple large language model (LLM) agents are created under various
roles with specialized prompts and access to different parts of information,
and the chat data is acquired by logging the conversation between the Listener
LLM and the Recsys LLM. To cover various conversation scenarios, for each
conversation, the Listener LLM is conditioned on a finetuned conversation goal.
Finally, all the LLMs are multimodal with audio and images, allowing a
simulation of multimodal recommendation and conversation. In the LLM-as-a-judge
and subjective evaluation experiments, TalkPlayData 2 achieved the proposed
goal in various aspects related to training a generative recommendation model
for music. TalkPlayData 2 and its generation code are open-sourced at
https://talkpl.ai/talkplaydata2.html.

</details>


### [58] [GeoGPT.RAG Technical Report](https://arxiv.org/abs/2509.09686)
*Fei Huang,Fan Wu,Zeqing Zhang,Qihao Wang,Long Zhang,Grant Michael Boquet,Hongyang Chen*

Main category: cs.IR

TL;DR: GeoGPT是一个开源的大语言模型系统，专为地球科学研究设计，通过检索增强生成(RAG)技术从专业知识库中获取信息来提升回答的准确性和领域相关性。


<details>
  <summary>Details</summary>
Motivation: 为了解决通用大语言模型在地球科学领域专业知识不足的问题，开发一个能够提供准确、可信赖的地球科学领域专业回答的AI系统。

Method: 集成RAG技术，构建GeoGPT Library专业语料库；微调嵌入模型和排序模型来优化检索质量；支持用户上传个性化知识库；开源核心RAG组件GeoEmbedding和GeoReranker。

Result: 显著提升了系统在地球科学应用中的检索精度和输出质量，能够生成准确、上下文相关的专业回答。

Conclusion: GeoGPT通过RAG技术和领域优化，为地球科学家提供了强大的AI工具，体现了对开放科学、协作和透明度的承诺，推动了地球科学研究的发展。

Abstract: GeoGPT is an open large language model system built to advance research in
the geosciences. To enhance its domain-specific capabilities, we integrated
Retrieval Augmented Generation(RAG), which augments model outputs with relevant
information retrieved from an external knowledge source. GeoGPT uses RAG to
draw from the GeoGPT Library, a specialized corpus curated for geoscientific
content, enabling it to generate accurate, context-specific answers. Users can
also create personalized knowledge bases by uploading their own publication
lists, allowing GeoGPT to retrieve and respond using user-provided materials.
To further improve retrieval quality and domain alignment, we fine-tuned both
the embedding model and a ranking model that scores retrieved passages by
relevance to the query. These enhancements optimize RAG for geoscience
applications and significantly improve the system's ability to deliver precise
and trustworthy outputs. GeoGPT reflects a strong commitment to open science
through its emphasis on collaboration, transparency, and community driven
development. As part of this commitment, we have open-sourced two core RAG
components-GeoEmbedding and GeoReranker-to support geoscientists, researchers,
and professionals worldwide with powerful, accessible AI tools.

</details>


### [59] [Demonstrating Narrative Pattern Discovery from Biomedical Literature](https://arxiv.org/abs/2509.09687)
*Hermann Kroll,Pascal Sackhoff,Bill Matthias Thang,Christin Katharina Kreutz,Wolf-Tilo Balke*

Main category: cs.IR

TL;DR: 本文介绍了一种名为叙事模式挖掘的新搜索功能，允许用户探索上下文相关的实体和实体交互，并通过领域专家访谈验证了原型的实用性。


<details>
  <summary>Details</summary>
Motivation: 数字图书馆需要为用户提供有效的访问路径，PubPharm作为德国药学专业信息服务，需要开发新的搜索功能来增强对生物医学文档集合的访问能力。

Method: 开发了叙事模式挖掘搜索功能，允许用户探索上下文相关的实体和实体交互，并通过与五位领域专家的访谈来验证原型的有用性。

Result: 通过专家访谈验证了叙事模式挖掘功能的实用性，表明该功能能够有效支持用户探索生物医学文档中的实体关系和交互。

Conclusion: 叙事模式挖掘是一种有前景的新搜索功能，能够增强数字图书馆对生物医学文档集合的访问和探索能力，得到了领域专家的认可。

Abstract: Digital libraries maintain extensive collections of knowledge and need to
provide effective access paths for their users. For instance, PubPharm, the
specialized information service for Pharmacy in Germany, provides and develops
access paths to their underlying biomedical document collection. In brief,
PubPharm supports traditional keyword-based search, search for chemical
structures, as well as novel graph-based discovery workflows, e.g., listing or
searching for interactions between different pharmaceutical entities. This
paper introduces a new search functionality, called narrative pattern mining,
allowing users to explore context-relevant entities and entity interactions. We
performed interviews with five domain experts to verify the usefulness of our
prototype.

</details>


### [60] [AI-Powered Assistant for Long-Term Access to RHIC Knowledge](https://arxiv.org/abs/2509.09688)
*Mohammad Atif,Vincent Garonne,Eric Lancon,Jerome Lauret,Alexandr Prozorov,Michal Vranovsky*

Main category: cs.IR

TL;DR: RHIC数据与分析保存计划开发了一个基于大语言模型的AI助手系统，通过自然语言访问文档、工作流和软件，旨在支持重离子碰撞实验数据的可重现性、教育和未来发现。


<details>
  <summary>Details</summary>
Motivation: 随着RHIC运行25年结束，保存其海量数据（约1EB）和嵌入的科学知识成为关键优先事项，需要确保科学遗产的可访问性和可用性。

Method: 基于大语言模型，采用检索增强生成和模型上下文协议，索引RHIC实验的结构化和非结构化内容，实现领域适应的交互。

Result: 系统已部署，报告了计算性能、多实验集成进展，以及为可持续和可解释的长期AI访问设计的架构特性。

Conclusion: 现代AI/ML工具可以显著提升科学遗产数据的可用性和可发现性，为大型科学实验的数据保存和知识传承提供了有效解决方案。

Abstract: As the Relativistic Heavy Ion Collider (RHIC) at Brookhaven National
Laboratory concludes 25 years of operation, preserving not only its vast data
holdings ($\sim$1 ExaByte) but also the embedded scientific knowledge becomes a
critical priority. The RHIC Data and Analysis Preservation Plan (DAPP)
introduces an AI-powered assistant system that provides natural language access
to documentation, workflows, and software, with the aim of supporting
reproducibility, education, and future discovery. Built upon Large Language
Models using Retrieval-Augmented Generation and the Model Context Protocol,
this assistant indexes structured and unstructured content from RHIC
experiments and enables domain-adapted interaction. We report on the
deployment, computational performance, ongoing multi-experiment integration,
and architectural features designed for a sustainable and explainable long-term
AI access. Our experience illustrates how modern AI/ML tools can transform the
usability and discoverability of scientific legacy data.

</details>


### [61] [Personas within Parameters: Fine-Tuning Small Language Models with Low-Rank Adapters to Mimic User Behaviors](https://arxiv.org/abs/2509.09689)
*Himanshu Thakur,Eshani Agrawal,Smruthi Mukund*

Main category: cs.IR

TL;DR: 提出使用冻结大语言模型提取文本用户表示，并通过微调小语言模型来构建高效的用户行为代理，解决了传统方法在用户行为模拟中的复杂性和可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统中用户行为模拟的长期挑战，特别是处理大规模表格用户-物品交互数据、克服预训练模型的归纳偏差，以及在数百万用户规模上实现高效模拟。

Method: 使用冻结的大语言模型提取鲁棒的文本用户表示，然后通过微调小语言模型来构建成本效益高、资源效率好的用户代理，并采用低秩适配器为不同用户群体训练个性化模型。

Result: 实验提供了有力的经验证据，表明该方法开发出的用户代理能够有效弥合推荐系统离线指标与实际性能之间的差距。

Conclusion: 该方法在可扩展性和性能之间找到了最佳平衡，为推荐系统的用户行为模拟提供了一种高效实用的解决方案。

Abstract: A long-standing challenge in developing accurate recommendation models is
simulating user behavior, mainly due to the complex and stochastic nature of
user interactions. Towards this, one promising line of work has been the use of
Large Language Models (LLMs) for simulating user behavior. However, aligning
these general-purpose large pre-trained models with user preferences
necessitates: (i) effectively and continously parsing large-scale tabular
user-item interaction data, (ii) overcoming pre-training-induced inductive
biases to accurately learn user specific knowledge, and (iii) achieving the
former two at scale for millions of users. While most previous works have
focused on complex methods to prompt an LLM or fine-tune it on tabular
interaction datasets, our approach shifts the focus to extracting robust
textual user representations using a frozen LLM and simulating cost-effective,
resource-efficient user agents powered by fine-tuned Small Language Models
(SLMs). Further, we showcase a method for training multiple low-rank adapters
for groups of users or \textit{persona}, striking an optimal balance between
scalability and performance of user behavior agents. Our experiments provide
compelling empirical evidence of the efficacy of our methods, demonstrating
that user agents developed using our approach have the potential to bridge the
gap between offline metrics and real-world performance of recommender systems.

</details>


### [62] [Powering Job Search at Scale: LLM-Enhanced Query Understanding in Job Matching Systems](https://arxiv.org/abs/2509.09690)
*Ping Liu,Jianqiang Shen,Qianqi Shen,Chunnan Yao,Kevin Kao,Dan Xu,Rajat Arora,Baofen Zheng,Caleb Johnson,Liangjie Hong,Jingwei Wu,Wenjing Zhang*

Main category: cs.IR

TL;DR: 提出基于大语言模型的统一查询理解框架，替代传统的多NER模型方法，通过联合建模用户查询和上下文信号来生成结构化解释，提升推荐准确性和个性化程度


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖多个任务特定的命名实体识别模型，存在架构脆弱、维护成本高、难以适应快速变化的分类体系和语言模式等问题

Method: 使用大语言模型构建统一查询理解框架，联合建模用户查询和上下文信号（如个人资料属性），生成结构化解释来驱动推荐系统

Result: 在线A/B测试中提升了相关性质量，同时显著降低了系统复杂度和运维开销

Conclusion: 该解决方案为动态网络应用中的查询理解提供了可扩展和适应性强的技术基础

Abstract: Query understanding is essential in modern relevance systems, where user
queries are often short, ambiguous, and highly context-dependent. Traditional
approaches often rely on multiple task-specific Named Entity Recognition models
to extract structured facets as seen in job search applications. However, this
fragmented architecture is brittle, expensive to maintain, and slow to adapt to
evolving taxonomies and language patterns. In this paper, we introduce a
unified query understanding framework powered by a Large Language Model (LLM),
designed to address these limitations. Our approach jointly models the user
query and contextual signals such as profile attributes to generate structured
interpretations that drive more accurate and personalized recommendations. The
framework improves relevance quality in online A/B testing while significantly
reducing system complexity and operational overhead. The results demonstrate
that our solution provides a scalable and adaptable foundation for query
understanding in dynamic web applications.

</details>


### [63] [Wave-Based Semantic Memory with Resonance-Based Retrieval: A Phase-Aware Alternative to Vector Embedding Stores](https://arxiv.org/abs/2509.09691)
*Aleksandr Listopad*

Main category: cs.IR

TL;DR: 提出基于波的语义记忆框架，通过共振干涉进行知识检索，相比传统向量方法能更好地保留振幅和相位信息，在语义相似性判断上表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统基于向量的记忆系统依赖余弦或内积相似度，虽然计算高效但本质上是相位不敏感的，无法捕捉对意义表示至关重要的共振现象。

Method: 将知识建模为波模式ψ(x)=A(x)e^{iφ(x)}，通过共振干涉进行检索，保留振幅和相位信息。实现ResonanceDB系统，支持百万级模式毫秒级延迟检索。

Result: 共振检索在向量方法失效的情况下（如相位偏移、否定和组合查询）展现出更高的判别能力。

Conclusion: 基于波的记忆系统是向量存储的可行替代方案，适用于AGI导向的推理和知识表示。

Abstract: Conventional vector-based memory systems rely on cosine or inner product
similarity within real-valued embedding spaces. While computationally
efficient, such approaches are inherently phase-insensitive and limited in
their ability to capture resonance phenomena crucial for meaning
representation. We propose Wave-Based Semantic Memory, a novel framework that
models knowledge as wave patterns $\psi(x) = A(x) e^{i\phi(x)}$ and retrieves
it through resonance-based interference. This approach preserves both amplitude
and phase information, enabling more expressive and robust semantic similarity.
We demonstrate that resonance-based retrieval achieves higher discriminative
power in cases where vector methods fail, including phase shifts, negations,
and compositional queries. Our implementation, ResonanceDB, shows scalability
to millions of patterns with millisecond latency, positioning wave-based memory
as a viable alternative to vector stores for AGI-oriented reasoning and
knowledge representation.

</details>


### [64] [A Research Vision for Web Search on Emerging Topics](https://arxiv.org/abs/2509.10212)
*Alisa Rieger,Stefan Dietze,Ran Yu*

Main category: cs.IR

TL;DR: 本文提出了一个研究愿景，旨在开发支持新兴主题知识获取的搜索引擎系统，帮助用户了解动态变化的信息并形成负责任的观点。


<details>
  <summary>Details</summary>
Motivation: 新兴主题的信息通常稀疏、动态变化且质量参差不齐，容易受到错误信息和偏见的影响，现有搜索引擎难以有效支持用户获取可靠知识。

Method: 提出了三个核心研究问题：理解现状、确定系统需求、构建新系统，并通过文献综述指出了可能的解决方向。

Result: 提出了一个完整的研究框架和愿景，但没有具体的实验结果，主要贡献在于理论框架的构建和研究方向的指引。

Conclusion: 开发能够支持新兴主题知识获取的搜索引擎系统具有重要意义，但面临诸多挑战，需要跨学科合作来解决信息质量、动态性和用户认知等方面的问题。

Abstract: We regularly encounter information on novel, emerging topics for which the
body of knowledge is still evolving, which can be linked, for instance, to
current events. A primary way to learn more about such topics is through web
search. However, information on emerging topics is sparse and evolves
dynamically as knowledge grows, making it uncertain and variable in quality and
trustworthiness and prone to deliberate or accidental manipulation,
misinformation, and bias. In this paper, we outline a research vision towards
search systems and interfaces that support effective knowledge acquisition,
awareness of the dynamic nature of topics, and responsible opinion formation
among people searching the web for information on emerging topics. To realize
this vision, we propose three overarching research questions, aimed at
understanding the status quo, determining requirements of systems aligned with
our vision, and building these systems. For each of the three questions, we
highlight relevant literature, including pointers on how they could be
addressed. Lastly, we discuss the challenges that will potentially arise in
pursuing the proposed vision.

</details>


### [65] [Model-agnostic post-hoc explainability for recommender systems](https://arxiv.org/abs/2509.10245)
*Irina Arévalo,Jose L Salmeron*

Main category: cs.IR

TL;DR: 该研究提出了一种模型无关的删除诊断方法，用于评估推荐系统中特定用户或项目对模型性能的影响，提高推荐系统的可解释性和透明度。


<details>
  <summary>Details</summary>
Motivation: 推荐系统通常使用复杂的特征嵌入和深度学习算法，虽然能提供精准推荐，但降低了系统的可解释性和透明度。需要一种方法来量化特定观察值对推荐系统的影响。

Method: 开发了一种删除诊断方法，通过比较完整模型与排除特定用户或项目后训练模型的性能差异，来量化该观察值对推荐系统的影响。该方法适用于神经协同过滤(NCF)和奇异值分解(SVD)等不同推荐算法。

Result: 在MovieLens和Amazon Reviews数据集上的实验表明，该方法能够提供对模型行为的深入洞察，并展示了在不同推荐范式中的通用性。

Conclusion: 提出的删除诊断方法是一种模型无关的工具，能够有效提高推荐系统的可解释性，帮助理解特定用户或项目对推荐结果的影响，适用于各种推荐算法。

Abstract: Recommender systems often benefit from complex feature embeddings and deep
learning algorithms, which deliver sophisticated recommendations that enhance
user experience, engagement, and revenue. However, these methods frequently
reduce the interpretability and transparency of the system. In this research,
we develop a systematic application, adaptation, and evaluation of deletion
diagnostics in the recommender setting. The method compares the performance of
a model to that of a similar model trained without a specific user or item,
allowing us to quantify how that observation influences the recommender, either
positively or negatively. To demonstrate its model-agnostic nature, the
proposal is applied to both Neural Collaborative Filtering (NCF), a widely used
deep learning-based recommender, and Singular Value Decomposition (SVD), a
classical collaborative filtering technique. Experiments on the MovieLens and
Amazon Reviews datasets provide insights into model behavior and highlight the
generality of the approach across different recommendation paradigms.

</details>


### [66] [Diversified recommendations of cultural activities with personalized determinantal point processes](https://arxiv.org/abs/2509.10392)
*Carole Ibrahim,Hiba Bederina,Daniel Cuesta,Laurent Montier,Cyrille Delabre,Jill-Jênn Vie*

Main category: cs.IR

TL;DR: 使用个性化行列式点过程(DPP)在推荐系统中平衡相关性和多样性，通过质量-多样性分解核函数来优先考虑用户偏好，并在生产环境中评估效果


<details>
  <summary>Details</summary>
Motivation: 在优化用户参与度的同时，有效多样化推荐内容而不影响核心业务指标是行业重大挑战，旨在扩大受众的文化实践

Method: 实现个性化DPP采样方法，基于质量-多样性分解相似性核函数，给予用户偏好更多权重，通过离线和在线指标评估效果

Result: 提出了完整的平台和实验代码并开源在GitHub上，为从业者提供了在生产环境中使用的实践见解

Conclusion: 个性化DPP是平衡推荐系统相关性和多样性的有效方法，能够在不损害核心指标的前提下实现文化实践的多样化推荐

Abstract: While optimizing recommendation systems for user engagement is a
well-established practice, effectively diversifying recommendations without
negatively impacting core business metrics remains a significant industry
challenge. In line with our initiative to broaden our audience's cultural
practices, this study investigates using personalized Determinantal Point
Processes (DPPs) to sample diverse and relevant recommendations. We rely on a
well-known quality-diversity decomposition of the similarity kernel to give
more weight to user preferences. In this paper, we present our implementations
of the personalized DPP sampling, evaluate the trade-offs between relevance and
diversity through both offline and online metrics, and give insights for
practitioners on their use in a production environment. For the sake of
reproducibility, we release the full code for our platform and experiments on
GitHub.

</details>


### [67] [RecoWorld: Building Simulated Environments for Agentic Recommender Systems](https://arxiv.org/abs/2509.10397)
*Fei Liu,Xinyu Lin,Hanchao Yu,Mingyuan Wu,Jianyu Wang,Qiang Zhang,Zhuokai Zhao,Yinglong Xia,Yao Zhang,Weiwei Li,Mingze Gao,Qifan Wang,Lizhu Zhang,Benyu Zhang,Xiangjun Fan*

Main category: cs.IR

TL;DR: RecoWorld是一个为智能推荐系统设计的模拟环境框架，通过双视图架构让用户模拟器和推荐代理进行多轮交互，利用LLM的推理能力来最大化用户留存率。


<details>
  <summary>Details</summary>
Motivation: 为智能推荐系统提供安全的训练环境，让代理能够在不影响真实用户的情况下从错误中学习，解决传统推荐系统缺乏用户反馈循环的问题。

Method: 采用双视图架构：用户模拟器审查推荐项目并更新心态，当检测到用户可能流失时生成反思指令；推荐代理通过整合用户指令和推理痕迹来调整推荐策略，形成动态反馈循环。支持多种内容表示形式（文本、多模态、语义ID）和多轮强化学习。

Result: 构建了一个支持多代理模拟的环境框架，能够模拟目标用户群体的响应，为推荐系统提供了重要的训练和测试平台。

Conclusion: RecoWorld是向用户与代理协同塑造个性化信息流的重要第一步，开启了"用户指导、推荐器响应"的新交互范式，共同优化用户留存和参与度。

Abstract: We present RecoWorld, a blueprint for building simulated environments
tailored to agentic recommender systems. Such environments give agents a proper
training space where they can learn from errors without impacting real users.
RecoWorld distinguishes itself with a dual-view architecture: a simulated user
and an agentic recommender engage in multi-turn interactions aimed at
maximizing user retention. The user simulator reviews recommended items,
updates its mindset, and when sensing potential user disengagement, generates
reflective instructions. The agentic recommender adapts its recommendations by
incorporating these user instructions and reasoning traces, creating a dynamic
feedback loop that actively engages users. This process leverages the
exceptional reasoning capabilities of modern LLMs. We explore diverse content
representations within the simulator, including text-based, multimodal, and
semantic ID modeling, and discuss how multi-turn RL enables the recommender to
refine its strategies through iterative interactions. RecoWorld also supports
multi-agent simulations, allowing creators to simulate the responses of
targeted user populations. It marks an important first step toward recommender
systems where users and agents collaboratively shape personalized information
streams. We envision new interaction paradigms where "user instructs,
recommender responds," jointly optimizing user retention and engagement.

</details>


### [68] [MatSKRAFT: A framework for large-scale materials knowledge extraction from scientific tables](https://arxiv.org/abs/2509.10448)
*Kausik Hira,Mohd Zaki,Mausam,N. M. Anoop Krishnan*

Main category: cs.IR

TL;DR: MatSKRAFT是一个从材料科学表格数据中自动提取和整合知识的计算框架，通过图神经网络处理表格数据，在性能和速度上显著优于现有大型语言模型


<details>
  <summary>Details</summary>
Motivation: 科学进步越来越依赖于整合海量文献知识，但大多数实验数据仍被困在半结构化格式中，难以进行系统提取和分析

Method: 将表格转换为基于图的表示，使用约束驱动的图神经网络（GNN）将科学原理直接编码到模型架构中

Result: 在性能提取上达到88.68 F1分数，在成分提取上达到71.35 F1分数，处理速度比现有模型快19-496倍，构建了包含超过535,000条目的综合数据库

Conclusion: 该方法能够揭示先前被忽视的材料特性组合，支持数据驱动的成分-特性关系发现，为材料科学发现奠定基础

Abstract: Scientific progress increasingly depends on synthesizing knowledge across
vast literature, yet most experimental data remains trapped in semi-structured
formats that resist systematic extraction and analysis. Here, we present
MatSKRAFT, a computational framework that automatically extracts and integrates
materials science knowledge from tabular data at unprecedented scale. Our
approach transforms tables into graph-based representations processed by
constraint-driven GNNs that encode scientific principles directly into model
architecture. MatSKRAFT significantly outperforms state-of-the-art large
language models, achieving F1 scores of 88.68 for property extraction and 71.35
for composition extraction, while processing data $19$-$496\times$ faster than
them (compared to the slowest and the fastest models, respectively) with modest
hardware requirements. Applied to nearly 69,000 tables from more than 47,000
research publications, we construct a comprehensive database containing over
535,000 entries, including 104,000 compositions that expand coverage beyond
major existing databases, pending manual validation. This systematic approach
reveals previously overlooked materials with distinct property combinations and
enables data-driven discovery of composition-property relationships forming the
cornerstone of materials and scientific discovery.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [69] [Structure Matters: Brain Graph Augmentation via Learnable Edge Masking for Data-efficient Psychiatric Diagnosis](https://arxiv.org/abs/2509.09744)
*Mujie Liu,Chenze Wang,Liping Chen,Nguyen Linh Dan Le,Niharika Tewari,Ting Dang,Jiangang Ma,Feng Xia*

Main category: cs.LG

TL;DR: SAM-BG是一个两阶段的自监督学习框架，通过结构语义保护来学习脑图表示，在精神疾病诊断中表现出色，特别是在小样本标注数据场景下。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督学习方法在脑图数据上往往依赖可能破坏关键结构语义的增强策略，而标注脑网络数据的稀缺性使得准确且可解释的精神疾病诊断面临挑战。

Method: 提出两阶段框架：1）预训练阶段在小标注子集上训练边缘掩码器捕获关键结构语义；2）自监督学习阶段使用提取的结构先验指导结构感知增强过程，学习更具语义意义和鲁棒性的表示。

Result: 在两个真实世界精神疾病数据集上的实验表明，SAM-BG优于最先进方法，特别是在小标注数据设置下，并发现了增强可解释性的临床相关连接模式。

Conclusion: SAM-BG通过结构语义保护的自监督学习方法，有效解决了脑图数据标注稀缺的问题，在精神疾病诊断中实现了更好的性能和可解释性。

Abstract: The limited availability of labeled brain network data makes it challenging
to achieve accurate and interpretable psychiatric diagnoses. While
self-supervised learning (SSL) offers a promising solution, existing methods
often rely on augmentation strategies that can disrupt crucial structural
semantics in brain graphs. To address this, we propose SAM-BG, a two-stage
framework for learning brain graph representations with structural semantic
preservation. In the pre-training stage, an edge masker is trained on a small
labeled subset to capture key structural semantics. In the SSL stage, the
extracted structural priors guide a structure-aware augmentation process,
enabling the model to learn more semantically meaningful and robust
representations. Experiments on two real-world psychiatric datasets demonstrate
that SAM-BG outperforms state-of-the-art methods, particularly in small-labeled
data settings, and uncovers clinically relevant connectivity patterns that
enhance interpretability. Our code is available at
https://github.com/mjliu99/SAM-BG.

</details>


### [70] [D-CAT: Decoupled Cross-Attention Transfer between Sensor Modalities for Unimodal Inference](https://arxiv.org/abs/2509.09747)
*Leen Daher,Zhaobo Wang,Malcolm Mielle*

Main category: cs.LG

TL;DR: D-CAT是一种解耦的跨注意力迁移框架，允许在推理时仅使用单一传感器，通过跨模态知识迁移提升性能，减少硬件冗余。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态迁移方法在训练和推理时都需要配对传感器数据，限制了在资源受限环境中的部署。需要一种方法能够在推理时仅使用单一传感器，同时利用其他模态的知识。

Method: 提出D-CAT框架，结合自注意力模块进行特征提取和新型跨注意力对齐损失，强制对齐不同传感器的特征空间，而不需要耦合两种模态的分类流程。

Result: 在三个多模态人类活动数据集上评估，在分布内场景中，从高性能模态（如视频到IMU）迁移可获得10%的F1分数提升；在分布外场景中，即使较弱的源模态也能改善目标性能。

Conclusion: D-CAT通过实现单传感器推理和跨模态知识迁移，减少了感知系统的硬件冗余，同时保持准确性，适用于成本敏感或自适应部署场景。

Abstract: Cross-modal transfer learning is used to improve multi-modal classification
models (e.g., for human activity recognition in human-robot collaboration).
However, existing methods require paired sensor data at both training and
inference, limiting deployment in resource-constrained environments where full
sensor suites are not economically and technically usable. To address this, we
propose Decoupled Cross-Attention Transfer (D-CAT), a framework that aligns
modality-specific representations without requiring joint sensor modality
during inference. Our approach combines a self-attention module for feature
extraction with a novel cross-attention alignment loss, which enforces the
alignment of sensors' feature spaces without requiring the coupling of the
classification pipelines of both modalities. We evaluate D-CAT on three
multi-modal human activity datasets (IMU, video, and audio) under both
in-distribution and out-of-distribution scenarios, comparing against uni-modal
models. Results show that in in-distribution scenarios, transferring from
high-performing modalities (e.g., video to IMU) yields up to 10% F1-score gains
over uni-modal training. In out-of-distribution scenarios, even weaker source
modalities (e.g., IMU to video) improve target performance, as long as the
target model isn't overfitted on the training data. By enabling single-sensor
inference with cross-modal knowledge, D-CAT reduces hardware redundancy for
perception systems while maintaining accuracy, which is critical for
cost-sensitive or adaptive deployments (e.g., assistive robots in homes with
variable sensor availability). Code is available at
https://github.com/Schindler-EPFL-Lab/D-CAT.

</details>


### [71] [Meta-Learning Reinforcement Learning for Crypto-Return Prediction](https://arxiv.org/abs/2509.09751)
*Junqiao Wang,Zhaoyang Guan,Guanyu Liu,Tianze Xia,Xianzhi Li,Shuo Yin,Xinyuan Song,Chuhan Cheng,Tianyu Shi,Alex Lee*

Main category: cs.LG

TL;DR: Meta-RL-Crypto是一个基于Transformer的统一架构，结合元学习和强化学习，创建了一个完全自改进的交易代理，用于加密货币回报预测，无需额外人工监督，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 加密货币回报预测极其困难，价格波动由快速变化的链上活动、新闻流和社交情绪驱动，且标记训练数据稀缺昂贵。需要一个能够自我改进、无需人工监督的交易代理。

Method: 使用基于Transformer的统一架构，结合元学习和强化学习。从指令调优的LLM开始，代理在闭环架构中迭代交替扮演三个角色（执行者、评判者和元评判者），利用多模态市场输入和内部偏好反馈，持续改进交易策略和评估标准。

Result: 在不同市场机制下的实验表明，Meta-RL-Crypto在真实市场的技术指标上表现良好，并且优于其他基于LLM的基线方法。

Conclusion: Meta-RL-Crypto通过结合元学习和强化学习的统一架构，成功创建了一个完全自改进的加密货币交易代理，无需人工监督，在复杂多变的市场环境中表现出优越性能。

Abstract: Predicting cryptocurrency returns is notoriously difficult: price movements
are driven by a fast-shifting blend of on-chain activity, news flow, and social
sentiment, while labeled training data are scarce and expensive. In this paper,
we present Meta-RL-Crypto, a unified transformer-based architecture that
unifies meta-learning and reinforcement learning (RL) to create a fully
self-improving trading agent. Starting from a vanilla instruction-tuned LLM,
the agent iteratively alternates between three roles-actor, judge, and
meta-judge-in a closed-loop architecture. This learning process requires no
additional human supervision. It can leverage multimodal market inputs and
internal preference feedback. The agent in the system continuously refines both
the trading policy and evaluation criteria. Experiments across diverse market
regimes demonstrate that Meta-RL-Crypto shows good performance on the technical
indicators of the real market and outperforming other LLM-based baselines.

</details>


### [72] [LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation](https://arxiv.org/abs/2509.09754)
*Yiqun Shen,Song Yuan,Zhengze Zhang,Xiaoliang Wang,Daxin Jiang,Nguyen Cam-Tu*

Main category: cs.LG

TL;DR: LAVa是一个统一的KV缓存压缩框架，通过最小化Transformer残差流中的信息损失来实现动态预算分配，在多种基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存压缩方法大多是启发式的，缺乏动态预算分配机制，无法有效处理长上下文推理的高内存需求。

Method: 通过分析层注意力输出损失，提出新的度量标准来比较不同头的缓存条目，实现层间和头间的动态预算分配，无需训练或多策略组合。

Result: 在LongBench、Needle-In-A-Haystack等基准测试中表现优越，发现动态层预算对生成任务关键，动态头预算对抽取任务重要。

Conclusion: LAVa是首个统一的缓存淘汰和动态预算分配策略，在各种任务类型中保持最佳性能，为KV缓存压缩提供了新的理论框架和实践方法。

Abstract: KV Cache is commonly used to accelerate LLM inference with long contexts, yet
its high memory demand drives the need for cache compression. Existing
compression methods, however, are largely heuristic and lack dynamic budget
allocation. To address this limitation, we introduce a unified framework for
cache compression by minimizing information loss in Transformer residual
streams. Building on it, we analyze the layer attention output loss and derive
a new metric to compare cache entries across heads, enabling layer-wise
compression with dynamic head budgets. Additionally, by contrasting cross-layer
information, we also achieve dynamic layer budgets. LAVa is the first unified
strategy for cache eviction and dynamic budget allocation that, unlike prior
methods, does not rely on training or the combination of multiple strategies.
Experiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and
InfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a
new insight: dynamic layer budgets are crucial for generation tasks (e.g., code
completion), while dynamic head budgets play a key role in extraction tasks
(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently
maintains top performance across task types. Our code is available at
https://github.com/MGDDestiny/Lava.

</details>


### [73] [Hybrid Adaptive Conformal Offline Reinforcement Learning for Fair Population Health Management](https://arxiv.org/abs/2509.09772)
*Sanjay Basu,Sadiq Y. Patel,Parth Sheth,Bhairavi Muralidharan,Namrata Elamaran,Aakriti Kinra,Rajaie Batniji*

Main category: cs.LG

TL;DR: HACO框架通过分离风险校准和偏好优化，为医疗补助人群提供安全、公平且可审计的决策支持，在控制不良事件风险的同时保持高安全覆盖率


<details>
  <summary>Details</summary>
Motivation: 为医疗补助人群提供安全、公平且可审计的人口健康管理服务，需要协调纵向外展服务并控制不良利用事件的风险

Method: 混合自适应符合离线强化学习(HACO)框架：训练轻量级风险模型、推导符合阈值屏蔽不安全行为、在安全子集上学习偏好策略

Result: 实现了强大的风险区分能力(AUC ~0.81)，校准阈值(τ ~0.038 at α=0.10)，保持高安全覆盖率，子组分析显示人口统计特征间存在系统性价值差异

Conclusion: 符合风险门控与离线强化学习无缝集成，为人口健康管理团队提供保守且可审计的决策支持

Abstract: Population health management programs for Medicaid populations coordinate
longitudinal outreach and services (e.g., benefits navigation, behavioral
health, social needs support, and clinical scheduling) and must be safe, fair,
and auditable. We present a Hybrid Adaptive Conformal Offline Reinforcement
Learning (HACO) framework that separates risk calibration from preference
optimization to generate conservative action recommendations at scale. In our
setting, each step involves choosing among common coordination actions (e.g.,
which member to contact, by which modality, and whether to route to a
specialized service) while controlling the near-term risk of adverse
utilization events (e.g., unplanned emergency department visits or
hospitalizations). Using a de-identified operational dataset from Waymark
comprising 2.77 million sequential decisions across 168,126 patients, HACO (i)
trains a lightweight risk model for adverse events, (ii) derives a conformal
threshold to mask unsafe actions at a target risk level, and (iii) learns a
preference policy on the resulting safe subset. We evaluate policies with a
version-agnostic fitted Q evaluation (FQE) on stratified subsets and audit
subgroup performance across age, sex, and race. HACO achieves strong risk
discrimination (AUC ~0.81) with a calibrated threshold ( {\tau} ~0.038 at
{\alpha} = 0.10), while maintaining high safe coverage. Subgroup analyses
reveal systematic differences in estimated value across demographics,
underscoring the importance of fairness auditing. Our results show that
conformal risk gating integrates cleanly with offline RL to deliver
conservative, auditable decision support for population health management
teams.

</details>


### [74] [One Head, Many Models: Cross-Attention Routing for Cost-Aware LLM Selection](https://arxiv.org/abs/2509.09782)
*Roshini Pulishetty,Mani Kishan Ghantasala,Keerthy Kaushik Dasoju,Niti Mangwani,Vishal Garimella,Aditya Mate,Somya Chatterjee,Yue Kang,Ehi Nosakhare,Sadid Hasan,Soundar Srinivasan*

Main category: cs.LG

TL;DR: 提出基于单头交叉注意力机制的统一路由框架，动态选择最优LLM，在RouterBench基准上实现6.6%的质量提升和2.9%的最大性能提升


<details>
  <summary>Details</summary>
Motivation: 解决不同计算成本和性能的大型语言模型在现实应用中规模化、成本效益部署的挑战

Method: 使用单头交叉注意力机制联合建模查询和模型嵌入，预测响应质量和生成成本，并提出指数奖励函数来平衡性能和成本

Result: 在RouterBench基准测试中，平均质量改进(AIQ)提升6.6%，最大性能提升2.9%，架构轻量且能有效跨领域泛化

Conclusion: 建立了一个新的成本感知LLM路由标准，提供了轻量级、高效且能稳定平衡性能与成本的解决方案

Abstract: The proliferation of large language models (LLMs) with varying computational
costs and performance profiles presents a critical challenge for scalable,
cost-effective deployment in real-world applications. We introduce a unified
routing framework that leverages a single-head cross-attention mechanism to
jointly model query and model embeddings, enabling dynamic selection of the
optimal LLM for each input query. Our approach is evaluated on RouterBench, a
large-scale, publicly available benchmark encompassing diverse LLM pools and
domains. By explicitly capturing fine-grained query-model interactions, our
router predicts both response quality and generation cost, achieving up to 6.6%
improvement in Average Improvement in Quality (AIQ) and 2.9% in maximum
performance over existing routers. To robustly balance performance and cost, we
propose an exponential reward function that enhances stability across user
preferences. The resulting architecture is lightweight, generalizes effectively
across domains, and demonstrates improved efficiency compared to prior methods,
establishing a new standard for cost-aware LLM routing.

</details>


### [75] [From the Gradient-Step Denoiser to the Proximal Denoiser and their associated convergent Plug-and-Play algorithms](https://arxiv.org/abs/2509.09793)
*Vincent Herfeld,Baudouin Denis de Senneville,Arthur Leclaire,Nicolas Papadakis*

Main category: cs.LG

TL;DR: 分析梯度步长去噪器及其在即插即用算法中的应用，该去噪器被训练为显式函数的最优梯度下降算子或邻近算子，同时保持最先进的去噪能力


<details>
  <summary>Details</summary>
Motivation: 即插即用优化算法通常使用现成的去噪器来替代图像先验的邻近算子或梯度下降算子，但这些先验通常是隐式的。研究旨在开发能够同时作为显式函数算子的先进去噪器

Method: 训练梯度步长去噪器，使其精确地成为显式函数的最优梯度下降算子或邻近算子，同时保持最先进的去噪性能

Result: 成功开发出梯度步长去噪器，该去噪器既能作为显式函数的优化算子，又能保持state-of-the-art的去噪能力

Conclusion: 梯度步长去噪器为即插即用算法提供了理论基础更强的替代方案，将隐式图像先验转化为显式函数表示，同时不牺牲去噪性能

Abstract: In this paper we analyze the Gradient-Step Denoiser and its usage in
Plug-and-Play algorithms. The Plug-and-Play paradigm of optimization algorithms
uses off the shelf denoisers to replace a proximity operator or a gradient
descent operator of an image prior. Usually this image prior is implicit and
cannot be expressed, but the Gradient-Step Denoiser is trained to be exactly
the gradient descent operator or the proximity operator of an explicit
functional while preserving state-of-the-art denoising capabilities.

</details>


### [76] [Distinguishing Startle from Surprise Events Based on Physiological Signals](https://arxiv.org/abs/2509.09799)
*Mansi Sharma,Alexandre Duchevet,Florian Daiber,Jean-Paul Imbert,Maurice Rekrut*

Main category: cs.LG

TL;DR: 本研究使用机器学习和多模态融合策略，基于生理信号区分惊吓和惊讶反应，最高准确率达85.7%，并能区分三种状态（惊吓、惊讶、基线）准确率达74.9%


<details>
  <summary>Details</summary>
Motivation: 意外事件会损害注意力和延迟决策，在航空等高危环境中造成严重安全风险。惊吓和惊讶反应以不同方式影响飞行员表现，但实践中难以区分，现有研究多单独研究这两种反应，缺乏对组合效应和生理数据区分方法的研究

Method: 使用机器学习方法和多模态融合策略，基于生理信号数据来区分惊吓和惊讶事件

Result: 能够可靠预测这些事件，SVM和Late Fusion方法获得最高平均准确率85.7%；扩展评估包含基线条件后，XGBoost和Late Fusion方法能区分惊吓、惊讶和基线三种状态，最高平均准确率达74.9%

Conclusion: 通过生理信号和机器学习方法可以有效区分惊吓和惊讶反应，为高风险环境中飞行员状态监测和安全管理提供了有效技术手段

Abstract: Unexpected events can impair attention and delay decision-making, posing
serious safety risks in high-risk environments such as aviation. In particular,
reactions like startle and surprise can impact pilot performance in different
ways, yet are often hard to distinguish in practice. Existing research has
largely studied these reactions separately, with limited focus on their
combined effects or how to differentiate them using physiological data. In this
work, we address this gap by distinguishing between startle and surprise events
based on physiological signals using machine learning and multi-modal fusion
strategies. Our results demonstrate that these events can be reliably
predicted, achieving a highest mean accuracy of 85.7% with SVM and Late Fusion.
To further validate the robustness of our model, we extended the evaluation to
include a baseline condition, successfully differentiating between Startle,
Surprise, and Baseline states with a highest mean accuracy of 74.9% with
XGBoost and Late Fusion.

</details>


### [77] [Revisiting Actor-Critic Methods in Discrete Action Off-Policy Reinforcement Learning](https://arxiv.org/abs/2509.09838)
*Reza Asad,Reza Babanezhad,Sharan Vaswani*

Main category: cs.LG

TL;DR: 本文重新审视了离散动作环境中的actor-critic方法，发现DSAC性能不佳的主要原因是actor和critic熵的耦合。通过解耦这两个组件并提出灵活的off-policy框架，方法在Atari游戏中达到了与DQN相当的性能。


<details>
  <summary>Details</summary>
Motivation: 解决离散动作环境中off-policy强化学习的问题，因为值基方法（如DQN）是默认选择，而常见的策略基方法要么无法有效利用off-policy数据（如PPO），要么在离散动作设置中表现不佳（如SAC）。

Method: 从离散SAC（DSAC）出发，解耦actor和critic的熵组件，提出灵活的off-policy actor-critic框架，允许使用m步Bellman算子进行critic更新，并将标准策略优化方法与熵正则化结合来实例化actor目标。

Result: 理论上证明了在表格设置中可以收敛到最优正则化值函数；实证表明这些方法在标准Atari游戏中可以达到DQN的性能水平，甚至在没有熵正则化或显式探索的情况下也能实现。

Conclusion: 通过解耦actor-critic的熵组件并设计灵活的框架，成功解决了离散动作环境中off-policy actor-critic方法的性能问题，为离散动作RL提供了新的有效解决方案。

Abstract: Value-based approaches such as DQN are the default methods for off-policy
reinforcement learning with discrete-action environments such as Atari. Common
policy-based methods are either on-policy and do not effectively learn from
off-policy data (e.g. PPO), or have poor empirical performance in the
discrete-action setting (e.g. SAC). Consequently, starting from discrete SAC
(DSAC), we revisit the design of actor-critic methods in this setting. First,
we determine that the coupling between the actor and critic entropy is the
primary reason behind the poor performance of DSAC. We demonstrate that by
merely decoupling these components, DSAC can have comparable performance as
DQN. Motivated by this insight, we introduce a flexible off-policy actor-critic
framework that subsumes DSAC as a special case. Our framework allows using an
m-step Bellman operator for the critic update, and enables combining standard
policy optimization methods with entropy regularization to instantiate the
resulting actor objective. Theoretically, we prove that the proposed methods
can guarantee convergence to the optimal regularized value function in the
tabular setting. Empirically, we demonstrate that these methods can approach
the performance of DQN on standard Atari games, and do so even without entropy
regularization or explicit exploration.

</details>


### [78] [HGEN: Heterogeneous Graph Ensemble Networks](https://arxiv.org/abs/2509.09843)
*Jiajun Shen,Yufei Jin,Yi He,Xingquan Zhu*

Main category: cs.LG

TL;DR: HGEN是首个针对异质图的集成学习框架，通过元路径和变换优化管道集成多个学习器，提升分类准确率


<details>
  <summary>Details</summary>
Motivation: 异质图中节点类型、节点特征和局部邻域拓扑的异质性给集成学习带来重大挑战，特别是在适应多样化图学习器方面

Method: 使用元路径结合随机丢弃创建等位基因GNN，通过残差注意力机制校准不同元路径的等位基因GNN，并使用相关性正则化项扩大不同元路径生成的嵌入矩阵差异

Result: 在五个异质网络上的实验验证HGEN始终以显著优势优于最先进的竞争对手

Conclusion: HGEN通过有效的集成学习框架成功解决了异质图学习中的挑战，提高了分类准确性

Abstract: This paper presents HGEN that pioneers ensemble learning for heterogeneous
graphs. We argue that the heterogeneity in node types, nodal features, and
local neighborhood topology poses significant challenges for ensemble learning,
particularly in accommodating diverse graph learners. Our HGEN framework
ensembles multiple learners through a meta-path and transformation-based
optimization pipeline to uplift classification accuracy. Specifically, HGEN
uses meta-path combined with random dropping to create Allele Graph Neural
Networks (GNNs), whereby the base graph learners are trained and aligned for
later ensembling. To ensure effective ensemble learning, HGEN presents two key
components: 1) a residual-attention mechanism to calibrate allele GNNs of
different meta-paths, thereby enforcing node embeddings to focus on more
informative graphs to improve base learner accuracy, and 2) a
correlation-regularization term to enlarge the disparity among embedding
matrices generated from different meta-paths, thereby enriching base learner
diversity. We analyze the convergence of HGEN and attest its higher
regularization magnitude over simple voting. Experiments on five heterogeneous
networks validate that HGEN consistently outperforms its state-of-the-art
competitors by substantial margin.

</details>


### [79] [Latency and Token-Aware Test-Time Compute](https://arxiv.org/abs/2509.09864)
*Jenny Y. Huang,Mehul Damani,Yousef El-Kurdi,Ramon Astudillo,Wei Sun*

Main category: cs.LG

TL;DR: 本文提出了一个动态计算分配框架，在推理时根据查询需求选择最佳生成策略（如beam search或best-of-N），同时考虑token成本和延迟时间，以优化LLM性能。


<details>
  <summary>Details</summary>
Motivation: 现有推理时扩展方法主要关注并行生成和token使用，忽略了增量解码方法和延迟时间的重要性，特别是在需要高效多查询的智能体工作流中。

Method: 构建动态计算分配框架，在每查询基础上决定应用哪种策略（beam search或best-of-N）以及分配多少计算资源，同时显式考虑token成本和时钟延迟。

Result: 在推理基准测试中，该方法始终优于静态策略，实现了良好的准确率-成本权衡，且具有实际部署可行性。

Conclusion: 动态计算分配方法能够有效优化LLM推理性能，在保持准确性的同时显著降低延迟和计算成本，特别适用于需要高效多查询的应用场景。

Abstract: Inference-time scaling has emerged as a powerful way to improve large
language model (LLM) performance by generating multiple candidate responses and
selecting among them. However, existing work on dynamic allocation for
test-time compute typically considers only parallel generation methods such as
best-of-N, overlooking incremental decoding methods like beam search, and has
largely ignored latency, focusing only on token usage. We formulate
inference-time scaling as a problem of dynamic compute allocation and method
selection, where the system must decide which strategy to apply and how much
compute to allocate on a per-query basis. Our framework explicitly incorporates
both token cost and wall-clock latency, the latter being critical for user
experience and particularly for agentic workflows where models must issue
multiple queries efficiently. Experiments on reasoning benchmarks show that our
approach consistently outperforms static strategies, achieving favorable
accuracy-cost trade-offs while remaining practical for deployment.

</details>


### [80] [Variational Neural Networks for Observable Thermodynamics (V-NOTS)](https://arxiv.org/abs/2509.09899)
*Christopher Eldred,François Gay-Balmaz,Vakhtang Putkaradze*

Main category: cs.LG

TL;DR: 本文提出了一种基于可观测变量的数据驱动计算框架，用于处理耗散动力系统的相空间演化问题，通过热力学拉格朗日方法和神经网络来保证热力学约束和熵增特性。


<details>
  <summary>Details</summary>
Motivation: 现有的数据驱动方法通常假设可以直接观测到相空间变量，但在耗散动力系统中，动量和熵等关键变量往往无法直接观测，这限制了传统方法的适用性。

Method: 构建基于热力学拉格朗日的新型方法，设计神经网络来保证热力学约束和熵的非递减演化，仅使用可观测变量进行相空间演化描述。

Result: 该方法能够基于有限的数据点和相对较少的系统参数，有效描述相空间演化过程。

Conclusion: 所提出的框架为处理无法直接观测相空间变量的耗散动力系统提供了一种有效的计算解决方案，具有较好的实用性和推广价值。

Abstract: Much attention has recently been devoted to data-based computing of evolution
of physical systems. In such approaches, information about data points from
past trajectories in phase space is used to reconstruct the equations of motion
and to predict future solutions that have not been observed before. However, in
many cases, the available data does not correspond to the variables that define
the system's phase space. We focus our attention on the important example of
dissipative dynamical systems. In that case, the phase space consists of
coordinates, momenta and entropies; however, the momenta and entropies cannot,
in general, be observed directly. To address this difficulty, we develop an
efficient data-based computing framework based exclusively on observable
variables, by constructing a novel approach based on the \emph{thermodynamic
Lagrangian}, and constructing neural networks that respect the thermodynamics
and guarantees the non-decreasing entropy evolution. We show that our network
can provide an efficient description of phase space evolution based on a
limited number of data points and a relatively small number of parameters in
the system.

</details>


### [81] [LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios](https://arxiv.org/abs/2509.09926)
*Jiahao Chen,Zhiyuan Huang,Yurou Liu,Bing Su*

Main category: cs.LG

TL;DR: 提出了LoFT框架，通过参数高效微调基础模型来处理长尾半监督学习问题，并在开放世界场景下扩展为LoFT-OW，显著提升了性能


<details>
  <summary>Details</summary>
Motivation: 现有的长尾半监督学习方法需要从零训练模型，容易产生过度自信和低质量伪标签的问题，需要更有效的解决方案

Method: 将长尾半监督学习扩展到基础模型微调范式，提出LoFT框架进行参数高效微调，生成更可靠的伪标签；针对开放世界场景提出LoFT-OW提升判别能力

Result: 在多个基准测试中表现出优越性能，即使只使用1%的无标签数据也能超越先前方法

Conclusion: 基于基础模型微调的长尾半监督学习方法能有效解决传统方法的局限性，在标准设置和开放世界场景下都具有优异表现

Abstract: Long-tailed learning has garnered increasing attention due to its wide
applicability in real-world scenarios. Among existing approaches, Long-Tailed
Semi-Supervised Learning (LTSSL) has emerged as an effective solution by
incorporating a large amount of unlabeled data into the imbalanced labeled
dataset. However, most prior LTSSL methods are designed to train models from
scratch, which often leads to issues such as overconfidence and low-quality
pseudo-labels. To address these challenges, we extend LTSSL into the foundation
model fine-tuning paradigm and propose a novel framework: LoFT (Long-tailed
semi-supervised learning via parameter-efficient Fine-Tuning). We demonstrate
that fine-tuned foundation models can generate more reliable pseudolabels,
thereby benefiting imbalanced learning. Furthermore, we explore a more
practical setting by investigating semi-supervised learning under open-world
conditions, where the unlabeled data may include out-of-distribution (OOD)
samples. To handle this problem, we propose LoFT-OW (LoFT under Open-World
scenarios) to improve the discriminative ability. Experimental results on
multiple benchmarks demonstrate that our method achieves superior performance
compared to previous approaches, even when utilizing only 1\% of the unlabeled
data compared with previous works.

</details>


### [82] [Multi-Play Combinatorial Semi-Bandit Problem](https://arxiv.org/abs/2509.09933)
*Shintaro Nakamura,Yuko Kuroki,Wei Chen*

Main category: cs.LG

TL;DR: 提出了多播放组合半赌博机(MP-CSB)模型，扩展了传统组合半赌博机问题，支持非负整数动作空间，解决了最优运输和背包等问题的局限性。提出了两种算法：基于Thompson采样的算法和最佳两用算法，分别在随机和对抗环境下实现对数级和次线性遗憾。


<details>
  <summary>Details</summary>
Motivation: 传统组合半赌博机(CSB)问题仅限于二元决策空间，无法处理涉及非负整数流或分配的重要应用场景，如最优运输和背包问题。需要扩展模型以支持更广泛的实际应用。

Method: 提出了MP-CSB模型，允许选择非负整数动作并观察单个臂的多个反馈。设计了两种算法：1)基于Thompson采样的算法，计算可行且实现O(log T)遗憾；2)最佳两用算法，在随机环境下实现方差依赖遗憾，在对抗环境下实现数据依赖的次线性遗憾。

Result: 理论分析表明，Thompson采样算法在随机环境下达到O(log T)分布依赖遗憾，最佳两用算法在随机环境下实现O(log T)方差依赖遗憾，在对抗环境下实现Õ(√T)最坏情况遗憾，且遗憾具有数据依赖性。数值实验显示提出的算法优于现有CSB方法。

Conclusion: MP-CSB模型成功扩展了组合半赌博机框架，提出的算法在理论和实验上都表现出色，为处理非负整数动作空间的组合优化问题提供了有效解决方案。

Abstract: In the combinatorial semi-bandit (CSB) problem, a player selects an action
from a combinatorial action set and observes feedback from the base arms
included in the action. While CSB is widely applicable to combinatorial
optimization problems, its restriction to binary decision spaces excludes
important cases involving non-negative integer flows or allocations, such as
the optimal transport and knapsack problems.To overcome this limitation, we
propose the multi-play combinatorial semi-bandit (MP-CSB), where a player can
select a non-negative integer action and observe multiple feedbacks from a
single arm in each round. We propose two algorithms for the MP-CSB. One is a
Thompson-sampling-based algorithm that is computationally feasible even when
the action space is exponentially large with respect to the number of arms, and
attains $O(\log T)$ distribution-dependent regret in the stochastic regime,
where $T$ is the time horizon. The other is a best-of-both-worlds algorithm,
which achieves $O(\log T)$ variance-dependent regret in the stochastic regime
and the worst-case $\tilde{\mathcal{O}}\left( \sqrt{T} \right)$ regret in the
adversarial regime. Moreover, its regret in adversarial one is data-dependent,
adapting to the cumulative loss of the optimal action, the total quadratic
variation, and the path-length of the loss sequence. Finally, we numerically
show that the proposed algorithms outperform existing methods in the CSB
literature.

</details>


### [83] [SciML Agents: Write the Solver, Not the Solution](https://arxiv.org/abs/2509.09936)
*Saarth Gaonkar,Xiang Zheng,Haocheng Xi,Rishabh Tiwari,Kurt Keutzer,Dmitriy Morozov,Michael W. Mahoney,Amir Gholami*

Main category: cs.LG

TL;DR: 本文探讨使用LLMs生成科学计算代码而非直接预测解函数，引入两个新数据集评估LLMs作为SciML代理的能力，发现适当提示和微调可使LLMs可靠解决简单ODE问题


<details>
  <summary>Details</summary>
Motivation: 传统科学机器学习方法直接预测目标值存在精度和鲁棒性挑战，本文探索使用LLMs编写基于数值算法的代码，将学习负担从解函数转移到领域感知的数值选择

Method: 引入诊断数据集和大规模ODE基准测试，评估开源和闭源LLM在无引导vs领域知识引导提示、现成vs微调变体下的表现，测量可执行性和数值有效性

Result: 在充分上下文和引导提示下，较新的指令跟随模型在两个评估标准上都达到高精度，开源系统无需微调表现强劲，较老或较小模型仍能从微调中受益

Conclusion: 精心设计的提示和微调可以产生能够可靠解决简单ODE问题的专用LLM代理，为科学计算任务中的LLM应用提供了新视角

Abstract: Recent work in scientific machine learning aims to tackle scientific tasks
directly by predicting target values with neural networks (e.g.,
physics-informed neural networks, neural ODEs, neural operators, etc.), but
attaining high accuracy and robustness has been challenging. We explore an
alternative view: use LLMs to write code that leverages decades of numerical
algorithms. This shifts the burden from learning a solution function to making
domain-aware numerical choices. We ask whether LLMs can act as SciML agents
that, given a natural-language ODE description, generate runnable code that is
scientifically appropriate, selecting suitable solvers (stiff vs. non-stiff),
and enforcing stability checks. There is currently no benchmark to measure this
kind of capability for scientific computing tasks. As such, we first introduce
two new datasets: a diagnostic dataset of adversarial "misleading" problems;
and a large-scale benchmark of 1,000 diverse ODE tasks. The diagnostic set
contains problems whose superficial appearance suggests stiffness, and that
require algebraic simplification to demonstrate non-stiffness; and the
large-scale benchmark spans stiff and non-stiff ODE regimes. We evaluate open-
and closed-source LLM models along two axes: (i) unguided versus guided
prompting with domain-specific knowledge; and (ii) off-the-shelf versus
fine-tuned variants. Our evaluation measures both executability and numerical
validity against reference solutions. We find that with sufficient context and
guided prompts, newer instruction-following models achieve high accuracy on
both criteria. In many cases, recent open-source systems perform strongly
without fine-tuning, while older or smaller models still benefit from
fine-tuning. Overall, our preliminary results indicate that careful prompting
and fine-tuning can yield a specialized LLM agent capable of reliably solving
simple ODE problems.

</details>


### [84] [DyKen-Hyena: Dynamic Kernel Generation via Cross-Modal Attention for Multimodal Intent Recognition](https://arxiv.org/abs/2509.09940)
*Yifei Wang,Wenbin Wang,Yong Luo*

Main category: cs.LG

TL;DR: DyKen-Hyena模型通过将多模态信息转化为动态卷积核来调制文本特征提取，而不是简单融合特征，在多模态意图识别任务上取得了SOTA效果


<details>
  <summary>Details</summary>
Motivation: 传统多模态意图识别方法通过注意力机制融合多模态特征，但可能引入意图无关的噪声信息，破坏主要语言特征，且无法实现细粒度的token级调制

Method: 提出DyKen-Hyena模型，将音频-视觉线索转化为动态的每token卷积核，直接调制文本特征提取过程，实现从特征融合到处理调制的转变

Result: 在MIntRec和MIntRec2.0基准测试中达到最先进水平，在out-of-scope检测中获得+10.46%的F1分数提升

Conclusion: 该方法创建了更鲁棒的意图表示，验证了细粒度调制方法的有效性

Abstract: Though Multimodal Intent Recognition (MIR) proves effective by utilizing rich
information from multiple sources (e.g., language, video, and audio), the
potential for intent-irrelevant and conflicting information across modalities
may hinder performance from being further improved. Most current models attempt
to fuse modalities by applying mechanisms like multi-head attention to unimodal
feature sequences and then adding the result back to the original
representation. This process risks corrupting the primary linguistic features
with noisy or irrelevant non-verbal signals, as it often fails to capture the
fine-grained, token-level influence where non-verbal cues should modulate, not
just augment, textual meaning. To address this, we introduce DyKen-Hyena, which
reframes the problem from feature fusion to processing modulation. Our model
translates audio-visual cues into dynamic, per-token convolutional kernels that
directly modulate textual feature extraction. This fine-grained approach
achieves state-of-the-art results on the MIntRec and MIntRec2.0 benchmarks.
Notably, it yields a +10.46% F1-score improvement in out-of-scope detection,
validating that our method creates a fundamentally more robust intent
representation.

</details>


### [85] [Adaptive Token Merging for Efficient Transformer Semantic Communication at the Edge](https://arxiv.org/abs/2509.09955)
*Omar Erak,Omar Alhussein,Hatem Abou-Zeid,Mehdi Bennis,Sami Muhaidat*

Main category: cs.LG

TL;DR: 提出无需训练的token合并框架，通过动态合并语义冗余token来压缩transformer表示，在保持精度的同时显著降低计算和通信成本


<details>
  <summary>Details</summary>
Motivation: 大规模transformer在语义通信中计算和通信成本过高，难以部署在资源受限的边缘设备上，需要一种无需重新训练的高效压缩方法

Method: 基于每层相似度阈值选择性合并语义冗余token，将合并策略发现建模为多目标优化问题，使用贝叶斯优化获得精度、推理成本和通信成本之间的帕累托最优权衡

Result: 在ImageNet分类上以30%更少的FLOPs和低于20%的原始通信成本达到原始transformer精度；在VQA任务上以不到1/3计算量和1/10带宽达到与完整LLaVA模型相当的性能

Conclusion: 该框架为资源受限的边缘智能场景提供了实用且通用的解决方案，具有跨信道条件的鲁棒性，并提供固有的隐私保护优势

Abstract: Large-scale transformers are central to modern semantic communication, yet
their high computational and communication costs hinder deployment on
resource-constrained edge devices. This paper introduces a training-free
framework for adaptive token merging, a novel mechanism that compresses
transformer representations at runtime by selectively merging semantically
redundant tokens under per-layer similarity thresholds. Unlike prior
fixed-ratio reduction, our approach couples merging directly to input
redundancy, enabling data-dependent adaptation that balances efficiency and
task relevance without retraining. We cast the discovery of merging strategies
as a multi-objective optimization problem and leverage Bayesian optimization to
obtain Pareto-optimal trade-offs between accuracy, inference cost, and
communication cost. On ImageNet classification, we match the accuracy of the
unmodified transformer with 30\% fewer floating-point operations per second and
under 20\% of the original communication cost, while for visual question
answering our method achieves performance competitive with the full LLaVA model
at less than one-third of the compute and one-tenth of the bandwidth. Finally,
we show that our adaptive merging is robust across varying channel conditions
and provides inherent privacy benefits, substantially degrading the efficacy of
model inversion attacks. Our framework provides a practical and versatile
solution for deploying powerful transformer models in resource-limited edge
intelligence scenarios.

</details>


### [86] [Limited Reference, Reliable Generation: A Two-Component Framework for Tabular Data Generation in Low-Data Regimes](https://arxiv.org/abs/2509.09960)
*Mingxuan Jiang,Yongxin Wang,Ziyue Dai,Yicun Liu,Hongyi Nie,Sen Liu,Hongfeng Chai*

Main category: cs.LG

TL;DR: ReFine是一个合成表格数据生成框架，通过从可解释模型提取符号规则嵌入提示词来指导生成，并采用双粒度过滤策略减少分布不平衡，在回归和分类任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有表格生成方法需要充足参考数据，在领域特定数据库中效果有限；基于提示的LLM难以捕捉数据集特定的特征-标签依赖关系且生成冗余数据，导致下游任务性能下降。

Method: 从可解释模型推导符号"if-then"规则并嵌入提示词，明确指导生成朝向领域特定特征分布；应用双粒度过滤策略抑制过采样模式并选择性精炼稀有但信息丰富的样本。

Result: 在各种回归和分类基准测试中，ReFine始终优于最先进方法，回归任务R平方绝对提升0.44，分类任务F1分数相对提升10.0%。

Conclusion: ReFine框架通过规则引导和智能过滤有效解决了表格数据生成中的领域适应性和分布不平衡问题，显著提升了生成数据的质量和下游任务性能。

Abstract: Synthetic tabular data generation is increasingly essential in data
management, supporting downstream applications when real-world and high-quality
tabular data is insufficient. Existing tabular generation approaches, such as
generative adversarial networks (GANs), diffusion models, and fine-tuned Large
Language Models (LLMs), typically require sufficient reference data, limiting
their effectiveness in domain-specific databases with scarce records. While
prompt-based LLMs offer flexibility without parameter tuning, they often fail
to capture dataset-specific feature-label dependencies and generate redundant
data, leading to degradation in downstream task performance. To overcome these
issues, we propose ReFine, a framework that (i) derives symbolic "if-then"
rules from interpretable models and embeds them into prompts to explicitly
guide generation toward domain-specific feature distribution, and (ii) applies
a dual-granularity filtering strategy that suppresses over-sampling patterns
and selectively refines rare but informative samples to reduce distributional
imbalance. Extensive experiments on various regression and classification
benchmarks demonstrate that ReFine consistently outperforms state-of-the-art
methods, achieving up to 0.44 absolute improvement in R-squared for regression
and 10.0 percent relative improvement in F1 score for classification tasks.

</details>


### [87] [Data-Driven Energy Estimation for Virtual Servers Using Combined System Metrics and Machine Learning](https://arxiv.org/abs/2509.09991)
*Amandip Sangha*

Main category: cs.LG

TL;DR: 提出一种基于机器学习的虚拟服务器能耗估计方法，无需物理功率测量接口，仅使用虚拟机资源利用率指标即可准确预测能耗


<details>
  <summary>Details</summary>
Motivation: 解决虚拟化环境（如云平台）中无法直接测量能耗的关键问题，为能源感知调度和成本优化提供支持

Method: 使用梯度提升回归器（Gradient Boosting Regressor），基于虚拟机收集的资源利用率指标来预测通过RAPL测量的主机能耗

Result: 在多样化工作负载实验中实现了高预测精度（0.90 ≤ R² ≤ 0.97），首次证明了无需特权主机访问的纯客户端能耗估计的可行性

Conclusion: 该方法能够实现虚拟化环境中的能源感知调度、成本优化和物理主机独立的能耗估计，填补了虚拟化环境中直接能耗测量不可行的关键空白

Abstract: This paper presents a machine learning-based approach to estimate the energy
consumption of virtual servers without access to physical power measurement
interfaces. Using resource utilization metrics collected from guest virtual
machines, we train a Gradient Boosting Regressor to predict energy consumption
measured via RAPL on the host. We demonstrate, for the first time, guest-only
resource-based energy estimation without privileged host access with
experiments across diverse workloads, achieving high predictive accuracy and
variance explained ($0.90 \leq R^2 \leq 0.97$), indicating the feasibility of
guest-side energy estimation. This approach can enable energy-aware scheduling,
cost optimization and physical host independent energy estimates in virtualized
environments. Our approach addresses a critical gap in virtualized environments
(e.g. cloud) where direct energy measurement is infeasible.

</details>


### [88] [Neural Scaling Laws for Deep Regression](https://arxiv.org/abs/2509.10000)
*Tilen Cadez,Kyoung-Min Kim*

Main category: cs.LG

TL;DR: 该论文实证研究了深度回归模型中的神经缩放定律，发现在扭曲范德瓦尔斯磁体的参数估计模型中，损失与训练数据集大小和模型容量之间存在幂律关系，缩放指数在1到2之间。


<details>
  <summary>Details</summary>
Motivation: 虽然神经缩放定律在大语言模型中很重要，但在深度回归模型中的应用仍未被充分探索，需要研究这些定律在回归任务中的表现。

Method: 使用扭曲范德瓦尔斯磁体的参数估计模型，采用全连接网络、残差网络和视觉变换器等多种架构，在不同数据集大小和模型容量下进行实验。

Result: 观察到损失与训练数据集大小和模型容量之间存在幂律关系，缩放指数范围为1到2，具体值取决于回归参数和模型细节。

Conclusion: 一致的缩放行为和大缩放指数表明，深度回归模型的性能可以随着数据量的增加而显著提高。

Abstract: Neural scaling laws--power-law relationships between generalization errors
and characteristics of deep learning models--are vital tools for developing
reliable models while managing limited resources. Although the success of large
language models highlights the importance of these laws, their application to
deep regression models remains largely unexplored. Here, we empirically
investigate neural scaling laws in deep regression using a parameter estimation
model for twisted van der Waals magnets. We observe power-law relationships
between the loss and both training dataset size and model capacity across a
wide range of values, employing various architectures--including fully
connected networks, residual networks, and vision transformers. Furthermore,
the scaling exponents governing these relationships range from 1 to 2, with
specific values depending on the regressed parameters and model details. The
consistent scaling behaviors and their large scaling exponents suggest that the
performance of deep regression models can improve substantially with increasing
data size.

</details>


### [89] [Intrinsic Dimension Estimating Autoencoder (IDEA) Using CancelOut Layer and a Projected Loss](https://arxiv.org/abs/2509.10011)
*Antoine Orioua,Philipp Krah,Julian Koellermeier*

Main category: cs.LG

TL;DR: IDEA是一种能够估计数据集内在维度并重建原始数据的自编码器，通过投影重建损失项和重加权双CancelOut层结构，在理论和实际流体动力学数据上都表现出良好性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在估计非线性流形数据集的内在维度时存在局限性，需要一种既能准确估计内在维度又能有效重建原始数据的统一框架。

Method: 提出IDEA自编码器，使用重加权双CancelOut层构建潜在空间，引入投影重建损失项通过连续移除潜在维度来指导训练。

Result: 在理论基准测试中表现出良好的准确性和通用性，在流体动力学数值解数据集上成功估计内在维度并重建原始解。

Conclusion: IDEA是一个有效且通用的内在维度估计和重建框架，适用于线性和非线性流形数据集，在理论和实际应用中均表现优异。

Abstract: This paper introduces the Intrinsic Dimension Estimating Autoencoder (IDEA),
which identifies the underlying intrinsic dimension of a wide range of datasets
whose samples lie on either linear or nonlinear manifolds. Beyond estimating
the intrinsic dimension, IDEA is also able to reconstruct the original dataset
after projecting it onto the corresponding latent space, which is structured
using re-weighted double CancelOut layers. Our key contribution is the
introduction of the projected reconstruction loss term, guiding the training of
the model by continuously assessing the reconstruction quality under the
removal of an additional latent dimension. We first assess the performance of
IDEA on a series of theoretical benchmarks to validate its robustness. These
experiments allow us to test its reconstruction ability and compare its
performance with state-of-the-art intrinsic dimension estimators. The
benchmarks show good accuracy and high versatility of our approach.
Subsequently, we apply our model to data generated from the numerical solution
of a vertically resolved one-dimensional free-surface flow, following a
pointwise discretization of the vertical velocity profile in the horizontal
direction, vertical direction, and time. IDEA succeeds in estimating the
dataset's intrinsic dimension and then reconstructs the original solution by
working directly within the projection space identified by the network.

</details>


### [90] [Exploring Expert Specialization through Unsupervised Training in Sparse Mixture of Experts](https://arxiv.org/abs/2509.10025)
*Strahinja Nikolic,Ilker Oguz,Demetri Psaltis*

Main category: cs.LG

TL;DR: 本文提出了一种稀疏专家混合变分自编码器（SMoE-VAE）架构，发现在无监督专家路由方面表现优于有监督基线，专家能够学习到超越人工定义类别边界的有意义子类别结构。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络的内部组织是深度学习可解释性的基本挑战，需要探索新的架构来揭示数据的内在结构。

Method: 采用稀疏专家混合变分自编码器（SMoE-VAE）架构，在QuickDraw数据集上测试，比较无监督专家路由与基于真实标签的有监督基线。

Result: 无监督路由始终获得更好的重建性能，专家学会识别有意义子类别结构，这些结构往往超越人工定义的类别边界。t-SNE可视化和重建分析显示MoE模型发现更符合模型目标的基本数据结构。

Conclusion: 研究数据集大小的影响揭示了数据量与专家专业化之间的权衡，为设计高效的MoE架构提供了指导，表明无监督方法在发现数据内在结构方面具有优势。

Abstract: Understanding the internal organization of neural networks remains a
fundamental challenge in deep learning interpretability. We address this
challenge by exploring a novel Sparse Mixture of Experts Variational
Autoencoder (SMoE-VAE) architecture. We test our model on the QuickDraw
dataset, comparing unsupervised expert routing against a supervised baseline
guided by ground-truth labels. Surprisingly, we find that unsupervised routing
consistently achieves superior reconstruction performance. The experts learn to
identify meaningful sub-categorical structures that often transcend
human-defined class boundaries. Through t-SNE visualizations and reconstruction
analysis, we investigate how MoE models uncover fundamental data structures
that are more aligned with the model's objective than predefined labels.
Furthermore, our study on the impact of dataset size provides insights into the
trade-offs between data quantity and expert specialization, offering guidance
for designing efficient MoE architectures.

</details>


### [91] [Sparse Coding Representation of 2-way Data](https://arxiv.org/abs/2509.10033)
*Boya Ma,Abram Magner,Maxwell McNeil,Petko Bogdanov*

Main category: cs.LG

TL;DR: 提出了一种用于稀疏字典编码的低秩编码模型AODL，通过凸松弛和交替优化解决多字典学习中的数据复杂度问题，在保持重建质量的同时获得更稀疏的解


<details>
  <summary>Details</summary>
Motivation: 稀疏字典编码在处理多维时空数据时需要同时学习多个字典和编码系数，这在大规模多字典场景中变得特别具有挑战性，因为编码系数对应于所有字典原子的组合

Method: 提出了一个低秩编码模型来处理双字典场景，开发了AODL凸松弛解决方案，通过稀疏编码矩阵和学习字典之间的交替优化来求解，并证明了其收敛性

Result: AODL在固定重建质量下相比非低秩和固定分析字典基线学习到高达90%更稀疏的解，学习到的字典揭示了训练样本中存在的可解释模式

Conclusion: AODL方法有效解决了多字典学习中的数据复杂度问题，在合成和真实数据集上展示了优异的数据重建和缺失值插补性能，同时提供了更好的稀疏性和可解释性

Abstract: Sparse dictionary coding represents signals as linear combinations of a few
dictionary atoms. It has been applied to images, time series, graph signals and
multi-way spatio-temporal data by jointly employing temporal and spatial
dictionaries. Data-agnostic analytical dictionaries, such as the discrete
Fourier transform, wavelets and graph Fourier, have seen wide adoption due to
efficient implementations and good practical performance. On the other hand,
dictionaries learned from data offer sparser and more accurate solutions but
require learning of both the dictionaries and the coding coefficients. This
becomes especially challenging for multi-dictionary scenarios since encoding
coefficients correspond to all atom combinations from the dictionaries. To
address this challenge, we propose a low-rank coding model for 2-dictionary
scenarios and study its data complexity. Namely, we establish a bound on the
number of samples needed to learn dictionaries that generalize to unseen
samples from the same distribution. We propose a convex relaxation solution,
called AODL, whose exact solution we show also solves the original problem. We
then solve this relaxation via alternating optimization between the sparse
coding matrices and the learned dictionaries, which we prove to be convergent.
We demonstrate its quality for data reconstruction and missing value imputation
in both synthetic and real-world datasets. For a fixed reconstruction quality,
AODL learns up to 90\% sparser solutions compared to non-low-rank and
analytical (fixed) dictionary baselines. In addition, the learned dictionaries
reveal interpretable insights into patterns present within the samples used for
training.

</details>


### [92] [Symbolic Feedforward Networks for Probabilistic Finite Automata: Exact Simulation and Learnability](https://arxiv.org/abs/2509.10034)
*Sahil Rajesh Dhayalkar*

Main category: cs.LG

TL;DR: 该论文提出了一个形式化理论，证明概率有限自动机(PFAs)可以通过符号前馈神经网络精确模拟。通过将状态分布表示为向量、转移表示为随机矩阵，实现了无需递归的并行、可解释、可微分PFA模拟。


<details>
  <summary>Details</summary>
Motivation: 统一概率自动机理论与神经网络架构，在严格的代数框架下弥合符号计算与深度学习之间的差距，实现概率自动机的可学习神经网络模拟。

Method: 使用符号前馈神经网络架构，将状态分布表示为向量，转移表示为随机矩阵，通过矩阵-向量乘积实现概率状态传播，采用软更新而非递归方式。

Result: 证明了PFAs与特定类别神经网络的等价性，展示了这些符号模拟器不仅具有表达性而且可学习：通过标准梯度下降优化在标记序列数据上训练，能够恢复真实PFAs的精确行为。

Conclusion: 该工作通过命题5.1形式化了可学习性这一核心贡献，为概率自动机理论与神经架构的统一提供了严格的代数框架，成功连接了符号计算与深度学习领域。

Abstract: We present a formal and constructive theory showing that probabilistic finite
automata (PFAs) can be exactly simulated using symbolic feedforward neural
networks. Our architecture represents state distributions as vectors and
transitions as stochastic matrices, enabling probabilistic state propagation
via matrix-vector products. This yields a parallel, interpretable, and
differentiable simulation of PFA dynamics using soft updates-without
recurrence. We formally characterize probabilistic subset construction,
$\varepsilon$-closure, and exact simulation via layered symbolic computation,
and prove equivalence between PFAs and specific classes of neural networks. We
further show that these symbolic simulators are not only expressive but
learnable: trained with standard gradient descent-based optimization on labeled
sequence data, they recover the exact behavior of ground-truth PFAs. This
learnability, formalized in Proposition 5.1, is the crux of this work. Our
results unify probabilistic automata theory with neural architectures under a
rigorous algebraic framework, bridging the gap between symbolic computation and
deep learning.

</details>


### [93] [FedRP: A Communication-Efficient Approach for Differentially Private Federated Learning Using Random Projection](https://arxiv.org/abs/2509.10041)
*Mohammad Hasan Narimani,Mostafa Tavassolipour*

Main category: cs.LG

TL;DR: FedRP是一种新颖的联邦学习算法，结合随机投影和ADMM优化框架，在保护隐私的同时降低通信成本，提供(ε,δ)-差分隐私保证。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在物联网和医疗数据分析等敏感领域面临用户隐私保护和通信成本管理的挑战，需要开发既能保护隐私又高效的算法。

Method: 提出FedRP算法，集成随机投影技术和ADMM优化框架，通过随机投影降低模型参数维度后再传输到中央服务器。

Result: 实验结果显示FedRP不仅保持高模型精度，在隐私保护和通信效率方面均优于现有方法，包括传统差分隐私方法和FedADMM。

Conclusion: FedRP算法成功解决了联邦学习中的隐私保护和通信效率问题，为敏感数据场景下的协作学习提供了有效的解决方案。

Abstract: Federated learning (FL) offers an innovative paradigm for collaborative model
training across decentralized devices, such as smartphones, balancing enhanced
predictive performance with the protection of user privacy in sensitive areas
like Internet of Things (IoT) and medical data analysis. Despite its
advantages, FL encounters significant challenges related to user privacy
protection against potential attacks and the management of communication costs.
This paper introduces a novel federated learning algorithm called FedRP, which
integrates random projection techniques with the Alternating Direction Method
of Multipliers (ADMM) optimization framework. This approach enhances privacy by
employing random projection to reduce the dimensionality of model parameters
prior to their transmission to a central server, reducing the communication
cost. The proposed algorithm offers a strong $(\epsilon, \delta)$-differential
privacy guarantee, demonstrating resilience against data reconstruction
attacks. Experimental results reveal that FedRP not only maintains high model
accuracy but also outperforms existing methods, including conventional
differential privacy approaches and FedADMM, in terms of both privacy
preservation and communication efficiency.

</details>


### [94] [Uncertainty-Aware Tabular Prediction: Evaluating VBLL-Enhanced TabPFN in Safety-Critical Medical Data](https://arxiv.org/abs/2509.10048)
*Madhushan Ramalingam*

Main category: cs.LG

TL;DR: 在三个医疗表格数据集上评估VBLL与TabPFN集成的不确定性校准性能，发现原始TabPFN始终优于集成版本


<details>
  <summary>Details</summary>
Motivation: 预测模型在医疗诊断等安全关键应用中需要可靠的不确定性估计，TabPFN是新兴的表格数据基础模型，VBLL是最先进的轻量级变分方法，研究两者集成的校准性能

Method: 在三个基准医疗表格数据集上进行实验，比较原始TabPFN和VBLL集成版本的性能

Result: 与预期相反，原始TabPFN在所有数据集上的不确定性校准性能都优于VBLL集成版本

Conclusion: VBLL集成并未改善TabPFN的不确定性校准性能，原始TabPFN表现更佳

Abstract: Predictive models are being increasingly used across a wide range of domains,
including safety-critical applications such as medical diagnosis and criminal
justice. Reliable uncertainty estimation is a crucial task in such settings.
Tabular Prior-data Fitted Network (TabPFN) is a recently proposed machine
learning foundation model for tabular dataset, which uses a generative
transformer architecture. Variational Bayesian Last Layers (VBLL) is a
state-of-the-art lightweight variational formulation that effectively improves
uncertainty estimation with minimal computational overhead. In this work we aim
to evaluate the performance of VBLL integrated with the recently proposed
TabPFN in uncertainty calibration. Our experiments, conducted on three
benchmark medical tabular datasets, compare the performance of the original
TabPFN and the VBLL-integrated version. Contrary to expectations, we observed
that original TabPFN consistently outperforms VBLL integrated TabPFN in
uncertainty calibration across all datasets.

</details>


### [95] [KAN-SR: A Kolmogorov-Arnold Network Guided Symbolic Regression Framework](https://arxiv.org/abs/2509.10089)
*Marco Andrea Bühler,Gonzalo Guillén-Gosálbez*

Main category: cs.LG

TL;DR: 提出基于Kolmogorov-Arnold网络(KAN)的符号回归框架KAN-SR，采用分治策略，结合深度学习技术和简化策略，能够准确恢复Feynman SRSD数据集的真实方程，并能精确建模生物过程系统动力学。


<details>
  <summary>Details</summary>
Motivation: 传统符号回归通常使用遗传编程方法，本文旨在利用深度学习技术改进符号回归，通过KAN网络和简化策略提高方程发现的准确性和效率。

Method: 使用Kolmogorov-Arnold网络(KAN)构建符号回归框架，采用分治方法，结合平移对称性和可分离性等简化策略，并与神经控制微分方程结合建模动力学系统。

Result: 成功恢复Feynman SRSD数据集的真实方程，并精确建模了硅内生物过程系统的动力学，为其他工程系统的动态建模开辟了新途径。

Conclusion: KAN-SR框架在符号回归任务中表现出色，结合深度学习技术和简化策略，能够有效发现数学方程并精确建模复杂动力学系统，具有广泛的工程应用前景。

Abstract: We introduce a novel symbolic regression framework, namely KAN-SR, built on
Kolmogorov Arnold Networks (KANs) which follows a divide-and-conquer approach.
Symbolic regression searches for mathematical equations that best fit a given
dataset and is commonly solved with genetic programming approaches. We show
that by using deep learning techniques, more specific KANs, and combining them
with simplification strategies such as translational symmetries and
separabilities, we are able to recover ground-truth equations of the Feynman
Symbolic Regression for Scientific Discovery (SRSD) dataset. Additionally, we
show that by combining the proposed framework with neural controlled
differential equations, we are able to model the dynamics of an in-silico
bioprocess system precisely, opening the door for the dynamic modeling of other
engineering systems.

</details>


### [96] [Cost-Free Personalization via Information-Geometric Projection in Bayesian Federated Learning](https://arxiv.org/abs/2509.10132)
*Nour Jamoussi,Giuseppe Serra,Photios A. Stavrou,Marios Kountouris*

Main category: cs.LG

TL;DR: 提出了一种基于信息几何投影的贝叶斯联邦学习个性化框架，通过将全局模型投影到用户本地模型的邻域，实现全局泛化与本地特化的可调节权衡。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯联邦学习方法通常依赖MCMC采样或变分推断，需要个性化机制来适应异构数据分布。本文旨在开发一种计算高效且能灵活权衡全局与本地性能的个性化方法。

Method: 采用信息几何投影框架，将全局模型投影到用户本地模型的统计流形邻域，证明该投影等价于计算统计流形上的重心，从而获得闭式解和零成本个性化。结合IVON优化器应用于变分学习设置。

Result: 在异构数据分布下的实证评估表明，该方法能有效平衡全局和本地性能，且计算开销极小。

Conclusion: 提出的信息几何投影框架为贝叶斯联邦学习提供了一种计算高效、可调节的个性化方法，能够在不增加计算成本的情况下实现全局模型与本地特化的良好平衡。

Abstract: Bayesian Federated Learning (BFL) combines uncertainty modeling with
decentralized training, enabling the development of personalized and reliable
models under data heterogeneity and privacy constraints. Existing approaches
typically rely on Markov Chain Monte Carlo (MCMC) sampling or variational
inference, often incorporating personalization mechanisms to better adapt to
local data distributions. In this work, we propose an information-geometric
projection framework for personalization in parametric BFL. By projecting the
global model onto a neighborhood of the user's local model, our method enables
a tunable trade-off between global generalization and local specialization.
Under mild assumptions, we show that this projection step is equivalent to
computing a barycenter on the statistical manifold, allowing us to derive
closed-form solutions and achieve cost-free personalization. We apply the
proposed approach to a variational learning setup using the Improved
Variational Online Newton (IVON) optimizer and extend its application to
general aggregation schemes in BFL. Empirical evaluations under heterogeneous
data distributions confirm that our method effectively balances global and
local performance with minimal computational overhead.

</details>


### [97] [BenchECG and xECG: a benchmark and baseline for ECG foundation models](https://arxiv.org/abs/2509.10151)
*Riccardo Lunelli,Angus Nicolson,Samuel Martin Pröll,Sebastian Johannes Reinstadler,Axel Bauer,Clemens Dlaska*

Main category: cs.LG

TL;DR: 提出了BenchECG标准化基准和xECG模型，通过统一的评估框架解决ECG基础模型缺乏公平比较的问题，xECG在多个数据集和任务上表现最优。


<details>
  <summary>Details</summary>
Motivation: ECG基础模型缺乏一致的评估标准，先前研究使用不同的任务和数据集，阻碍了公平比较，需要建立标准化基准来推动ECG表示学习的发展。

Method: 开发BenchECG标准化基准，包含公开ECG数据集和多样化任务；提出基于xLSTM的xECG模型，采用SimDINOv2自监督学习进行训练。

Result: xECG在BenchECG基准上取得了最佳分数，是唯一在所有数据集和任务上都表现优异的公开可用模型，超越了现有最先进方法。

Conclusion: BenchECG通过标准化评估实现了严格比较，xECG为未来ECG基础模型设立了新的性能基准，将加速ECG表示学习领域的进展。

Abstract: Electrocardiograms (ECGs) are inexpensive, widely used, and well-suited to
deep learning. Recently, interest has grown in developing foundation models for
ECGs - models that generalise across diverse downstream tasks. However,
consistent evaluation has been lacking: prior work often uses narrow task
selections and inconsistent datasets, hindering fair comparison. Here, we
introduce BenchECG, a standardised benchmark comprising a comprehensive suite
of publicly available ECG datasets and versatile tasks. We also propose xECG,
an xLSTM-based recurrent model trained with SimDINOv2 self-supervised learning,
which achieves the best BenchECG score compared to publicly available
state-of-the-art models. In particular, xECG is the only publicly available
model to perform strongly on all datasets and tasks. By standardising
evaluation, BenchECG enables rigorous comparison and aims to accelerate
progress in ECG representation learning. xECG achieves superior performance
over earlier approaches, defining a new baseline for future ECG foundation
models.

</details>


### [98] [FedBiF: Communication-Efficient Federated Learning via Bits Freezing](https://arxiv.org/abs/2509.10161)
*Shiwei Li,Qunwei Li,Haozhao Wang,Ruixuan Li,Jianbin Lin,Wenliang Zhong*

Main category: cs.LG

TL;DR: 提出了FedBiF联邦学习框架，通过在本地训练期间直接学习量化模型参数，逐比特更新参数来减少通信开销，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习存在显著的通信开销问题，现有量化方法通常在本地训练后应用量化，导致量化误差影响模型精度。

Method: 服务器先量化模型参数并发送给客户端，每个客户端每次只更新多比特参数表示中的单个比特，冻结其余比特，实现逐比特更新策略。

Result: 在5个常用数据集上的IID和非IID设置下，FedBiF实现了优异的通信压缩，促进模型稀疏性，仅使用1bpp上行和3bpp下行通信即可达到与FedAvg相当的精度。

Conclusion: FedBiF框架有效解决了联邦学习的通信开销问题，通过直接学习量化参数和逐比特更新策略，在保持精度的同时显著减少通信成本。

Abstract: Federated learning (FL) is an emerging distributed machine learning paradigm
that enables collaborative model training without sharing local data. Despite
its advantages, FL suffers from substantial communication overhead, which can
affect training efficiency. Recent efforts have mitigated this issue by
quantizing model updates to reduce communication costs. However, most existing
methods apply quantization only after local training, introducing quantization
errors into the trained parameters and potentially degrading model accuracy. In
this paper, we propose Federated Bit Freezing (FedBiF), a novel FL framework
that directly learns quantized model parameters during local training. In each
communication round, the server first quantizes the model parameters and
transmits them to the clients. FedBiF then allows each client to update only a
single bit of the multi-bit parameter representation, freezing the remaining
bits. This bit-by-bit update strategy reduces each parameter update to one bit
while maintaining high precision in parameter representation. Extensive
experiments are conducted on five widely used datasets under both IID and
Non-IID settings. The results demonstrate that FedBiF not only achieves
superior communication compression but also promotes sparsity in the resulting
models. Notably, FedBiF attains accuracy comparable to FedAvg, even when using
only 1 bit-per-parameter (bpp) for uplink and 3 bpp for downlink communication.
The code is available at https://github.com/Leopold1423/fedbif-tpds25.

</details>


### [99] [Federated Multi-Agent Reinforcement Learning for Privacy-Preserving and Energy-Aware Resource Management in 6G Edge Networks](https://arxiv.org/abs/2509.10163)
*Francisco Javier Esono Nkulu Andong,Qi Min*

Main category: cs.LG

TL;DR: 提出了一种基于联邦多智能体强化学习（Fed-MARL）的6G网络资源管理框架，通过跨层编排实现隐私保护、能效优化和实时资源分配


<details>
  <summary>Details</summary>
Motivation: 6G网络向超密集智能边缘环境发展，需要在严格隐私、移动性和能耗约束下实现高效资源管理

Method: 使用深度循环Q网络（DRQN）学习去中心化策略，结合椭圆曲线Diffie-Hellman密钥交换的安全聚合协议保护隐私，将问题建模为部分可观测多智能体马尔可夫决策过程

Result: 仿真显示Fed-MARL在任务成功率、延迟、能效和公平性方面优于集中式MARL和启发式基线方法，同时确保隐私保护和可扩展性

Conclusion: 该框架为6G异构边缘设备提供了有效的隐私保护实时资源管理解决方案，能够满足URLLC、eMBB和mMTC等6G特定服务需求

Abstract: As sixth-generation (6G) networks move toward ultra-dense, intelligent edge
environments, efficient resource management under stringent privacy, mobility,
and energy constraints becomes critical. This paper introduces a novel
Federated Multi-Agent Reinforcement Learning (Fed-MARL) framework that
incorporates cross-layer orchestration of both the MAC layer and application
layer for energy-efficient, privacy-preserving, and real-time resource
management across heterogeneous edge devices. Each agent uses a Deep Recurrent
Q-Network (DRQN) to learn decentralized policies for task offloading, spectrum
access, and CPU energy adaptation based on local observations (e.g., queue
length, energy, CPU usage, and mobility). To protect privacy, we introduce a
secure aggregation protocol based on elliptic curve Diffie Hellman key
exchange, which ensures accurate model updates without exposing raw data to
semi-honest adversaries. We formulate the resource management problem as a
partially observable multi-agent Markov decision process (POMMDP) with a
multi-objective reward function that jointly optimizes latency, energy
efficiency, spectral efficiency, fairness, and reliability under 6G-specific
service requirements such as URLLC, eMBB, and mMTC. Simulation results
demonstrate that Fed-MARL outperforms centralized MARL and heuristic baselines
in task success rate, latency, energy efficiency, and fairness, while ensuring
robust privacy protection and scalability in dynamic, resource-constrained 6G
edge networks.

</details>


### [100] [A Symmetry-Integrated Approach to Surface Code Decoding](https://arxiv.org/abs/2509.10164)
*Hoshitaro Ohnishi,Hideo Mukai*

Main category: cs.LG

TL;DR: 提出一种通过神经网络近似综合征测量的连续函数来重新优化表面码解码器的技术，提高了量子纠错解码的准确性


<details>
  <summary>Details</summary>
Motivation: 传统解码器只能获取误差概率分布，由于输入预测结果的不唯一性，需要解决这个问题以提高解码精度

Method: 使用神经网络数学插值近似综合征测量，将解码问题重构为回归问题，采用多层感知机、卷积神经网络、循环神经网络和Transformer等架构

Result: 在码距5和7的所有情况下，重新优化的解码器都比原始模型精度更高，证明了该方法的普适有效性

Conclusion: 将表面码解码问题重新构建为可通过深度学习解决的回归问题是一个有用的策略

Abstract: Quantum error correction, which utilizes logical qubits that are encoded as
redundant multiple physical qubits to find and correct errors in physical
qubits, is indispensable for practical quantum computing. Surface code is
considered to be a promising encoding method with a high error threshold that
is defined by stabilizer generators. However, previous methods have suffered
from the problem that the decoder acquires solely the error probability
distribution because of the non-uniqueness of correct prediction obtained from
the input. To circumvent this problem, we propose a technique to reoptimize the
decoder model by approximating syndrome measurements with a continuous function
that is mathematically interpolated by neural network. We evaluated the
improvement in accuracy of a multilayer perceptron based decoder for code
distances of 5 and 7 as well as for decoders based on convolutional and
recurrent neural networks and transformers for a code distance of 5. In all
cases, the reoptimized decoder gave better accuracy than the original models,
demonstrating the universal effectiveness of the proposed method that is
independent of code distance or network architecture. These results suggest
that re-framing the problem of surface code decoding into a regression problem
that can be tackled by deep learning is a useful strategy.

</details>


### [101] [The Hidden Width of Deep ResNets: Tight Error Bounds and Phase Diagrams](https://arxiv.org/abs/2509.10167)
*Lénaïc Chizat*

Main category: cs.LG

TL;DR: 该论文研究了深度残差网络(ResNets)的梯度训练动态，证明了在深度趋于无穷时训练过程收敛到神经平均ODE动态，并分析了不同残差缩放参数对特征学习的影响。


<details>
  <summary>Details</summary>
Motivation: 研究大深度残差网络从标准随机初始化开始的训练动态，特别是深度趋于无穷时的极限行为，以及不同参数缩放对特征学习能力的影响。

Method: 使用数学分析框架，将ResNet的前向和后向传播视为随机平均ODE的近似，通过混沌传播理论证明训练动态的收敛性，并进行数值实验验证理论结果。

Result: 证明了深度趋于无穷时训练动态收敛到神经平均ODE；确定了导致完全特征学习的残差缩放参数为Θ(√D/LM)；给出了模型输出与极限之间的误差界限O(1/L + √D/√LM)。

Conclusion: 深度ResNets的训练动态在深度趋于无穷时收敛到平均ODE，特定的残差缩放参数能够实现完全特征学习，而过大参数会导致懒惰训练机制。

Abstract: We study the gradient-based training of large-depth residual networks
(ResNets) from standard random initializations. We show that with a diverging
depth $L$, a fixed embedding dimension $D$, and an arbitrary hidden width $M$,
the training dynamics converges to a Neural Mean ODE training dynamics.
Remarkably, the limit is independent of the scaling of $M$, covering practical
cases of, say, Transformers, where $M$ (the number of hidden units or attention
heads per layer) is typically of the order of $D$. For a residual scale
$\Theta_D\big(\frac{\alpha}{LM}\big)$, we obtain the error bound
$O_D\big(\frac{1}{L}+ \frac{\alpha}{\sqrt{LM}}\big)$ between the model's output
and its limit after a fixed number gradient of steps, and we verify empirically
that this rate is tight. When $\alpha=\Theta(1)$, the limit exhibits complete
feature learning, i.e. the Mean ODE is genuinely non-linearly parameterized. In
contrast, we show that $\alpha \to \infty$ yields a \lazy ODE regime where the
Mean ODE is linearly parameterized. We then focus on the particular case of
ResNets with two-layer perceptron blocks, for which we study how these scalings
depend on the embedding dimension $D$. We show that for this model, the only
residual scale that leads to complete feature learning is
$\Theta\big(\frac{\sqrt{D}}{LM}\big)$. In this regime, we prove the error bound
$O\big(\frac{1}{L}+ \frac{\sqrt{D}}{\sqrt{LM}}\big)$ between the ResNet and its
limit after a fixed number of gradient steps, which is also empirically tight.
Our convergence results rely on a novel mathematical perspective on ResNets :
(i) due to the randomness of the initialization, the forward and backward pass
through the ResNet behave as the stochastic approximation of certain mean ODEs,
and (ii) by propagation of chaos (that is, asymptotic independence of the
units) this behavior is preserved through the training dynamics.

</details>


### [102] [P3D: Scalable Neural Surrogates for High-Resolution 3D Physics Simulations with Global Context](https://arxiv.org/abs/2509.10186)
*Benjamin Holzschuh,Georg Kohl,Florian Redinger,Nils Thuerey*

Main category: cs.LG

TL;DR: 提出了一个可扩展的框架，用于学习高分辨率3D物理模拟的确定性和概率性神经代理模型，通过混合CNN-Transformer架构在速度和精度上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决高分辨率3D物理模拟计算成本高的问题，需要开发高效的神经代理模型来替代传统数值模拟方法。

Method: 采用混合CNN-Transformer骨干架构，支持在小块模拟域上进行预训练，然后融合获得全局解，并通过序列到序列模型包含长程依赖关系。

Result: 在14种不同类型3D PDE动力学学习任务中显著优于基线方法，可扩展到512^3空间分辨率的高分辨率各向同性湍流，并能作为扩散模型生成不同雷诺数下高度湍流3D通道流的概率样本。

Conclusion: 该框架为高分辨率3D物理模拟提供了高效准确的神经代理解决方案，在保持精度的同时大幅降低了计算和内存需求。

Abstract: We present a scalable framework for learning deterministic and probabilistic
neural surrogates for high-resolution 3D physics simulations. We introduce a
hybrid CNN-Transformer backbone architecture targeted for 3D physics
simulations, which significantly outperforms existing architectures in terms of
speed and accuracy. Our proposed network can be pretrained on small patches of
the simulation domain, which can be fused to obtain a global solution,
optionally guided via a fast and scalable sequence-to-sequence model to include
long-range dependencies. This setup allows for training large-scale models with
reduced memory and compute requirements for high-resolution datasets. We
evaluate our backbone architecture against a large set of baseline methods with
the objective to simultaneously learn the dynamics of 14 different types of
PDEs in 3D. We demonstrate how to scale our model to high-resolution isotropic
turbulence with spatial resolutions of up to $512^3$. Finally, we demonstrate
the versatility of our network by training it as a diffusion model to produce
probabilistic samples of highly turbulent 3D channel flows across varying
Reynolds numbers, accurately capturing the underlying flow statistics.

</details>


### [103] [Hadamard-Riemannian Optimization for Margin-Variance Ensemble](https://arxiv.org/abs/2509.10189)
*Zexu Jin*

Main category: cs.LG

TL;DR: 提出了一种新的集成学习框架，通过将边际方差显式纳入损失函数，联合优化负期望边际和其方差，从而提高模型的鲁棒性和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 传统的基于边际的集成方法主要关注最大化期望边际，但忽视了边际方差的关键作用，这限制了模型的泛化能力并在噪声或不平衡数据集中容易过拟合。此外，传统方法在概率单纯形中优化集成权重存在计算效率低和可扩展性挑战。

Method: 通过将边际方差显式纳入损失函数，联合优化负期望边际和其方差；通过将集成权重重新参数化到单位球面上，简化优化过程并提高计算效率。

Result: 在多个基准数据集上的广泛实验表明，该方法始终优于传统的基于边际的集成技术。

Conclusion: 所提出的方法通过考虑边际方差和优化权重参数化，有效提高了集成学习的鲁棒性和泛化性能，具有实际应用价值。

Abstract: Ensemble learning has been widely recognized as a pivotal technique for
boosting predictive performance by combining multiple base models.
Nevertheless, conventional margin-based ensemble methods predominantly focus on
maximizing the expected margin while neglecting the critical role of margin
variance, which inherently restricts the generalization capability of the model
and heightens its vulnerability to overfitting, particularly in noisy or
imbalanced datasets. Additionally, the conventional approach of optimizing
ensemble weights within the probability simplex often introduces computational
inefficiency and scalability challenges, complicating its application to
large-scale problems. To tackle these limitations, this paper introduces a
novel ensemble learning framework that explicitly incorporates margin variance
into the loss function. Our method jointly optimizes the negative expected
margin and its variance, leading to enhanced robustness and improved
generalization performance. Moreover, by reparameterizing the ensemble weights
onto the unit sphere, we substantially simplify the optimization process and
improve computational efficiency. Extensive experiments conducted on multiple
benchmark datasets demonstrate that the proposed approach consistently
outperforms traditional margin-based ensemble techniques, underscoring its
effectiveness and practical utility.

</details>


### [104] [A Certifiable Machine Learning-Based Pipeline to Predict Fatigue Life of Aircraft Structures](https://arxiv.org/abs/2509.10227)
*Ángel Ladrón,Miguel Sánchez-Domínguez,Javier Rozalén,Fernando R. Sánchez,Javier de Vicente,Lucas Lacasa,Eusebio Valero,Gonzalo Rubio*

Main category: cs.LG

TL;DR: 本文提出了一种基于机器学习的飞机机翼疲劳寿命预测管道，通过飞行参数快速估计不同机翼位置的疲劳寿命，作为传统工程方法的补充。


<details>
  <summary>Details</summary>
Motivation: 传统疲劳寿命预测方法虽然可靠但耗时且流程复杂，需要多团队协作和大量计算资源。机器学习方法可以提供快速迭代和泛化能力，作为传统模拟的补充指导决策。

Method: 开发基于机器学习的管道，根据飞机整个运行寿命中不同任务的飞行参数，估计机翼不同位置的疲劳寿命。

Result: 在真实的疲劳寿命估计用例中验证了管道的准确性，提供了全面的统计验证和不确定性量化，能够减少昂贵的模拟需求。

Conclusion: 该机器学习管道通过降低计算和人力资源需求，有效补充了传统疲劳寿命预测方法，为航空航天安全提供了更高效的解决方案。

Abstract: Fatigue life prediction is essential in both the design and operational
phases of any aircraft, and in this sense safety in the aerospace industry
requires early detection of fatigue cracks to prevent in-flight failures.
Robust and precise fatigue life predictors are thus essential to ensure safety.
Traditional engineering methods, while reliable, are time consuming and involve
complex workflows, including steps such as conducting several Finite Element
Method (FEM) simulations, deriving the expected loading spectrum, and applying
cycle counting techniques like peak-valley or rainflow counting. These steps
often require collaboration between multiple teams and tools, added to the
computational time and effort required to achieve fatigue life predictions.
Machine learning (ML) offers a promising complement to traditional fatigue life
estimation methods, enabling faster iterations and generalization, providing
quick estimates that guide decisions alongside conventional simulations.
  In this paper, we present a ML-based pipeline that aims to estimate the
fatigue life of different aircraft wing locations given the flight parameters
of the different missions that the aircraft will be operating throughout its
operational life. We validate the pipeline in a realistic use case of fatigue
life estimation, yielding accurate predictions alongside a thorough statistical
validation and uncertainty quantification. Our pipeline constitutes a
complement to traditional methodologies by reducing the amount of costly
simulations and, thereby, lowering the required computational and human
resources.

</details>


### [105] [Prompt Injection Attacks on LLM Generated Reviews of Scientific Publications](https://arxiv.org/abs/2509.10248)
*Janis Keuper*

Main category: cs.LG

TL;DR: 本文通过系统评估发现：简单的提示词注入攻击对LLM审稿过程高度有效（可达100%接受率），且LLM审稿普遍存在接受偏向（>95%），这对LLM在同行评审中的使用讨论具有重要影响。


<details>
  <summary>Details</summary>
Motivation: 针对近期关于作者使用隐藏提示词注入操纵审稿分数的报道，研究这种攻击的可行性和技术成功率，以影响关于LLM在科学同行评审中使用的重要讨论。

Method: 使用多种LLM对2024年ICLR会议的1000篇论文进行系统评估，测试简单提示词注入的效果。

Result: 1) 非常简单提示词注入高度有效，达到100%接受率；2) LLM审稿普遍偏向接受（许多模型>95%）。

Conclusion: 研究结果对LLM在同行评审中的使用讨论具有重大影响，揭示了系统漏洞和固有偏见。

Abstract: The ongoing intense discussion on rising LLM usage in the scientific
peer-review process has recently been mingled by reports of authors using
hidden prompt injections to manipulate review scores. Since the existence of
such "attacks" - although seen by some commentators as "self-defense" - would
have a great impact on the further debate, this paper investigates the
practicability and technical success of the described manipulations. Our
systematic evaluation uses 1k reviews of 2024 ICLR papers generated by a wide
range of LLMs shows two distinct results: I) very simple prompt injections are
indeed highly effective, reaching up to 100% acceptance scores. II) LLM reviews
are generally biased toward acceptance (>95% in many models). Both results have
great impact on the ongoing discussions on LLM usage in peer-review.

</details>


### [106] [Property prediction for ionic liquids without prior structural knowledge using limited experimental data: A data-driven neural recommender system leveraging transfer learning](https://arxiv.org/abs/2509.10273)
*Sahil Sethi,Kai Sundmacher,Caroline Ganzer*

Main category: cs.LG

TL;DR: 提出基于神经推荐系统的迁移学习框架，利用COSMO-RS模拟数据和稀疏实验数据，准确预测离子液体的五种关键热物理性质


<details>
  <summary>Details</summary>
Motivation: 离子液体具有可定制的物理化学性质，但由于化学设计空间巨大和实验数据有限，准确预测其热物理性质仍然具有挑战性

Method: 两阶段方法：首先在固定温度压力下使用COSMO-RS模拟数据预训练神经推荐系统模型，学习阳离子和阴离子的性质特异性结构嵌入；然后使用这些嵌入和不同温度压力下的实验数据微调简单前馈神经网络

Result: 该框架支持性质内和跨性质知识迁移，预训练的密度、粘度和热容模型用于微调所有五个目标性质模型，其中四个性质的性能显著提升。模型对未见过的离子液体具有鲁棒外推能力，可为超过70万种离子液体组合提供性质预测

Conclusion: 这项工作展示了结合模拟数据和迁移学习来克服实验数据稀疏性的有效性，为过程设计中的离子液体筛选提供了可扩展的解决方案

Abstract: Ionic liquids (ILs) have emerged as versatile replacements for traditional
solvents because their physicochemical properties can be precisely tailored to
various applications. However, accurately predicting key thermophysical
properties remains challenging due to the vast chemical design space and the
limited availability of experimental data. In this study, we present a
data-driven transfer learning framework that leverages a neural recommender
system (NRS) to enable reliable property prediction for ILs using sparse
experimental datasets. The approach involves a two-stage process: first,
pre-training NRS models on COSMO-RS-based simulated data at fixed temperature
and pressure to learn property-specific structural embeddings for cations and
anions; and second, fine-tuning simple feedforward neural networks using these
embeddings with experimental data at varying temperatures and pressures. In
this work, five essential IL properties are considered: density, viscosity,
surface tension, heat capacity, and melting point. The framework supports both
within-property and cross-property knowledge transfer. Notably, pre-trained
models for density, viscosity, and heat capacity are used to fine-tune models
for all five target properties, achieving improved performance by a substantial
margin for four of them. The model exhibits robust extrapolation to previously
unseen ILs. Moreover, the final trained models enable property prediction for
over 700,000 IL combinations, offering a scalable solution for IL screening in
process design. This work highlights the effectiveness of combining simulated
data and transfer learning to overcome sparsity in the experimental data.

</details>


### [107] [Proof of AutoML: SDN based Secure Energy Trading with Blockchain in Disaster Case](https://arxiv.org/abs/2509.10291)
*Salih Toprak,Muge Erel-Ozcevik*

Main category: cs.LG

TL;DR: 提出基于SDN和AutoML的Proof of AutoML架构，利用机器学习回归模型生成随机数作为区块链nonce，用于灾难场景下的安全能源交易


<details>
  <summary>Details</summary>
Motivation: 在传统能源基础设施受损的灾难场景中，需要确保太阳能家庭与移动充电单元之间能源交易的安全性和可追溯性，而区块链网络需要强大的随机nonce生成机制

Method: 采用SDN架构实现灵活的数据流和能源路由控制，使用五种AutoML选择的回归模型（梯度提升、LightGBM、随机森林、额外树和K近邻）生成随机nonce，通过9000样本数据集评估模型的随机性而非准确性

Result: 随机性分析显示随机森林和额外树回归器完全依赖随机性，梯度提升、K近邻和LightGBM分别达到97.6%、98.8%和99.9%的随机性得分，树基集成模型表现最佳

Conclusion: 某些机器学习模型，特别是树基集成模型，可作为区块链安全、SDN基础的能源交易基础设施中有效且轻量级的nonce生成器，适用于灾难恢复场景

Abstract: In disaster scenarios where conventional energy infrastructure is
compromised, secure and traceable energy trading between solar-powered
households and mobile charging units becomes a necessity. To ensure the
integrity of such transactions over a blockchain network, robust and
unpredictable nonce generation is vital. This study proposes an SDN-enabled
architecture where machine learning regressors are leveraged not for their
accuracy, but for their potential to generate randomized values suitable as
nonce candidates. Therefore, it is newly called Proof of AutoML. Here, SDN
allows flexible control over data flows and energy routing policies even in
fragmented or degraded networks, ensuring adaptive response during emergencies.
Using a 9000-sample dataset, we evaluate five AutoML-selected regression models
- Gradient Boosting, LightGBM, Random Forest, Extra Trees, and K-Nearest
Neighbors - not by their prediction accuracy, but by their ability to produce
diverse and non-deterministic outputs across shuffled data inputs. Randomness
analysis reveals that Random Forest and Extra Trees regressors exhibit complete
dependency on randomness, whereas Gradient Boosting, K-Nearest Neighbors and
LightGBM show strong but slightly lower randomness scores (97.6%, 98.8% and
99.9%, respectively). These findings highlight that certain machine learning
models, particularly tree-based ensembles, may serve as effective and
lightweight nonce generators within blockchain-secured, SDN-based energy
trading infrastructures resilient to disaster conditions.

</details>


### [108] [Generalizing Beyond Suboptimality: Offline Reinforcement Learning Learns Effective Scheduling through Random Data](https://arxiv.org/abs/2509.10303)
*Jesse van Remmerden,Zaharah Bukhsh,Yingqian Zhang*

Main category: cs.LG

TL;DR: 提出CDQAC离线强化学习算法，直接从历史数据学习作业车间调度策略，无需在线交互，在样本效率和数据质量方面表现优异


<details>
  <summary>Details</summary>
Motivation: 传统在线强化学习方法需要大量模拟环境交互且样本效率低，无法捕捉真实世界复杂性，需要直接从历史数据学习调度策略的方法

Method: CDQAC算法结合分位数critic和延迟策略更新，估计每个机器-操作对的回报分布而非直接选择，能够从多样化数据源学习

Result: CDQAC显著优于原始数据生成启发式方法，超越最先进的离线和在线RL基线，仅需10-20个训练实例即可学习高质量策略

Conclusion: CDQAC是高效的离线RL调度方法，特别适合从随机启发式生成的数据中学习，在样本效率和数据质量要求方面具有优势

Abstract: The Job-Shop Scheduling Problem (JSP) and Flexible Job-Shop Scheduling
Problem (FJSP), are canonical combinatorial optimization problems with
wide-ranging applications in industrial operations. In recent years, many
online reinforcement learning (RL) approaches have been proposed to learn
constructive heuristics for JSP and FJSP. Although effective, these online RL
methods require millions of interactions with simulated environments that may
not capture real-world complexities, and their random policy initialization
leads to poor sample efficiency. To address these limitations, we introduce
Conservative Discrete Quantile Actor-Critic (CDQAC), a novel offline RL
algorithm that learns effective scheduling policies directly from historical
data, eliminating the need for costly online interactions, while maintaining
the ability to improve upon suboptimal training data. CDQAC couples a
quantile-based critic with a delayed policy update, estimating the return
distribution of each machine-operation pair rather than selecting pairs
outright. Our extensive experiments demonstrate CDQAC's remarkable ability to
learn from diverse data sources. CDQAC consistently outperforms the original
data-generating heuristics and surpasses state-of-the-art offline and online RL
baselines. In addition, CDQAC is highly sample efficient, requiring only 10-20
training instances to learn high-quality policies. Surprisingly, we find that
CDQAC performs better when trained on data generated by a random heuristic than
when trained on higher-quality data from genetic algorithms and priority
dispatching rules.

</details>


### [109] [GraphCSVAE: Graph Categorical Structured Variational Autoencoder for Spatiotemporal Auditing of Physical Vulnerability Towards Sustainable Post-Disaster Risk Reduction](https://arxiv.org/abs/2509.10308)
*Joshua Dimasaka,Christian Geiß,Robert Muir-Wood,Emily So*

Main category: cs.LG

TL;DR: 提出了GraphCSVAE框架，通过深度学习、图表示和分类概率推理整合卫星时间序列数据和专家知识，用于建模灾害物理脆弱性，并在孟加拉国和塞拉利昂的灾害案例中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有灾害风险评估方法在物理脆弱性建模方面进展有限，限制了决策者对联合国仙台框架进展的评估能力，需要开发新的数据驱动框架来填补这一空白。

Method: 开发了Graph Categorical Structured Variational Autoencoder (GraphCSVAE)概率框架，整合深度学习、图表示和分类概率推理，使用时间序列卫星数据和专家先验知识，引入弱监督一阶转移矩阵来反映物理脆弱性的时空分布变化。

Result: 在两个灾害频发且社会经济弱势地区（孟加拉国沿海社区和塞拉利昂弗里敦市）成功揭示了灾后物理脆弱性的区域动态，提供了局部时空审计和可持续减灾策略的宝贵见解。

Conclusion: GraphCSVAE框架能够有效建模物理脆弱性，为灾后风险评估和减灾策略制定提供了新的数据驱动方法，有助于实现联合国仙台框架的目标。

Abstract: In the aftermath of disasters, many institutions worldwide face challenges in
continually monitoring changes in disaster risk, limiting the ability of key
decision-makers to assess progress towards the UN Sendai Framework for Disaster
Risk Reduction 2015-2030. While numerous efforts have substantially advanced
the large-scale modeling of hazard and exposure through Earth observation and
data-driven methods, progress remains limited in modeling another equally
important yet challenging element of the risk equation: physical vulnerability.
To address this gap, we introduce Graph Categorical Structured Variational
Autoencoder (GraphCSVAE), a novel probabilistic data-driven framework for
modeling physical vulnerability by integrating deep learning, graph
representation, and categorical probabilistic inference, using time-series
satellite-derived datasets and prior expert belief systems. We introduce a
weakly supervised first-order transition matrix that reflects the changes in
the spatiotemporal distribution of physical vulnerability in two
disaster-stricken and socioeconomically disadvantaged areas: (1) the
cyclone-impacted coastal Khurushkul community in Bangladesh and (2) the
mudslide-affected city of Freetown in Sierra Leone. Our work reveals
post-disaster regional dynamics in physical vulnerability, offering valuable
insights into localized spatiotemporal auditing and sustainable strategies for
post-disaster risk reduction.

</details>


### [110] [ARMA Block: A CNN-Based Autoregressive and Moving Average Module for Long-Term Time Series Forecasting](https://arxiv.org/abs/2509.10324)
*Myung Jin Kim,YeongHyeon Park,Il Dong Yun*

Main category: cs.LG

TL;DR: 提出一个基于ARIMA模型启发的简单卷积模块ARMA，用于长期时间序列预测，包含趋势捕捉和局部变化修正两个组件，能直接进行多步预测并在多个基准数据集上取得竞争性精度。


<details>
  <summary>Details</summary>
Motivation: 传统ARIMA模型需要迭代多步预测且难以扩展到多变量设置，需要一种简单有效的模块来实现直接多步预测并保持架构简洁性。

Method: 设计包含两个卷积组件的模块：一个用于捕捉趋势（自回归），另一个用于细化局部变化（移动平均），直接进行多步预测并易于扩展到多变量场景。

Result: 在九个广泛使用的基准数据集上实验表明，ARMA方法实现了竞争性精度，特别是在具有强趋势变化的数据集上表现优异，同时保持了架构简洁性。

Conclusion: 该模块不仅实现了有效的长期时间序列预测，还固有地编码了绝对位置信息，有潜力作为序列模型中位置嵌入的轻量级替代方案。

Abstract: This paper proposes a simple yet effective convolutional module for long-term
time series forecasting. The proposed block, inspired by the Auto-Regressive
Integrated Moving Average (ARIMA) model, consists of two convolutional
components: one for capturing the trend (autoregression) and the other for
refining local variations (moving average). Unlike conventional ARIMA, which
requires iterative multi-step forecasting, the block directly performs
multi-step forecasting, making it easily extendable to multivariate settings.
Experiments on nine widely used benchmark datasets demonstrate that our method
ARMA achieves competitive accuracy, particularly on datasets exhibiting strong
trend variations, while maintaining architectural simplicity. Furthermore,
analysis shows that the block inherently encodes absolute positional
information, suggesting its potential as a lightweight replacement for
positional embeddings in sequential models.

</details>


### [111] [Physics-informed sensor coverage through structure preserving machine learning](https://arxiv.org/abs/2509.10363)
*Benjamin David Shaffer,Brooks Kinch,Joseph Klobusicky,M. Ani Hsieh,Nathaniel Trask*

Main category: cs.LG

TL;DR: 提出了一种基于条件神经Whitney形式的数字孪生框架，用于自适应源定位，结合有限元外微积分和Transformer算子学习，保持离散守恒性并实时适应传感器数据。


<details>
  <summary>Details</summary>
Motivation: 为了解决复杂流体输运系统中源定位问题，需要构建能够保持物理结构、实时适应传感器数据并保证数值稳定性的数字孪生模型。

Method: 使用条件神经Whitney形式(CNWF)构建数字孪生，结合FEEC的数值保证和Transformer算子学习。采用交错方案在数字孪生评估和Lloyd算法引导传感器布置之间交替进行。

Result: 实验表明，与物理无关的Transformer架构相比，在复杂几何形状中强制物理约束时精度更高，结构保持为源识别提供了有效的归纳偏置。

Conclusion: 结构保持的数字孪生框架能够有效实现自适应源定位，物理约束的强制执行提高了复杂几何中的定位精度，正则性作为定位的充分条件发挥了重要作用。

Abstract: We present a machine learning framework for adaptive source localization in
which agents use a structure-preserving digital twin of a coupled
hydrodynamic-transport system for real-time trajectory planning and data
assimilation. The twin is constructed with conditional neural Whitney forms
(CNWF), coupling the numerical guarantees of finite element exterior calculus
(FEEC) with transformer-based operator learning. The resulting model preserves
discrete conservation, and adapts in real time to streaming sensor data. It
employs a conditional attention mechanism to identify: a reduced Whitney-form
basis; reduced integral balance equations; and a source field, each compatible
with given sensor measurements. The induced reduced-order environmental model
retains the stability and consistency of standard finite-element simulation,
yielding a physically realizable, regular mapping from sensor data to the
source field. We propose a staggered scheme that alternates between evaluating
the digital twin and applying Lloyd's algorithm to guide sensor placement, with
analysis providing conditions for monotone improvement of a coverage
functional. Using the predicted source field as an importance function within
an optimal-recovery scheme, we demonstrate recovery of point sources under
continuity assumptions, highlighting the role of regularity as a sufficient
condition for localization. Experimental comparisons with physics-agnostic
transformer architectures show improved accuracy in complex geometries when
physical constraints are enforced, indicating that structure preservation
provides an effective inductive bias for source identification.

</details>


### [112] [A Discrepancy-Based Perspective on Dataset Condensation](https://arxiv.org/abs/2509.10367)
*Tong Chen,Raghavendra Selvan*

Main category: cs.LG

TL;DR: 本文提出了一个统一框架，将数据集压缩任务从传统的泛化性能扩展到包括鲁棒性、隐私保护等更多目标，并使用差异度量来形式化定义分布逼近问题。


<details>
  <summary>Details</summary>
Motivation: 现有数据集压缩方法主要关注泛化性能，但缺乏统一的理论框架。作者希望建立一个更通用的形式化定义，能够涵盖多种目标而不仅仅是泛化能力。

Method: 提出基于差异度量的统一框架，使用概率分布间的距离度量来形式化数据集压缩问题，将任务从单纯的泛化扩展到鲁棒性、隐私保护等多个维度。

Result: 建立了一个理论框架，能够统一现有的数据集压缩方法，并为扩展DC目标到更广泛的应用场景提供了理论基础。

Conclusion: 该框架为数据集压缩提供了更全面的理论支撑，使其能够适应包括鲁棒性、隐私保护在内的多种实际需求，推动了该领域的发展。

Abstract: Given a dataset of finitely many elements $\mathcal{T} = \{\mathbf{x}_i\}_{i
= 1}^N$, the goal of dataset condensation (DC) is to construct a synthetic
dataset $\mathcal{S} = \{\tilde{\mathbf{x}}_j\}_{j = 1}^M$ which is
significantly smaller ($M \ll N$) such that a model trained from scratch on
$\mathcal{S}$ achieves comparable or even superior generalization performance
to a model trained on $\mathcal{T}$. Recent advances in DC reveal a close
connection to the problem of approximating the data distribution represented by
$\mathcal{T}$ with a reduced set of points. In this work, we present a unified
framework that encompasses existing DC methods and extend the task-specific
notion of DC to a more general and formal definition using notions of
discrepancy, which quantify the distance between probability distribution in
different regimes. Our framework broadens the objective of DC beyond
generalization, accommodating additional objectives such as robustness,
privacy, and other desirable properties.

</details>


### [113] [Data distribution impacts the performance and generalisability of contrastive learning-based foundation models of electrocardiograms](https://arxiv.org/abs/2509.10369)
*Gul Rukh Khattak,Konstantinos Patlatzoglou,Joseph Barker,Libor Pastika,Boroumand Zeidaabadi,Ahmed El-Medany,Hesham Aggour,Yixiu Liang,Antonio H. Ribeiro,Jeffrey Annis,Antonio Luiz Pinho Ribeiro,Junbo Ge,Daniel B. Kramer,Jonathan W. Waks,Evan Brittain,Nicholas Peters,Fu Siong Ng,Arunashis Sau*

Main category: cs.LG

TL;DR: 对比学习在ECG预训练中受队列组成影响，多中心多样化队列提升分布内准确率但降低分布外泛化能力，提出IDB策略增强鲁棒性


<details>
  <summary>Details</summary>
Motivation: 探索对比学习在自监督预训练中对队列组成的依赖性，特别是人口统计学特征、健康状况和人群多样性对下游任务性能的影响

Method: 提出CAPE基础模型，在四大洲五个队列(n=5,203,352)上进行预训练，系统评估队列特征对性能的影响，并提出In-Distribution Batch (IDB)策略来保持队列内一致性

Result: 发现下游性能取决于预训练队列的分布特性，多中心多样化队列提高分布内准确率但编码了队列特异性伪影，降低了分布外泛化能力。IDB策略有效增强了OOD鲁棒性

Conclusion: 这项工作为开发临床公平和可泛化的基础模型提供了重要见解，强调了队列组成对模型性能的关键影响

Abstract: Contrastive learning is a widely adopted self-supervised pretraining
strategy, yet its dependence on cohort composition remains underexplored. We
present Contrasting by Patient Augmented Electrocardiograms (CAPE) foundation
model and pretrain on four cohorts (n = 5,203,352), from diverse populations
across three continents (North America, South America, Asia). We systematically
assess how cohort demographics, health status, and population diversity
influence the downstream performance for prediction tasks also including two
additional cohorts from another continent (Europe). We find that downstream
performance depends on the distributional properties of the pretraining cohort,
including demographics and health status. Moreover, while pretraining with a
multi-centre, demographically diverse cohort improves in-distribution accuracy,
it reduces out-of-distribution (OOD) generalisation of our contrastive approach
by encoding cohort-specific artifacts. To address this, we propose the
In-Distribution Batch (IDB) strategy, which preserves intra-cohort consistency
during pretraining and enhances OOD robustness. This work provides important
insights for developing clinically fair and generalisable foundation models.

</details>


### [114] [Flow Straight and Fast in Hilbert Space: Functional Rectified Flow](https://arxiv.org/abs/2509.10384)
*Jianxin Zhang,Clayton Scott*

Main category: cs.LG

TL;DR: 本文提出了无限维希尔伯特空间中整流流的严格函数形式化框架，建立了基于连续性方程叠加原理的函数整流流理论，并扩展到函数流匹配和概率流ODE，实验显示优于现有函数生成模型。


<details>
  <summary>Details</summary>
Motivation: 虽然许多生成模型已在有限维欧几里得空间发展并推广到无限维设置，但整流流向无限维空间的扩展仍未被探索。现有函数流匹配理论存在限制性的测度理论假设，需要更一般的框架。

Method: 基于无限维空间中连续性方程的叠加原理，建立了函数整流流的严格数学框架。该方法自然地扩展到函数流匹配和函数概率流ODE，将其解释为整流流的非线性推广。

Result: 实验证明该方法相比现有函数生成模型具有更优越的性能。理论框架移除了现有函数流匹配理论中的限制性测度理论假设。

Conclusion: 成功建立了无限维希尔伯特空间中整流流的函数形式化框架，提供了更一般的函数生成模型理论基础，并在实验中展现了性能优势。

Abstract: Many generative models originally developed in finite-dimensional Euclidean
space have functional generalizations in infinite-dimensional settings.
However, the extension of rectified flow to infinite-dimensional spaces remains
unexplored. In this work, we establish a rigorous functional formulation of
rectified flow in an infinite-dimensional Hilbert space. Our approach builds
upon the superposition principle for continuity equations in an
infinite-dimensional space. We further show that this framework extends
naturally to functional flow matching and functional probability flow ODEs,
interpreting them as nonlinear generalizations of rectified flow. Notably, our
extension to functional flow matching removes the restrictive measure-theoretic
assumptions in the existing theory of \citet{kerrigan2024functional}.
Furthermore, we demonstrate experimentally that our method achieves superior
performance compared to existing functional generative models.

</details>


### [115] [Vendi Information Gain for Active Learning and its Application to Ecology](https://arxiv.org/abs/2509.10390)
*Quan Nguyen,Adji Bousso Dieng*

Main category: cs.LG

TL;DR: 提出Vendi信息增益(VIG)主动学习策略，通过考虑数据集层面的预测不确定性来选择最具信息量和多样性的图像进行标注，在Snapshot Serengeti数据集上仅用不到10%的标签就达到了接近全监督的预测精度。


<details>
  <summary>Details</summary>
Motivation: 相机陷阱监测生物多样性时，物种识别因标注资源有限成为主要瓶颈。传统主动学习方法只关注单个预测的不确定性，而忽略了整个数据集层面的不确定性。

Method: 开发Vendi信息增益(VIG)主动学习策略，选择能够最大程度减少数据集整体预测不确定性的图像进行标注，同时考虑信息量和多样性。

Result: 在Snapshot Serengeti数据集上，VIG仅使用不到10%的标签就达到了接近全监督的预测精度，在各种指标和批次大小下均优于标准基线方法，并在特征空间中收集了更多样化的数据。

Conclusion: VIG方法在数据有限的环境中具有广泛的适用性，对于生物多样性监测具有重要价值，能够显著减少标注需求同时保持高预测性能。

Abstract: While monitoring biodiversity through camera traps has become an important
endeavor for ecological research, identifying species in the captured image
data remains a major bottleneck due to limited labeling resources. Active
learning -- a machine learning paradigm that selects the most informative data
to label and train a predictive model -- offers a promising solution, but
typically focuses on uncertainty in the individual predictions without
considering uncertainty across the entire dataset. We introduce a new active
learning policy, Vendi information gain (VIG), that selects images based on
their impact on dataset-wide prediction uncertainty, capturing both
informativeness and diversity. Applied to the Snapshot Serengeti dataset, VIG
achieves impressive predictive accuracy close to full supervision using less
than 10% of the labels. It consistently outperforms standard baselines across
metrics and batch sizes, collecting more diverse data in the feature space. VIG
has broad applicability beyond ecology, and our results highlight its value for
biodiversity monitoring in data-limited environments.

</details>


### [116] [Inpainting-Guided Policy Optimization for Diffusion Large Language Models](https://arxiv.org/abs/2509.10396)
*Siyan Zhao,Mengchen Liu,Jing Huang,Miao Liu,Chenyu Wang,Bo Liu,Yuandong Tian,Guan Pang,Sean Bell,Aditya Grover,Feiyu Chen*

Main category: cs.LG

TL;DR: IGPO是一种针对掩码扩散大语言模型的强化学习框架，利用inpainting能力指导探索，通过插入部分真实推理轨迹来提高样本效率和性能


<details>
  <summary>Details</summary>
Motivation: 解决强化学习对齐LLM时的探索挑战，包括稀疏奖励信号和样本浪费问题，利用dLLM独特的inpainting能力来引导探索

Method: 提出IGPO框架，在在线采样时策略性地插入部分真实推理轨迹，结合监督微调和强化学习，并采用基于熵的过滤等技术

Result: 在GSM8K、Math500和AMC三个数学基准测试中取得显著提升，为全注意力掩码dLLM实现了新的最先进结果

Conclusion: IGPO有效解决了dLLM强化学习中的探索效率问题，证明了inpainting能力在指导策略优化中的价值

Abstract: Masked diffusion large language models (dLLMs) are emerging as promising
alternatives to autoregressive LLMs, offering competitive performance while
supporting unique generation capabilities such as inpainting. We explore how
inpainting can inform RL algorithm design for dLLMs. Aligning LLMs with
reinforcement learning faces an exploration challenge: sparse reward signals
and sample waste when models fail to discover correct solutions. While this
inefficiency affects LLMs broadly, dLLMs offer a distinctive opportunity--their
inpainting ability can guide exploration. We introduce IGPO (Inpainting Guided
Policy Optimization), an RL framework that strategically inserts partial
ground-truth reasoning traces during online sampling. Unlike providing full
solutions, inpainting steers exploration toward promising trajectory spaces
while preserving self-generated reasoning, bridging supervised fine-tuning and
reinforcement learning. We apply IGPO to group-based optimization methods such
as GRPO, where exploration failures cause zero advantages and gradients. IGPO
restores meaningful gradients while improving sample efficiency. We also
propose supervised fine-tuning on synthetically rewritten concise traces that
better align with dLLM generation patterns. With additional techniques
including entropy-based filtering, our training recipe yields substantial gains
across three mathematical benchmarks--GSM8K, Math500, and AMC--achieving new
state-of-the-art results for full-attention masked dLLMs.

</details>


### [117] [Multipole Semantic Attention: A Fast Approximation of Softmax Attention for Pretraining](https://arxiv.org/abs/2509.10406)
*Rupert Mitchell,Kristian Kersting*

Main category: cs.LG

TL;DR: MuSe是一种高效的注意力机制近似方法，通过结合语义聚类和多极展开来降低Transformer的二次计算复杂度，在保持性能的同时显著提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer中softmax注意力机制在序列长度上的二次计算复杂度问题，通过更高效的近似方法来降低计算成本，同时保持模型性能。

Method: 采用语义聚类和多极展开相结合的方法：1）在学习的表示空间中分别对查询和键进行聚类；2）使用层次化的两阶段注意力机制；3）引入偶极校正来捕捉簇内方向方差；4）作为标准注意力的即插即用替代方案。

Result: 在8k上下文长度下比CUDNN Flash Attention快3倍，相对平方误差低于20%；在30M参数模型的端到端预训练中，16k上下文长度下实现12.2%的运行时间减少，仅损失0.36%的性能。

Conclusion: 多极近似方法为高效Transformer预训练提供了可行的解决方案，通过分离聚类和层次化块分解在保持模型性能的同时显著降低了计算复杂度。

Abstract: We present Multipole Semantic Attention (MuSe), an efficient approximation of
softmax attention that combines semantic clustering with multipole expansions
from computational physics. Our method addresses the quadratic computational
complexity of transformers in the context length by clustering queries and keys
separately in their learned representation spaces, enabling a hierarchical
two-stage attention mechanism. Unlike prior clustering approaches that group
only keys or use unified clustering, we maintain separate clusterings that
respect attention's asymmetric treatment of these spaces. We augment
centroid-based (monopole) approximations with dipole corrections that capture
directional variance within clusters, preserving richer information during
training. The method operates as a drop-in replacement for standard attention,
requiring only hyperparameter specification without architectural
modifications. Our approach achieves $\mathcal{O}(NCD)$ complexity for acausal
attention with $C$ clusters and $\mathcal{O}(NCD \log N)$ for causal attention.
On isolated attention layers, we demonstrate $3\times$ speedup over CUDNN Flash
Attention at 8k context length, with relative squared errors below 20%. For
causal attention, we develop a hierarchical block decomposition that combines
exact local computation with efficient long-range approximation. In end-to-end
pretraining of a 30M parameter model on book-length texts with 16k context, we
achieve 12.2% runtime reduction with only 0.36% loss degradation, establishing
the viability of multipole approximations for efficient transformer
pretraining.

</details>


### [118] [Run-Time Monitoring of ERTMS/ETCS Control Flow by Process Mining](https://arxiv.org/abs/2509.10419)
*Francesco Vitale,Tommaso Zoppi,Francesco Flammini,Nicola Mazzocca*

Main category: cs.LG

TL;DR: 本文提出使用过程挖掘和机器学习技术来实时检测和定位铁路控制系统ERTMS/ETCS L2中的控制流异常，提高系统韧性。


<details>
  <summary>Details</summary>
Motivation: 随着铁路系统复杂性和关键性增加，尽管有严格的验证和认证流程，运行时仍可能出现设计时未知的故障、系统环境变化或网络威胁，需要增强系统韧性。

Method: 采用过程挖掘技术从执行轨迹中学习系统实际控制流，进行在线一致性检查；使用无监督机器学习进行异常定位，将偏差关联到关键系统组件。

Result: 在ERTMS/ETCS L2的RBC/RBC交接参考场景中测试，方法能够以高精度、高效率和高可解释性检测和定位异常。

Conclusion: 过程挖掘结合机器学习为铁路控制系统提供了有效的运行时异常检测和定位解决方案，增强了系统在面对未知变化和威胁时的韧性。

Abstract: Ensuring the resilience of computer-based railways is increasingly crucial to
account for uncertainties and changes due to the growing complexity and
criticality of those systems. Although their software relies on strict
verification and validation processes following well-established best-practices
and certification standards, anomalies can still occur at run-time due to
residual faults, system and environmental modifications that were unknown at
design-time, or other emergent cyber-threat scenarios. This paper explores
run-time control-flow anomaly detection using process mining to enhance the
resilience of ERTMS/ETCS L2 (European Rail Traffic Management System / European
Train Control System Level 2). Process mining allows learning the actual
control flow of the system from its execution traces, thus enabling run-time
monitoring through online conformance checking. In addition, anomaly
localization is performed through unsupervised machine learning to link
relevant deviations to critical system components. We test our approach on a
reference ERTMS/ETCS L2 scenario, namely the RBC/RBC Handover, to show its
capability to detect and localize anomalies with high accuracy, efficiency, and
explainability.

</details>


### [119] [Understanding Outer Optimizers in Local SGD: Learning Rates, Momentum, and Acceleration](https://arxiv.org/abs/2509.10439)
*Ahmed Khaled,Satyen Kale,Arthur Douillard,Chi Jin,Rob Fergus,Manzil Zaheer*

Main category: cs.LG

TL;DR: 本文研究了Local SGD中外部优化器的作用，证明了新的收敛保证，发现调整外部学习率可以在优化误差和随机梯度噪声方差之间权衡，并弥补内部学习率的不良调整。理论表明外部学习率有时应大于1，并扩展到动量、加速等场景。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习需要大批量训练和分布式计算，通信成为主要瓶颈。Local SGD能减少通信开销，但现有研究主要关注本地优化过程的超参数，对外部优化器及其超参数的选择不够清晰。

Method: 通过理论分析证明Local SGD的收敛保证，研究外部学习率的作用机制，扩展到动量优化器和加速方法，并提出数据依赖的分析方法。使用标准语言模型进行实验验证。

Result: 理论表明调整外部学习率能够权衡优化误差和随机梯度噪声方差，补偿内部学习率的不良调整。外部学习率有时应大于1，动量调整和加速方法能改善收敛速率。

Conclusion: 外部优化器在Local SGD中扮演关键角色，适当调整外部学习率能显著改善算法性能，动量加速方法能进一步提升收敛效率，为分布式机器学习提供了重要理论指导。

Abstract: Modern machine learning often requires training with large batch size,
distributed data, and massively parallel compute hardware (like mobile and
other edge devices or distributed data centers). Communication becomes a major
bottleneck in such settings but methods like Local Stochastic Gradient Descent
(Local SGD) show great promise in reducing this additional communication
overhead. Local SGD consists of three parts: a local optimization process, an
aggregation mechanism, and an outer optimizer that uses the aggregated updates
from the nodes to produce a new model. While there exists an extensive
literature on understanding the impact of hyperparameters in the local
optimization process, the choice of outer optimizer and its hyperparameters is
less clear. We study the role of the outer optimizer in Local SGD, and prove
new convergence guarantees for the algorithm. In particular, we show that
tuning the outer learning rate allows us to (a) trade off between optimization
error and stochastic gradient noise variance, and (b) make up for ill-tuning of
the inner learning rate. Our theory suggests that the outer learning rate
should sometimes be set to values greater than $1$. We extend our results to
settings where we use momentum in the outer optimizer, and we show a similar
role for the momentum-adjusted outer learning rate. We also study acceleration
in the outer optimizer and show that it improves the convergence rate as a
function of the number of communication rounds, improving upon the convergence
rate of prior algorithms that apply acceleration locally. Finally, we also
introduce a novel data-dependent analysis of Local SGD that yields further
insights on outer learning rate tuning. We conduct comprehensive experiments
with standard language models and various outer optimizers to validate our
theory.

</details>
