<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 11]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Energy-Guided Diffusion Sampling for Long-Term User Behavior Prediction in Reinforcement Learning-based Recommendation](https://arxiv.org/abs/2510.12815)
*Xiaocong Chen,Siyu Wang,Lina Yao*

Main category: cs.IR

TL;DR: 提出DAC4Rec框架，将扩散过程与强化学习结合，通过扩散模型的去噪能力增强离线RL算法的鲁棒性，并引入Q值引导的策略优化来处理次优轨迹，在六个真实世界离线数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习推荐系统面临数据效率低、依赖预收集轨迹的问题，现有方法难以处理噪声数据且无法有效捕捉长期用户偏好，导致推荐策略次优。

Method: 集成扩散过程与强化学习，利用扩散模型的去噪能力；采用Q值引导的策略优化；引入基于能量的采样策略减少推荐生成的随机性。

Result: 在六个真实世界离线数据集和在线模拟环境中验证了DAC4Rec的有效性，能够优化长期用户偏好，且扩散策略可无缝集成到其他常用RL算法中。

Conclusion: DAC4Rec通过扩散增强的actor-critic框架有效解决了离线RL推荐系统的挑战，展示了在建模复杂用户偏好方面的优越性能和广泛适用性。

Abstract: Reinforcement learning-based recommender systems (RL4RS) have gained
attention for their ability to adapt to dynamic user preferences. However,
these systems face challenges, particularly in offline settings, where data
inefficiency and reliance on pre-collected trajectories limit their broader
applicability. While offline reinforcement learning methods leverage extensive
datasets to address these issues, they often struggle with noisy data and fail
to capture long-term user preferences, resulting in suboptimal recommendation
policies. To overcome these limitations, we propose Diffusion-enhanced
Actor-Critic for Offline RL4RS (DAC4Rec), a novel framework that integrates
diffusion processes with reinforcement learning to model complex user
preferences more effectively. DAC4Rec leverages the denoising capabilities of
diffusion models to enhance the robustness of offline RL algorithms and
incorporates a Q-value-guided policy optimization strategy to better handle
suboptimal trajectories. Additionally, we introduce an energy-based sampling
strategy to reduce randomness during recommendation generation, ensuring more
targeted and reliable outcomes. We validate the effectiveness of DAC4Rec
through extensive experiments on six real-world offline datasets and in an
online simulation environment, demonstrating its ability to optimize long-term
user preferences. Furthermore, we show that the proposed diffusion policy can
be seamlessly integrated into other commonly used RL algorithms in RL4RS,
highlighting its versatility and wide applicability.

</details>


### [2] [Maximum In-Support Return Modeling for Dynamic Recommendation with Language Model Prior](https://arxiv.org/abs/2510.12816)
*Xiaocong Chen,Siyu Wang,Lina Yao*

Main category: cs.IR

TL;DR: MDT4Rec是一个基于决策变换器的离线强化学习推荐系统框架，通过将轨迹拼接从训练阶段转移到动作推断，并使用预训练大语言模型初始化来改善从次优历史数据中学习的能力。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习推荐系统在现实场景中面临的两个主要挑战：从次优历史数据中学习和表示复杂的用户-物品交互。现实世界中的用户反馈数据往往不理想或稀疏。

Method: 1) 将轨迹拼接从训练阶段转移到动作推断阶段，允许系统在必要时缩短历史上下文；2) 使用预训练大语言模型初始化决策变换器进行知识迁移；3) 用多层感知机替换线性嵌入层以获得更灵活的表征；4) 使用低秩适应高效微调少量参数。

Result: 在五个公共数据集和在线模拟环境中的评估表明，MDT4Rec优于现有方法。

Conclusion: MDT4Rec通过创新的轨迹拼接策略和有效的知识迁移机制，成功解决了强化学习推荐系统在现实场景中的关键挑战，实现了更好的性能。

Abstract: Reinforcement Learning-based recommender systems (RLRS) offer an effective
way to handle sequential recommendation tasks but often face difficulties in
real-world settings, where user feedback data can be sub-optimal or sparse. In
this paper, we introduce MDT4Rec, an offline RLRS framework that builds on the
Decision Transformer (DT) to address two major challenges: learning from
sub-optimal histories and representing complex user-item interactions. First,
MDT4Rec shifts the trajectory stitching procedure from the training phase to
action inference, allowing the system to shorten its historical context when
necessary and thereby ignore negative or unsuccessful past experiences. Second,
MDT4Rec initializes DT with a pre-trained large language model (LLM) for
knowledge transfer, replaces linear embedding layers with Multi-Layer
Perceptrons (MLPs) for more flexible representations, and employs Low-Rank
Adaptation (LoRA) to efficiently fine-tune only a small subset of parameters.
We evaluate MDT4Rec on five public datasets and in an online simulation
environment, demonstrating that it outperforms existing methods.

</details>


### [3] [Post-hoc Popularity Bias Correction in GNN-based Collaborative Filtering](https://arxiv.org/abs/2510.12959)
*Md Aminul Islam,Elena Zheleva,Ren Wang*

Main category: cs.IR

TL;DR: 提出了一种后处理流行度去偏方法(PPD)，用于纠正基于图神经网络的协同过滤中的流行度偏差，无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 用户历史交互数据存在长尾分布，少数热门物品占据大部分交互。直接训练会导致CF模型学习到流行度偏差，降低个性化推荐质量。GNN的消息传递机制会进一步传播和放大这种偏差。

Method: PPD方法通过估计交互级别的流行度，使用流行度方向向量从节点表示中移除流行度成分，从而减少偏差同时保留用户偏好。该方法直接作用于预训练嵌入，无需重新训练。

Result: 实验结果表明，该方法在基于GNN的CF中优于现有的流行度偏差纠正方法。

Conclusion: PPD方法能有效纠正GNN-based CF中的流行度偏差，提高推荐质量，且具有无需重新训练的优势。

Abstract: User historical interaction data is the primary signal for learning user
preferences in collaborative filtering (CF). However, the training data often
exhibits a long-tailed distribution, where only a few items have the majority
of interactions. CF models trained directly on such imbalanced data are prone
to learning popularity bias, which reduces personalization and leads to
suboptimal recommendation quality. Graph Neural Networks (GNNs), while
effective for CF due to their message passing mechanism, can further propagate
and amplify popularity bias through their aggregation process. Existing
approaches typically address popularity bias by modifying training objectives
but fail to directly counteract the bias propagated during GNN's neighborhood
aggregation. Applying weights to interactions during aggregation can help
alleviate this problem, yet it risks distorting model learning due to unstable
node representations in the early stages of training. In this paper, we propose
a Post-hoc Popularity Debiasing (PPD) method that corrects for popularity bias
in GNN-based CF and operates directly on pre-trained embeddings without
requiring retraining. By estimating interaction-level popularity and removing
popularity components from node representations via a popularity direction
vector, PPD reduces bias while preserving user preferences. Experimental
results show that our method outperforms state-of-the-art approaches for
popularity bias correction in GNN-based CF.

</details>


### [4] [Retrieval-in-the-Chain: Bootstrapping Large Language Models for Generative Retrieval](https://arxiv.org/abs/2510.13095)
*Yingchen zhang,Ruqing zhang,Jiafeng Guo,Wenjun Peng,Sen Li,Fuyu Lv*

Main category: cs.IR

TL;DR: 提出R4R框架，通过结构化推理增强生成式检索，在推理过程中交替进行约束解码和推理更新，无需额外模型或训练。


<details>
  <summary>Details</summary>
Motivation: 现有生成式检索方法主要利用LLMs的生成能力，而忽视了其推理能力可能带来的提升。初步研究发现自由形式的思维链推理虽然有效但存在冗长和对齐不佳的问题。

Method: R4R将自由形式推理转换为紧凑的结构化格式，在检索过程中迭代优化推理。使用单一LLM交替进行约束解码生成文档标识符和基于检索结果更新推理。

Result: 在Natural Questions、MS MARCO和真实世界商品搜索基准上的广泛实验验证了R4R的有效性。

Conclusion: R4R证明了显式推理能够显著提升生成式检索性能，提供了一种无需额外训练的高效推理增强框架。

Abstract: Generative retrieval (GR) is an emerging paradigm that leverages large
language models (LLMs) to autoregressively generate document identifiers
(docids) relevant to a given query. Prior works have focused on leveraging the
generative capabilities of LLMs to improve GR, while overlooking that their
reasoning capabilities could likewise help. This raises a key question: Can
explicit reasoning benefit GR? To investigate, we first conduct a preliminary
study where an LLM is prompted to generate free-form chain-of-thought (CoT)
reasoning before performing constrained docid decoding. Although this method
outperforms standard GR, the generated reasoning tends to be verbose and poorly
aligned with the docid space. These limitations motivate the development of a
reasoning mechanism better tailored to GR.
  Therefore, we propose Reason-for-Retrieval (R4R), a reasoning-augmented
framework for GR that converts free-form CoT reasoning into a compact,
structured format, and iteratively refines the reasoning during the retrieval
process. R4R augments an existing GR method by leveraging a reasoning-capable
LLM that has been instruction-tuned for GR. At inference time, R4R first uses
the LLM to generate an initial structured reasoning; then the same LLM
alternates between (i) constrained decoding with the chosen GR method to
produce candidate docids and (ii) updating the reasoning based on retrieval
results to improve the next round. R4R does not require additional models or
training, and instead a single LLM serves as both the reasoning generator and
the retriever. Extensive experiments on Natural Questions, MS MARCO, and a
real-world item-search benchmark validate the effectiveness of R4R.

</details>


### [5] [ReMindRAG: Low-Cost LLM-Guided Knowledge Graph Traversal for Efficient RAG](https://arxiv.org/abs/2510.13193)
*Yikuan Hu,Jifeng Zhu,Lanrui Tang,Chen Huang*

Main category: cs.IR

TL;DR: REMINDRAG是一种基于知识图谱的检索增强生成系统，通过LLM引导的图遍历和记忆重放机制，在提升系统效果的同时保持成本效率。


<details>
  <summary>Details</summary>
Motivation: 现有的KG-RAG系统在系统效果和成本效率之间难以达到有效协同，要么性能不令人满意，要么消耗过多的LLM提示词和推理时间。

Method: 采用LLM引导的图遍历，包括节点探索、节点利用和记忆重放，通过在KG边嵌入中记忆遍历经验来实现无训练的记忆机制。

Result: 理论和实验验证了REMINDRAG的有效性，在多个基准数据集和LLM骨干网络上均优于现有基线方法。

Conclusion: REMINDRAG通过创新的记忆重放机制成功解决了KG-RAG系统中效果与效率的平衡问题，为知识图谱增强的检索生成系统提供了新的解决方案。

Abstract: Knowledge graphs (KGs), with their structured representation capabilities,
offer promising avenue for enhancing Retrieval Augmented Generation (RAG)
systems, leading to the development of KG-RAG systems. Nevertheless, existing
methods often struggle to achieve effective synergy between system
effectiveness and cost efficiency, leading to neither unsatisfying performance
nor excessive LLM prompt tokens and inference time. To this end, this paper
proposes REMINDRAG, which employs an LLM-guided graph traversal featuring node
exploration, node exploitation, and, most notably, memory replay, to improve
both system effectiveness and cost efficiency. Specifically, REMINDRAG
memorizes traversal experience within KG edge embeddings, mirroring the way
LLMs "memorize" world knowledge within their parameters, but in a train-free
manner. We theoretically and experimentally confirm the effectiveness of
REMINDRAG, demonstrating its superiority over existing baselines across various
benchmark datasets and LLM backbones. Our code is available at
https://github.com/kilgrims/ReMindRAG.

</details>


### [6] [LLM-guided Hierarchical Retrieval](https://arxiv.org/abs/2510.13217)
*Nilesh Gupta,Wei-Cheng Chang,Ngot Bui,Cho-Jui Hsieh,Inderjit S. Dhillon*

Main category: cs.IR

TL;DR: LATTICE是一个分层检索框架，通过语义树结构组织语料库，使LLM能够以对数搜索复杂度在大规模语料库中进行推理和导航。


<details>
  <summary>Details</summary>
Motivation: 解决现代IR系统处理复杂多面查询时面临的挑战：传统检索-重排范式受限于基于嵌入的检索，参数生成方法难以更新新信息，长上下文方法在大规模文档集合中计算不可行。

Method: 采用两阶段方法：离线阶段通过自底向上聚合或自顶向下分割策略将语料库组织成语义层次结构；在线阶段使用搜索LLM导航该树结构，并提出遍历算法从局部LLM输出估计校准的潜在相关性分数。

Result: 在推理密集的BRIGHT基准测试中实现最先进的零样本性能，Recall@100提升9%，nDCG@10提升5%；与微调的SOTA方法DIVER-v2在静态语料库评估子集上获得可比结果。

Conclusion: LATTICE框架通过分层检索有效解决了LLM在大规模语料库中的推理导航问题，实现了高效且准确的检索性能。

Abstract: Modern IR systems are increasingly tasked with answering complex,
multi-faceted queries that require deep reasoning rather than simple keyword or
semantic matching. While LLM-based IR has shown great promise, the prevailing
retrieve-then-rerank paradigm inherits the limitations of embedding-based
retrieval; parametric generative approaches are difficult to update with new
information; and long-context methods that place the entire corpus in context
are computationally infeasible for large document collections. To address these
challenges, we introduce LATTICE, a hierarchical retrieval framework that
enables an LLM to reason over and navigate large corpora with logarithmic
search complexity by imposing a semantic tree structure on the corpus. Our
approach consists of two stages: (1) an offline phase that organizes the corpus
into a semantic hierarchy via either a bottom-up agglomerative strategy or a
top-down divisive strategy using multi-level summaries and (2) an online
traversal phase where a search LLM navigates this tree. A central challenge in
such LLM-guided search is that the model's relevance judgments are noisy,
context-dependent, and unaware of the hierarchy, making cross-branch and
cross-level comparisons difficult. To overcome this, we propose a traversal
algorithm that estimates calibrated latent relevance scores from local LLM
outputs and aggregates them into a global path relevance metric. Our
training-free framework achieves state-of-the-art zero-shot performance on the
reasoning-intensive BRIGHT benchmark, demonstrating up to 9% improvement in
Recall@100 and 5% in nDCG@10 over the next best zero-shot baseline.
Furthermore, compared to the fine-tuned SOTA method DIVER-v2, LATTICE attains
comparable results on BRIGHT subsets that use a static corpus for evaluation.

</details>


### [7] [Beyond Static LLM Policies: Imitation-Enhanced Reinforcement Learning for Recommendation](https://arxiv.org/abs/2510.13229)
*Yi Zhang,Lili Xie,Ruihong Qiu,Jiajun Liu,Sen Wang*

Main category: cs.IR

TL;DR: 提出一种基于离线强化学习的新框架，通过模仿LLM生成的轨迹来改进推荐系统，避免直接使用LLM带来的延迟和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: LLM在推荐系统中具有巨大潜力，但直接部署面临API调用延迟、幻觉和偏见等挑战，需要更高效的解决方案。

Method: 使用逆强化学习从LLM演示中提取奖励模型，通过模仿学习训练RL策略，无需微调LLM。

Result: 在两个基准数据集上的实验表明，该方法优于最先进的基于RL和上下文学习的基线方法。

Conclusion: 提出的框架成功将LLM的语义理解能力转移到高效的RL策略中，解决了LLM直接部署的挑战。

Abstract: Recommender systems (RecSys) have become critical tools for enhancing user
engagement by delivering personalized content across diverse digital platforms.
Recent advancements in large language models (LLMs) demonstrate significant
potential for improving RecSys, primarily due to their exceptional
generalization capabilities and sophisticated contextual understanding, which
facilitate the generation of flexible and interpretable recommendations.
However, the direct deployment of LLMs as primary recommendation policies
presents notable challenges, including persistent latency issues stemming from
frequent API calls and inherent model limitations such as hallucinations and
biases. To address these issues, this paper proposes a novel offline
reinforcement learning (RL) framework that leverages imitation learning from
LLM-generated trajectories. Specifically, inverse reinforcement learning is
employed to extract robust reward models from LLM demonstrations. This approach
negates the need for LLM fine-tuning, thereby substantially reducing
computational overhead. Simultaneously, the RL policy is guided by the
cumulative rewards derived from these demonstrations, effectively transferring
the semantic insights captured by the LLM. Comprehensive experiments conducted
on two benchmark datasets validate the effectiveness of the proposed method,
demonstrating superior performance when compared against state-of-the-art
RL-based and in-context learning baselines. The code can be found at
https://github.com/ArronDZhang/IL-Rec.

</details>


### [8] [Improving Visual Recommendation on E-commerce Platforms Using Vision-Language Models](https://arxiv.org/abs/2510.13359)
*Yuki Yada,Sho Akiyama,Ryo Watanabe,Yuta Ueno,Yusuke Shido,Andre Rusli*

Main category: cs.IR

TL;DR: 在Mercari电商平台上应用SigLIP视觉语言模型进行产品推荐，通过微调获得图像编码器，在离线评估中nDCG@5提升9.1%，在线A/B测试中点击率提升50%、转化率提升14%。


<details>
  <summary>Details</summary>
Motivation: 在拥有数千万月活用户的大型电商平台上，推荐视觉相似产品对于帮助用户高效发现符合偏好的商品至关重要。

Method: 使用Mercari三个月内收集的100万产品图像-标题对微调SigLIP视觉语言模型，开发用于推荐系统的图像编码器生成商品嵌入。

Result: 离线分析：nDCG@5相比基线提升9.1%；在线A/B测试：点击率提升50%，转化率提升14%。

Conclusion: 基于视觉语言模型的编码器在电商产品推荐中有效，为基于视觉相似性的推荐系统开发提供了实用见解。

Abstract: On large-scale e-commerce platforms with tens of millions of active monthly
users, recommending visually similar products is essential for enabling users
to efficiently discover items that align with their preferences. This study
presents the application of a vision-language model (VLM) -- which has
demonstrated strong performance in image recognition and image-text retrieval
tasks -- to product recommendations on Mercari, a major consumer-to-consumer
marketplace used by more than 20 million monthly users in Japan. Specifically,
we fine-tuned SigLIP, a VLM employing a sigmoid-based contrastive loss, using
one million product image-title pairs from Mercari collected over a three-month
period, and developed an image encoder for generating item embeddings used in
the recommendation system. Our evaluation comprised an offline analysis of
historical interaction logs and an online A/B test in a production environment.
In offline analysis, the model achieved a 9.1% improvement in nDCG@5 compared
with the baseline. In the online A/B test, the click-through rate improved by
50% whereas the conversion rate improved by 14% compared with the existing
model. These results demonstrate the effectiveness of VLM-based encoders for
e-commerce product recommendations and provide practical insights into the
development of visual similarity-based recommendation systems.

</details>


### [9] [MADREC: A Multi-Aspect Driven LLM Agent for Explainable and Adaptive Recommendation](https://arxiv.org/abs/2510.13371)
*Jiin Park,Misuk Kim*

Main category: cs.IR

TL;DR: 提出了MADRec，一个基于LLM的多方面驱动推荐代理，通过无监督提取评论中的多方面信息构建用户和物品画像，支持直接推荐、序列推荐和解释生成。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推荐系统大多局限于简单文本生成或静态提示推理，无法捕捉用户偏好和真实交互的复杂性。

Method: 使用基于方面类别的总结生成结构化画像，应用重排序构建高密度输入，当真实物品缺失时通过自反馈机制动态调整推理标准。

Result: 在多个领域的实验中，MADRec在准确性和可解释性方面优于传统和基于LLM的基线方法，人工评估进一步证实了生成解释的说服力。

Conclusion: MADRec展示了自主LLM推荐代理在捕捉复杂用户偏好和提供可解释推荐方面的有效性。

Abstract: Recent attempts to integrate large language models (LLMs) into recommender
systems have gained momentum, but most remain limited to simple text generation
or static prompt-based inference, failing to capture the complexity of user
preferences and real-world interactions. This study proposes the Multi-Aspect
Driven LLM Agent MADRec, an autonomous LLM-based recommender that constructs
user and item profiles by unsupervised extraction of multi-aspect information
from reviews and performs direct recommendation, sequential recommendation, and
explanation generation. MADRec generates structured profiles via
aspect-category-based summarization and applies Re-Ranking to construct
high-density inputs. When the ground-truth item is missing from the output, the
Self-Feedback mechanism dynamically adjusts the inference criteria. Experiments
across multiple domains show that MADRec outperforms traditional and LLM-based
baselines in both precision and explainability, with human evaluation further
confirming the persuasiveness of the generated explanations.

</details>


### [10] [RAG Meets Temporal Graphs: Time-Sensitive Modeling and Retrieval for Evolving Knowledge](https://arxiv.org/abs/2510.13590)
*Jiale Han,Austin Cheung,Yubai Wei,Zheng Yu,Xusheng Wang,Bing Zhu,Yi Yang*

Main category: cs.IR

TL;DR: 提出了Temporal GraphRAG (TG-RAG)方法，通过构建双层时间图来使RAG系统具备时间感知能力，解决现有RAG方法在处理时间敏感知识时的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统主要忽略知识的时效性，缺乏有效的时间感知表示，且评估大多基于静态语料库，无法应对知识随时间演变的挑战。

Method: 构建包含时间知识图和时间层次图的双层时间图，为每个时间节点生成多粒度时间摘要，支持增量更新，并在推理时动态检索查询时间范围内的子图。

Result: TG-RAG在时间敏感问答任务上显著优于现有基线方法，证明了其在处理时间知识和增量更新方面的有效性。

Conclusion: TG-RAG通过显式建模时间信息，有效解决了RAG系统的时间感知问题，为处理动态演化的知识提供了可行方案。

Abstract: Knowledge is inherently time-sensitive and continuously evolves over time.
Although current Retrieval-Augmented Generation (RAG) systems enrich LLMs with
external knowledge, they largely ignore this temporal nature. This raises two
challenges for RAG. First, current RAG methods lack effective time-aware
representations. Same facts of different time are difficult to distinguish with
vector embeddings or conventional knowledge graphs. Second, most RAG
evaluations assume a static corpus, leaving a blind spot regarding update costs
and retrieval stability as knowledge evolves. To make RAG time-aware, we
propose Temporal GraphRAG (TG-RAG), which models external corpora as a bi-level
temporal graph consisting of a temporal knowledge graph with timestamped
relations and a hierarchical time graph. Multi-granularity temporal summaries
are generated for each time node to capture both key events and broader trends
at that time. The design supports incremental updates by extracting new
temporal facts from the incoming corpus and merging them into the existing
graph. The temporal graph explicitly represents identical facts at different
times as distinct edges to avoid ambiguity, and the time hierarchy graph allows
only generating reports for new leaf time nodes and their ancestors, ensuring
effective and efficient updates. During inference, TG-RAG dynamically retrieves
a subgraph within the temporal and semantic scope of the query, enabling
precise evidence gathering. Moreover, we introduce ECT-QA, a time-sensitive
question-answering dataset featuring both specific and abstract queries, along
with a comprehensive evaluation protocol designed to assess incremental update
capabilities of RAG systems. Extensive experiments show that TG-RAG
significantly outperforms existing baselines, demonstrating the effectiveness
of our method in handling temporal knowledge and incremental updates.

</details>


### [11] [HyMiRec: A Hybrid Multi-interest Learning Framework for LLM-based Sequential Recommendation](https://arxiv.org/abs/2510.13738)
*Jingyi Zhou,Cheng Chen,Kai Zuo,Manjie Xu,Zhendong Fu,Yibo Chen,Xu Tang,Yao Hu*

Main category: cs.IR

TL;DR: 提出HyMiRec混合多兴趣序列推荐框架，解决LLM在推荐系统中长序列建模和兴趣多样性不足的问题


<details>
  <summary>Details</summary>
Motivation: 现有LLM推荐方法存在两个关键限制：1）由于推理延迟和特征获取带宽限制，通常截断用户行为序列，丢失长期偏好信号；2）大多依赖单嵌入的下一个物品预测，忽略了用户兴趣的多面性，限制了推荐多样性

Method: HyMiRec框架包含：1）轻量级推荐器从长序列提取粗粒度兴趣嵌入；2）LLM推荐器捕获细粒度兴趣嵌入；3）基于余弦相似度的残差码本，高效压缩和复用用户历史嵌入；4）解耦多兴趣学习模块，使用多个兴趣查询自适应学习分离的兴趣信号

Result: 在基准数据集和工业数据集上的广泛实验显示，HyMiRec优于现有最先进方法。在线A/B测试表明，在真实推荐系统中带来持续改进

Conclusion: HyMiRec通过混合架构有效解决了LLM推荐中的长序列建模和兴趣多样性问题，在多个数据集和实际系统中都表现出优越性能

Abstract: Large language models (LLMs) have recently demonstrated strong potential for
sequential recommendation. However, current LLM-based approaches face critical
limitations in modeling users' long-term and diverse interests. First, due to
inference latency and feature fetching bandwidth constraints, existing methods
typically truncate user behavior sequences to include only the most recent
interactions, resulting in the loss of valuable long-range preference signals.
Second, most current methods rely on next-item prediction with a single
predicted embedding, overlooking the multifaceted nature of user interests and
limiting recommendation diversity. To address these challenges, we propose
HyMiRec, a hybrid multi-interest sequential recommendation framework, which
leverages a lightweight recommender to extracts coarse interest embeddings from
long user sequences and an LLM-based recommender to captures refined interest
embeddings. To alleviate the overhead of fetching features, we introduce a
residual codebook based on cosine similarity, enabling efficient compression
and reuse of user history embeddings. To model the diverse preferences of
users, we design a disentangled multi-interest learning module, which leverages
multiple interest queries to learn disentangles multiple interest signals
adaptively, allowing the model to capture different facets of user intent.
Extensive experiments are conducted on both benchmark datasets and a collected
industrial dataset, demonstrating our effectiveness over existing
state-of-the-art methods. Furthermore, online A/B testing shows that HyMiRec
brings consistent improvements in real-world recommendation systems.

</details>
