<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 86]
- [cs.IR](#cs.IR) [Total: 11]
- [cs.LG](#cs.LG) [Total: 152]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Uncovering the Vulnerability of Large Language Models in the Financial Domain via Risk Concealment](https://arxiv.org/abs/2509.10546)
*Gang Cheng,Haibo Jin,Wenbin Zhang,Haohan Wang,Jun Zhuang*

Main category: cs.CL

TL;DR: 本文提出Risk-Concealment Attacks (RCA)多轮攻击框架，通过在金融领域迭代隐藏监管风险，成功绕过主流LLMs的防护机制，平均攻击成功率93.18%，揭示了当前对齐技术的重大安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有红队研究主要针对有害内容，忽视了金融领域的监管风险。随着LLMs在金融应用中的广泛集成，需要专门评估其在金融语境下的安全性和合规性。

Method: 提出了Risk-Concealment Attacks (RCA)多轮攻击框架，通过迭代隐藏监管风险来诱导LLMs生成看似合规但实际违反监管的回应。构建了FIN-Bench金融领域基准测试集进行系统性评估。

Result: 在FIN-Bench上的实验显示，RCA成功绕过了9个主流LLMs，平均攻击成功率达93.18%，其中GPT-4.1为98.28%，OpenAI o1为97.56%。

Conclusion: 研究揭示了当前对齐技术在金融领域存在严重漏洞，迫切需要开发更强的领域感知监管机制，为构建更健壮的LLM对齐技术提供了实践洞见。

Abstract: Large Language Models (LLMs) are increasingly integrated into financial
applications, yet existing red-teaming research primarily targets harmful
content, largely neglecting regulatory risks. In this work, we aim to
investigate the vulnerability of financial LLMs through red-teaming approaches.
We introduce Risk-Concealment Attacks (RCA), a novel multi-turn framework that
iteratively conceals regulatory risks to provoke seemingly compliant yet
regulatory-violating responses from LLMs. To enable systematic evaluation, we
construct FIN-Bench, a domain-specific benchmark for assessing LLM safety in
financial contexts. Extensive experiments on FIN-Bench demonstrate that RCA
effectively bypasses nine mainstream LLMs, achieving an average attack success
rate (ASR) of 93.18%, including 98.28% on GPT-4.1 and 97.56% on OpenAI o1.
These findings reveal a critical gap in current alignment techniques and
underscore the urgent need for stronger moderation mechanisms in financial
domains. We hope this work offers practical insights for advancing robust and
domain-aware LLM alignment.

</details>


### [2] [No Answer Needed: Predicting LLM Answer Accuracy from Question-Only Linear Probes](https://arxiv.org/abs/2509.10625)
*Iván Vicente Moreno Cencerrado,Arnau Padrés Masdemont,Anton Gonzalvez Hawthorne,David Demitri Africa,Lorenzo Pacchiardi*

Main category: cs.CL

TL;DR: 该研究通过提取LLM在回答问题前的激活状态，训练线性探针来预测模型即将给出的答案是否正确，发现这种"提前正确性方向"在不同模型和数据集上都能有效预测答案正确性，且预测能力在中间层达到饱和。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型是否能够在生成答案之前就预知自己回答的正确性，以深入理解LLM的内部工作机制和自我评估能力。

Method: 在三个开源模型家族（7B-70B参数）上，提取问题读取后但答案生成前的激活状态，训练线性探针来预测即将生成答案的正确性，并在分布内和分布外知识数据集上进行测试。

Result: 线性探针在通用琐事问题和多样化知识数据集上都能有效预测答案正确性，优于黑盒基线和口头化置信度预测。预测能力在中间层达到饱和，但在数学推理问题上泛化能力较差。模型说"我不知道"时与探针得分高度相关。

Conclusion: LLM在计算过程中确实形成了自我评估能力，这种内部表征可以提前预测答案正确性，为理解LLM内部机制提供了重要发现，但数学推理方面的局限性表明这种能力存在领域特异性。

Abstract: Do large language models (LLMs) anticipate when they will answer correctly?
To study this, we extract activations after a question is read but before any
tokens are generated, and train linear probes to predict whether the model's
forthcoming answer will be correct. Across three open-source model families
ranging from 7 to 70 billion parameters, projections on this "in-advance
correctness direction" trained on generic trivia questions predict success in
distribution and on diverse out-of-distribution knowledge datasets,
outperforming black-box baselines and verbalised predicted confidence.
Predictive power saturates in intermediate layers, suggesting that
self-assessment emerges mid-computation. Notably, generalisation falters on
questions requiring mathematical reasoning. Moreover, for models responding "I
don't know", doing so strongly correlates with the probe score, indicating that
the same direction also captures confidence. By complementing previous results
on truthfulness and other behaviours obtained with probes and sparse
auto-encoders, our work contributes essential findings to elucidate LLM
internals.

</details>


### [3] [Interdisciplinary Research in Conversation: A Case Study in Computational Morphology for Language Documentation](https://arxiv.org/abs/2509.10644)
*Enora Rice,Katharina von der Wense,Alexis Palmer*

Main category: cs.CL

TL;DR: 本文认为计算形态学与语言文档工作之间存在脱节，需要通过用户中心设计来重新调整研究方向，并通过GlossLM案例研究展示了当前模型在真实使用场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 计算形态学的研究成果在实际语言文档工作中应用有限，存在研究与实践之间的错位，需要通过用户中心设计来弥合这一差距。

Method: 采用用户中心设计原则，通过案例研究分析GlossLM模型，并进行了小规模用户研究（三位文档语言学家参与）。

Result: 尽管GlossLM模型在指标上表现优异，但在实际文档语境中无法满足核心可用性需求，揭示了模型约束、标签标准化、分割和个性化等方面的新研究问题。

Conclusion: 以用户为中心不仅能产生更有效的工具，还能发掘更丰富、更相关的研究方向，避免研究脱离实际应用场景。

Abstract: Computational morphology has the potential to support language documentation
through tasks like morphological segmentation and the generation of Interlinear
Glossed Text (IGT). However, our research outputs have seen limited use in
real-world language documentation settings. This position paper situates the
disconnect between computational morphology and language documentation within a
broader misalignment between research and practice in NLP and argues that the
field risks becoming decontextualized and ineffectual without systematic
integration of User-Centered Design (UCD). To demonstrate how principles from
UCD can reshape the research agenda, we present a case study of GlossLM, a
state-of-the-art multilingual IGT generation model. Through a small-scale user
study with three documentary linguists, we find that despite strong metric
based performance, the system fails to meet core usability needs in real
documentation contexts. These insights raise new research questions around
model constraints, label standardization, segmentation, and personalization. We
argue that centering users not only produces more effective tools, but surfaces
richer, more relevant research directions

</details>


### [4] [Context Copying Modulation: The Role of Entropy Neurons in Managing Parametric and Contextual Knowledge Conflicts](https://arxiv.org/abs/2509.10663)
*Zineddine Tighidet,Andrea Mogini,Hedi Ben-younes,Jiali Mei,Patrick Gallinari,Benjamin Piwowarski*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型中熵神经元在抑制上下文复制行为中的作用，特别是在处理上下文信息与参数知识冲突时的机制。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在面对与内部参数知识冲突的上下文信息时行为不一致，缺乏对预期结果分布的一般解释。最近研究发现了一类称为熵神经元的特殊神经元，它们对模型输出熵有显著影响但对预测标记排序影响不大。

Method: 通过研究熵神经元在解决上下文信息与参数知识冲突中的作用，验证这些神经元是否参与抑制transformer中的上下文复制行为。采用神经元消融技术来分析其对生成过程的影响。

Result: 研究表明熵神经元确实负责抑制各种大型语言模型中的上下文复制行为，消融这些神经元会导致生成过程发生显著变化。

Conclusion: 这些发现增强了我们对大型语言模型在处理冲突信息时内部动态机制的理解，为解释模型在上下文与参数知识冲突时的行为提供了新的见解。

Abstract: The behavior of Large Language Models (LLMs) when facing contextual
information that conflicts with their internal parametric knowledge is
inconsistent, with no generally accepted explanation for the expected outcome
distribution. Recent work has identified in autoregressive transformer models a
class of neurons -- called entropy neurons -- that produce a significant effect
on the model output entropy while having an overall moderate impact on the
ranking of the predicted tokens. In this paper, we investigate the preliminary
claim that these neurons are involved in inhibiting context copying behavior in
transformers by looking at their role in resolving conflicts between contextual
and parametric information. We show that entropy neurons are responsible for
suppressing context copying across a range of LLMs, and that ablating them
leads to a significant change in the generation process. These results enhance
our understanding of the internal dynamics of LLMs when handling conflicting
information.

</details>


### [5] [Pluralistic Alignment for Healthcare: A Role-Driven Framework](https://arxiv.org/abs/2509.10685)
*Jiayou Zhong,Anudeex Shetty,Chao Jia,Xuanrui Lin,Usman Naseem*

Main category: cs.CL

TL;DR: 提出EthosAgents方法，通过模拟多样化视角和价值观来改进大型语言模型在医疗等敏感领域的多元对齐，在7个不同规模的模型中验证了有效性


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法在医疗领域存在不足，个人、文化、情境因素会影响多元主义，需要确保模型输出反映不同人群的多样化价值观和观点

Method: EthosAgents方法，一种轻量级、可泛化的多元对齐方法，通过模拟多样化视角和价值观来实现对齐

Result: 在7个不同规模的开源和闭源模型上验证了该方法对所有三种模式都能提升多元对齐效果

Conclusion: 医疗相关的多元主义需要适应性强且具有规范意识的方法，这些发现为模型在其他高风险领域更好地尊重多样性提供了见解

Abstract: As large language models are increasingly deployed in sensitive domains such
as healthcare, ensuring their outputs reflect the diverse values and
perspectives held across populations is critical. However, existing alignment
approaches, including pluralistic paradigms like Modular Pluralism, often fall
short in the health domain, where personal, cultural, and situational factors
shape pluralism. Motivated by the aforementioned healthcare challenges, we
propose a first lightweight, generalizable, pluralistic alignment approach,
EthosAgents, designed to simulate diverse perspectives and values. We
empirically show that it advances the pluralistic alignment for all three modes
across seven varying-sized open and closed models. Our findings reveal that
health-related pluralism demands adaptable and normatively aware approaches,
offering insights into how these models can better respect diversity in other
high-stakes domains.

</details>


### [6] [Struct-Bench: A Benchmark for Differentially Private Structured Text Generation](https://arxiv.org/abs/2509.10696)
*Shuaiqi Wang,Vikas Raunak,Arturs Backurs,Victor Reis,Pei Zhou,Sihao Chen,Longqi Yang,Zinan Lin,Sergey Yekhanin,Giulia Fanti*

Main category: cs.CL

TL;DR: 提出了Struct-Bench框架和基准测试，用于评估包含自然语言的结构化数据集的差分隐私合成数据生成方法，通过上下文无关文法表示数据结构，包含7个数据集和标准化评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有的合成数据评估技术（如FID）难以捕捉结构化数据集的结构特性和相关性，特别是在企业环境中，结构化数据（如表格式数据）通常包含自然语言字段或组件。

Method: 提出Struct-Bench框架，要求用户使用上下文无关文法（CFG）表示数据集结构，包含5个真实世界和2个合成生成的数据集，每个数据集都标注了CFG，并提供不同指标的参考实现和排行榜。

Result: 这些数据集对最先进的差分隐私合成数据生成方法构成了重大挑战，并通过案例研究展示了如何使用Struct-Bench改进Private Evolution在结构化数据上的合成数据质量。

Conclusion: Struct-Bench为研究人员提供了一个标准化的评估平台，用于基准测试和研究隐私保护的合成数据生成方法，框架和排行榜已公开提供。

Abstract: Differentially private (DP) synthetic data generation is a promising
technique for utilizing private datasets that otherwise cannot be exposed for
model training or other analytics. While much research literature has focused
on generating private unstructured text and image data, in enterprise settings,
structured data (e.g., tabular) is more common, often including natural
language fields or components. Existing synthetic data evaluation techniques
(e.g., FID) struggle to capture the structural properties and correlations of
such datasets. In this work, we propose Struct-Bench, a framework and benchmark
for evaluating synthetic datasets derived from structured datasets that contain
natural language data. The Struct-Bench framework requires users to provide a
representation of their dataset structure as a Context-Free Grammar (CFG). Our
benchmark comprises 5 real-world and 2 synthetically generated datasets, each
annotated with CFGs. We show that these datasets demonstrably present a great
challenge even for state-of-the-art DP synthetic data generation methods.
Struct-Bench also includes reference implementations of different metrics and a
leaderboard, thereby providing researchers a standardized evaluation platform
to benchmark and investigate privacy-preserving synthetic data generation
methods. Further, we also present a case study showing how to use Struct-Bench
to improve the synthetic data quality of Private Evolution (PE) on structured
data. The benchmark and the leaderboard have been publicly made available at
https://struct-bench.github.io.

</details>


### [7] [A Survey on Retrieval And Structuring Augmented Generation with Large Language Models](https://arxiv.org/abs/2509.10697)
*Pengcheng Jiang,Siru Ouyang,Yizhu Jiao,Ming Zhong,Runchu Tian,Jiawei Han*

Main category: cs.CL

TL;DR: 这篇综述论文探讨了检索与结构化增强生成(RAS)方法，通过整合动态信息检索和结构化知识表示来解决LLMs在实际应用中的幻觉生成、知识过时和领域专业知识有限等关键挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在实际部署中面临幻觉生成、知识过时和领域专业知识有限等关键挑战，需要集成外部知识来增强模型能力。

Method: 论文系统性地研究了：(1)检索机制(稀疏、稠密和混合方法)；(2)文本结构化技术(分类构建、层次分类和信息抽取)；(3)结构化表示与LLMs的集成方法(基于提示的方法、推理框架和知识嵌入技术)。

Result: 论文全面概述了RAS方法的技术挑战(检索效率、结构质量和知识集成)和研究机会(多模态检索、跨语言结构和交互式系统)，为研究者和实践者提供了深入的见解。

Conclusion: RAS增强生成通过整合动态检索和结构化知识表示，为解决LLMs的实际应用限制提供了有效途径，并为未来研究指明了方向。

Abstract: Large Language Models (LLMs) have revolutionized natural language processing
with their remarkable capabilities in text generation and reasoning. However,
these models face critical challenges when deployed in real-world applications,
including hallucination generation, outdated knowledge, and limited domain
expertise. Retrieval And Structuring (RAS) Augmented Generation addresses these
limitations by integrating dynamic information retrieval with structured
knowledge representations. This survey (1) examines retrieval mechanisms
including sparse, dense, and hybrid approaches for accessing external
knowledge; (2) explore text structuring techniques such as taxonomy
construction, hierarchical classification, and information extraction that
transform unstructured text into organized representations; and (3) investigate
how these structured representations integrate with LLMs through prompt-based
methods, reasoning frameworks, and knowledge embedding techniques. It also
identifies technical challenges in retrieval efficiency, structure quality, and
knowledge integration, while highlighting research opportunities in multimodal
retrieval, cross-lingual structures, and interactive systems. This
comprehensive overview provides researchers and practitioners with insights
into RAS methods, applications, and future directions.

</details>


### [8] [SearchInstruct: Enhancing Domain Adaptation via Retrieval-Based Instruction Dataset Creation](https://arxiv.org/abs/2509.10708)
*Iman Barati,Mostafa Amiri,Heshaam Faili*

Main category: cs.CL

TL;DR: SearchInstruct是一种创新的方法，用于构建高质量的指令数据集来进行监督微调(SFT)，通过有限的人工生成问题扩展和动态检索领域相关资源来生成答案。


<details>
  <summary>Details</summary>
Motivation: 针对特定领域创建合适的SFT训练数据集具有挑战性，因为存在独特的领域约束和数据稀缺问题，需要一种系统化的方法来提高数据集质量和多样性。

Method: 从少量领域特定的人工生成问题开始，使用大语言模型系统性地扩展问题，然后动态检索领域相关资源为每个扩展问题生成准确且上下文适当的答案。

Result: 实验评估表明SearchInstruct提高了SFT数据集的多样性和质量，在专业领域带来了可衡量的LLM性能改进，还能有效促进模型编辑等任务。

Conclusion: SearchInstruct为构建高质量SFT数据集提供了一种有效方法，不仅改善了模型性能，还支持模型更新，并通过开源代码促进社区采用和可复现性。

Abstract: Supervised Fine-Tuning (SFT) is essential for training large language models
(LLMs), significantly enhancing critical capabilities such as instruction
following and in-context learning. Nevertheless, creating suitable training
datasets tailored for specific domains remains challenging due to unique domain
constraints and data scarcity. In this paper, we propose SearchInstruct, an
innovative method explicitly designed to construct high quality instruction
datasets for SFT. Our approach begins with a limited set of domain specific,
human generated questions, which are systematically expanded using a large
language model. Subsequently, domain relevant resources are dynamically
retrieved to generate accurate and contextually appropriate answers for each
augmented question. Experimental evaluation demonstrates that SearchInstruct
enhances both the diversity and quality of SFT datasets, leading to measurable
improvements in LLM performance within specialized domains. Additionally, we
show that beyond dataset generation, the proposed method can also effectively
facilitate tasks such as model editing, enabling efficient updates to existing
models. To facilitate reproducibility and community adoption, we provide full
implementation details, the complete set of generated instruction response
pairs, and the source code in a publicly accessible Git repository:
[https://github.com/mostafaamiri/SearchInstruct](https://github.com/mostafaamiri/SearchInstruct)

</details>


### [9] [PolyTruth: Multilingual Disinformation Detection using Transformer-Based Language Models](https://arxiv.org/abs/2509.10737)
*Zaur Gouliev,Jennifer Waters,Chengqian Wang*

Main category: cs.CL

TL;DR: 本文系统比较了五种多语言Transformer模型在虚假信息检测任务上的表现，并发布了包含60,486个声明对的PolyTruth Disinfo Corpus多语言数据集，覆盖25种语言。


<details>
  <summary>Details</summary>
Motivation: 虚假信息在跨语言环境中快速传播，但大多数AI模型仅在英语上进行基准测试，缺乏对多语言环境下虚假信息检测能力的系统评估。

Method: 使用mBERT、XLM、XLM-RoBERTa、RemBERT和mT5五种多语言Transformer模型，在PolyTruth Disinfo Corpus数据集上进行虚假vs真实声明的分类任务评估。

Result: 不同模型表现存在差异，RemBERT在整体准确率上表现更好，特别是在低资源语言中表现出色，而mBERT和XLM在训练数据稀缺时存在明显局限性。

Conclusion: 研究揭示了AI系统在多语言虚假信息检测方面的潜力和当前局限性，为实际部署提供了重要参考，并公开数据集以促进进一步研究。

Abstract: Disinformation spreads rapidly across linguistic boundaries, yet most AI
models are still benchmarked only on English. We address this gap with a
systematic comparison of five multilingual transformer models: mBERT, XLM,
XLM-RoBERTa, RemBERT, and mT5 on a common fake-vs-true machine learning
classification task. While transformer-based language models have demonstrated
notable success in detecting disinformation in English, their effectiveness in
multilingual contexts still remains up for debate. To facilitate evaluation, we
introduce PolyTruth Disinfo Corpus, a novel corpus of 60,486 statement pairs
(false claim vs. factual correction) spanning over twenty five languages that
collectively cover five language families and a broad topical range from
politics, health, climate, finance, and conspiracy, half of which are
fact-checked disinformation claims verified by an augmented MindBugs Discovery
dataset. Our experiments revealed performance variations. Models such as
RemBERT achieved better overall accuracy, particularly excelling in
low-resource languages, whereas models like mBERT and XLM exhibit considerable
limitations when training data is scarce. We provide a discussion of these
performance patterns and implications for real-world deployment. The dataset is
publicly available on our GitHub repository to encourage further
experimentation and advancement. Our findings illuminate both the potential and
the current limitations of AI systems for multilingual disinformation
detection.

</details>


### [10] [Reasoning Under Uncertainty: Exploring Probabilistic Reasoning Capabilities of LLMs](https://arxiv.org/abs/2509.10739)
*Mobina Pournemat,Keivan Rezaei,Gaurang Sriramanan,Arman Zarei,Jiaxiang Fu,Yang Wang,Hamid Eghbalzadeh,Soheil Feizi*

Main category: cs.CL

TL;DR: 该研究首次全面评估了大语言模型在离散概率分布上的推理能力，发现大模型在概率推理任务上表现更好，但对符号表示敏感且长上下文性能下降明显


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在语言理解和生成方面取得了广泛成功，但在需要概率推理的任务上表现出不明确且不一致的行为，因此需要系统评估其概率推理能力

Method: 通过三个精心设计的任务（模式识别、最大似然估计和样本生成）来评估模型，让模型基于概率分布的观测结果来回答关于联合分布或其条件分布的查询

Result: 发现大小模型之间存在明显的性能差距，大模型展现出更强的推理能力和令人惊讶的样本生成能力，但也存在对概率结果表示符号的敏感性，以及随着上下文长度增加性能下降超过60%的显著限制

Conclusion: 研究结果提供了对大语言模型概率推理能力的详细理解，并确定了未来改进的关键方向

Abstract: Despite widespread success in language understanding and generation, large
language models (LLMs) exhibit unclear and often inconsistent behavior when
faced with tasks that require probabilistic reasoning. In this work, we present
the first comprehensive study of the reasoning capabilities of LLMs over
explicit discrete probability distributions. Given observations from a
probability distribution, we evaluate models on three carefully designed tasks,
mode identification, maximum likelihood estimation, and sample generation, by
prompting them to provide responses to queries about either the joint
distribution or its conditionals. These tasks thus probe a range of
probabilistic skills, including frequency analysis, marginalization, and
generative behavior. Through comprehensive empirical evaluations, we
demonstrate that there exists a clear performance gap between smaller and
larger models, with the latter demonstrating stronger inference and surprising
capabilities in sample generation. Furthermore, our investigations reveal
notable limitations, including sensitivity to variations in the notation
utilized to represent probabilistic outcomes and performance degradation of
over 60% as context length increases. Together, our results provide a detailed
understanding of the probabilistic reasoning abilities of LLMs and identify key
directions for future improvement.

</details>


### [11] [Automated MCQA Benchmarking at Scale: Evaluating Reasoning Traces as Retrieval Sources for Domain Adaptation of Small Language Models](https://arxiv.org/abs/2509.10744)
*Ozan Gokdemir,Neil Getty,Robert Underwood,Sandeep Madireddy,Franck Cappello,Arvind Ramanathan,Ian T. Foster,Rick L. Stevens*

Main category: cs.CL

TL;DR: 提出了一个从科学论文自动生成多选题评测基准的框架，在放射和癌症生物学领域生成16,000多道题目，发现推理轨迹检索能显著提升小模型性能


<details>
  <summary>Details</summary>
Motivation: 随着科学知识的快速增长，需要与时俱进的评测基准来测试语言模型对最新多样化文献的理解能力

Method: 开发了从PDF解析、语义分块、问题生成到模型评估的全自动化流水线，使用22,000篇开放获取论文生成MCQA基准，并比较了基础模型、基于论文语义块的RAG和基于GPT-4推理轨迹检索的性能

Result: 推理轨迹检索方法在合成和专家标注的基准上都持续提升性能，多个小模型在2023年Astro放射与癌症生物学考试中超越了GPT-4

Conclusion: 该框架能够快速生成科学领域的评测基准，推理轨迹检索是提升小模型科学问答性能的有效方法

Abstract: As scientific knowledge grows at an unprecedented pace, evaluation benchmarks
must evolve to reflect new discoveries and ensure language models are tested on
current, diverse literature. We propose a scalable, modular framework for
generating multiple-choice question-answering (MCQA) benchmarks directly from
large corpora of scientific papers. Our pipeline automates every stage of MCQA
creation, including PDF parsing, semantic chunking, question generation, and
model evaluation. As a case study, we generate more than 16,000 MCQs from
22,000 open-access articles in radiation and cancer biology. We then evaluate a
suite of small language models (1.1B-14B parameters) on these questions,
comparing baseline accuracy with retrieval-augmented generation (RAG) from
paper-derived semantic chunks and from reasoning traces distilled from GPT-4.1.
We find that reasoning-trace retrieval consistently improves performance on
both synthetic and expert-annotated benchmarks, enabling several small models
to surpass GPT-4 on the 2023 Astro Radiation and Cancer Biology exam.

</details>


### [12] [RECAP: Transparent Inference-Time Emotion Alignment for Medical Dialogue Systems](https://arxiv.org/abs/2509.10746)
*Adarsh Srinivasan,Jacob Dineen,Muhammad Umar Afzal,Muhammad Uzair Sarfraz,Irbaz B. Riaz,Ben Zhou*

Main category: cs.CL

TL;DR: RECAP是一个推理时框架，通过结构化情感推理增强医疗AI的情感智能，无需重新训练模型，在情感推理方面提升10-28%，同时保持医学准确性和可审计性。


<details>
  <summary>Details</summary>
Motivation: 医疗场景中的大语言模型往往忽略情感线索，提供医学上正确但情感上平淡的建议，这在患者处于脆弱状态时需要共情沟通来建立安全和信任。

Method: RECAP框架（Reflect-Extract-Calibrate-Align-Produce）将共情分解为透明的评估理论阶段，使用维度Likert信号，通过模块化、理论基础的提示技术增强情感推理。

Result: 在EmoBench、SECEU和EQ-Bench基准测试中，RECAP使8B模型的情感推理提升22-28%，更大模型提升10-13%。临床医生评估确认了更优越的共情沟通能力。

Conclusion: 模块化、理论基础的提示技术可以系统性地增强医疗AI的情感智能，同时保持部署所需的问责制，为医疗AI的情感沟通提供了可审计的解决方案。

Abstract: Large language models in healthcare often miss critical emotional cues,
delivering medically sound but emotionally flat advice. This is especially
problematic in clinical contexts where patients are distressed and vulnerable,
and require empathic communication to support safety, adherence, and trust. We
present RECAP (Reflect-Extract-Calibrate-Align-Produce), an inference-time
framework that adds structured emotional reasoning without retraining. By
decomposing empathy into transparent appraisal-theoretic stages and exposing
per-dimension Likert signals, RECAP produces nuanced, auditable responses.
Across EmoBench, SECEU, and EQ-Bench, RECAP improves emotional reasoning by
22-28% on 8B models and 10-13% on larger models over zero-shot baselines.
Clinician evaluations further confirm superior empathetic communication. RECAP
shows that modular, theory-grounded prompting can systematically enhance
emotional intelligence in medical AI while preserving the accountability
required for deployment.

</details>


### [13] [Judge Q: Trainable Queries for Optimized Information Retention in KV Cache Eviction](https://arxiv.org/abs/2509.10798)
*Yijun Liu,Yixuan Wang,Yuzhuang Xu,Shiyu Ji,Yang Xu,Qingfu Zhu,Wanxiang Che*

Main category: cs.CL

TL;DR: 提出Judge Q方法，通过软令牌列表训练嵌入层，使查询能更好捕获全局信息来评估KV缓存重要性，在相同缓存预算下相比现有方法性能下降更小。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存淘汰方法过度关注局部信息，可能忽略重要全局信息，影响内存使用和解码效率。

Method: 提出Judge Q训练方法，在输入序列末尾拼接软令牌列表，训练这些令牌对原始输入序列的注意力图与实际解码令牌对齐，从而让软令牌对应的查询能有效捕获全局信息。

Result: 在Llama-3.1-8B-Instruct和Mistral-7B-Instruct-v0.3模型上实验，LongBench提升约1分，RULER提升超过3分。

Conclusion: 该方法能以最小训练开销无缝集成到现有开源模型中，提升KV缓存淘汰场景下的性能。

Abstract: Large language models (LLMs) utilize key-value (KV) cache to store historical
information during sequence processing. The size of KV cache grows linearly as
the length of the sequence extends, which seriously affects memory usage and
decoding efficiency. Current methods for KV cache eviction typically utilize
the last window from the pre-filling phase as queries to compute the KV
importance scores for eviction. Although this scheme is simple to implement, it
tends to overly focus on local information, potentially leading to the neglect
or omission of crucial global information. To mitigate this issue, we propose
Judge Q, a novel training method which incorporates a soft token list. This
method only tunes the model's embedding layer at a low training cost. By
concatenating the soft token list at the end of the input sequence, we train
these tokens' attention map to the original input sequence to align with that
of the actual decoded tokens. In this way, the queries corresponding to the
soft tokens can effectively capture global information and better evaluate the
importance of the keys and values within the KV cache, thus maintaining
decoding quality when KV cache is evicted. Under the same eviction budget, our
method exhibits less performance degradation compared to existing eviction
approaches. We validate our approach through experiments conducted on models
such as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks
including LongBench, RULER, and Needle-in-a-Haystack. Results indicate an
improvement of approximately 1 point on the LongBench and over 3 points on
RULER. This proposed methodology can be seamlessly integrated into existing
open-source models with minimal training overhead, thereby enhancing
performance in KV cache eviction scenarios.

</details>


### [14] [Towards Automated Error Discovery: A Study in Conversational AI](https://arxiv.org/abs/2509.10833)
*Dominic Petrak,Thy Thy Tran,Iryna Gurevych*

Main category: cs.CL

TL;DR: 提出了Automated Error Discovery框架和SEEED方法，通过改进的Soft Nearest Neighbor Loss和Label-Based Sample Ranking来检测对话AI中的未知错误，在多个数据集上优于GPT-4o和Phi-4等基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的对话代理虽然流畅连贯，但仍会产生不良行为（错误），且现有LLM难以检测指令中未明确指定的错误，特别是当生成模型更新或用户行为变化时。

Method: 提出了SEEED方法：1）改进Soft Nearest Neighbor Loss，增强负样本的距离权重；2）引入Label-Based Sample Ranking选择高对比度样本；3）基于编码器的错误检测方法。

Result: 在多个错误标注对话数据集上，SEEED优于GPT-4o和Phi-4等基线模型，检测未知错误的准确率提升高达8个百分点，并在未知意图检测方面表现出强泛化能力。

Conclusion: SEEED方法有效解决了对话AI中未知错误的检测问题，通过改进的表示学习方法实现了更好的错误检测性能，为对话系统的安全部署提供了可靠保障。

Abstract: Although LLM-based conversational agents demonstrate strong fluency and
coherence, they still produce undesirable behaviors (errors) that are
challenging to prevent from reaching users during deployment. Recent research
leverages large language models (LLMs) to detect errors and guide
response-generation models toward improvement. However, current LLMs struggle
to identify errors not explicitly specified in their instructions, such as
those arising from updates to the response-generation model or shifts in user
behavior. In this work, we introduce Automated Error Discovery, a framework for
detecting and defining errors in conversational AI, and propose SEEED (Soft
Clustering Extended Encoder-Based Error Detection), as an encoder-based
approach to its implementation. We enhance the Soft Nearest Neighbor Loss by
amplifying distance weighting for negative samples and introduce Label-Based
Sample Ranking to select highly contrastive examples for better representation
learning. SEEED outperforms adapted baselines -- including GPT-4o and Phi-4 --
across multiple error-annotated dialogue datasets, improving the accuracy for
detecting unknown errors by up to 8 points and demonstrating strong
generalization to unknown intent detection.

</details>


### [15] [Evaluating Large Language Models for Evidence-Based Clinical Question Answering](https://arxiv.org/abs/2509.10843)
*Can Wang,Yiqun Chen*

Main category: cs.CL

TL;DR: 该研究评估了LLMs在循证临床问答中的表现，发现准确性在结构化指南中最高(90%)，在叙述性指南和系统综述中较低(60-70%)。检索增强提示显著提升准确性，但需要精准检索而非随机摘要。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在生物医学和临床应用中取得进展，需要严格评估其回答基于证据的复杂问题的能力，以了解其在循证医学中的实际应用潜力。

Method: 使用来自Cochrane系统综述和临床指南的多源基准测试，采用GPT-4o-mini和GPT-5模型，比较不同来源和临床领域的性能，并测试检索增强提示的效果。

Result: 结构化指南准确性最高(90%)，叙述性指南和系统综述较低(60-70%)。提供金标准摘要可将准确性提升至0.79，相关PubMed摘要提升至0.23，随机摘要反而降低准确性。准确性与被引次数强相关。

Conclusion: LLMs在循证临床问答中既有潜力也有局限，检索增强提示是提升准确性的有效策略，但需要针对性检索。按专业和问题类型分层评估对理解模型性能至关重要。

Abstract: Large Language Models (LLMs) have demonstrated substantial progress in
biomedical and clinical applications, motivating rigorous evaluation of their
ability to answer nuanced, evidence-based questions. We curate a multi-source
benchmark drawing from Cochrane systematic reviews and clinical guidelines,
including structured recommendations from the American Heart Association and
narrative guidance used by insurers. Using GPT-4o-mini and GPT-5, we observe
consistent performance patterns across sources and clinical domains: accuracy
is highest on structured guideline recommendations (90%) and lower on narrative
guideline and systematic review questions (60--70%). We also find a strong
correlation between accuracy and the citation count of the underlying
systematic reviews, where each doubling of citations is associated with roughly
a 30% increase in the odds of a correct answer. Models show moderate ability to
reason about evidence quality when contextual information is supplied. When we
incorporate retrieval-augmented prompting, providing the gold-source abstract
raises accuracy on previously incorrect items to 0.79; providing top 3 PubMed
abstracts (ranked by semantic relevance) improves accuracy to 0.23, while
random abstracts reduce accuracy (0.10, within temperature variation). These
effects are mirrored in GPT-4o-mini, underscoring that source clarity and
targeted retrieval -- not just model size -- drive performance. Overall, our
results highlight both the promise and current limitations of LLMs for
evidence-based clinical question answering. Retrieval-augmented prompting
emerges as a useful strategy to improve factual accuracy and alignment with
source evidence, while stratified evaluation by specialty and question type
remains essential to understand current knowledge access and to contextualize
model performance.

</details>


### [16] [RanAT4BIE: Random Adversarial Training for Biomedical Information Extraction](https://arxiv.org/abs/2509.11191)
*Jian Chen,Shengyi Lv,Leilei Su*

Main category: cs.CL

TL;DR: 提出了随机对抗训练(RAT)框架，在生物医学信息抽取任务中显著提升模型性能同时大幅降低计算成本


<details>
  <summary>Details</summary>
Motivation: 传统对抗训练虽然能提升预训练语言模型在生物医学信息抽取任务中的性能，但带来了巨大的计算开销，需要寻找效率解决方案

Method: 基于PubMedBERT架构，将随机采样机制与对抗训练原则策略性结合，在保持模型泛化性和鲁棒性的同时减少计算成本

Result: RAT在生物医学信息抽取任务中表现出优于基线模型的性能，实现了模型性能与计算效率的平衡

Conclusion: RAT是一个具有变革性的生物医学自然语言处理框架，为模型性能和计算效率提供了平衡解决方案

Abstract: We introduce random adversarial training (RAT), a novel framework
successfully applied to biomedical information extraction (BioIE) tasks.
Building on PubMedBERT as the foundational architecture, our study first
validates the effectiveness of conventional adversarial training in enhancing
pre-trained language models' performance on BioIE tasks. While adversarial
training yields significant improvements across various performance metrics, it
also introduces considerable computational overhead. To address this
limitation, we propose RAT as an efficiency solution for biomedical information
extraction. This framework strategically integrates random sampling mechanisms
with adversarial training principles, achieving dual objectives: enhanced model
generalization and robustness while significantly reducing computational costs.
Through comprehensive evaluations, RAT demonstrates superior performance
compared to baseline models in BioIE tasks. The results highlight RAT's
potential as a transformative framework for biomedical natural language
processing, offering a balanced solution to the model performance and
computational efficiency.

</details>


### [17] [GAPrune: Gradient-Alignment Pruning for Domain-Aware Embeddings](https://arxiv.org/abs/2509.10844)
*Yixuan Tang,Yi Yang*

Main category: cs.CL

TL;DR: GAPrune是一个针对领域特定嵌入模型的剪枝框架，通过考虑领域重要性和保持通用语言基础，在50%稀疏度下性能损失小于2.5%，并在重训练后提升领域性能


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法对所有参数一视同仁，无法区分通用语义表示和领域特定模式，导致在资源受限环境中部署大型嵌入模型时剪枝决策不理想

Method: 使用Fisher信息衡量重要性，通过通用领域梯度对齐评估参数行为，结合领域对齐重要性(DAI)评分进行剪枝，低DAI分数表示参数对领域任务不重要或在领域与通用目标间产生冲突

Result: 在FinMTEB和ChemTEB两个领域基准测试中，GAPrune在50%稀疏度下一次剪枝性能保持在密集模型的2.5%以内，重训练100步后在FinMTEB上提升4.51%，在ChemTEB上提升1.73%

Conclusion: 原则性剪枝策略可以实现模型压缩和增强领域专业化，为研究社区提供了新的开发方法

Abstract: Domain-specific embedding models have shown promise for applications that
require specialized semantic understanding, such as coding agents and financial
retrieval systems, often achieving higher performance gains than general
models. However, state-of-the-art embedding models are typically based on LLMs,
which contain billions of parameters, making deployment challenging in
resource-constrained environments. Model compression through pruning offers a
promising solution, but existing pruning methods treat all parameters
uniformly, failing to distinguish between general semantic representations and
domain-specific patterns, leading to suboptimal pruning decisions. Thus, we
propose GAPrune, a pruning framework that addresses this challenge by
considering both domain importance and preserving general linguistic
foundation. Our method uses Fisher Information to measure importance and
general-domain gradient alignment to assess parameter behavior, then combines
these signals using our Domain Alignment Importance (DAI) scoring. Lower DAI
scores indicate that the parameter is either less important for the domain task
or creates conflicts between domain and general objectives. Experiments on two
domain benchmarks, FinMTEB and ChemTEB, show that GAPrune maintains performance
within 2.5% of dense models in one-shot pruning at 50% sparsity, while
outperforming all baselines. With retraining in 100 steps, GAPrune achieves
+4.51% improvement on FinMTEB and +1.73% on ChemTEB, demonstrating that our
pruning strategy not only preserves but enhances domain-specific capabilities.
Our findings demonstrate that principled pruning strategies can achieve model
compression and enhanced domain specialization, providing the research
community with a new approach for development.

</details>


### [18] [Text2Sign Diffusion: A Generative Approach for Gloss-Free Sign Language Production](https://arxiv.org/abs/2509.10845)
*Liqian Feng,Lintao Wang,Kun Hu,Dehui Kong,Zhiyong Wang*

Main category: cs.CL

TL;DR: 提出Text2SignDiff，一种基于扩散模型的免gloss手语生成方法，通过跨模态对齐和潜在扩散过程直接从文本生成手语序列


<details>
  <summary>Details</summary>
Motivation: 现有手语生成方法依赖gloss作为中间表示，但gloss标注稀缺且语言特定，限制了方法的灵活性和泛化能力

Method: 使用潜在扩散模型从噪声潜在代码和文本联合生成手语序列，设计跨模态对齐器学习共享潜在空间，通过非自回归迭代去噪过程减少误差累积

Result: 在PHOENIX14T和How2Sign数据集上达到最先进性能，证明了方法的有效性

Conclusion: Text2SignDiff成功实现了免gloss的手语生成，通过扩散模型和跨模态对齐技术提升了手语生成的准确性和上下文相关性

Abstract: Sign language production (SLP) aims to translate spoken language sentences
into a sequence of pose frames in a sign language, bridging the communication
gap and promoting digital inclusion for deaf and hard-of-hearing communities.
Existing methods typically rely on gloss, a symbolic representation of sign
language words or phrases that serves as an intermediate step in SLP. This
limits the flexibility and generalization of SLP, as gloss annotations are
often unavailable and language-specific. Therefore, we present a novel
diffusion-based generative approach - Text2Sign Diffusion (Text2SignDiff) for
gloss-free SLP. Specifically, a gloss-free latent diffusion model is proposed
to generate sign language sequences from noisy latent sign codes and spoken
text jointly, reducing the potential error accumulation through a
non-autoregressive iterative denoising process. We also design a cross-modal
signing aligner that learns a shared latent space to bridge visual and textual
content in sign and spoken languages. This alignment supports the conditioned
diffusion-based process, enabling more accurate and contextually relevant sign
language generation without gloss. Extensive experiments on the commonly used
PHOENIX14T and How2Sign datasets demonstrate the effectiveness of our method,
achieving the state-of-the-art performance.

</details>


### [19] [A funny companion: Distinct neural responses to perceived AI- versus human- generated humor](https://arxiv.org/abs/2509.10847)
*Xiaohui Rao,Hanlin Wu,Zhenguang G. Cai*

Main category: cs.CL

TL;DR: 研究发现AI幽默与人类幽默在行为评分上相似，但神经生理数据显示AI幽默引发更小的N400（认知努力减少）和更大的LPP（情绪反应增强），表明大脑对AI幽默有积极且强烈的反应。


<details>
  <summary>Details</summary>
Motivation: 随着AI伴侣能够进行类人交流（包括讲笑话），了解人们对AI幽默的认知和情感反应变得日益重要。

Method: 使用脑电图（EEG）比较人们处理AI与人类来源幽默的方式，分析行为评分和神经生理数据（N400和LPP效应）。

Result: 行为分析显示参与者认为AI和人类幽默同样有趣。神经数据显示AI幽默引发更小的N400（认知努力减少）和更大的LPP（情绪反应增强），且随时间呈现不同的动态模式。

Conclusion: 大脑对AI幽默表现出令人惊讶的积极强烈反应，幽默在促进人机社交互动中具有重要潜力，认知适应AI语言模式可带来增强的情感奖励。

Abstract: As AI companions become capable of human-like communication, including
telling jokes, understanding how people cognitively and emotionally respond to
AI humor becomes increasingly important. This study used electroencephalography
(EEG) to compare how people process humor from AI versus human sources.
Behavioral analysis revealed that participants rated AI and human humor as
comparably funny. However, neurophysiological data showed that AI humor
elicited a smaller N400 effect, suggesting reduced cognitive effort during the
processing of incongruity. This was accompanied by a larger Late Positive
Potential (LPP), indicating a greater degree of surprise and emotional
response. This enhanced LPP likely stems from the violation of low initial
expectations regarding AI's comedic capabilities. Furthermore, a key temporal
dynamic emerged: human humor showed habituation effects, marked by an
increasing N400 and a decreasing LPP over time. In contrast, AI humor
demonstrated increasing processing efficiency and emotional reward, with a
decreasing N400 and an increasing LPP. This trajectory reveals how the brain
can dynamically update its predictive model of AI capabilities. This process of
cumulative reinforcement challenges "algorithm aversion" in humor, as it
demonstrates how cognitive adaptation to AI's language patterns can lead to an
intensified emotional reward. Additionally, participants' social attitudes
toward AI modulated these neural responses, with higher perceived AI
trustworthiness correlating with enhanced emotional engagement. These findings
indicate that the brain responds to AI humor with surprisingly positive and
intense reactions, highlighting humor's potential for fostering genuine
engagement in human-AI social interaction.

</details>


### [20] [Pre-Storage Reasoning for Episodic Memory: Shifting Inference Burden to Memory for Personalized Dialogue](https://arxiv.org/abs/2509.10852)
*Sangyeop Kim,Yohan Lee,Sanghwa Kim,Hyunjong Kim,Sungzoon Cho*

Main category: cs.CL

TL;DR: PREMem是一种新的对话AI记忆方法，通过在记忆构建阶段而非响应生成阶段进行复杂推理，将细粒度记忆片段分类并建立跨会话关系，显著提升了各种规模模型的性能


<details>
  <summary>Details</summary>
Motivation: 当前对话AI系统在长期记忆方面将过多推理负担放在响应生成阶段，导致性能严重依赖模型规模，需要一种能够降低推理时计算需求的方法

Method: PREMem在记忆预存储阶段提取事实性、经验性和主观性细粒度记忆片段，建立跨会话的显式关系，捕捉扩展、转换和含义等演化模式

Result: 实验显示所有模型规模都有显著性能提升，小模型能达到与大基线模型相当的结果，且在受限token预算下仍保持有效性

Conclusion: PREMem通过将复杂推理从推理阶段转移到记忆构建阶段，有效降低了对话交互时的计算需求，同时提升了长期记忆性能

Abstract: Effective long-term memory in conversational AI requires synthesizing
information across multiple sessions. However, current systems place excessive
reasoning burden on response generation, making performance significantly
dependent on model sizes. We introduce PREMem (Pre-storage Reasoning for
Episodic Memory), a novel approach that shifts complex reasoning processes from
inference to memory construction. PREMem extracts fine-grained memory fragments
categorized into factual, experiential, and subjective information; it then
establishes explicit relationships between memory items across sessions,
capturing evolution patterns like extensions, transformations, and
implications. By performing this reasoning during pre-storage rather than when
generating a response, PREMem creates enriched representations while reducing
computational demands during interactions. Experiments show significant
performance improvements across all model sizes, with smaller models achieving
results comparable to much larger baselines while maintaining effectiveness
even with constrained token budgets. Code and dataset are available at
https://github.com/sangyeop-kim/PREMem.

</details>


### [21] [Quantifier Scope Interpretation in Language Learners and LLMs](https://arxiv.org/abs/2509.10860)
*Shaohua Fang,Yue Li,Yan Cong*

Main category: cs.CL

TL;DR: 本研究通过跨语言方法分析大型语言模型在英语和中文中处理量词范围解释的能力，发现大多数模型偏好表层范围解释，与人类倾向一致，但不同模型在近似人类行为方面存在差异。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究大型语言模型如何处理不同语言中多量词句子的解释歧义，特别是比较英语和中文中量词范围解释的差异，以及模型是否能够模拟人类的解释模式。

Method: 采用跨语言研究方法，使用概率评估解释可能性，通过人类相似性(HS)分数量化LLMs模拟人类表现的程度，比较不同模型架构、规模和预训练数据语言背景的影响。

Result: 结果显示大多数LLMs偏好表层范围解释，与人类倾向一致；只有部分模型在逆向范围偏好上区分英语和中文，反映人类相似模式；HS分数显示模型在近似人类行为方面存在变异性。

Conclusion: LLMs在量词范围解释方面具有与人类对齐的潜力，但模型架构、规模和预训练数据的语言背景显著影响其近似人类解释的程度。

Abstract: Sentences with multiple quantifiers often lead to interpretive ambiguities,
which can vary across languages. This study adopts a cross-linguistic approach
to examine how large language models (LLMs) handle quantifier scope
interpretation in English and Chinese, using probabilities to assess
interpretive likelihood. Human similarity (HS) scores were used to quantify the
extent to which LLMs emulate human performance across language groups. Results
reveal that most LLMs prefer the surface scope interpretations, aligning with
human tendencies, while only some differentiate between English and Chinese in
the inverse scope preferences, reflecting human-similar patterns. HS scores
highlight variability in LLMs' approximation of human behavior, but their
overall potential to align with humans is notable. Differences in model
architecture, scale, and particularly models' pre-training data language
background, significantly influence how closely LLMs approximate human
quantifier scope interpretations.

</details>


### [22] [Term2Note: Synthesising Differentially Private Clinical Notes from Medical Terms](https://arxiv.org/abs/2509.10882)
*Yuping Wu,Viktor Schlegel,Warren Del-Pinto,Srinivasan Nandakumar,Iqra Zahid,Yidan Sun,Usama Farghaly Omar,Amirah Jasmine,Arun-Kumar Kaliya-Perumal,Chun Shen Tham,Gabriel Connors,Anil A Bharath,Goran Nenadic*

Main category: cs.CL

TL;DR: Term2Note是一种在强差分隐私约束下生成临床笔记的方法，通过分离内容和形式，基于隐私保护的医学术语生成结构化笔记内容，在保持数据效用的同时提供正式隐私保证。


<details>
  <summary>Details</summary>
Motivation: 在医疗等高风险领域，使用真实训练数据受到隐私泄露的严重限制，需要找到在隐私保护和数据效用之间平衡的解决方案。

Method: 通过结构性地分离内容和形式，基于差分隐私医学术语生成分段的笔记内容，每个部分受不同的隐私约束控制，并使用差分隐私质量最大化器选择高质量输出。

Result: 实验结果显示Term2Note生成的合成笔记在统计特性上与真实临床笔记高度一致，基于合成笔记训练的多标签分类模型性能与使用真实数据训练的模型相当。

Conclusion: Term2Note在比现有基线更少假设的情况下，在保真度和效用方面都实现了显著改进，表明其作为使用敏感临床笔记的隐私保护替代方案的潜力。

Abstract: Training data is fundamental to the success of modern machine learning
models, yet in high-stakes domains such as healthcare, the use of real-world
training data is severely constrained by concerns over privacy leakage. A
promising solution to this challenge is the use of differentially private (DP)
synthetic data, which offers formal privacy guarantees while maintaining data
utility. However, striking the right balance between privacy protection and
utility remains challenging in clinical note synthesis, given its domain
specificity and the complexity of long-form text generation. In this paper, we
present Term2Note, a methodology to synthesise long clinical notes under strong
DP constraints. By structurally separating content and form, Term2Note
generates section-wise note content conditioned on DP medical terms, with each
governed by separate DP constraints. A DP quality maximiser further enhances
synthetic notes by selecting high-quality outputs. Experimental results show
that Term2Note produces synthetic notes with statistical properties closely
aligned with real clinical notes, demonstrating strong fidelity. In addition,
multi-label classification models trained on these synthetic notes perform
comparably to those trained on real data, confirming their high utility.
Compared to existing DP text generation baselines, Term2Note achieves
substantial improvements in both fidelity and utility while operating under
fewer assumptions, suggesting its potential as a viable privacy-preserving
alternative to using sensitive clinical notes.

</details>


### [23] [CultureSynth: A Hierarchical Taxonomy-Guided and Retrieval-Augmented Framework for Cultural Question-Answer Synthesis](https://arxiv.org/abs/2509.10886)
*Xinyu Zhang,Pei Zhang,Shuang Luo,Jialong Tang,Yu Wan,Baosong Yang,Fei Huang*

Main category: cs.CL

TL;DR: CultureSynth是一个新的文化能力评估框架，包含多语言文化分类法和基于RAG的问答对生成方法，用于评估LLMs的文化能力。


<details>
  <summary>Details</summary>
Motivation: 现有文化能力评估存在分类法碎片化、领域特定性和依赖人工标注的问题，需要更全面和可扩展的评估方法。

Method: 提出CultureSynth框架：(1)包含12个主要和130个次要主题的多层次多语言文化分类法；(2)基于检索增强生成(RAG)的方法，利用事实知识合成文化相关问答对。

Result: 创建了包含19,360个条目和4,149个手动验证条目的CultureSynth-7基准测试集。评估14个不同规模的LLM显示，ChatGPT-4o-Latest和Qwen2.5-72B-Instruct表现最佳，发现3B参数是基本文化能力的最低要求，模型存在架构偏见和地理差异。

Conclusion: CultureSynth为开发文化感知AI系统提供了可扩展框架，减少了对人工标注的依赖，有助于提升LLMs的全球文化能力。

Abstract: Cultural competence, defined as the ability to understand and adapt to
multicultural contexts, is increasingly vital for large language models (LLMs)
in global environments. While several cultural benchmarks exist to assess LLMs'
cultural competence, current evaluations suffer from fragmented taxonomies,
domain specificity, and heavy reliance on manual data annotation. To address
these limitations, we introduce CultureSynth, a novel framework comprising (1)
a comprehensive hierarchical multilingual cultural taxonomy covering 12 primary
and 130 secondary topics, and (2) a Retrieval-Augmented Generation (RAG)-based
methodology leveraging factual knowledge to synthesize culturally relevant
question-answer pairs. The CultureSynth-7 synthetic benchmark contains 19,360
entries and 4,149 manually verified entries across 7 languages. Evaluation of
14 prevalent LLMs of different sizes reveals clear performance stratification
led by ChatGPT-4o-Latest and Qwen2.5-72B-Instruct. The results demonstrate that
a 3B-parameter threshold is necessary for achieving basic cultural competence,
models display varying architectural biases in knowledge processing, and
significant geographic disparities exist across models. We believe that
CultureSynth offers a scalable framework for developing culturally aware AI
systems while reducing reliance on manual annotation\footnote{Benchmark is
available at https://github.com/Eyr3/CultureSynth.}.

</details>


### [24] [Aligning ESG Controversy Data with International Guidelines through Semi-Automatic Ontology Construction](https://arxiv.org/abs/2509.10922)
*Tsuyoshi Iwata,Guillaume Comte,Melissa Flores,Ryoma Kondo,Ryohei Hisano*

Main category: cs.CL

TL;DR: 本文提出了一种半自动方法，使用轻量级本体设计、形式化模式建模和大语言模型，将ESG新闻事件转化为结构化知识图谱，以解决非财务风险数据与规范性框架对齐的挑战。


<details>
  <summary>Details</summary>
Motivation: 环境、社会和治理(ESG)数据在监管和投资中的重要性日益增长，但将新闻中的争议相关数据与联合国全球契约等原则性规范性框架对齐存在重大挑战，因为这些框架语言抽象、缺乏标准化分类法。

Method: 采用半自动方法，结合轻量级本体设计、形式化模式建模和大语言模型，将规范性原则转化为可重用的RDF模板，从新闻内容中提取相关信息并构建结构化知识图谱。

Result: 开发了一个可扩展且透明的框架，能够识别和解释与国际可持续发展指南的不合规情况，将报道的事件与特定框架原则相连接。

Conclusion: 该方法为解决ESG数据与规范性框架对齐问题提供了有效的技术方案，实现了从非结构化新闻到结构化知识表示的转化，有助于提升ESG风险评估的准确性和可解释性。

Abstract: The growing importance of environmental, social, and governance data in
regulatory and investment contexts has increased the need for accurate,
interpretable, and internationally aligned representations of non-financial
risks, particularly those reported in unstructured news sources. However,
aligning such controversy-related data with principle-based normative
frameworks, such as the United Nations Global Compact or Sustainable
Development Goals, presents significant challenges. These frameworks are
typically expressed in abstract language, lack standardized taxonomies, and
differ from the proprietary classification systems used by commercial data
providers. In this paper, we present a semi-automatic method for constructing
structured knowledge representations of environmental, social, and governance
events reported in the news. Our approach uses lightweight ontology design,
formal pattern modeling, and large language models to convert normative
principles into reusable templates expressed in the Resource Description
Framework. These templates are used to extract relevant information from news
content and populate a structured knowledge graph that links reported incidents
to specific framework principles. The result is a scalable and transparent
framework for identifying and interpreting non-compliance with international
sustainability guidelines.

</details>


### [25] [Introducing Spotlight: A Novel Approach for Generating Captivating Key Information from Documents](https://arxiv.org/abs/2509.10935)
*Ankan Mullick,Sombit Bose,Rounak Saha,Ayan Kumar Bhowmick,Aditya Vempaty,Prasenjit Dey,Ravi Kokku,Pawan Goyal,Niloy Ganguly*

Main category: cs.CL

TL;DR: Spotlight是一种新颖的信息提取范式，通过突出文档中最吸引人的内容来生成简洁、引人入胜的叙事，与传统追求全面覆盖的摘要不同，它选择性地强调有趣内容以促进读者深度参与。


<details>
  <summary>Details</summary>
Motivation: 传统摘要方法过于注重全面性而忽略了内容的吸引力和读者参与度，需要一种能够突出文档中最引人注目内容的新方法来提升阅读体验和参与价值。

Method: 采用两阶段方法：首先在基准数据上对大型语言模型进行微调，然后通过直接偏好优化（DPO）进行对齐，以生成高质量的spotlight内容。

Result: 综合评估表明，所提出的模型不仅能够精确识别关键元素，还能显著提升可读性并增强原始文档的参与价值。

Conclusion: Spotlight范式通过选择性强调文档中最引人入胜的内容，有效提升了信息提取的质量和读者参与度，为文档摘要领域提供了新的研究方向。

Abstract: In this paper, we introduce Spotlight, a novel paradigm for information
extraction that produces concise, engaging narratives by highlighting the most
compelling aspects of a document. Unlike traditional summaries, which
prioritize comprehensive coverage, spotlights selectively emphasize intriguing
content to foster deeper reader engagement with the source material. We
formally differentiate spotlights from related constructs and support our
analysis with a detailed benchmarking study using new datasets curated for this
work. To generate high-quality spotlights, we propose a two-stage approach:
fine-tuning a large language model on our benchmark data, followed by alignment
via Direct Preference Optimization (DPO). Our comprehensive evaluation
demonstrates that the resulting model not only identifies key elements with
precision but also enhances readability and boosts the engagement value of the
original document.

</details>


### [26] [An Interpretable Benchmark for Clickbait Detection and Tactic Attribution](https://arxiv.org/abs/2509.10937)
*Lihi Nofar,Tomer Portal,Aviv Elbaz,Alexander Apartsin,Yehudit Aperstein*

Main category: cs.CL

TL;DR: 提出了一种可解释的点击诱饵检测模型，不仅能识别点击诱饵标题，还能归因于特定的语言操纵策略，使用合成数据集和两阶段检测框架。


<details>
  <summary>Details</summary>
Motivation: 点击诱饵标题的泛滥对信息可信度和用户信任构成挑战，现有机器学习方法缺乏可解释性限制了实际应用。

Method: 使用系统增强真实新闻标题生成的合成数据集，采用两阶段框架：第一阶段比较微调BERT与LLM的检测性能，第二阶段用BERT分类器预测具体点击诱饵策略。

Result: 开发了包含检测和策略归因的可解释点击诱饵分析系统，并公开了数据集供研究社区使用。

Conclusion: 这项工作推动了透明可信AI系统的发展，用于对抗操纵性媒体内容。

Abstract: The proliferation of clickbait headlines poses significant challenges to the
credibility of information and user trust in digital media. While recent
advances in machine learning have improved the detection of manipulative
content, the lack of explainability limits their practical adoption. This paper
presents a model for explainable clickbait detection that not only identifies
clickbait titles but also attributes them to specific linguistic manipulation
strategies. We introduce a synthetic dataset generated by systematically
augmenting real news headlines using a predefined catalogue of clickbait
strategies. This dataset enables controlled experimentation and detailed
analysis of model behaviour. We present a two-stage framework for automatic
clickbait analysis comprising detection and tactic attribution. In the first
stage, we compare a fine-tuned BERT classifier with large language models
(LLMs), specifically GPT-4.0 and Gemini 2.4 Flash, under both zero-shot
prompting and few-shot prompting enriched with illustrative clickbait headlines
and their associated persuasive tactics. In the second stage, a dedicated
BERT-based classifier predicts the specific clickbait strategies present in
each headline. This work advances the development of transparent and
trustworthy AI systems for combating manipulative media content. We share the
dataset with the research community at
https://github.com/LLM-HITCS25S/ClickbaitTacticsDetection

</details>


### [27] [EmoBench-Reddit: A Hierarchical Benchmark for Evaluating the Emotional Intelligence of Multimodal Large Language Models](https://arxiv.org/abs/2509.11101)
*Haokun Li,Yazhou Zhang,Jizhi Ding,Qiuchi Li,Peng Zhang*

Main category: cs.CL

TL;DR: EmoBench-Reddit是一个用于评估多模态大语言模型情感理解能力的分层基准数据集，包含350个来自Reddit的精心标注样本，每个样本包含图像、文本和情感类别，并设计了从基础感知到高级认知的多层次任务。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态大语言模型评估基准主要关注客观的视觉问答或图像描述任务，缺乏对复杂主观情感理解能力的评估，需要专门的情感理解基准来填补这一空白。

Method: 从Reddit社交媒体平台收集350个样本，每个样本包含图像、用户提供的文本和情感类别（悲伤、幽默、讽刺、快乐），通过用户标签确认情感类别。设计分层任务框架，包含6个多选题和1个开放式问题，从基础感知到高级认知逐步深入。使用Claude 4 AI辅助和人工验证确保标注质量。

Result: 构建了EmoBench-Reddit数据集，包含350个高质量标注样本，建立了从视觉元素识别到场景推理、意图理解和深度共情的多层次评估体系。

Conclusion: EmoBench-Reddit为多模态大语言模型的情感理解能力提供了全面的评估基准，填补了现有基准在主观情感理解方面的不足，有助于推动模型在复杂情感认知方面的发展。

Abstract: With the rapid advancement of Multimodal Large Language Models (MLLMs), they
have demonstrated exceptional capabilities across a variety of vision-language
tasks. However, current evaluation benchmarks predominantly focus on objective
visual question answering or captioning, inadequately assessing the models'
ability to understand complex and subjective human emotions. To bridge this
gap, we introduce EmoBench-Reddit, a novel, hierarchical benchmark for
multimodal emotion understanding. The dataset comprises 350 meticulously
curated samples from the social media platform Reddit, each containing an
image, associated user-provided text, and an emotion category (sad, humor,
sarcasm, happy) confirmed by user flairs. We designed a hierarchical task
framework that progresses from basic perception to advanced cognition, with
each data point featuring six multiple-choice questions and one open-ended
question of increasing difficulty. Perception tasks evaluate the model's
ability to identify basic visual elements (e.g., colors, objects), while
cognition tasks require scene reasoning, intent understanding, and deep empathy
integrating textual context. We ensured annotation quality through a
combination of AI assistance (Claude 4) and manual verification.

</details>


### [28] [Fluid Language Model Benchmarking](https://arxiv.org/abs/2509.11106)
*Valentin Hofmann,David Heineman,Ian Magnusson,Kyle Lo,Jesse Dodge,Maarten Sap,Pang Wei Koh,Chun Wang,Hannaneh Hajishirzi,Noah A. Smith*

Main category: cs.CL

TL;DR: Fluid Benchmarking是一种新的语言模型评估方法，通过项目反应理论和动态项目选择来提高评估效率、有效性、减少方差和饱和问题


<details>
  <summary>Details</summary>
Motivation: 解决语言模型基准测试面临的挑战：全面评估成本高、基准测试无法准确测量目标能力、标签错误和基准饱和导致评估质量下降

Method: 基于项目反应理论估计项目响应模型，使用推断量动态选择评估项目，类似于教育中的计算机化自适应测试

Result: 在效率、有效性、方差和饱和四个维度上都优于随机项目抽样和其他基线方法，在MMLU上使用50倍更少的项目实现了更高的有效性和更小的方差

Conclusion: 语言模型基准测试可以通过超越静态评估来显著改进，Fluid Benchmarking的两个组件分别提高有效性和减少方差

Abstract: Language model (LM) benchmarking faces several challenges: comprehensive
evaluations are costly, benchmarks often fail to measure the intended
capabilities, and evaluation quality can degrade due to labeling errors and
benchmark saturation. Although various strategies have been proposed to
mitigate these issues, they tend to address individual aspects in isolation,
neglecting broader questions about overall evaluation quality. Here, we
introduce Fluid Benchmarking, a new evaluation approach that advances LM
benchmarking across multiple dimensions. Inspired by psychometrics, Fluid
Benchmarking is based on the insight that the relative value of benchmark items
depends on an LM's capability level, suggesting that evaluation should adapt to
each LM. Methodologically, Fluid Benchmarking estimates an item response model
based on existing LM evaluation results and uses the inferred quantities to
select evaluation items dynamically, similar to computerized adaptive testing
in education. In our experiments, we compare Fluid Benchmarking against the
common practice of random item sampling as well as more sophisticated
baselines, including alternative methods grounded in item response theory. We
examine four dimensions -- efficiency, validity, variance, and saturation --
and find that Fluid Benchmarking achieves superior performance in all of them
(e.g., higher validity and less variance on MMLU with fifty times fewer items).
Our analysis shows that the two components of Fluid Benchmarking have distinct
effects: item response theory, used to map performance into a latent ability
space, increases validity, while dynamic item selection reduces variance.
Overall, our results suggest that LM benchmarking can be substantially improved
by moving beyond static evaluation.

</details>


### [29] [We Argue to Agree: Towards Personality-Driven Argumentation-Based Negotiation Dialogue Systems for Tourism](https://arxiv.org/abs/2509.11118)
*Priyanshu Priya,Saurav Dudhate,Desai Vishesh Yasheshbhai,Asif Ekbal*

Main category: cs.CL

TL;DR: 提出PAN-DG任务和PACT数据集，通过个性驱动的论证式谈判对话生成，提升谈判系统的个性化和推理能力


<details>
  <summary>Details</summary>
Motivation: 将论证机制融入谈判对话系统可以改善冲突解决，而加入个性属性能增强系统适应性，使其更符合个人偏好和风格

Method: 提出PAN-DG任务，构建PACT数据集（包含三种个性配置文件），使用LLM生成高质量对话，并进行预训练与微调LLM的对比实验

Result: 自动和人工评估显示数据集质量高，微调后的LLM能有效生成个性驱动的理性谈判响应

Conclusion: PACT数据集有效提升了谈判对话系统的个性化和推理能力，为该领域未来研究奠定了基础

Abstract: Integrating argumentation mechanisms into negotiation dialogue systems
improves conflict resolution through exchanges of arguments and critiques.
Moreover, incorporating personality attributes enhances adaptability by
aligning interactions with individuals' preferences and styles. To advance
these capabilities in negotiation dialogue systems, we propose a novel
Personality-driven Argumentation-based Negotiation Dialogue Generation (PAN-DG)
task. To support this task, we introduce PACT, a dataset of Personality-driven
Argumentation-based negotiation Conversations for Tourism sector. This dataset,
generated using Large Language Models (LLMs), features three distinct
personality profiles, viz. Argumentation Profile, Preference Profile, and
Buying Style Profile to simulate a variety of negotiation scenarios involving
diverse personalities. Thorough automatic and manual evaluations indicate that
the dataset comprises high-quality dialogues. Further, we conduct comparative
experiments between pre-trained and fine-tuned LLMs for the PAN-DG task.
Multi-dimensional evaluation demonstrates that the fine-tuned LLMs effectively
generate personality-driven rational responses during negotiations. This
underscores the effectiveness of PACT in enhancing personalization and
reasoning capabilities in negotiation dialogue systems, thereby establishing a
foundation for future research in this domain.

</details>


### [30] [Joint Effects of Argumentation Theory, Audio Modality and Data Enrichment on LLM-Based Fallacy Classification](https://arxiv.org/abs/2509.11127)
*Hongxu Zhou,Hylke Westerdijk,Khondoker Ittehadul Islam*

Main category: cs.CL

TL;DR: 本研究探讨上下文和情感语调元数据如何影响大语言模型在谬误分类任务中的推理和性能，特别是在政治辩论场景中。研究发现情感元数据会偏向将陈述标记为"诉诸情感"谬误，反而降低逻辑推理能力。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在政治辩论谬误分类任务中，上下文信息和情感语调元数据对模型推理性能的影响，探索不同提示策略的有效性。

Method: 使用美国总统辩论数据，通过Qwen-3 (8B)模型对六种谬误类型进行分类。采用三种输入设置（纯文本、文本+上下文、文本+上下文+情感语调元数据），比较两种理论性Chain-of-Thought框架（语用辩证法和论证周期表）与基线提示的效果。

Result: 理论性提示可以提高可解释性，有时也能提高准确性，但添加上下文和情感语调元数据通常会导致性能下降。情感语调元数据使模型偏向将陈述标记为"诉诸情感"谬误，恶化了逻辑推理。基础提示往往优于增强提示。

Conclusion: 额外的输入信息可能导致注意力分散，反而恶化LLMs的谬误分类性能。情感元数据引入的偏见对逻辑推理有负面影响，简单的提示策略在某些情况下可能更有效。

Abstract: This study investigates how context and emotional tone metadata influence
large language model (LLM) reasoning and performance in fallacy classification
tasks, particularly within political debate settings. Using data from U.S.
presidential debates, we classify six fallacy types through various prompting
strategies applied to the Qwen-3 (8B) model. We introduce two theoretically
grounded Chain-of-Thought frameworks: Pragma-Dialectics and the Periodic Table
of Arguments, and evaluate their effectiveness against a baseline prompt under
three input settings: text-only, text with context, and text with both context
and audio-based emotional tone metadata. Results suggest that while theoretical
prompting can improve interpretability and, in some cases, accuracy, the
addition of context and especially emotional tone metadata often leads to
lowered performance. Emotional tone metadata biases the model toward labeling
statements as \textit{Appeal to Emotion}, worsening logical reasoning. Overall,
basic prompts often outperformed enhanced ones, suggesting that attention
dilution from added inputs may worsen rather than improve fallacy
classification in LLMs.

</details>


### [31] [When Smiley Turns Hostile: Interpreting How Emojis Trigger LLMs' Toxicity](https://arxiv.org/abs/2509.11141)
*Shiyao Cui,Xijia Feng,Yingkang Wang,Junxiao Yang,Zhexin Zhang,Biplab Sikdar,Hongning Wang,Han Qiu,Minlie Huang*

Main category: cs.CL

TL;DR: 研究发现表情符号可以触发大型语言模型生成有毒内容，通过自动化构建含表情符号的提示词，在7个主流LLM和5种语言中验证了这一现象，并揭示了表情符号作为异质语义通道绕过安全机制的机制。


<details>
  <summary>Details</summary>
Motivation: 观察到表情符号可能触发LLM生成有毒内容，旨在研究：(1)表情符号是否能明显增强LLM的毒性生成；(2)如何解释这一现象。

Method: 通过自动化构建含表情符号的提示词来微妙表达有毒意图，在7个著名LLM和5种主流语言上进行实验，并进行模型级解释（语义认知、序列生成、分词）和预训练语料分析。

Result: 实验表明含表情符号的提示词容易诱导毒性生成，表情符号可作为异质语义通道绕过安全机制，预训练语料中的表情符号相关数据污染与毒性生成行为存在潜在关联。

Conclusion: 表情符号确实能显著增强LLM的毒性生成能力，这一现象源于表情符号作为异质语义通道的特性以及预训练语料中的数据污染问题，需要引起安全机制设计的重视。

Abstract: Emojis are globally used non-verbal cues in digital communication, and
extensive research has examined how large language models (LLMs) understand and
utilize emojis across contexts. While usually associated with friendliness or
playfulness, it is observed that emojis may trigger toxic content generation in
LLMs. Motivated by such a observation, we aim to investigate: (1) whether
emojis can clearly enhance the toxicity generation in LLMs and (2) how to
interpret this phenomenon. We begin with a comprehensive exploration of
emoji-triggered LLM toxicity generation by automating the construction of
prompts with emojis to subtly express toxic intent. Experiments across 5
mainstream languages on 7 famous LLMs along with jailbreak tasks demonstrate
that prompts with emojis could easily induce toxicity generation. To understand
this phenomenon, we conduct model-level interpretations spanning semantic
cognition, sequence generation and tokenization, suggesting that emojis can act
as a heterogeneous semantic channel to bypass the safety mechanisms. To pursue
deeper insights, we further probe the pre-training corpus and uncover potential
correlation between the emoji-related data polution with the toxicity
generation behaviors. Supplementary materials provide our implementation code
and data. (Warning: This paper contains potentially sensitive contents)

</details>


### [32] [Text2Mem: A Unified Memory Operation Language for Memory Operating System](https://arxiv.org/abs/2509.11145)
*Felix Wang,Boyu Chen,Kerun Xu,Bo Tang,Feiyu Xiong,Zhiyu Li*

Main category: cs.CL

TL;DR: Text2Mem是一个统一的内存操作语言，为LLM代理提供从自然语言到可靠执行的标准路径，通过JSON模式定义表达性操作集，确保跨后端的安全性和可移植性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理内存框架功能有限，只提供基本操作（编码、检索、删除），缺乏高级操作（合并、升级、降级、拆分等），且没有正式可执行规范，导致跨系统行为不可预测。

Method: 设计Text2Mem语言，定义紧凑而表达性强的操作集；每个指令表示为基于JSON的模式实例；解析器转换为类型化操作对象；验证器确保正确性；适配器映射到SQL原型后端或实际内存框架；集成模型服务如嵌入和摘要。

Result: 建立了第一个标准化的内存控制基础，确保安全性、确定性和跨异构后端的可移植性；所有结果通过统一执行合约返回。

Conclusion: Text2Mem为代理内存控制提供了标准化基础，解决了现有框架的功能局限和规范缺失问题，通过统一语言和验证机制实现了可靠的内存操作执行。

Abstract: Large language model agents increasingly depend on memory to sustain long
horizon interaction, but existing frameworks remain limited. Most expose only a
few basic primitives such as encode, retrieve, and delete, while higher order
operations like merge, promote, demote, split, lock, and expire are missing or
inconsistently supported. Moreover, there is no formal and executable
specification for memory commands, leaving scope and lifecycle rules implicit
and causing unpredictable behavior across systems. We introduce Text2Mem, a
unified memory operation language that provides a standardized pathway from
natural language to reliable execution. Text2Mem defines a compact yet
expressive operation set aligned with encoding, storage, and retrieval. Each
instruction is represented as a JSON based schema instance with required fields
and semantic invariants, which a parser transforms into typed operation objects
with normalized parameters. A validator ensures correctness before execution,
while adapters map typed objects either to a SQL prototype backend or to real
memory frameworks. Model based services such as embeddings or summarization are
integrated when required. All results are returned through a unified execution
contract. This design ensures safety, determinism, and portability across
heterogeneous backends. We also outline Text2Mem Bench, a planned benchmark
that separates schema generation from backend execution to enable systematic
evaluation. Together, these components establish the first standardized
foundation for memory control in agents.

</details>


### [33] [Differentially-private text generation degrades output language quality](https://arxiv.org/abs/2509.11176)
*Erion Çano,Ivan Habernal*

Main category: cs.CL

TL;DR: 研究分析了在差分隐私约束下微调的大型语言模型对文本质量和实用性的影响，发现更强的隐私保护会导致文本更短、语法正确性下降、词汇多样性减少，以及下游分类任务准确率降低。


<details>
  <summary>Details</summary>
Motivation: 虽然使用差分隐私微调LLMs来保护用户隐私变得流行，但DP微调对LLM生成文本的语言质量和实用性的影响尚未得到充分研究。

Method: 对五个大型语言模型在三个语料库上使用四种隐私级别进行微调，评估生成文本的长度、语法正确性、词汇多样性，并测试在下游分类任务（如书籍类型识别和死因识别）中的实用性。

Result: 更强的隐私约束导致文本长度减少至少77%，语法正确性下降至少9%，二元语法多样性减少至少10%，下游分类任务的准确率也下降。

Conclusion: 差分隐私微调虽然保护隐私，但会显著降低生成文本的质量和实用性，这可能影响合成数据的实际使用价值。

Abstract: Ensuring user privacy by synthesizing data from large language models (LLMs)
tuned under differential privacy (DP) has become popular recently. However, the
impact of DP fine-tuned LLMs on the quality of the language and the utility of
the texts they produce has not been investigated. In this work, we tune five
LLMs with three corpora under four levels of privacy and assess the length, the
grammatical correctness, and the lexical diversity of the text outputs they
produce. We also probe the utility of the synthetic outputs in downstream
classification tasks such as book genre recognition based on book descriptions
and cause of death recognition based on verbal autopsies. The results indicate
that LLMs tuned under stronger privacy constrains produce texts that are
shorter by at least 77 %, that are less grammatically correct by at least 9 %,
and are less diverse by at least 10 % in bi-gram diversity. Furthermore, the
accuracy they reach in downstream classification tasks decreases, which might
be detrimental to the usefulness of the generated synthetic data.

</details>


### [34] [Optimal Brain Restoration for Joint Quantization and Sparsification of LLMs](https://arxiv.org/abs/2509.11177)
*Hang Guo,Yawei Li,Luca Benini*

Main category: cs.CL

TL;DR: OBR是一个无需训练的二阶优化框架，通过误差补偿机制联合量化和稀疏化，在W4A4KV4量化和50%稀疏度下实现4.72倍加速和6.4倍内存减少


<details>
  <summary>Details</summary>
Motivation: 随着单一压缩方法逐渐接近极限，需要探索联合量化与稀疏化的新方案，但两者在权重分布要求上存在冲突：量化偏好紧凑范围，而剪枝需要高方差

Method: 提出Optimal Brain Restoration (OBR)框架，基于二阶Hessian目标，通过代理近似和组误差补偿得到闭式解，无需训练即可对齐剪枝和量化

Result: 在现有LLM上实现激进的W4A4KV4量化和50%稀疏度，相比FP16密集基线获得4.72倍加速和6.4倍内存减少

Conclusion: OBR提供了一种有效的联合压缩方案，解决了量化与稀疏化的冲突问题，为LLM压缩开辟了新方向

Abstract: Recent advances in Large Language Model (LLM) compression, such as
quantization and pruning, have achieved notable success. However, as these
techniques gradually approach their respective limits, relying on a single
method for further compression has become increasingly challenging. In this
work, we explore an alternative solution by combining quantization and
sparsity. This joint approach, though promising, introduces new difficulties
due to the inherently conflicting requirements on weight distributions:
quantization favors compact ranges, while pruning benefits from high variance.
To attack this problem, we propose Optimal Brain Restoration (OBR), a general
and training-free framework that aligns pruning and quantization by error
compensation between both. OBR minimizes performance degradation on downstream
tasks by building on a second-order Hessian objective, which is then
reformulated into a tractable problem through surrogate approximation and
ultimately reaches a closed-form solution via group error compensation.
Experiments show that OBR enables aggressive W4A4KV4 quantization with 50%
sparsity on existing LLMs, and delivers up to 4.72x speedup and 6.4x memory
reduction compared to the FP16-dense baseline.

</details>


### [35] [The Prompt Engineering Report Distilled: Quick Start Guide for Life Sciences](https://arxiv.org/abs/2509.11295)
*Valentin Romanov,Steven A Niederer*

Main category: cs.CL

TL;DR: 该论文将58种提示工程技术提炼为6种核心方法（零样本、少样本、思维生成、集成、自我批评和分解），为生命科学领域提供实用的提示工程指南，旨在提高研究效率和质量。


<details>
  <summary>Details</summary>
Motivation: 解决研究人员在使用大型语言模型时面临的高认知投入问题，通过系统化的提示工程技术简化生命科学工作流程，实现远超初始学习时间投入的效率提升。

Method: 分析2025年提示报告中58种文本提示工程技术，提炼出6种核心方法，结合生命科学具体用例（文献总结、数据提取、编辑任务等）进行详细说明，提供结构化建议和常见陷阱分析。

Result: 开发了一套针对生命科学领域的实用提示工程框架，包括具体的提示结构建议、避免常见错误的方法，以及跨平台工具（OpenAI、Google、Anthropic、Perplexity）的效果分析。

Conclusion: 提示工程应作为现有数据处理和文档编辑实践的补充而非替代，通过系统化的提示方法可以实现从机会性提示到高效、低摩擦的系统实践转变，从而提升研究质量。

Abstract: Developing effective prompts demands significant cognitive investment to
generate reliable, high-quality responses from Large Language Models (LLMs). By
deploying case-specific prompt engineering techniques that streamline
frequently performed life sciences workflows, researchers could achieve
substantial efficiency gains that far exceed the initial time investment
required to master these techniques. The Prompt Report published in 2025
outlined 58 different text-based prompt engineering techniques, highlighting
the numerous ways prompts could be constructed. To provide actionable
guidelines and reduce the friction of navigating these various approaches, we
distil this report to focus on 6 core techniques: zero-shot, few-shot
approaches, thought generation, ensembling, self-criticism, and decomposition.
We breakdown the significance of each approach and ground it in use cases
relevant to life sciences, from literature summarization and data extraction to
editorial tasks. We provide detailed recommendations for how prompts should and
shouldn't be structured, addressing common pitfalls including multi-turn
conversation degradation, hallucinations, and distinctions between reasoning
and non-reasoning models. We examine context window limitations, agentic tools
like Claude Code, while analyzing the effectiveness of Deep Research tools
across OpenAI, Google, Anthropic and Perplexity platforms, discussing current
limitations. We demonstrate how prompt engineering can augment rather than
replace existing established individual practices around data processing and
document editing. Our aim is to provide actionable guidance on core prompt
engineering principles, and to facilitate the transition from opportunistic
prompting to an effective, low-friction systematic practice that contributes to
higher quality research.

</details>


### [36] [Ko-PIQA: A Korean Physical Commonsense Reasoning Dataset with Cultural Context](https://arxiv.org/abs/2509.11303)
*Dasol Choi,Jungwhan Kim,Guijin Son*

Main category: cs.CL

TL;DR: Ko-PIQA是一个韩语物理常识推理数据集，包含文化特定元素，填补了英语主导数据集的多样性空白，通过多阶段过滤和人工验证构建了441个高质量问答对。


<details>
  <summary>Details</summary>
Motivation: 现有的物理常识推理数据集（如PIQA）主要基于英语，缺乏文化多样性。为了促进多语言和文化包容性的常识推理研究，需要构建包含文化背景的韩语数据集。

Method: 从301万个网络爬取问题开始，采用多阶段过滤方法，使用三个语言模型识别11,553个PIQA风格问题，通过GPT-4o精炼和人工验证，最终获得441个高质量问答对。

Result: 评估了七个语言模型，最佳模型准确率达到83.22%，最差模型仅为59.86%。模型在文化特定场景（如泡菜、韩服、泡菜冰箱等）上表现尤其困难。

Conclusion: Ko-PIQA既可作为韩语语言模型的基准测试，也为更包容的常识推理研究奠定了基础，突显了文化多样性数据集的重要性。

Abstract: Physical commonsense reasoning datasets like PIQA are predominantly
English-centric and lack cultural diversity. We introduce Ko-PIQA, a Korean
physical commonsense reasoning dataset that incorporates cultural context.
Starting from 3.01 million web-crawled questions, we employed a multi-stage
filtering approach using three language models to identify 11,553 PIQA-style
questions. Through GPT-4o refinement and human validation, we obtained 441
high-quality question-answer pairs. A key feature of Ko-PIQA is its cultural
grounding: 19.7\% of questions contain culturally specific elements like
traditional Korean foods (kimchi), clothing (hanbok), and specialized
appliances (kimchi refrigerators) that require culturally-aware reasoning
beyond direct translation. We evaluate seven language models on Ko-PIQA, with
the best model achieving 83.22\% accuracy while the weakest reaches only
59.86\%, demonstrating significant room for improvement. Models particularly
struggle with culturally specific scenarios, highlighting the importance of
culturally diverse datasets. Ko-PIQA serves as both a benchmark for Korean
language models and a foundation for more inclusive commonsense reasoning
research. The dataset and code will be publicly available.

</details>


### [37] [!MSA at AraHealthQA 2025 Shared Task: Enhancing LLM Performance for Arabic Clinical Question Answering through Prompt Engineering and Ensemble Learning](https://arxiv.org/abs/2509.11365)
*Mohamed Tarek,Seif Ahmed,Mohamed Basem*

Main category: cs.CL

TL;DR: 本文介绍了在AraHealthQA-2025共享任务Track 2中取得第二名的方法，使用Gemini 2.5 Flash模型，通过few-shot提示、数据预处理和集成策略在多项选择题和开放式问答任务中表现优异


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯语临床健康问答的挑战，提升在多种医疗问答场景下的准确性和响应质量

Method: 对于子任务1（选择题）：使用Gemini 2.5 Flash模型，采用few-shot提示、数据集预处理和三种提示配置的集成方法；对于子任务2（开放式问答）：使用统一提示，结合角色扮演（阿拉伯医学专家）、few-shot示例和后处理技术

Result: 在两个子任务中都获得了第二名：多项选择题问答和开放式问答任务

Conclusion: 提出的基于Gemini 2.5 Flash的方法在阿拉伯语临床健康问答中表现优异，few-shot提示和集成策略有效提升了模型性能

Abstract: We present our systems for Track 2 (General Arabic Health QA, MedArabiQ) of
the AraHealthQA-2025 shared task, where our methodology secured 2nd place in
both Sub-Task 1 (multiple-choice question answering) and Sub-Task 2 (open-ended
question answering) in Arabic clinical contexts. For Sub-Task 1, we leverage
the Gemini 2.5 Flash model with few-shot prompting, dataset preprocessing, and
an ensemble of three prompt configurations to improve classification accuracy
on standard, biased, and fill-in-the-blank questions. For Sub-Task 2, we employ
a unified prompt with the same model, incorporating role-playing as an Arabic
medical expert, few-shot examples, and post-processing to generate concise
responses across fill-in-the-blank, patient-doctor Q&A, GEC, and paraphrased
variants.

</details>


### [38] [Transformer Enhanced Relation Classification: A Comparative Analysis of Contextuality, Data Efficiency and Sequence Complexity](https://arxiv.org/abs/2509.11374)
*Bowen Jing,Yang Cui,Tianpeng Huang*

Main category: cs.CL

TL;DR: 本文系统比较了基于Transformer和非Transformer的深度学习模型在关系抽取任务上的性能，发现Transformer模型显著优于传统模型


<details>
  <summary>Details</summary>
Motivation: 在大语言模型时代，关系抽取作为信息抽取的重要环节，需要系统评估不同深度学习方法的性能差异，特别是比较Transformer架构与传统架构的效果

Method: 使用PA-LSTM、C-GCN、AGGCN等非Transformer架构，以及BERT、RoBERTa、R-BERT等Transformer架构，在TACRED、TACREV、RE-TACRED数据集上进行实验，评估不同句子长度和训练数据比例下的性能

Result: Transformer模型在micro F1指标上达到80-90%，显著优于非Transformer模型的64-67%

Conclusion: Transformer架构在关系抽取任务上表现出显著优势，同时论文还回顾了监督关系分类的研究历程，并讨论了大语言模型在关系抽取中的作用和现状

Abstract: In the era of large language model, relation extraction (RE) plays an
important role in information extraction through the transformation of
unstructured raw text into structured data (Wadhwa et al., 2023). In this
paper, we systematically compare the performance of deep supervised learning
approaches without transformers and those with transformers. We used a series
of non-transformer architectures such as PA-LSTM(Zhang et al., 2017),
C-GCN(Zhang et al., 2018), and AGGCN(attention guide GCN)(Guo et al., 2019),
and a series of transformer architectures such as BERT, RoBERTa, and R-BERT(Wu
and He, 2019). Our comparison included traditional metrics like micro F1, as
well as evaluations in different scenarios, varying sentence lengths, and
different percentages of the dataset for training. Our experiments were
conducted on TACRED, TACREV, and RE-TACRED. The results show that
transformer-based models outperform non-transformer models, achieving micro F1
scores of 80-90% compared to 64-67% for non-transformer models. Additionally,
we briefly review the research journey in supervised relation classification
and discuss the role and current status of large language models (LLMs) in
relation extraction.

</details>


### [39] [Continually Adding New Languages to Multilingual Language Models](https://arxiv.org/abs/2509.11414)
*Abraham Toluwase Owodunni,Sachin Kumar*

Main category: cs.CL

TL;DR: 提出了Layer-Selective LoRA (LayRA)方法，通过选择性冻结和适配层来解决多语言模型添加新语言时的灾难性遗忘问题，无需原始预训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有多语言模型需要重新训练才能支持新语言，成本高昂且原始预训练数据通常不可得。继续预训练会导致灾难性遗忘，而经验回放等方法因缺乏原始数据而无法应用。

Method: 提出Layer-Selective LoRA (LayRA)方法，在初始层和最终层添加低秩适配器(LoRA)，同时冻结中间层。基于多语言模型在初始层编码源语言、中间层用英语推理、最终层翻译回源语言的特点。

Result: 在添加Galician、Swahili和Urdu语言的实验中，LayRA在保持原有语言能力的同时，在新语言学习上与传统LoRA方法相当，提供了最佳的性能平衡。通过模型算术，还能为目标语言赋予指令跟随能力。

Conclusion: LayRA方法有效解决了多语言模型持续扩展时的灾难性遗忘问题，无需原始预训练数据，为新语言添加提供了实用且高效的解决方案。

Abstract: Multilingual language models are trained on a fixed set of languages, and to
support new languages, the models need to be retrained from scratch. This is an
expensive endeavor and is often infeasible, as model developers tend not to
release their pre-training data. Naive approaches, such as continued
pretraining, suffer from catastrophic forgetting; however, mitigation
strategies like experience replay cannot be applied due to the lack of original
pretraining data. In this work, we investigate the problem of continually
adding new languages to a multilingual model, assuming access to pretraining
data in only the target languages. We explore multiple approaches to address
this problem and propose Layer-Selective LoRA (LayRA), which adds Low-Rank
Adapters (LoRA) to selected initial and final layers while keeping the rest of
the model frozen. LayRA builds on two insights: (1) LoRA reduces forgetting,
and (2) multilingual models encode inputs in the source language in the initial
layers, reason in English in intermediate layers, and translate back to the
source language in final layers. We experiment with adding multiple
combinations of Galician, Swahili, and Urdu to pretrained language models and
evaluate each method on diverse multilingual tasks. We find that LayRA provides
the overall best tradeoff between preserving models' capabilities in previously
supported languages, while being competitive with existing approaches such as
LoRA in learning new languages. We also demonstrate that using model
arithmetic, the adapted models can be equipped with strong instruction
following abilities without access to any instruction tuning data in the target
languages.

</details>


### [40] [A Transformer-Based Cross-Platform Analysis of Public Discourse on the 15-Minute City Paradigm](https://arxiv.org/abs/2509.11443)
*Gaurab Chhetri,Darrell Anderson,Boniphace Kutela,Subasish Das*

Main category: cs.CL

TL;DR: 本研究首次对15分钟城市概念在Twitter、Reddit和新闻媒体上的公众意见进行多平台情感分析，使用压缩Transformer模型和Llama-3-8B进行标注，发现压缩模型表现竞争力强，挑战了大模型必要的假设。


<details>
  <summary>Details</summary>
Motivation: 研究15分钟城市概念在不同社交媒体平台上的公众情感倾向，探索压缩模型在多平台情感分析中的有效性，为城市规划讨论提供可扩展的情感分类方案。

Method: 使用压缩Transformer模型（DistilRoBERTa、DistilBERT、MiniLM、ELECTRA、TinyBERT）和Llama-3-8B进行标注，采用分层5折交叉验证，处理长短文本，支持一致性标注和可重复评估。

Result: DistilRoBERTa获得最高F1分数（0.8292），TinyBERT效率最佳，MiniLM跨平台一致性最好。新闻数据因类别不平衡导致性能虚高，Reddit存在摘要损失，Twitter提供中等挑战。

Conclusion: 压缩模型在多平台情感分析中表现竞争力强，挑战了大模型必要的传统观念，识别了平台特定的权衡，为城市规划讨论中的可扩展实时情感分类提供了方向。

Abstract: This study presents the first multi-platform sentiment analysis of public
opinion on the 15-minute city concept across Twitter, Reddit, and news media.
Using compressed transformer models and Llama-3-8B for annotation, we classify
sentiment across heterogeneous text domains. Our pipeline handles long-form and
short-form text, supports consistent annotation, and enables reproducible
evaluation. We benchmark five models (DistilRoBERTa, DistilBERT, MiniLM,
ELECTRA, TinyBERT) using stratified 5-fold cross-validation, reporting
F1-score, AUC, and training time. DistilRoBERTa achieved the highest F1
(0.8292), TinyBERT the best efficiency, and MiniLM the best cross-platform
consistency. Results show News data yields inflated performance due to class
imbalance, Reddit suffers from summarization loss, and Twitter offers moderate
challenge. Compressed models perform competitively, challenging assumptions
that larger models are necessary. We identify platform-specific trade-offs and
propose directions for scalable, real-world sentiment classification in urban
planning discourse.

</details>


### [41] [CognitiveSky: Scalable Sentiment and Narrative Analysis for Decentralized Social Media](https://arxiv.org/abs/2509.11444)
*Gaurab Chhetri,Anandi Dutta,Subasish Das*

Main category: cs.CL

TL;DR: CognitiveSky是一个开源、可扩展的框架，用于在去中心化社交媒体Bluesky上进行情感、情绪和叙事分析，使用基于transformer的模型处理大规模用户生成内容，并通过动态仪表板可视化分析结果。


<details>
  <summary>Details</summary>
Motivation: 随着去中心化社交媒体平台的出现，需要新的工具来实时分析公共话语。Bluesky作为Twitter/X的联邦替代方案，为计算社会科学提供了新的机会和挑战。

Method: 通过Bluesky API获取数据，应用基于transformer的模型对大规模用户生成内容进行标注，生成结构化可分析输出，并构建动态仪表板可视化情感、活动和话题模式。

Result: 构建了一个完全基于免费层基础设施的框架，实现了低运营成本和高可访问性，能够有效监控心理健康讨论等领域的公共话语。

Conclusion: CognitiveSky通过将大语言模型与去中心化网络结合，为数字生态系统转型时代的计算社会科学提供了一个透明、可扩展的工具，适用于虚假信息检测、危机响应和公民情感分析等多个领域。

Abstract: The emergence of decentralized social media platforms presents new
opportunities and challenges for real-time analysis of public discourse. This
study introduces CognitiveSky, an open-source and scalable framework designed
for sentiment, emotion, and narrative analysis on Bluesky, a federated Twitter
or X.com alternative. By ingesting data through Bluesky's Application
Programming Interface (API), CognitiveSky applies transformer-based models to
annotate large-scale user-generated content and produces structured and
analyzable outputs. These summaries drive a dynamic dashboard that visualizes
evolving patterns in emotion, activity, and conversation topics. Built entirely
on free-tier infrastructure, CognitiveSky achieves both low operational cost
and high accessibility. While demonstrated here for monitoring mental health
discourse, its modular design enables applications across domains such as
disinformation detection, crisis response, and civic sentiment analysis. By
bridging large language models with decentralized networks, CognitiveSky offers
a transparent, extensible tool for computational social science in an era of
shifting digital ecosystems.

</details>


### [42] [CEMTM: Contextual Embedding-based Multimodal Topic Modeling](https://arxiv.org/abs/2509.11465)
*Amirhossein Abaskohi,Raymond Li,Chuyuan Li,Shafiq Joty,Giuseppe Carenini*

Main category: cs.CL

TL;DR: CEMTM是一种上下文增强的多模态主题模型，能够从包含文本和图像的短文档和长文档中推断出连贯且可解释的主题结构，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态主题模型在处理包含多个图像的文档时存在重复编码问题，且难以保持跨模态的语义一致性，需要一种能够有效处理多图像文档并保持可解释性的新方法。

Method: 基于微调的大型视觉语言模型获取上下文嵌入，采用分布注意力机制加权标记级贡献，通过重建目标将基于主题的表示与文档嵌入对齐，确保跨模态语义一致性。

Result: 在六个多模态基准测试中 consistently 优于单模态和多模态基线，平均LLM得分达到2.61，在下游少样本检索任务中表现有效，能够捕捉复杂领域（如科学文章）中的视觉基础语义。

Conclusion: CEMTM通过上下文增强和分布注意力机制，成功解决了多图像文档处理问题，实现了跨模态语义一致性，为多模态主题建模提供了有效的解决方案。

Abstract: We introduce CEMTM, a context-enhanced multimodal topic model designed to
infer coherent and interpretable topic structures from both short and long
documents containing text and images. CEMTM builds on fine-tuned large vision
language models (LVLMs) to obtain contextualized embeddings, and employs a
distributional attention mechanism to weight token-level contributions to topic
inference. A reconstruction objective aligns topic-based representations with
the document embedding, encouraging semantic consistency across modalities.
Unlike existing approaches, CEMTM can process multiple images per document
without repeated encoding and maintains interpretability through explicit
word-topic and document-topic distributions. Extensive experiments on six
multimodal benchmarks show that CEMTM consistently outperforms unimodal and
multimodal baselines, achieving a remarkable average LLM score of 2.61. Further
analysis shows its effectiveness in downstream few-shot retrieval and its
ability to capture visually grounded semantics in complex domains such as
scientific articles.

</details>


### [43] [Improving LLMs' Learning for Coreference Resolution](https://arxiv.org/abs/2509.11466)
*Yujian Gan,Yuan Liang,Yanni Lin,Juntao Yu,Massimo Poesio*

Main category: cs.CL

TL;DR: 本文针对大型语言模型在指代消解任务中的幻觉和性能不足问题，提出了反向训练联合推理和迭代文档生成两种新方法，有效提升了基于LLM的指代消解性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大型语言模型的指代消解方法（如问答模板和文档模板方法）存在幻觉问题和性能不足，需要改进这些方法的有效性。

Method: 提出了两种新技术：1）反向训练联合推理 - 改进问答模板方法；2）迭代文档生成 - 消除生成源文本中的幻觉并提升指代消解性能。

Result: 实验表明反向训练提升了问答模板方法的性能，迭代文档生成有效消除了幻觉并增强了指代消解能力。

Conclusion: 整合这些方法和技术为基于LLM的指代消解提供了有效且鲁棒的解决方案。

Abstract: Coreference Resolution (CR) is crucial for many NLP tasks, but existing LLMs
struggle with hallucination and under-performance. In this paper, we
investigate the limitations of existing LLM-based approaches to CR-specifically
the Question-Answering (QA) Template and Document Template methods and propose
two novel techniques: Reversed Training with Joint Inference and Iterative
Document Generation. Our experiments show that Reversed Training improves the
QA Template method, while Iterative Document Generation eliminates
hallucinations in the generated source text and boosts coreference resolution.
Integrating these methods and techniques offers an effective and robust
solution to LLM-based coreference resolution.

</details>


### [44] [ClaimIQ at CheckThat! 2025: Comparing Prompted and Fine-Tuned Language Models for Verifying Numerical Claims](https://arxiv.org/abs/2509.11492)
*Anirban Saha Anik,Md Fahimul Kabir Chowdhury,Andrew Wyckoff,Sagnik Ray Choudhury*

Main category: cs.CL

TL;DR: 本文介绍了CLEF 2025 CheckThat! Lab任务3的系统，采用零样本提示和LoRA微调两种方法进行数值和时间声明验证，通过多种证据选择策略提升性能，但测试集表现下降显示泛化挑战。


<details>
  <summary>Details</summary>
Motivation: 解决数值和时间声明的自动验证问题，探索如何通过检索证据和不同建模方法来提高事实核查的准确性和鲁棒性。

Method: 使用两种互补方法：1）基于指令调优大语言模型的零样本提示；2）使用参数高效的LoRA进行监督微调。同时研究多种证据选择策略，包括完整文档输入和基于BM25及MiniLM的top-k句子过滤。

Result: 最佳性能模型LLaMA通过LoRA微调在英文验证集上表现强劲，但在测试集上出现显著性能下降，表明存在泛化问题。

Conclusion: 证据粒度选择和模型适应对于构建鲁棒的数字事实验证系统至关重要，需要进一步解决泛化挑战。

Abstract: This paper presents our system for Task 3 of the CLEF 2025 CheckThat! Lab,
which focuses on verifying numerical and temporal claims using retrieved
evidence. We explore two complementary approaches: zero-shot prompting with
instruction-tuned large language models (LLMs) and supervised fine-tuning using
parameter-efficient LoRA. To enhance evidence quality, we investigate several
selection strategies, including full-document input and top-k sentence
filtering using BM25 and MiniLM. Our best-performing model LLaMA fine-tuned
with LoRA achieves strong performance on the English validation set. However, a
notable drop in the test set highlights a generalization challenge. These
findings underscore the importance of evidence granularity and model adaptation
for robust numerical fact verification.

</details>


### [45] [AKCIT-FN at CheckThat! 2025: Switching Fine-Tuned SLMs and LLM Prompting for Multilingual Claim Normalization](https://arxiv.org/abs/2509.11496)
*Fabrycio Leite Nakano Almada,Kauan Divino Pouso Mariano,Maykon Adriell Dutra,Victor Emanuel da Silva Monteiro,Juliana Resplande Sant'Anna Gomes,Arlindo Rodrigues Galvão Filho,Anderson da Silva Soares*

Main category: cs.CL

TL;DR: 本文介绍了在CLEF-2025 CheckThat! Task 2中提出的声明规范化方法，使用微调的小型语言模型处理有监督语言，使用大型语言模型提示处理零样本语言，在20种语言中的15种获得前三名。


<details>
  <summary>Details</summary>
Motivation: 声明规范化是将非正式的社交媒体帖子转化为简洁、自包含陈述的关键步骤，是自动化事实核查流程中的重要环节。该任务需要处理20种语言，包括13种有监督语言和7种零样本语言。

Method: 采用双管齐下的方法：对于有监督的高资源语言使用微调的小型语言模型(SLMs)，对于零样本语言使用大型语言模型(LLM)提示技术。所有实现代码和配置都公开可用。

Result: 在20种语言中的15种获得前三名，其中8种语言获得第二名（包括5种零样本语言）。葡萄牙语的平均METEOR得分为0.5290，排名第三。零样本策略特别有效。

Conclusion: 该方法证明了结合微调SLMs和LLM提示的有效性，特别是在零样本场景中表现出色，为多语言声明规范化任务提供了实用的解决方案。

Abstract: Claim normalization, the transformation of informal social media posts into
concise, self-contained statements, is a crucial step in automated
fact-checking pipelines. This paper details our submission to the CLEF-2025
CheckThat! Task~2, which challenges systems to perform claim normalization
across twenty languages, divided into thirteen supervised (high-resource) and
seven zero-shot (no training data) tracks.
  Our approach, leveraging fine-tuned Small Language Models (SLMs) for
supervised languages and Large Language Model (LLM) prompting for zero-shot
scenarios, achieved podium positions (top three) in fifteen of the twenty
languages. Notably, this included second-place rankings in eight languages,
five of which were among the seven designated zero-shot languages, underscoring
the effectiveness of our LLM-based zero-shot strategy. For Portuguese, our
initial development language, our system achieved an average METEOR score of
0.5290, ranking third. All implementation artifacts, including inference,
training, evaluation scripts, and prompt configurations, are publicly available
at https://github.com/ju-resplande/checkthat2025_normalization.

</details>


### [46] [DeDisCo at the DISRPT 2025 Shared Task: A System for Discourse Relation Classification](https://arxiv.org/abs/2509.11498)
*Zhuoxuan Ju,Jingni Wu,Abhishek Purushothama,Amir Zeldes*

Main category: cs.CL

TL;DR: DeDisCo系统在DISRPT 2025语篇关系分类任务中测试了mt5编码器和Qwen解码器两种方法，使用数据增强和语言学特征，获得71.28的宏观准确率


<details>
  <summary>Details</summary>
Motivation: 参与DISRPT 2025共享任务，探索不同神经网络架构在语篇关系分类中的效果，并解决低资源语言的数据稀缺问题

Method: 使用基于mt5的编码器方法和基于Qwen模型的解码器方法，通过从英语自动翻译的数据进行数据增强，并加入额外的语言学特征

Result: 系统取得了71.28的宏观准确率分数，并提供了结果解释和错误分析

Conclusion: DeDisCo系统在语篇关系分类任务中表现良好，数据增强和语言学特征对提升低资源语言性能有积极作用

Abstract: This paper presents DeDisCo, Georgetown University's entry in the DISRPT 2025
shared task on discourse relation classification. We test two approaches, using
an mt5-based encoder and a decoder based approach using the openly available
Qwen model. We also experiment on training with augmented dataset for
low-resource languages using matched data translated automatically from
English, as well as using some additional linguistic features inspired by
entries in previous editions of the Shared Task. Our system achieves a
macro-accuracy score of 71.28, and we provide some interpretation and error
analysis for our results.

</details>


### [47] [Unsupervised Candidate Ranking for Lexical Substitution via Holistic Sentence Semantics](https://arxiv.org/abs/2509.11513)
*Zhongyang Hu,Naijie Gu,Xiangzhi Tao,Tianhui Gu,Yibing Zhou*

Main category: cs.CL

TL;DR: 本文提出两种基于注意力权重和积分梯度的方法来改进词汇替换任务中的候选词排序，通过建模目标词与上下文之间的双向影响来更准确表征语义变化。


<details>
  <summary>Details</summary>
Motivation: 现有词汇替换方法主要关注目标位置的语义变化或依赖多指标参数调优，难以准确建模候选替换对目标词和上下文的双向影响。

Method: 提出两种方法：基于注意力权重的方法和基于积分梯度的方法，用于衡量上下文词对目标词的影响，并结合原始句与替换句的语义相似性来排序候选词。

Result: 在LS07和SWORDS数据集上的实验表明，两种方法都提高了排序性能。

Conclusion: 基于注意力权重和积分梯度的方法能有效改进词汇替换中的候选词排序，通过建模双向影响来更好地表征语义变化。

Abstract: A key subtask in lexical substitution is ranking the given candidate words. A
common approach is to replace the target word with a candidate in the original
sentence and feed the modified sentence into a model to capture semantic
differences before and after substitution. However, effectively modeling the
bidirectional influence of candidate substitution on both the target word and
its context remains challenging. Existing methods often focus solely on
semantic changes at the target position or rely on parameter tuning over
multiple evaluation metrics, making it difficult to accurately characterize
semantic variation. To address this, we investigate two approaches: one based
on attention weights and another leveraging the more interpretable integrated
gradients method, both designed to measure the influence of context tokens on
the target token and to rank candidates by incorporating semantic similarity
between the original and substituted sentences. Experiments on the LS07 and
SWORDS datasets demonstrate that both approaches improve ranking performance.

</details>


### [48] [LVLMs are Bad at Overhearing Human Referential Communication](https://arxiv.org/abs/2509.11514)
*Zhengxiang Wang,Weiling Li,Panagiotis Kaliosis,Owen Rambow,Susan E. Brennan*

Main category: cs.CL

TL;DR: 研究评估了7种最先进的大型视觉语言模型作为旁听者理解人类自发对话中新颖指代表达的能力，发现当前模型在协作对象匹配任务中表现不佳，且无法随着对话轮次增加而持续改进。


<details>
  <summary>Details</summary>
Motivation: 理解自发对话中的指代表达对于具身智能体在现实世界中执行任务至关重要，需要整合语言、视觉和对话交互的理解能力。

Method: 使用7种最先进的大型视觉语言模型作为旁听者，分析人类在协作对象匹配任务中的自发对话语料库，评估模型理解指代表达的能力。

Result: 当前LVLMs在此任务上表现具有挑战性，所有模型都无法随着对话轮次增加而显示出持续的性能改进。

Conclusion: 该研究揭示了当前视觉语言模型在理解人类自发对话中指代表达方面的局限性，并发布了语料库和代码以促进未来研究。

Abstract: During spontaneous conversations, speakers collaborate on novel referring
expressions, which they can then re-use in subsequent conversations.
Understanding such referring expressions is an important ability for an
embodied agent, so that it can carry out tasks in the real world. This requires
integrating and understanding language, vision, and conversational interaction.
We study the capabilities of seven state-of-the-art Large Vision Language
Models (LVLMs) as overhearers to a corpus of spontaneous conversations between
pairs of human discourse participants engaged in a collaborative
object-matching task. We find that such a task remains challenging for current
LVLMs and they all fail to show a consistent performance improvement as they
overhear more conversations from the same discourse participants repeating the
same task for multiple rounds. We release our corpus and code for
reproducibility and to facilitate future research.

</details>


### [49] [PeruMedQA: Benchmarking Large Language Models (LLMs) on Peruvian Medical Exams -- Dataset Construction and Evaluation](https://arxiv.org/abs/2509.11517)
*Rodrigo M. Carrillo-Larco,Jesus Lovón Melgarejo,Manuel Castillo-Cara,Gusseppe Bravo-Rocca*

Main category: cs.CL

TL;DR: 该研究构建了秘鲁医学考试问答数据集PeruMedQA，评估了多个医学大语言模型在西班牙语医学问题上的表现，发现medgemma-27b-text-it表现最佳，微调后的medgemma-4b-it也能媲美更大参数模型。


<details>
  <summary>Details</summary>
Motivation: 探索医学大语言模型在西班牙语和拉丁美洲国家医学问题上的表现，为拉丁美洲地区的医学AI应用提供指导。

Method: 构建包含8,380个问题的PeruMedQA数据集，选择8个医学LLM进行零样本测试，使用PEFT和LoRA技术微调medgemma-4b-it模型。

Result: medgemma-27b-text-it表现最佳，正确率超过90%；参数量小于100亿的模型正确率低于60%；微调后的medgemma-4b-it能媲美700亿参数模型。

Conclusion: 对于需要西班牙语国家医学知识的AI应用，推荐使用medgemma-27b-text-it或微调版的medgemma-4b-it。

Abstract: BACKGROUND: Medical large language models (LLMS) have demonstrated remarkable
performance in answering medical examinations. However, the extent to which
this high performance is transferable to medical questions in Spanish and from
a Latin American country remains unexplored. This knowledge is crucial as
LLM-based medical applications gain traction in Latin America. AIMS: to build a
dataset of questions from medical examinations taken by Peruvian physicians
pursuing specialty training; to fine-tune a LLM on this dataset; to evaluate
and compare the performance in terms of accuracy between vanilla LLMs and the
fine-tuned LLM. METHODS: We curated PeruMedQA, a multiple-choice
question-answering (MCQA) datasets containing 8,380 questions spanning 12
medical domains (2018-2025). We selected eight medical LLMs including
medgemma-4b-it and medgemma-27b-text-it, and developed zero-shot task-specific
prompts to answer the questions appropriately. We employed parameter-efficient
fine tuning (PEFT)and low-rant adaptation (LoRA) to fine-tune medgemma-4b-it
utilizing all questions except those from 2025 (test set). RESULTS:
medgemma-27b-text-it outperformed all other models, achieving a proportion of
correct answers exceeding 90% in several instances. LLMs with <10 billion
parameters exhibited <60% of correct answers, while some exams yielded results
<50%. The fine-tuned version of medgemma-4b-it emerged victorious agains all
LLMs with <10 billion parameters and rivaled a LLM with 70 billion parameters
across various examinations. CONCLUSIONS: For medical AI application and
research that require knowledge bases from Spanish-speaking countries and those
exhibiting similar epidemiological profiles to Peru's, interested parties
should utilize medgemma-27b-text-it or a fine-tuned version of medgemma-4b-it.

</details>


### [50] [On the Distinctive Co-occurrence Characteristics of Antonymy](https://arxiv.org/abs/2509.11534)
*Zhihan Cao,Hiroaki Yamada,Takenobu Tokunaga*

Main category: cs.CL

TL;DR: 本研究通过比较反义词与其他语义关系在词性间的共现模式，发现反义词具有高共现强度、偏好线性顺序和短距离共现三个显著特征


<details>
  <summary>Details</summary>
Motivation: 反义词在文本中频繁共现已被广泛观察，但缺乏与其他语义关系的系统比较，无法确定这种共现模式是否反义词特有

Method: 使用稳健的共现度量方法，比较反义词与三种其他语义关系在不同词性间的共现模式

Result: 反义词在三个维度上具有独特性：高共现强度、偏好线性顺序、短距离共现

Conclusion: 反义词的共现模式确实具有独特性，这为词汇语义学和自然语言处理提供了重要见解

Abstract: Antonymy has long received particular attention in lexical semantics.
Previous studies have shown that antonym pairs frequently co-occur in text,
across genres and parts of speech, more often than would be expected by chance.
However, whether this co-occurrence pattern is distinctive of antonymy remains
unclear, due to a lack of comparison with other semantic relations. This work
fills the gap by comparing antonymy with three other relations across parts of
speech using robust co-occurrence metrics. We find that antonymy is distinctive
in three respects: antonym pairs co-occur with high strength, in a preferred
linear order, and within short spans. All results are available online.

</details>


### [51] [HARP: Hallucination Detection via Reasoning Subspace Projection](https://arxiv.org/abs/2509.11536)
*Junjie Hu,Gang Tu,ShengYu Cheng,Jinxin Li,Jinting Wang,Rui Chen,Zhilong Zhou,Dongbo Shan*

Main category: cs.CL

TL;DR: HARP是一个新的幻觉检测框架，通过将LLM隐藏状态空间分解为语义子空间和推理子空间，并利用推理子空间投影来检测幻觉，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法在分离语义和推理信息以及保持鲁棒性方面存在困难，需要更有效的检测框架来解决LLM在关键决策中的可靠性问题。

Method: 提出HARP框架：1）证明LLM隐藏状态空间可分解为语义子空间和推理子空间；2）通过SVD分解Unembedding层参数获得子空间基向量；3）将隐藏状态投影到推理子空间作为检测特征

Result: 在多个数据集上实现最先进的性能，TriviaQA上AUROC达到92.8%，比之前最佳方法提升7.5%，特征维度降至原始的5%，噪声过滤效果显著

Conclusion: HARP通过子空间分解和投影方法有效解决了幻觉检测中的语义-推理分离和鲁棒性问题，为LLM可靠性提供了重要技术支撑

Abstract: Hallucinations in Large Language Models (LLMs) pose a major barrier to their
reliable use in critical decision-making. Although existing hallucination
detection methods have improved accuracy, they still struggle with
disentangling semantic and reasoning information and maintaining robustness. To
address these challenges, we propose HARP (Hallucination detection via
reasoning subspace projection), a novel hallucination detection framework. HARP
establishes that the hidden state space of LLMs can be decomposed into a direct
sum of a semantic subspace and a reasoning subspace, where the former encodes
linguistic expression and the latter captures internal reasoning processes.
Moreover, we demonstrate that the Unembedding layer can disentangle these
subspaces, and by applying Singular Value Decomposition (SVD) to its
parameters, the basis vectors spanning the semantic and reasoning subspaces are
obtained. Finally, HARP projects hidden states onto the basis vectors of the
reasoning subspace, and the resulting projections are then used as input
features for hallucination detection in LLMs. By using these projections, HARP
reduces the dimension of the feature to approximately 5% of the original,
filters out most noise, and achieves enhanced robustness. Experiments across
multiple datasets show that HARP achieves state-of-the-art hallucination
detection performance; in particular, it achieves an AUROC of 92.8% on
TriviaQA, outperforming the previous best method by 7.5%.

</details>


### [52] [HiChunk: Evaluating and Enhancing Retrieval-Augmented Generation with Hierarchical Chunking](https://arxiv.org/abs/2509.11552)
*Wensheng Lu,Keyu Chen,Ruizhi Qiao,Xing Sun*

Main category: cs.CL

TL;DR: 本文提出了HiCBench评估基准和HiChunk文档分块框架，用于解决RAG系统中文档分块质量评估不足的问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG评估基准由于证据稀疏性问题，无法有效评估文档分块质量，需要开发专门的评估工具和改进的分块方法。

Method: 提出HiCBench评估基准（包含人工标注的多级分块点和合成的证据密集QA对）和HiChunk框架（基于微调LLM的多级文档结构化框架，结合Auto-Merge检索算法）。

Result: 实验表明HiCBench能有效评估不同分块方法在整个RAG流程中的影响，HiChunk在合理时间消耗下实现了更好的分块质量，提升了RAG系统整体性能。

Conclusion: HiCBench填补了RAG系统文档分块评估的空白，HiChunk框架通过改进分块质量显著提升了RAG系统的检索和生成效果。

Abstract: Retrieval-Augmented Generation (RAG) enhances the response capabilities of
language models by integrating external knowledge sources. However, document
chunking as an important part of RAG system often lacks effective evaluation
tools. This paper first analyzes why existing RAG evaluation benchmarks are
inadequate for assessing document chunking quality, specifically due to
evidence sparsity. Based on this conclusion, we propose HiCBench, which
includes manually annotated multi-level document chunking points, synthesized
evidence-dense quetion answer(QA) pairs, and their corresponding evidence
sources. Additionally, we introduce the HiChunk framework, a multi-level
document structuring framework based on fine-tuned LLMs, combined with the
Auto-Merge retrieval algorithm to improve retrieval quality. Experiments
demonstrate that HiCBench effectively evaluates the impact of different
chunking methods across the entire RAG pipeline. Moreover, HiChunk achieves
better chunking quality within reasonable time consumption, thereby enhancing
the overall performance of RAG systems.

</details>


### [53] [D$^2$HScore: Reasoning-Aware Hallucination Detection via Semantic Breadth and Depth Analysis in LLMs](https://arxiv.org/abs/2509.11569)
*Yue Ding,Xiaofang Zhu,Tianze Xia,Junfei Wu,Xinlong Chen,Qiang Liu,Liang Wang*

Main category: cs.CL

TL;DR: 提出D²HScore框架，通过分析LLM内部表示层的分散性和漂移性来检测幻觉，无需训练和标签，在多个基准测试中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 解决LLM生成非事实内容（幻觉）的问题，特别是在金融、安全和医疗等高风险领域确保输出可靠性

Method: 基于LLM的多层结构和自回归解码过程，提出D²HScore框架：1）层内分散性 - 量化每层token表示的语义多样性；2）层间漂移性 - 跟踪关键token表示在层间的渐进变换，使用注意力信号指导token选择

Result: 在5个开源LLM和5个广泛使用的基准测试上进行大量实验，D²HScore始终优于现有的无需训练基线方法

Conclusion: D²HScore通过捕捉推理过程中表示的水平和垂直动态，为幻觉检测提供了可解释且轻量级的代理方法

Abstract: Although large Language Models (LLMs) have achieved remarkable success, their
practical application is often hindered by the generation of non-factual
content, which is called "hallucination". Ensuring the reliability of LLMs'
outputs is a critical challenge, particularly in high-stakes domains such as
finance, security, and healthcare. In this work, we revisit hallucination
detection from the perspective of model architecture and generation dynamics.
Leveraging the multi-layer structure and autoregressive decoding process of
LLMs, we decompose hallucination signals into two complementary dimensions: the
semantic breadth of token representations within each layer, and the semantic
depth of core concepts as they evolve across layers. Based on this insight, we
propose \textbf{D$^2$HScore (Dispersion and Drift-based Hallucination Score)},
a training-free and label-free framework that jointly measures: (1)
\textbf{Intra-Layer Dispersion}, which quantifies the semantic diversity of
token representations within each layer; and (2) \textbf{Inter-Layer Drift},
which tracks the progressive transformation of key token representations across
layers. To ensure drift reflects the evolution of meaningful semantics rather
than noisy or redundant tokens, we guide token selection using attention
signals. By capturing both the horizontal and vertical dynamics of
representation during inference, D$^2$HScore provides an interpretable and
lightweight proxy for hallucination detection. Extensive experiments across
five open-source LLMs and five widely used benchmarks demonstrate that
D$^2$HScore consistently outperforms existing training-free baselines.

</details>


### [54] [Bhaasha, Bhasa, Zaban: A Survey for Low-Resourced Languages in South Asia -- Current Stage and Challenges](https://arxiv.org/abs/2509.11570)
*Sampoorna Poria,Xiaolei Huang*

Main category: cs.CL

TL;DR: 单一评估南亚语言大语言模型状况，揭示数据缺口、模型异象和评测标准问题，提出有针对性的解决方案


<details>
  <summary>Details</summary>
Motivation: 大语言模型在英语领域发展迅速，但南亚语言作为低资源语言被忽视，需要系统性评估当前模型状况和挑战

Method: 综合评估2020年以来的NLP研究，重点分析基于Transformer的模型（BERT、T5、GPT等），从数据、模型和任务三个维度进行调研

Result: 发现南亚语言存在严重数据缺口（特别是健康领域）、混码现象、缺乏标准化评测基准等问题，模型发展不平衡

Conclusion: 需要增强NLP社区对南亚语言的关注，推动有针对性的数据维护、统一的评测标准和更公平的语言表征

Abstract: Rapid developments of large language models have revolutionized many NLP
tasks for English data. Unfortunately, the models and their evaluations for
low-resource languages are being overlooked, especially for languages in South
Asia. Although there are more than 650 languages in South Asia, many of them
either have very limited computational resources or are missing from existing
language models. Thus, a concrete question to be answered is: Can we assess the
current stage and challenges to inform our NLP community and facilitate model
developments for South Asian languages? In this survey, we have comprehensively
examined current efforts and challenges of NLP models for South Asian languages
by retrieving studies since 2020, with a focus on transformer-based models,
such as BERT, T5, & GPT. We present advances and gaps across 3 essential
aspects: data, models, & tasks, such as available data sources, fine-tuning
strategies, & domain applications. Our findings highlight substantial issues,
including missing data in critical domains (e.g., health), code-mixing, and
lack of standardized evaluation benchmarks. Our survey aims to raise awareness
within the NLP community for more targeted data curation, unify benchmarks
tailored to cultural and linguistic nuances of South Asia, and encourage an
equitable representation of South Asian languages. The complete list of
resources is available at: https://github.com/trust-nlp/LM4SouthAsia-Survey.

</details>


### [55] [Analyzing Information-Seeking Behaviors in a Hakka AI Chatbot: A Cognitive-Pragmatic Study](https://arxiv.org/abs/2509.11591)
*Chu-Hsuan Lee,Chen-Chi Chang,Hung-Shin Lee,Yun-Hsiang Hsu,Ching-Yuan Chen*

Main category: cs.CL

TL;DR: 本研究通过分析7077条用户话语，探讨生成式AI聊天机器人TALKA如何支持客家话学习，发现AI能有效促进认知发展和文化认同。


<details>
  <summary>Details</summary>
Motivation: 随着濒危语言面临消失风险，需要结合技术手段和文化教学策略来保护语言。研究旨在了解生成式AI如何支持低资源语言学习者的认知发展和文化连接。

Method: 采用基于布鲁姆认知分类和对话行为分类的双层分析框架，对7077条用户话语进行标注分析，涵盖6个认知层次和11种对话行为类型。

Result: 生成式AI聊天机器人能有效支持语言学习，帮助学习者更自信地表达并与文化身份建立联系，特别是在理解用户思维和沟通方式的设计下。

Conclusion: AI介导的对话促进了低资源语言学习者的认知发展、语用协商和社会文化归属，为技术支持语言保护和教育实践提供了新见解。

Abstract: With many endangered languages at risk of disappearing, efforts to preserve
them now rely more than ever on using technology alongside culturally informed
teaching strategies. This study examines user behaviors in TALKA, a generative
AI-powered chatbot designed for Hakka language engagement, by employing a
dual-layered analytical framework grounded in Bloom's Taxonomy of cognitive
processes and dialogue act categorization. We analyzed 7,077 user utterances,
each carefully annotated according to six cognitive levels and eleven dialogue
act types. These included a variety of functions, such as asking for
information, requesting translations, making cultural inquiries, and using
language creatively. Pragmatic classifications further highlight how different
types of dialogue acts--such as feedback, control commands, and social
greetings--align with specific cognitive intentions. The results suggest that
generative AI chatbots can support language learning in meaningful
ways--especially when they are designed with an understanding of how users
think and communicate. They may also help learners express themselves more
confidently and connect with their cultural identity. The TALKA case provides
empirical insights into how AI-mediated dialogue facilitates cognitive
development in low-resource language learners, as well as pragmatic negotiation
and socio-cultural affiliation. By focusing on AI-assisted language learning,
this study offers new insights into how technology can support language
preservation and educational practice.

</details>


### [56] [Dynamic Span Interaction and Graph-Aware Memory for Entity-Level Sentiment Classification](https://arxiv.org/abs/2509.11604)
*Md. Mithun Hossain,Sanjara,Md. Shakil Hossain,Sudipto Chaki*

Main category: cs.CL

TL;DR: SpanEIT是一个新颖的实体级情感分类框架，通过动态跨度交互和图感知记忆机制增强实体-情感关系建模，在多个数据集上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 解决实体级情感分类中的关键挑战：实体与情感表达的复杂交互建模、跨句依赖关系捕捉、以及通过共指消确保同一实体多提及的情感一致性

Method: 构建基于跨度的实体和情感短语表示，使用双向注意力进行细粒度交互，图注意力网络捕捉句法和共现关系，共指感知记忆模块确保文档级一致性

Result: 在FSAD、BARU和IMDB数据集上，SpanEIT在准确率和F1分数上均优于最先进的transformer和混合基线方法

Conclusion: SpanEIT框架有效解决了实体级情感分析的复杂挑战，在细粒度情感分析应用中具有重要潜力，特别是在社交媒体监控和客户反馈分析领域

Abstract: Entity-level sentiment classification involves identifying the sentiment
polarity linked to specific entities within text. This task poses several
challenges: effectively modeling the subtle and complex interactions between
entities and their surrounding sentiment expressions; capturing dependencies
that may span across sentences; and ensuring consistent sentiment predictions
for multiple mentions of the same entity through coreference resolution.
Additionally, linguistic phenomena such as negation, ambiguity, and overlapping
opinions further complicate the analysis. These complexities make entity-level
sentiment classification a difficult problem, especially in real-world, noisy
textual data. To address these issues, we propose SpanEIT, a novel framework
integrating dynamic span interaction and graph-aware memory mechanisms for
enhanced entity-sentiment relational modeling. SpanEIT builds span-based
representations for entities and candidate sentiment phrases, employs
bidirectional attention for fine-grained interactions, and uses a graph
attention network to capture syntactic and co-occurrence relations. A
coreference-aware memory module ensures entity-level consistency across
documents. Experiments on FSAD, BARU, and IMDB datasets show SpanEIT
outperforms state-of-the-art transformer and hybrid baselines in accuracy and
F1 scores. Ablation and interpretability analyses validate the effectiveness of
our approach, underscoring its potential for fine-grained sentiment analysis in
applications like social media monitoring and customer feedback analysis.

</details>


### [57] [HalluDetect: Detecting, Mitigating, and Benchmarking Hallucinations in Conversational Systems](https://arxiv.org/abs/2509.11619)
*Spandan Anaokar,Shrey Ganatra,Harshvivek Kashid,Swapnil Bhattacharyya,Shruti Nair,Reshma Sekhar,Siddharth Manohar,Rahul Hemrajani,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 开发了HalluDetect幻觉检测系统，在LLaMA 3.1 8B模型上实现69% F1分数，比基线提升25.44%。AgentBot架构将幻觉降至每轮0.4159次，同时保持96.13%的token准确率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在工业应用中普遍存在幻觉问题，限制了其在关键应用中的可靠性，特别是在消费者投诉聊天机器人等高风险领域。

Method: 开发基于LLM的HalluDetect幻觉检测系统，并对五种聊天机器人架构进行基准测试，评估其幻觉减少效果。

Result: HalluDetect系统达到69%的F1分数，显著优于基线检测器。AgentBot架构在最小化幻觉（每轮0.4159次）的同时保持最高token准确率（96.13%）。

Conclusion: 优化的推理策略可以显著提高事实准确性，为幻觉缓解提供了可扩展框架，该方法可推广到其他高风险领域，增强对LLM驱动助手的信任。

Abstract: Large Language Models (LLMs) are widely used in industry but remain prone to
hallucinations, limiting their reliability in critical applications. This work
addresses hallucination reduction in consumer grievance chatbots built using
LLaMA 3.1 8B Instruct, a compact model frequently used in industry. We develop
HalluDetect, an LLM-based hallucination detection system that achieves an F1
score of 69% outperforming baseline detectors by 25.44%. Benchmarking five
chatbot architectures, we find that out of them, AgentBot minimizes
hallucinations to 0.4159 per turn while maintaining the highest token accuracy
(96.13%), making it the most effective mitigation strategy. Our findings
provide a scalable framework for hallucination mitigation, demonstrating that
optimized inference strategies can significantly improve factual accuracy.
While applied to consumer law, our approach generalizes to other high-risk
domains, enhancing trust in LLM-driven assistants. We will release the code and
dataset

</details>


### [58] [AesBiasBench: Evaluating Bias and Alignment in Multimodal Language Models for Personalized Image Aesthetic Assessment](https://arxiv.org/abs/2509.11620)
*Kun Li,Lai-Man Po,Hongzheng Yang,Xuyuan Xu,Kangcheng Liu,Yuzhi Zhao*

Main category: cs.CL

TL;DR: 提出了AesBiasBench基准来评估多模态大语言模型在图像美学评估中的刻板印象偏见和与人类偏好的对齐程度，发现小模型偏见更强，大模型更接近人类偏好，身份信息会加剧偏见。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在个性化图像美学评估中可能存在基于人口统计因素的微妙偏见，需要系统评估这些偏见及其与人类真实偏好的对齐情况。

Method: 构建AesBiasBench基准，包含三个子任务（美学感知、评估、共情），引入结构化指标（IFD、NRD、AAS）来评估偏见和对齐，测试了19个MLLM模型。

Result: 小模型表现出更强的刻板印象偏见，大模型与人类偏好更对齐；加入身份信息通常会加剧偏见，特别是在情感判断方面。

Conclusion: 在主观视觉语言任务中，身份感知的评估框架非常重要，需要关注模型偏见问题。

Abstract: Multimodal Large Language Models (MLLMs) are increasingly applied in
Personalized Image Aesthetic Assessment (PIAA) as a scalable alternative to
expert evaluations. However, their predictions may reflect subtle biases
influenced by demographic factors such as gender, age, and education. In this
work, we propose AesBiasBench, a benchmark designed to evaluate MLLMs along two
complementary dimensions: (1) stereotype bias, quantified by measuring
variations in aesthetic evaluations across demographic groups; and (2)
alignment between model outputs and genuine human aesthetic preferences. Our
benchmark covers three subtasks (Aesthetic Perception, Assessment, Empathy) and
introduces structured metrics (IFD, NRD, AAS) to assess both bias and
alignment. We evaluate 19 MLLMs, including proprietary models (e.g., GPT-4o,
Claude-3.5-Sonnet) and open-source models (e.g., InternVL-2.5, Qwen2.5-VL).
Results indicate that smaller models exhibit stronger stereotype biases,
whereas larger models align more closely with human preferences. Incorporating
identity information often exacerbates bias, particularly in emotional
judgments. These findings underscore the importance of identity-aware
evaluation frameworks in subjective vision-language tasks.

</details>


### [59] [EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI](https://arxiv.org/abs/2509.11648)
*Sai Kartheek Reddy Kasu*

Main category: cs.CL

TL;DR: EthicsMH是一个包含125个心理健康伦理场景的数据集，用于评估AI系统在治疗和精神病学背景下的伦理推理能力，填补了现有基准在心理健康领域伦理困境评估的空白。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在心理健康等敏感领域的部署引发了伦理推理、公平性和责任对齐的迫切问题，现有道德和临床决策基准无法充分捕捉心理健康实践中独特的伦理困境。

Method: 开发了EthicsMH数据集，包含125个场景，每个场景都有结构化字段：多种决策选项、专家对齐的推理、预期模型行为、现实世界影响和多利益相关者观点。

Result: 虽然规模较小且采用模型辅助生成，但EthicsMH建立了一个连接AI伦理和心理健康决策的任务框架，能够评估决策准确性、解释质量以及与专业规范的契合度。

Conclusion: 通过发布这个数据集，旨在提供一个可以通过社区和专家贡献扩展的种子资源，促进开发能够负责任处理社会最微妙决策的AI系统。

Abstract: The deployment of large language models (LLMs) in mental health and other
sensitive domains raises urgent questions about ethical reasoning, fairness,
and responsible alignment. Yet, existing benchmarks for moral and clinical
decision-making do not adequately capture the unique ethical dilemmas
encountered in mental health practice, where confidentiality, autonomy,
beneficence, and bias frequently intersect. To address this gap, we introduce
Ethical Reasoning in Mental Health (EthicsMH), a pilot dataset of 125 scenarios
designed to evaluate how AI systems navigate ethically charged situations in
therapeutic and psychiatric contexts. Each scenario is enriched with structured
fields, including multiple decision options, expert-aligned reasoning, expected
model behavior, real-world impact, and multi-stakeholder viewpoints. This
structure enables evaluation not only of decision accuracy but also of
explanation quality and alignment with professional norms. Although modest in
scale and developed with model-assisted generation, EthicsMH establishes a task
framework that bridges AI ethics and mental health decision-making. By
releasing this dataset, we aim to provide a seed resource that can be expanded
through community and expert contributions, fostering the development of AI
systems capable of responsibly handling some of society's most delicate
decisions.

</details>


### [60] [A Dynamic Knowledge Update-Driven Model with Large Language Models for Fake News Detection](https://arxiv.org/abs/2509.11687)
*Di Jin,Jun Yang,Xiaobao Wang,Junwei Zhang,Shuqi Li,Dongxiao He*

Main category: cs.CL

TL;DR: DYNAMO模型通过知识图谱动态更新和大型语言模型集成，解决假新闻检测中的知识真实性和语义深度挖掘问题，在真实数据集上表现最佳


<details>
  <summary>Details</summary>
Motivation: 互联网和社交媒体快速发展，海量复杂信息中区分可信新闻面临挑战。新闻事件的突发性和不稳定性导致真实性标签可能变化，需要获取最新事件更新。现有检索增强生成方法存在检索内容可信度不足和噪声信息干扰问题

Method: 构建新闻领域特定知识图谱，使用蒙特卡洛树搜索分解复杂新闻并逐步验证，从已验证的真实新闻文本和推理路径中提取更新新知识

Result: 在两个真实世界数据集上实现了最佳性能

Conclusion: DYNAMO模型通过动态知识更新机制有效解决了假新闻检测中的关键问题，确保了新知识的真实性和新闻语义的深度挖掘

Abstract: As the Internet and social media evolve rapidly, distinguishing credible news
from a vast amount of complex information poses a significant challenge. Due to
the suddenness and instability of news events, the authenticity labels of news
can potentially shift as events develop, making it crucial for fake news
detection to obtain the latest event updates. Existing methods employ
retrieval-augmented generation to fill knowledge gaps, but they suffer from
issues such as insufficient credibility of retrieved content and interference
from noisy information. We propose a dynamic knowledge update-driven model for
fake news detection (DYNAMO), which leverages knowledge graphs to achieve
continuous updating of new knowledge and integrates with large language models
to fulfill dual functions: news authenticity detection and verification of new
knowledge correctness, solving the two key problems of ensuring the
authenticity of new knowledge and deeply mining news semantics. Specifically,
we first construct a news-domain-specific knowledge graph. Then, we use Monte
Carlo Tree Search to decompose complex news and verify them step by step.
Finally, we extract and update new knowledge from verified real news texts and
reasoning paths. Experimental results demonstrate that DYNAMO achieves the best
performance on two real-world datasets.

</details>


### [61] [CoachMe: Decoding Sport Elements with a Reference-Based Coaching Instruction Generation Model](https://arxiv.org/abs/2509.11698)
*Wei-Hsin Yeh,Yu-An Su,Chih-Ning Chen,Yi-Hsueh Lin,Calvin Ku,Wen-Hsin Chiu,Min-Chun Hu,Lun-Wei Ku*

Main category: cs.CL

TL;DR: CoachMe是一个基于参考的运动指导模型，通过分析学习者动作与参考动作在时间和物理层面的差异，提供精准的体育专项指导，在花样滑冰和拳击项目中显著优于GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 运动指导对于运动员技术精进至关重要，但现有多模态模型在生成精确、体育专项的指导方面仍面临挑战，主要由于体育领域的高度专业性和需要提供信息丰富的指导。

Method: 提出CoachMe参考基准模型，分析学习者动作与参考动作在时间和物理层面的差异，实现领域知识学习和教练式思维过程的获取，能够有效识别运动错误并提供改进反馈。

Result: CoachMe在花样滑冰的G-Eval评估中比GPT-4o高出31.6%，在拳击项目中高出58.3%，能够详细阐述错误并提供相应的改进方法。

Conclusion: CoachMe通过从通用动作学习并利用有限数据，很好地适应了特定体育项目如滑冰和拳击，提供了高质量的指导而非仅仅是教练语气但缺乏关键信息的方向性建议。

Abstract: Motion instruction is a crucial task that helps athletes refine their
technique by analyzing movements and providing corrective guidance. Although
recent advances in multimodal models have improved motion understanding,
generating precise and sport-specific instruction remains challenging due to
the highly domain-specific nature of sports and the need for informative
guidance. We propose CoachMe, a reference-based model that analyzes the
differences between a learner's motion and a reference under temporal and
physical aspects. This approach enables both domain-knowledge learning and the
acquisition of a coach-like thinking process that identifies movement errors
effectively and provides feedback to explain how to improve. In this paper, we
illustrate how CoachMe adapts well to specific sports such as skating and
boxing by learning from general movements and then leveraging limited data.
Experiments show that CoachMe provides high-quality instructions instead of
directions merely in the tone of a coach but without critical information.
CoachMe outperforms GPT-4o by 31.6% in G-Eval on figure skating and by 58.3% on
boxing. Analysis further confirms that it elaborates on errors and their
corresponding improvement methods in the generated instructions. You can find
CoachMe here: https://motionxperts.github.io/

</details>


### [62] [Room acoustics affect communicative success in hybrid meeting spaces: a pilot study](https://arxiv.org/abs/2509.11709)
*Robert Einig,Stefan Janscha,Jonas Schuster,Julian Koch,Martin Hagmueller,Barbara Schuppler*

Main category: cs.CL

TL;DR: 本研究探讨了改善大学研讨室声学环境对混合会议沟通效果的影响，发现声学干预能改善混合会议中的沟通效果


<details>
  <summary>Details</summary>
Motivation: 疫情期间混合会议日益普及，但研讨室的声学设计常被忽视，不良声学环境会导致误解、语音清晰度下降和疲劳等问题

Method: 在格拉茨技术大学研讨室进行声学干预前后对比研究，记录两组人员在声学改善前后的沟通表现

Result: 尽管样本量小未达到统计显著性，但结果明确显示空间声学干预改善了混合会议的沟通效果

Conclusion: 声学设计对混合会议质量至关重要，改善房间声学环境能有效支持更好的混合会议沟通

Abstract: Since the COVID-19 pandemic in 2020, universities and companies have
increasingly integrated hybrid features into their meeting spaces, or even
created dedicated rooms for this purpose. While the importance of a fast and
stable internet connection is often prioritized, the acoustic design of seminar
rooms is frequently overlooked. Poor acoustics, particularly excessive
reverberation, can lead to issues such as misunderstandings, reduced speech
intelligibility or cognitive and vocal fatigue. This pilot study investigates
whether room acoustic interventions in a seminar room at Graz University of
Technology support better communication in hybrid meetings. For this purpose,
we recorded two groups of persons twice, once before and once after improving
the acoustics of the room. Our findings -- despite not reaching statistical
significance due to the small sample size - indicate clearly that our spatial
interventions improve communicative success in hybrid meetings. To make the
paper accessible also for readers from the speech communication community, we
explain room acoustics background, relevant for the interpretation of our
results.

</details>


### [63] [An Agentic Toolkit for Adaptive Information Extraction from Regulatory Documents](https://arxiv.org/abs/2509.11773)
*Gaye Colakoglu,Gürkan Solmaz,Jonathan Fürst*

Main category: cs.CL

TL;DR: 本文提出了一种针对欧盟建筑产品性能声明(DoP)文档的领域特定智能体系统，通过规划器-执行器-响应器架构解决文档格式多样性带来的自动化信息提取挑战


<details>
  <summary>Details</summary>
Motivation: 欧盟法规要求的性能声明文档在布局、语言、模式和格式上差异很大，给自动化键值对提取和问答带来困难，现有静态或纯LLM的信息提取流程存在幻觉问题且无法适应这种结构多样性

Method: 采用领域特定的状态感知智能体系统，通过规划器-执行器-响应器架构，动态推断用户意图、检测文档模态并协调工具使用，实现可追溯的鲁棒推理，避免工具误用或执行循环

Result: 在精心策划的DoP数据集上的评估显示，该系统在不同格式和语言下都表现出改进的鲁棒性

Conclusion: 该系统为受监管工作流程中的结构化数据提取提供了一个可扩展的解决方案，能够有效处理文档格式多样性带来的挑战

Abstract: Declaration of Performance (DoP) documents, mandated by EU regulation,
certify the performance of construction products. While some of their content
is standardized, DoPs vary widely in layout, language, schema, and format,
posing challenges for automated key-value pair extraction (KVP) and question
answering (QA). Existing static or LLM-only IE pipelines often hallucinate and
fail to adapt to this structural diversity. Our domain-specific, stateful
agentic system addresses these challenges through a planner-executor-responder
architecture. The system infers user intent, detects document modality, and
orchestrates tools dynamically for robust, traceable reasoning while avoiding
tool misuse or execution loops. Evaluation on a curated DoP dataset
demonstrates improved robustness across formats and languages, offering a
scalable solution for structured data extraction in regulated workflows.

</details>


### [64] [User eXperience Perception Insights Dataset (UXPID): Synthetic User Feedback from Public Industrial Forums](https://arxiv.org/abs/2509.11777)
*Mikhail Kulyabin,Jan Joosten,Choro Ulan uulu,Nuno Miguel Martins Pacheco,Fabian Ries,Filippos Petridis,Jan Bosch,Helena Holmström Olsson*

Main category: cs.CL

TL;DR: 本文提出了UXPID数据集，包含7130条合成的工业自动化论坛用户反馈，用于UX分析和AI模型训练


<details>
  <summary>Details</summary>
Motivation: 工业论坛中的客户反馈是宝贵但未被充分利用的信息源，传统分析方法难以处理其非结构化和领域特定特性

Method: 使用大型语言模型系统分析论坛讨论，标注UX洞察、用户期望、严重性评分和主题分类，构建结构化JSON数据集

Result: 创建了包含7130条多帖子评论的UXPID数据集，每条记录都包含丰富的元数据和上下文对话信息

Conclusion: UXPID数据集为UX研究、需求分析和AI驱动的反馈处理提供了有价值的资源，特别是在隐私限制下无法获取真实数据时

Abstract: Customer feedback in industrial forums reflect a rich but underexplored
source of insight into real-world product experience. These publicly shared
discussions offer an organic view of user expectations, frustrations, and
success stories shaped by the specific contexts of use. Yet, harnessing this
information for systematic analysis remains challenging due to the unstructured
and domain-specific nature of the content. The lack of structure and
specialized vocabulary makes it difficult for traditional data analysis
techniques to accurately interpret, categorize, and quantify the feedback,
thereby limiting its potential to inform product development and support
strategies. To address these challenges, this paper presents the User
eXperience Perception Insights Dataset (UXPID), a collection of 7130
artificially synthesized and anonymized user feedback branches extracted from a
public industrial automation forum. Each JavaScript object notation (JSON)
record contains multi-post comments related to specific hardware and software
products, enriched with metadata and contextual conversation data. Leveraging a
large language model (LLM), each branch is systematically analyzed and
annotated for UX insights, user expectations, severity and sentiment ratings,
and topic classifications. The UXPID dataset is designed to facilitate research
in user requirements, user experience (UX) analysis, and AI-driven feedback
processing, particularly where privacy and licensing restrictions limit access
to real-world data. UXPID supports the training and evaluation of
transformer-based models for tasks such as issue detection, sentiment analysis,
and requirements extraction in the context of technical forums.

</details>


### [65] [When Curiosity Signals Danger: Predicting Health Crises Through Online Medication Inquiries](https://arxiv.org/abs/2509.11802)
*Dvora Goncharok,Arbel Shifman,Alexander Apartsin,Yehudit Aperstein*

Main category: cs.CL

TL;DR: 该研究创建了一个标注数据集，用于识别在线医疗论坛中药物相关问题的关键性，并比较了传统机器学习方法和大型语言模型在检测高风险问题方面的性能。


<details>
  <summary>Details</summary>
Motivation: 在线医疗论坛包含大量患者关于药物使用的疑问，其中一些问题可能预示着混淆、误用甚至健康危机。及时检测这些关键问题对于干预和提高患者安全至关重要。

Method: 研究构建了一个手动标注临床风险关键性的药物相关问题数据集，使用TF-IDF文本表示的六种传统机器学习分类器，以及三种基于大型语言模型的深度上下文理解分类方法进行性能对比。

Result: 研究结果表明传统方法和现代方法都具备支持实时分诊和警报系统的潜力，为数字健康空间中的关键健康事件早期预警提供了可行方案。

Conclusion: 该研究提供了一个公开可用的标注数据集，鼓励在患者生成数据、自然语言处理和健康事件早期预警系统交叉领域的进一步研究，有助于改善患者安全和医疗干预时效性。

Abstract: Online medical forums are a rich and underutilized source of insight into
patient concerns, especially regarding medication use. Some of the many
questions users pose may signal confusion, misuse, or even the early warning
signs of a developing health crisis. Detecting these critical questions that
may precede severe adverse events or life-threatening complications is vital
for timely intervention and improving patient safety. This study introduces a
novel annotated dataset of medication-related questions extracted from online
forums. Each entry is manually labelled for criticality based on clinical risk
factors. We benchmark the performance of six traditional machine learning
classifiers using TF-IDF textual representations, alongside three
state-of-the-art large language model (LLM)-based classification approaches
that leverage deep contextual understanding. Our results highlight the
potential of classical and modern methods to support real-time triage and alert
systems in digital health spaces. The curated dataset is made publicly
available to encourage further research at the intersection of
patient-generated data, natural language processing, and early warning systems
for critical health events. The dataset and benchmark are available at:
https://github.com/Dvora-coder/LLM-Medication-QA-Risk-Classifier-MediGuard.

</details>


### [66] [From Fuzzy Speech to Medical Insight: Benchmarking LLMs on Noisy Patient Narratives](https://arxiv.org/abs/2509.11803)
*Eden Mama,Liel Sheri,Yehudit Aperstein,Alexander Apartsin*

Main category: cs.CL

TL;DR: 提出了一个名为Noisy Diagnostic Benchmark (NDB)的合成数据集，用于测试大型语言模型在嘈杂、非正式的患者自述文本中的诊断能力


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要基于干净、结构化的临床文本，无法真实反映LLM在实际嘈杂、模糊的患者自述条件下的表现

Method: 创建包含不同语言噪声水平、模糊语言和普通人术语的合成患者自述数据集，使用BERT-based和T5等先进模型进行微调和评估

Result: 开发了NDB基准测试，包含临床一致的情景和真实诊断标注，涵盖不同沟通清晰度，以反映真实世界的报告风格

Conclusion: NDB基准测试支持可重复性和未来研究，有助于压力测试和比较LLM在真实语言条件下的诊断能力

Abstract: The widespread adoption of large language models (LLMs) in healthcare raises
critical questions about their ability to interpret patient-generated
narratives, which are often informal, ambiguous, and noisy. Existing benchmarks
typically rely on clean, structured clinical text, offering limited insight
into model performance under realistic conditions. In this work, we present a
novel synthetic dataset designed to simulate patient self-descriptions
characterized by varying levels of linguistic noise, fuzzy language, and
layperson terminology. Our dataset comprises clinically consistent scenarios
annotated with ground-truth diagnoses, spanning a spectrum of communication
clarity to reflect diverse real-world reporting styles. Using this benchmark,
we fine-tune and evaluate several state-of-the-art models (LLMs), including
BERT-based and encoder-decoder T5 models. To support reproducibility and future
research, we release the Noisy Diagnostic Benchmark (NDB), a structured dataset
of noisy, synthetic patient descriptions designed to stress-test and compare
the diagnostic capabilities of large language models (LLMs) under realistic
linguistic conditions. We made the benchmark available for the community:
https://github.com/lielsheri/PatientSignal

</details>


### [67] [PledgeTracker: A System for Monitoring the Fulfilment of Pledges](https://arxiv.org/abs/2509.11804)
*Yulong Chen,Michael Sejr Schlichtkrull,Zhenyun Deng,David Corney,Nasim Asl,Joshua Salisbury,Andrew Dudfield,Andreas Vlachos*

Main category: cs.CL

TL;DR: PledgeTracker是一个将政治承诺验证转化为结构化事件时间线构建的系统，通过多步骤证据检索、时间线构建和履行过滤模块来跟踪政治承诺的履行情况。


<details>
  <summary>Details</summary>
Motivation: 现有方法将政治承诺验证简化为文档分类任务，忽略了其动态性、时序性和多文档特性，需要一种能够处理增量证据和动态更新的方法。

Method: 系统包含三个核心组件：(1)多步骤证据检索模块；(2)时间线构建模块；(3)履行过滤模块，通过构建结构化事件时间线来捕捉承诺履行的演变过程。

Result: 与专业事实核查人员在真实工作流程中合作评估，证明系统在检索相关证据和减少人工验证工作量方面具有有效性。

Conclusion: PledgeTracker提供了一种可解释且结构化的方法来跟踪政治承诺履行，能够更好地处理动态更新的多源信息。

Abstract: Political pledges reflect candidates' policy commitments, but tracking their
fulfilment requires reasoning over incremental evidence distributed across
multiple, dynamically updated sources. Existing methods simplify this task into
a document classification task, overlooking its dynamic, temporal and
multi-document nature. To address this issue, we introduce
\textsc{PledgeTracker}, a system that reformulates pledge verification into
structured event timeline construction. PledgeTracker consists of three core
components: (1) a multi-step evidence retrieval module; (2) a timeline
construction module and; (3) a fulfilment filtering module, allowing the
capture of the evolving nature of pledge fulfilment and producing interpretable
and structured timelines. We evaluate PledgeTracker in collaboration with
professional fact-checkers in real-world workflows, demonstrating its
effectiveness in retrieving relevant evidence and reducing human verification
effort.

</details>


### [68] [SCDTour: Embedding Axis Ordering and Merging for Interpretable Semantic Change Detection](https://arxiv.org/abs/2509.11818)
*Taichi Aida,Danushka Bollegala*

Main category: cs.CL

TL;DR: SCDTour方法通过排序和合并可解释轴来解决语义变化检测中可解释性与性能之间的权衡问题，在保持高可解释性的同时维持了检测性能


<details>
  <summary>Details</summary>
Motivation: 解决语义变化检测中可解释性与性能之间的权衡问题 - 提高可解释性通常会导致SCD性能下降，反之亦然

Method: SCDTour方法通过考虑(a)嵌入空间中轴之间的语义相似性，以及(b)每个轴对语义变化的贡献程度，来排序和合并可解释轴

Result: 实验结果表明SCDTour在保持语义变化检测性能的同时维持了高可解释性，聚合排序后的轴产生了更精细的词义集合，在SCD任务中达到了与原始全维嵌入相当或更好的性能

Conclusion: SCDTour有效平衡了可解释性和SCD性能，通过少量精炼轴实现了语义变化的有意义解释

Abstract: In Semantic Change Detection (SCD), it is a common problem to obtain
embeddings that are both interpretable and high-performing. However, improving
interpretability often leads to a loss in the SCD performance, and vice versa.
To address this problem, we propose SCDTour, a method that orders and merges
interpretable axes to alleviate the performance degradation of SCD. SCDTour
considers both (a) semantic similarity between axes in the embedding space, as
well as (b) the degree to which each axis contributes to semantic change.
Experimental results show that SCDTour preserves performance in semantic change
detection while maintaining high interpretability. Moreover, agglomerating the
sorted axes produces a more refined set of word senses, which achieves
comparable or improved performance against the original full-dimensional
embeddings in the SCD task. These findings demonstrate that SCDTour effectively
balances interpretability and SCD performance, enabling meaningful
interpretation of semantic shifts through a small number of refined axes.
Source code is available at https://github.com/LivNLP/svp-tour .

</details>


### [69] [MOOM: Maintenance, Organization and Optimization of Memory in Ultra-Long Role-Playing Dialogues](https://arxiv.org/abs/2509.11860)
*Weishu Chen,Jinyi Tang,Zhouhui Hou,Shihao Han,Mingjie Zhan,Zhiyuan Huang,Delong Liu,Jiawei Guo,Zhicheng Zhao,Fei Su*

Main category: cs.CL

TL;DR: MOOM是一个基于文学理论的双分支记忆插件，通过建模情节发展和角色刻画来控制超长对话中的记忆增长，并整合了遗忘机制来约束记忆容量。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在人类-机器人角色扮演场景中超长对话中记忆不受控制增长的问题，需要一种能够维持对话连贯性同时控制记忆容量的方法。

Method: 提出MOOM双分支记忆插件：一个分支总结多时间尺度的情节冲突，另一个分支提取用户角色档案；整合基于"竞争抑制"记忆理论的遗忘机制来约束记忆容量。

Result: MOOM在ZH-4O中文超长对话数据集上表现优于所有最先进的记忆提取方法，需要更少的大语言模型调用次数，同时保持可控的记忆容量。

Conclusion: MOOM通过结合文学理论和记忆理论，有效解决了超长角色扮演对话中的记忆控制问题，为相关领域提供了新的解决方案和数据集支持。

Abstract: Memory extraction is crucial for maintaining coherent ultra-long dialogues in
human-robot role-playing scenarios. However, existing methods often exhibit
uncontrolled memory growth. To address this, we propose MOOM, the first
dual-branch memory plugin that leverages literary theory by modeling plot
development and character portrayal as core storytelling elements.
Specifically, one branch summarizes plot conflicts across multiple time scales,
while the other extracts the user's character profile. MOOM further integrates
a forgetting mechanism, inspired by the ``competition-inhibition'' memory
theory, to constrain memory capacity and mitigate uncontrolled growth.
Furthermore, we present ZH-4O, a Chinese ultra-long dialogue dataset
specifically designed for role-playing, featuring dialogues that average 600
turns and include manually annotated memory information. Experimental results
demonstrate that MOOM outperforms all state-of-the-art memory extraction
methods, requiring fewer large language model invocations while maintaining a
controllable memory capacity.

</details>


### [70] [Growing Perspectives: Modelling Embodied Perspective Taking and Inner Narrative Development Using Large Language Models](https://arxiv.org/abs/2509.11868)
*Sabrina Patania,Luca Annese,Anna Lambiase,Anita Pellegrini,Tom Foulsham,Azzurra Ruggeri,Silvia Rossi,Silvia Serino,Dimitri Ognibene*

Main category: cs.CL

TL;DR: 本研究探讨PerspAct系统如何将ReAct范式与大型语言模型结合，基于Selman理论模拟观点采择的发展阶段，通过导演任务评估GPT生成发展一致性叙事的能力及其对协作表现的影响。


<details>
  <summary>Details</summary>
Motivation: 语言和具身观点采择对人类协作至关重要，但现有计算模型很少同时处理这两个方面。研究旨在探索如何将具身观点采择与语言能力整合到LLMs中，以更好地模拟发展动态。

Method: 使用扩展的导演任务，评估GPT生成符合特定发展阶段内部叙事的能力，并从定性（动作选择）和定量（任务效率）角度分析这些叙事如何影响协作表现。

Result: GPT能在任务执行前可靠地生成发展一致的叙事，但在交互过程中往往转向更高级阶段；较高发展阶段通常提升协作效果，而早期阶段在复杂情境中产生更多变结果。

Conclusion: 研究发现语言交流有助于精化内部表征，强调在结合语言和具身任务时评估内部言语的重要性，展示了在LLMs中整合具身观点采择和语言的潜力。

Abstract: Language and embodied perspective taking are essential for human
collaboration, yet few computational models address both simultaneously. This
work investigates the PerspAct system [1], which integrates the ReAct (Reason
and Act) paradigm with Large Language Models (LLMs) to simulate developmental
stages of perspective taking, grounded in Selman's theory [2]. Using an
extended director task, we evaluate GPT's ability to generate internal
narratives aligned with specified developmental stages, and assess how these
influence collaborative performance both qualitatively (action selection) and
quantitatively (task efficiency). Results show that GPT reliably produces
developmentally-consistent narratives before task execution but often shifts
towards more advanced stages during interaction, suggesting that language
exchanges help refine internal representations. Higher developmental stages
generally enhance collaborative effectiveness, while earlier stages yield more
variable outcomes in complex contexts. These findings highlight the potential
of integrating embodied perspective taking and language in LLMs to better model
developmental dynamics and stress the importance of evaluating internal speech
during combined linguistic and embodied tasks.

</details>


### [71] [Uncertainty in Authorship: Why Perfect AI Detection Is Mathematically Impossible](https://arxiv.org/abs/2509.11915)
*Aadil Gani Ganie*

Main category: cs.CL

TL;DR: 本文通过量子不确定性类比，论证了AI文本检测存在根本性限制：检测精度与文本自然性之间存在权衡，当AI文本高度拟人时，完美检测在理论上不可行


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型发展，区分人类与AI生成文本变得越来越困难。作者希望探讨文本作者身份检测的理论极限，借鉴量子力学中的测不准原理来理解这一挑战

Method: 采用概念类比方法，将量子系统中的精度与扰动张力平行应用于文本检测领域。分析现有检测方法（风格计量学、数字水印、神经分类器）的内在局限性

Result: 研究发现检测精度提升往往导致AI输出改变，使其他特征可靠性下降。当AI文本高度模仿人类写作时，完美检测不仅在技术上困难，在理论上也不可能实现

Conclusion: AI文本检测挑战不仅仅是工具问题，而是反映了语言本质中更深层次、不可避免的张力。这对作者身份、伦理和政策具有重要启示意义

Abstract: As large language models (LLMs) become more advanced, it is increasingly
difficult to distinguish between human-written and AI-generated text. This
paper draws a conceptual parallel between quantum uncertainty and the limits of
authorship detection in natural language. We argue that there is a fundamental
trade-off: the more confidently one tries to identify whether a text was
written by a human or an AI, the more one risks disrupting the text's natural
flow and authenticity. This mirrors the tension between precision and
disturbance found in quantum systems. We explore how current detection
methods--such as stylometry, watermarking, and neural classifiers--face
inherent limitations. Enhancing detection accuracy often leads to changes in
the AI's output, making other features less reliable. In effect, the very act
of trying to detect AI authorship introduces uncertainty elsewhere in the text.
Our analysis shows that when AI-generated text closely mimics human writing,
perfect detection becomes not just technologically difficult but theoretically
impossible. We address counterarguments and discuss the broader implications
for authorship, ethics, and policy. Ultimately, we suggest that the challenge
of AI-text detection is not just a matter of better tools--it reflects a
deeper, unavoidable tension in the nature of language itself.

</details>


### [72] [Designing LLMs for cultural sensitivity: Evidence from English-Japanese translation](https://arxiv.org/abs/2509.11921)
*Helene Tenzer,Oumnia Abidi,Stefan Feuerriegel*

Main category: cs.CL

TL;DR: 本研究分析了不同LLM设计在英日职场邮件翻译中的文化敏感性，发现文化定制提示可以改善文化适应性。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM能够生成近乎完美的字面翻译，但在跨文化沟通中是否支持文化适宜的交流仍不清楚，特别是在英语-日语职场邮件翻译场景中。

Method: 采用混合方法研究，比较三种提示策略：(1)简单翻译提示，(2)指定收件人文化背景的受众定向提示，(3)包含日本沟通规范明确指导的教学提示，并通过文化特定语言模式分析和母语者对翻译语气适当性的评估。

Result: 研究发现文化定制的提示策略能够改善翻译的文化适应性。

Conclusion: 基于研究结果，为在多语言环境中设计文化包容性LLM提供了建议。

Abstract: Large language models (LLMs) are increasingly used in everyday communication,
including multilingual interactions across different cultural contexts. While
LLMs can now generate near-perfect literal translations, it remains unclear
whether LLMs support culturally appropriate communication. In this paper, we
analyze the cultural sensitivity of different LLM designs when applied to
English-Japanese translations of workplace e-mails. Here, we vary the prompting
strategies: (1) naive "just translate" prompts, (2) audience-targeted prompts
specifying the recipient's cultural background, and (3) instructional prompts
with explicit guidance on Japanese communication norms. Using a mixed-methods
study, we then analyze culture-specific language patterns to evaluate how well
translations adapt to cultural norms. Further, we examine the appropriateness
of the tone of the translations as perceived by native speakers. We find that
culturally-tailored prompting can improve cultural fit, based on which we offer
recommendations for designing culturally inclusive LLMs in multilingual
settings.

</details>


### [73] [Spec-LLaVA: Accelerating Vision-Language Models with Dynamic Tree-Based Speculative Decoding](https://arxiv.org/abs/2509.11961)
*Mingxiao Huo,Jiayi Zhang,Hewei Wang,Jinfeng Xu,Zheyu Chen,Huilin Tai,Yijun Chen*

Main category: cs.CL

TL;DR: Spec-LLaVA使用推测解码技术加速视觉语言模型，通过轻量级草稿模型预测未来token，目标模型并行验证，实现3.28倍加速且不损失生成质量。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型(VLMs)的自回归推理速度慢，限制了其在实时应用中的部署，需要一种在不牺牲输出质量的前提下加速推理的方法。

Method: 采用推测解码技术，将轻量级草稿VLM与大型目标模型配对：草稿模型推测未来token，目标模型并行验证，并使用基于动态树的验证算法根据草稿模型置信度自适应扩展和剪枝推测分支。

Result: 在MS COCO域外图像上，Spec-LLaVA在LLaVA-1.5(7B, 13B)上实现了最高3.28倍的解码加速，且生成质量无损失。

Conclusion: 该工作提出了一个使用动态树结构推测解码的无损加速框架，为实用实时多模态助手开辟了道路，轻量级草稿模型设计使其适用于资源受限或设备端部署场景。

Abstract: Vision-Language Models (VLMs) enable powerful multimodal reasoning but suffer
from slow autoregressive inference, limiting their deployment in real-time
applications. We introduce Spec-LLaVA, a system that applies speculative
decoding to accelerate VLMs without sacrificing output quality. Spec-LLaVA
pairs a lightweight draft VLM with a large target model: the draft speculates
future tokens, which the target verifies in parallel, allowing multiple tokens
to be generated per step. To maximize efficiency, we design a dynamic
tree-based verification algorithm that adaptively expands and prunes
speculative branches using draft model confidence. On MS COCO out-of-domain
images, Spec-LLaVA achieves up to 3.28$\times$ faster decoding on LLaVA-1.5
(7B, 13B) with no loss in generation quality. This work presents a lossless
acceleration framework for VLMs using dynamic tree-structured speculative
decoding, opening a path toward practical real-time multimodal assistants.
Importantly, the lightweight draft model design makes the framework amenable to
resource-constrained or on-device deployment settings.

</details>


### [74] [ToolRM: Outcome Reward Models for Tool-Calling Large Language Models](https://arxiv.org/abs/2509.11963)
*Mayank Agarwal,Ibrahim Abdelaziz,Kinjal Basu,Merve Unuvar,Luis A. Lastras,Yara Rizk,Pavan Kapanipathi*

Main category: cs.CL

TL;DR: 提出了FC-RewardBench基准测试来评估奖励模型在工具调用场景中的性能，发现现有模型在评估工具使用方面存在不足，并提出了基于结果的新训练框架，显著提升了工具使用的奖励建模效果。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地与外部工具交互，现有的基于自然语言输出的奖励模型难以有效评估工具使用的推理和执行过程，需要专门的工具使用奖励建模方法。

Method: 开发了FC-RewardBench基准测试系统评估奖励模型性能；提出了使用开源大语言模型合成数据来训练基于结果的奖励模型的框架；训练了1.7B到14B参数规模的模型。

Result: 训练的专用奖励模型在7个域外基准测试中持续优于通用基线模型，平均下游任务性能提升达25%，并通过奖励引导过滤实现了数据高效微调。

Conclusion: 工具使用需要专门的奖励建模方法，基于结果的奖励模型训练框架能显著提升工具调用场景的评估性能，为LLM与工具的高效交互提供了重要支持。

Abstract: As large language models (LLMs) increasingly interact with external tools,
reward modeling for tool use has become a critical yet underexplored area.
Existing reward models, trained primarily on natural language outputs, struggle
to evaluate tool-based reasoning and execution. To quantify this gap, we
introduce FC-RewardBench, the first benchmark designed to systematically assess
reward models' performance in tool-calling scenarios. Our analysis shows that
current reward models often miss key signals of effective tool use,
highlighting the need for domain-specific modeling. To address this, we propose
a training framework for outcome-based reward models using data synthesized
from permissively licensed, open-weight LLMs. We train models ranging from 1.7B
to 14B parameters and evaluate them across seven out-of-domain benchmarks.
These models consistently outperform general-purpose baselines, achieving up to
25\% average improvement in downstream task performance and enabling
data-efficient fine-tuning through reward-guided filtering.

</details>


### [75] [Query-Focused Extractive Summarization for Sentiment Explanation](https://arxiv.org/abs/2509.11989)
*Ahmed Moubtahij,Sylvie Ratté,Yazid Attabi,Maxime Dumas*

Main category: cs.CL

TL;DR: 本文提出了一个多偏差框架来解决查询聚焦摘要任务中查询与源文档之间的语言差异问题，通过情感偏差和查询扩展方法在情感解释问题上取得了优于基线模型的效果。


<details>
  <summary>Details</summary>
Motivation: 客户反馈的情感分析需要从大量文本中确定情感原因，查询聚焦摘要任务常因查询与源文档之间的语言差异而受阻，需要开发领域无关的通用解决方案。

Method: 提出了多偏差框架来弥合语言差异，针对情感解释问题制定了专门的方法，包括基于情感的偏差和查询扩展技术。

Result: 在真实世界的专有情感感知QFS数据集上，实验结果显示该方法优于基线模型。

Conclusion: 多偏差框架和专门的情感处理方法有效提升了查询聚焦摘要任务在情感解释方面的性能，为实际应用提供了可行的解决方案。

Abstract: Constructive analysis of feedback from clients often requires determining the
cause of their sentiment from a substantial amount of text documents. To assist
and improve the productivity of such endeavors, we leverage the task of
Query-Focused Summarization (QFS). Models of this task are often impeded by the
linguistic dissonance between the query and the source documents. We propose
and substantiate a multi-bias framework to help bridge this gap at a
domain-agnostic, generic level; we then formulate specialized approaches for
the problem of sentiment explanation through sentiment-based biases and query
expansion. We achieve experimental results outperforming baseline models on a
real-world proprietary sentiment-aware QFS dataset.

</details>


### [76] [Text Adaptation to Plain Language and Easy Read via Automatic Post-Editing Cycles](https://arxiv.org/abs/2509.11991)
*Jesús Calleja,David Ponce,Thierry Etchegoyhen*

Main category: cs.CL

TL;DR: Vicomtech团队使用大型语言模型迭代后编辑方法，在CLEARS挑战赛的西班牙语文本简化任务中获得第一名（普通语言）和第二名（易读语言）的成绩


<details>
  <summary>Details</summary>
Motivation: 参与CLEARS挑战赛，目标是开发有效的文本简化方法，将复杂文本转换为普通语言和易读语言，提高文本的可读性

Method: 采用大型语言模型的自动后编辑方法，通过迭代生成连续的文本简化版本，直到可读性和相似性指标显示无法进一步优化

Result: 在官方评估指标的平均得分上，团队提交的方案在普通语言简化任务中获得第一名，在易读语言简化任务中获得第二名

Conclusion: 基于大型语言模型的迭代后编辑方法在文本简化任务中表现优异，证明了该方法在提高文本可读性方面的有效性

Abstract: We describe Vicomtech's participation in the CLEARS challenge on text
adaptation to Plain Language and Easy Read in Spanish. Our approach features
automatic post-editing of different types of initial Large Language Model
adaptations, where successive adaptations are generated iteratively until
readability and similarity metrics indicate that no further adaptation
refinement can be successfully performed. Taking the average of all official
metrics, our submissions achieved first and second place in Plain language and
Easy Read adaptation, respectively.

</details>


### [77] [Steering Language Models in Multi-Token Generation: A Case Study on Tense and Aspect](https://arxiv.org/abs/2509.12065)
*Alina Klerings,Jannik Brinkmann,Daniel Ruffinelli,Simone Ponzetto*

Main category: cs.CL

TL;DR: 该研究通过线性判别分析发现大语言模型在残差空间中编码动词时态和体貌的独立正交方向，并通过概念导向实现了对这两个语法特征的控制，发现导向强度、位置和持续时间是影响多标记生成效果的关键参数。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型如何内部编码句法知识，特别是多维层次语法现象（动词时态和体貌）的表征方式，探索对这些语法特征的控制方法。

Method: 使用线性判别分析识别动词时态和体貌在残差空间中的正交方向，通过概念导向在三个生成任务中实现因果控制，并研究多标记生成中影响有效导向的关键因素。

Result: 发现模型以结构化的、类似人类的方式编码时态和体貌，但有效控制需要精细调整导向强度、位置和持续时间等参数，以避免主题偏移和退化等副作用。

Conclusion: 大语言模型编码语法知识具有结构性组织特征，但对语法特征的有效生成控制需要手动调优或自动化优化，涉及多个敏感参数的协调。

Abstract: Large language models (LLMs) are able to generate grammatically well-formed
text, but how do they encode their syntactic knowledge internally? While prior
work has focused largely on binary grammatical contrasts, in this work, we
study the representation and control of two multidimensional hierarchical
grammar phenomena - verb tense and aspect - and for each, identify distinct,
orthogonal directions in residual space using linear discriminant analysis.
Next, we demonstrate causal control over both grammatical features through
concept steering across three generation tasks. Then, we use these identified
features in a case study to investigate factors influencing effective steering
in multi-token generation. We find that steering strength, location, and
duration are crucial parameters for reducing undesirable side effects such as
topic shift and degeneration. Our findings suggest that models encode tense and
aspect in structurally organized, human-like ways, but effective control of
such features during generation is sensitive to multiple factors and requires
manual tuning or automated optimization.

</details>


### [78] [SENSE models: an open source solution for multilingual and multimodal semantic-based tasks](https://arxiv.org/abs/2509.12093)
*Salima Mdhaffar,Haroun Elleuch,Chaimae Chellaf,Ha Nguyen,Yannick Estève*

Main category: cs.CL

TL;DR: SENSE是一个开源的多语言语音-文本共享嵌入模型，基于SAMU-XLSR框架，通过师生框架将自监督语音编码器与语言无关的文本编码器在话语级别对齐，在多项多语言多模态语义任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 受到SAMU-XLSR框架和Meta AI的SONAR模型启发，旨在开发一个能够将语音和文本表示在语义空间中对齐的多语言模型，以支持跨语言和跨模态的语义理解任务。

Method: 采用师生框架，选择更强的教师文本模型和更好的初始语音编码器来更新原始SAMU-XLSR方法，通过SpeechBrain工具包实现模型训练和使用。

Result: SENSE模型在多语言多模态语义任务中取得了极具竞争力的性能表现，展示了其在语义对齐方面的有效性。

Conclusion: 该研究不仅发布了实用的SENSE模型，还提供了关于语义对齐语音编码器如何捕获语义的新见解，为多语言语音-文本表示学习提供了有价值的贡献。

Abstract: This paper introduces SENSE (Shared Embedding for N-lingual Speech and tExt),
an open-source solution inspired by the SAMU-XLSR framework and conceptually
similar to Meta AI's SONAR models. These approaches rely on a teacher-student
framework to align a self-supervised speech encoder with the language-agnostic
continuous representations of a text encoder at the utterance level. We
describe how the original SAMU-XLSR method has been updated by selecting a
stronger teacher text model and a better initial speech encoder. The source
code for training and using SENSE models has been integrated into the
SpeechBrain toolkit, and the first SENSE model we trained has been publicly
released. We report experimental results on multilingual and multimodal
semantic tasks, where our SENSE model achieves highly competitive performance.
Finally, this study offers new insights into how semantics are captured in such
semantically aligned speech encoders.

</details>


### [79] [Is 'Hope' a person or an idea? A pilot benchmark for NER: comparing traditional NLP tools and large language models on ambiguous entities](https://arxiv.org/abs/2509.12098)
*Payam Latifi*

Main category: cs.CL

TL;DR: 本研究比较了6个NER系统（3个传统NLP工具和3个LLM）在小型标注数据集上的性能，发现LLM在上下文敏感实体识别上表现更好，但传统工具在结构化标签上更稳定。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型与传统NLP工具在命名实体识别任务上的性能差异，为模型选择提供实证依据。

Method: 使用包含119个标记、5种实体类型的手动标注黄金标准数据集，比较6个系统（NLTK、spaCy、Stanza、Gemini-1.5-flash、DeepSeek-V3、Qwen-3-4B）的F1分数性能。

Result: LLM在识别上下文敏感实体（如人名）方面普遍优于传统工具，Gemini获得最高平均F1分数。但传统系统如Stanza在结构化标签（地点、日期）上表现更一致。LLM在处理时间表达式和多词组织时存在变异性。

Conclusion: 虽然LLM提供更好的上下文理解能力，但传统工具在特定任务上仍具有竞争力，模型选择应根据具体需求决定。

Abstract: This pilot study presents a small-scale but carefully annotated benchmark of
Named Entity Recognition (NER) performance across six systems: three non-LLM
NLP tools (NLTK, spaCy, Stanza) and three general-purpose large language models
(LLMs: Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B). The dataset contains 119
tokens covering five entity types (PERSON, LOCATION, ORGANIZATION, DATE, TIME).
We evaluated each system's output against the manually annotated gold standard
dataset using F1-score. The results show that LLMs generally outperform
conventional tools in recognizing context-sensitive entities like person names,
with Gemini achieving the highest average F1-score. However, traditional
systems like Stanza demonstrate greater consistency in structured tags such as
LOCATION and DATE. We also observed variability among LLMs, particularly in
handling temporal expressions and multi-word organizations. Our findings
highlight that while LLMs offer improved contextual understanding, traditional
tools remain competitive in specific tasks, informing model selection.

</details>


### [80] [In-domain SSL pre-training and streaming ASR](https://arxiv.org/abs/2509.12101)
*Jarod Duret,Salima Mdhaffar,Gaëlle Laperrière,Ryan Whetten,Audrey Galametz,Catherine Kobus,Marion-Cécile Martin,Jo Oleiwan,Yannick Estève*

Main category: cs.CL

TL;DR: 该研究证明在航空交通管制领域进行领域特定的自监督预训练能显著提升ASR性能，提出的流式处理方法在低延迟约束下进一步降低了词错误率。


<details>
  <summary>Details</summary>
Motivation: 针对航空交通管制这一安全关键领域，需要开发高精度、低延迟的语音识别系统，但通用语音编码器在该领域表现不佳，因此研究领域特定的自监督学习方法。

Method: 使用4.5千小时无标注ATC数据训练BEST-RQ模型，然后在有监督ATC数据集上微调；提出分块注意力和动态卷积技术实现实时流式处理。

Result: 领域自适应预训练在标准ATC基准测试中显著优于通用语音编码器（w2v-BERT 2.0和HuBERT），大幅降低了词错误率；流式方法在严格延迟约束下进一步提升了性能。

Conclusion: 为ATC数据专门定制自监督学习表示是实现更准确、高效ASR系统的实用途径，特别适合安全关键的实际应用场景。

Abstract: In this study, we investigate the benefits of domain-specific self-supervised
pre-training for both offline and streaming ASR in Air Traffic Control (ATC)
environments. We train BEST-RQ models on 4.5k hours of unlabeled ATC data, then
fine-tune on a smaller supervised ATC set. To enable real-time processing, we
propose using chunked attention and dynamic convolutions, ensuring low-latency
inference. We compare these in-domain SSL models against state-of-the-art,
general-purpose speech encoders such as w2v-BERT 2.0 and HuBERT. Results show
that domain-adapted pre-training substantially improves performance on standard
ATC benchmarks, significantly reducing word error rates when compared to models
trained on broad speech corpora. Furthermore, the proposed streaming approach
further improves word error rate under tighter latency constraints, making it
particularly suitable for safety-critical aviation applications. These findings
highlight that specializing SSL representations for ATC data is a practical
path toward more accurate and efficient ASR systems in real-world operational
settings.

</details>


### [81] [GTA: Supervised-Guided Reinforcement Learning for Text Classification with Large Language Models](https://arxiv.org/abs/2509.12108)
*Min Zeng,Jinfei Sun,Xueyou Luo,Caiquan Liu,Shiqi Zhang,Li Xie,Xiaoxin Chen*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In natural language processing tasks, pure reinforcement learning (RL)
fine-tuning methods often suffer from inefficient exploration and slow
convergence; while supervised fine-tuning (SFT) methods, although efficient in
training, have limited performance ceiling and less solid theoretical
foundation compared to RL. To address efficiency-capability trade-off, we
propose the Guess-Think-Answer (GTA) framework that combines the efficiency of
SFT with the capability gains of RL in a unified training paradigm. GTA works
by having the model first produce a provisional guess (optimized via
cross-entropy loss), then reflect on this guess before generating the final
answer, with RL rewards shaping both the final output and the format of the
entire GTA structure. This hybrid approach achieves both faster convergence
than pure RL and higher performance ceiling than pure SFT. To mitigate gradient
conflicts between the two training signals, we employ loss masking and gradient
constraints. Empirical results on four text classification benchmarks
demonstrate that GTA substantially accelerates convergence while outperforming
both standalone SFT and RL baselines.

</details>


### [82] [CBP-Tuning: Efficient Local Customization for Black-box Large Language Models](https://arxiv.org/abs/2509.12112)
*Jiaxuan Zhao,Naibin Gu,Yuchen Feng,Xiyu Liu,Peng Fu,Zheng Lin,Weiping Wang*

Main category: cs.CL

TL;DR: CBP-Tuning是一个新颖的黑盒提示调优框架，通过在服务器端训练提示生成器捕获领域能力，在用户端进行无需梯度的优化，实现高效本地定制化同时保护双向隐私。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型定制化成本高的问题，以及云端服务模式下提供商难以支持大规模个性化定制、用户面临隐私风险的双重挑战。

Method: 两阶段框架：1）服务器端训练提示生成器捕获领域特定和任务无关能力；2）用户端进行无需梯度的优化，为单个任务定制软提示，无需访问模型权重或上传隐私数据。

Result: 在常识推理、医疗和金融领域设置中的评估显示，CBP-Tuning相比基线方法表现出优越性能，在任务无关处理和隐私保护方面具有优势。

Conclusion: CBP-Tuning框架通过创新的双向隐私保护方法，有效解决了LLM定制化的成本和隐私问题，仅需每个任务一个定制化向量即可实现有效适配。

Abstract: The high costs of customizing large language models (LLMs) fundamentally
limit their adaptability to user-specific needs. Consequently, LLMs are
increasingly offered as cloud-based services, a paradigm that introduces
critical limitations: providers struggle to support personalized customization
at scale, while users face privacy risks when exposing sensitive data. To
address this dual challenge, we propose Customized Black-box Prompt Tuning
(CBP-Tuning), a novel framework that facilitates efficient local customization
while preserving bidirectional privacy. Specifically, we design a two-stage
framework: (1) a prompt generator trained on the server-side to capture
domain-specific and task-agnostic capabilities, and (2) user-side gradient-free
optimization that tailors soft prompts for individual tasks. This approach
eliminates the need for users to access model weights or upload private data,
requiring only a single customized vector per task while achieving effective
adaptation. Furthermore, the evaluation of CBP-Tuning in the commonsense
reasoning, medical and financial domain settings demonstrates superior
performance compared to baselines, showcasing its advantages in task-agnostic
processing and privacy preservation.

</details>


### [83] [XplaiNLP at CheckThat! 2025: Multilingual Subjectivity Detection with Finetuned Transformers and Prompt-Based Inference with Large Language Models](https://arxiv.org/abs/2509.12130)
*Ariana Sahitaj,Jiaao Li,Pia Wenzel Neves,Fedor Splitt,Premtim Sahitaj,Charlott Jakob,Veronika Solopova,Vera Schmitt*

Main category: cs.CL

TL;DR: XplaiNLP在CheckThat! 2025多语言主观性检测任务中提交了两种方法：监督微调transformer编码器和零样本提示LLMs，在意大利语单语任务中获得第一名，罗马尼亚语零样本设置中获得第三名。


<details>
  <summary>Details</summary>
Motivation: 解决多语言主观性检测任务，特别是在低资源语言和零样本场景下的性能挑战，探索不同方法在不同语言环境下的有效性。

Method: 使用两种方法：(1) 对EuroBERT、XLM-RoBERTa和German-BERT进行监督微调，使用单语和机器翻译训练数据；(2) 零样本提示使用o3-mini进行基于规则的标注，gpt-4.1-mini进行对比重写和比较推理。

Result: 意大利语单语任务F1分数0.8104（第一名，基线0.6941）；罗马尼亚语零样本设置F1分数0.7917（第三名，基线0.6461）；多语言任务表现可靠；希腊语超越基线；德语表现有竞争力；乌克兰语和波兰语略低于基线。

Conclusion: 监督微调方法在资源充足语言中表现优异，零样本方法在特定语言中有效，但在低资源跨语言场景中泛化仍具挑战性，需要针对不同语言特点采用不同策略。

Abstract: This notebook reports the XplaiNLP submission to the CheckThat! 2025 shared
task on multilingual subjectivity detection. We evaluate two approaches: (1)
supervised fine-tuning of transformer encoders, EuroBERT, XLM-RoBERTa, and
German-BERT, on monolingual and machine-translated training data; and (2)
zero-shot prompting using two LLMs: o3-mini for Annotation (rule-based
labelling) and gpt-4.1-mini for DoubleDown (contrastive rewriting) and
Perspective (comparative reasoning). The Annotation Approach achieves 1st place
in the Italian monolingual subtask with an F_1 score of 0.8104, outperforming
the baseline of 0.6941. In the Romanian zero-shot setting, the fine-tuned
XLM-RoBERTa model obtains an F_1 score of 0.7917, ranking 3rd and exceeding the
baseline of 0.6461. The same model also performs reliably in the multilingual
task and improves over the baseline in Greek. For German, a German-BERT model
fine-tuned on translated training data from typologically related languages
yields competitive performance over the baseline. In contrast, performance in
the Ukrainian and Polish zero-shot settings falls slightly below the respective
baselines, reflecting the challenge of generalization in low-resource
cross-lingual scenarios.

</details>


### [84] [Pun Unintended: LLMs and the Illusion of Humor Understanding](https://arxiv.org/abs/2509.12158)
*Alessandro Zangari,Matteo Marcuzzo,Andrea Albarelli,Mohammad Taher Pilehvar,Jose Camacho-Collados*

Main category: cs.CL

TL;DR: 论文指出虽然大语言模型在双关语检测方面有潜力，但其理解往往停留在表面层次，缺乏人类理解的细微差别。通过系统分析和重构现有双关语基准测试，研究发现微小的双关语变化就足以误导大语言模型。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型对双关语的理解深度，揭示其与人类理解的差距，探索模型在处理语言幽默方面的局限性。

Method: 系统分析和重构现有双关语基准测试，进行细微的双关语变化，并对最新的大语言模型进行人类评估。

Result: 研究表明大语言模型对双关语的理解较为肤浅，微小的语言变化就能误导模型，显示出在处理语言幽默方面的鲁棒性挑战。

Conclusion: 大语言模型在双关语理解方面仍存在显著局限，需要更深入的语言理解和上下文感知能力来达到人类水平的幽默理解。

Abstract: Puns are a form of humorous wordplay that exploits polysemy and phonetic
similarity. While LLMs have shown promise in detecting puns, we show in this
paper that their understanding often remains shallow, lacking the nuanced grasp
typical of human interpretation. By systematically analyzing and reformulating
existing pun benchmarks, we demonstrate how subtle changes in puns are
sufficient to mislead LLMs. Our contributions include comprehensive and nuanced
pun detection benchmarks, human evaluation across recent LLMs, and an analysis
of the robustness challenges these models face in processing puns.

</details>


### [85] [RAGs to Riches: RAG-like Few-shot Learning for Large Language Model Role-playing](https://arxiv.org/abs/2509.12168)
*Timothy Rupprecht,Enfu Nan,Arash Akbari,Arman Akbari,Lei Lu,Priyanka Maan,Sean Duffy,Pu Zhao,Yumei He,David Kaeli,Yanzhi Wang*

Main category: cs.CL

TL;DR: 提出了RAGs-to-Riches提示框架，通过检索增强生成方法改进LLM角色扮演性能，在面对敌对用户时能保持角色一致性并提高真实性


<details>
  <summary>Details</summary>
Motivation: 现有少样本学习方法在LLM角色扮演中容易导致模型脱离角色，特别是在与敌对用户互动时可能产生有害行为，需要更稳健的解决方案

Method: 将LLM角色扮演重新表述为文本检索问题，利用精心策划的参考演示来调节LLM响应，引入IOO和IOR两个新颖的token级ROUGE指标进行评估

Result: 在与敌对用户模拟互动中，该方法在推理过程中从参考演示中平均多整合35%的token，在453次角色扮演互动中被一致评为更真实且更频繁保持角色一致

Conclusion: 该方法为构建稳健、人类对齐的LLM角色扮演框架提供了可扩展策略

Abstract: Role-playing Large language models (LLMs) are increasingly deployed in
high-stakes domains such as healthcare, education, and governance, where
failures can directly impact user trust and well-being. A cost effective
paradigm for LLM role-playing is few-shot learning, but existing approaches
often cause models to break character in unexpected and potentially harmful
ways, especially when interacting with hostile users. Inspired by
Retrieval-Augmented Generation (RAG), we reformulate LLM role-playing into a
text retrieval problem and propose a new prompting framework called
RAGs-to-Riches, which leverages curated reference demonstrations to condition
LLM responses. We evaluate our framework with LLM-as-a-judge preference voting
and introduce two novel token-level ROUGE metrics: Intersection over Output
(IOO) to quantity how much an LLM improvises and Intersection over References
(IOR) to measure few-shot demonstrations utilization rate during the evaluation
tasks. When simulating interactions with a hostile user, our prompting strategy
incorporates in its responses during inference an average of 35% more tokens
from the reference demonstrations. As a result, across 453 role-playing
interactions, our models are consistently judged as being more authentic, and
remain in-character more often than zero-shot and in-context Learning (ICL)
methods. Our method presents a scalable strategy for building robust,
human-aligned LLM role-playing frameworks.

</details>


### [86] [Preservation of Language Understanding Capabilities in Speech-aware Large Language Models](https://arxiv.org/abs/2509.12171)
*Marek Kubis,Paweł Skórzewski,Iwona Christop,Mateusz Czyżnikiewicz,Jakub Kubiak,Łukasz Bondaruk,Marcin Lewandowski*

Main category: cs.CL

TL;DR: C3T是一个新的基准测试，用于评估语音感知大语言模型的性能，通过文本任务和语音克隆技术来量化模型在语音输入时语言理解能力的保持程度。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏评估语音感知大语言模型在语音输入模式下语言理解能力保持程度的标准化基准，需要量化模型在不同说话人群体间的公平性和跨模态鲁棒性。

Method: 利用文本任务和语音克隆文本转语音模型，将文本输入转换为语音输入，系统性地测试模型在语音模态下的性能表现。

Result: 开发了C3T基准测试框架，能够量化模型在语音输入时的能力保持程度，并评估模型对不同说话人群体的公平性。

Conclusion: C3T提供了一个有效的基准测试工具，有助于评估和改进语音感知大语言模型的跨模态能力和公平性。

Abstract: The paper presents C3T (Cross-modal Capabilities Conservation Test), a new
benchmark for assessing the performance of speech-aware large language models.
The benchmark utilizes textual tasks and a voice cloning text-to-speech model
to quantify the extent to which language understanding capabilities are
preserved when the model is accessed via speech input. C3T quantifies the
fairness of the model for different categories of speakers and its robustness
across text and speech modalities.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [87] [DSRAG: A Domain-Specific Retrieval Framework Based on Document-derived Multimodal Knowledge Graph](https://arxiv.org/abs/2509.10467)
*Mengzheng Yang,Yanfei Ren,David Osei Opoku,Ruochang Li,Peng Ren,Chunxiao Xing*

Main category: cs.IR

TL;DR: 提出了DSRAG框架，通过多模态知识图谱增强检索增强生成，提升领域特定问答的准确性和可靠性


<details>
  <summary>Details</summary>
Motivation: 解决通用大语言模型在领域特定任务中的知识幻觉和领域适应性不足问题，传统RAG在领域知识准确性和上下文建模方面仍有局限

Method: 利用领域特定文档构建多模态知识图谱（包含文本、图像、表格），引入语义剪枝和结构化子图检索机制，结合知识图谱上下文和向量检索结果

Result: 使用Langfuse多维评分机制评估显示，该方法在领域特定问答方面表现优异

Conclusion: 多模态知识图谱与检索增强生成的整合有效提升了领域特定问答性能

Abstract: Current general-purpose large language models (LLMs) commonly exhibit
knowledge hallucination and insufficient domain-specific adaptability in
domain-specific tasks, limiting their effectiveness in specialized question
answering scenarios. Retrieval-augmented generation (RAG) effectively tackles
these challenges by integrating external knowledge to enhance accuracy and
relevance. However, traditional RAG still faces limitations in domain knowledge
accuracy and context modeling.To enhance domain-specific question answering
performance, this work focuses on a graph-based RAG framework, emphasizing the
critical role of knowledge graph quality during the generation process. We
propose DSRAG (Domain-Specific RAG), a multimodal knowledge graph-driven
retrieval-augmented generation framework designed for domain-specific
applications. Our approach leverages domain-specific documents as the primary
knowledge source, integrating heterogeneous information such as text, images,
and tables to construct a multimodal knowledge graph covering both conceptual
and instance layers. Building on this foundation, we introduce semantic pruning
and structured subgraph retrieval mechanisms, combining knowledge graph context
and vector retrieval results to guide the language model towards producing more
reliable responses. Evaluations using the Langfuse multidimensional scoring
mechanism show that our method excels in domain-specific question answering,
validating the efficacy of integrating multimodal knowledge graphs with
retrieval-augmented generation.

</details>


### [88] [Learning Decomposed Contextual Token Representations from Pretrained and Collaborative Signals for Generative Recommendation](https://arxiv.org/abs/2509.10468)
*Yifan Liu,Yaokun Liu,Zelin Li,Zhenrui Yue,Gyuseok Lee,Ruichen Yao,Yang Zhang,Dong Wang*

Main category: cs.IR

TL;DR: DECOR是一个统一的生成式推荐框架，通过上下文标记组合和分解嵌入融合来解决语义重建与用户交互建模之间的目标不一致问题，在三个真实数据集上超越了现有最佳基线。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐器的两阶段范式存在目标不一致问题：标记化阶段优化语义重建，而推荐器训练阶段优化用户交互建模，导致静态标记化效果不佳和预训练语义知识丢失。

Method: 提出DECOR框架，包含上下文标记组合（根据用户交互上下文精炼标记嵌入）和分解嵌入融合（将预训练码本嵌入与新学习的协同嵌入集成）。

Result: 在三个真实世界数据集上的实验表明，DECOR在推荐性能上持续优于最先进的基线方法。

Conclusion: DECOR通过统一框架有效解决了生成式推荐中的目标不一致问题，既保留了预训练语义又增强了标记嵌入的适应性。

Abstract: Recent advances in generative recommenders adopt a two-stage paradigm: items
are first tokenized into semantic IDs using a pretrained tokenizer, and then
large language models (LLMs) are trained to generate the next item via
sequence-to-sequence modeling. However, these two stages are optimized for
different objectives: semantic reconstruction during tokenizer pretraining
versus user interaction modeling during recommender training. This objective
misalignment leads to two key limitations: (i) suboptimal static tokenization,
where fixed token assignments fail to reflect diverse usage contexts; and (ii)
discarded pretrained semantics, where pretrained knowledge - typically from
language model embeddings - is overwritten during recommender training on user
interactions. To address these limitations, we propose to learn DEcomposed
COntextual Token Representations (DECOR), a unified framework that preserves
pretrained semantics while enhancing the adaptability of token embeddings.
DECOR introduces contextualized token composition to refine token embeddings
based on user interaction context, and decomposed embedding fusion that
integrates pretrained codebook embeddings with newly learned collaborative
embeddings. Experiments on three real-world datasets demonstrate that DECOR
consistently outperforms state-of-the-art baselines in recommendation
performance. Our code will be made available upon publication.

</details>


### [89] [Real-Time RAG for the Identification of Supply Chain Vulnerabilities](https://arxiv.org/abs/2509.10469)
*Jesse Ponnock,Grace Kenneally,Michael Robert Briggs,Elinor Yeo,Tyrone Patterson III,Nicholas Kinberg,Matthew Kalinowski,David Hechtman*

Main category: cs.IR

TL;DR: 该研究提出了一种结合RAG预处理、检索技术和网络爬虫的创新方法，用于供应链分析的时效性提升，通过实验验证了嵌入检索模型微调对性能提升的关键作用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的知识受限于训练截止日期，无法处理新兴及时信息，而供应链分析需要持续更新的海量数据，因此需要解决将新信息及时整合到增强LLM中的延迟问题。

Method: 集成新兴的检索增强生成(RAG)预处理和检索技术与先进的网络爬虫技术，通过实验评估这些技术的组合效果，包括嵌入检索模型微调、自适应迭代检索等方法。

Result: 嵌入检索模型微调始终提供最显著的性能提升，自适应迭代检索在复杂供应链查询上表现更佳，而LLM微调改进有限且资源成本更高，向下查询抽象在实践中显著优于向上抽象。

Conclusion: 在供应链分析中应用RAG系统时，检索质量至关重要，嵌入检索模型微调是最有效的性能提升方法，自适应检索策略能进一步优化复杂查询处理。

Abstract: New technologies in generative AI can enable deeper analysis into our
nation's supply chains but truly informative insights require the continual
updating and aggregation of massive data in a timely manner. Large Language
Models (LLMs) offer unprecedented analytical opportunities however, their
knowledge base is constrained to the models' last training date, rendering
these capabilities unusable for organizations whose mission impacts rely on
emerging and timely information. This research proposes an innovative approach
to supply chain analysis by integrating emerging Retrieval-Augmented Generation
(RAG) preprocessing and retrieval techniques with advanced web-scraping
technologies. Our method aims to reduce latency in incorporating new
information into an augmented-LLM, enabling timely analysis of supply chain
disruptors. Through experimentation, this study evaluates the combinatorial
effects of these techniques towards timeliness and quality trade-offs. Our
results suggest that in applying RAG systems to supply chain analysis,
fine-tuning the embedding retrieval model consistently provides the most
significant performance gains, underscoring the critical importance of
retrieval quality. Adaptive iterative retrieval, which dynamically adjusts
retrieval depth based on context, further enhances performance, especially on
complex supply chain queries. Conversely, fine-tuning the LLM yields limited
improvements and higher resource costs, while techniques such as downward query
abstraction significantly outperforms upward abstraction in practice.

</details>


### [90] [ReFineG: Synergizing Small Supervised Models and LLMs for Low-Resource Grounded Multimodal NER](https://arxiv.org/abs/2509.10975)
*Jielong Tang,Shuang Wang,Zhenxing Wang,Jianxing Yu,Jian Yin*

Main category: cs.IR

TL;DR: ReFineG是一个三阶段协作框架，结合小监督模型和冻结的MLLMs来解决低资源GMNER问题，在CCKS2025竞赛中获得第二名


<details>
  <summary>Details</summary>
Motivation: 现有监督方法依赖昂贵的多模态标注且在低资源领域表现不佳，MLLMs存在领域知识冲突问题，需要解决这些挑战

Method: 三阶段框架：1)训练阶段通过领域感知NER数据合成策略转移LLM知识；2)精炼阶段基于不确定度机制保留置信预测；3)接地阶段通过类比推理增强视觉接地

Result: 在CCKS2025 GMNER共享任务中排名第二，F1得分0.6461，证明了在有限标注下的有效性

Conclusion: ReFineG框架成功整合了小监督模型和MLLMs的优势，有效解决了低资源GMNER中的领域知识冲突和标注成本问题

Abstract: Grounded Multimodal Named Entity Recognition (GMNER) extends traditional NER
by jointly detecting textual mentions and grounding them to visual regions.
While existing supervised methods achieve strong performance, they rely on
costly multimodal annotations and often underperform in low-resource domains.
Multimodal Large Language Models (MLLMs) show strong generalization but suffer
from Domain Knowledge Conflict, producing redundant or incorrect mentions for
domain-specific entities. To address these challenges, we propose ReFineG, a
three-stage collaborative framework that integrates small supervised models
with frozen MLLMs for low-resource GMNER. In the Training Stage, a domain-aware
NER data synthesis strategy transfers LLM knowledge to small models with
supervised training while avoiding domain knowledge conflicts. In the
Refinement Stage, an uncertainty-based mechanism retains confident predictions
from supervised models and delegates uncertain ones to the MLLM. In the
Grounding Stage, a multimodal context selection algorithm enhances visual
grounding through analogical reasoning. In the CCKS2025 GMNER Shared Task,
ReFineG ranked second with an F1 score of 0.6461 on the online leaderboard,
demonstrating its effectiveness with limited annotations.

</details>


### [91] [Membership Inference Attacks on Recommender System: A Survey](https://arxiv.org/abs/2509.11080)
*Jiajie He,Yuechun Gu,Keke Chen,Xintong Chen*

Main category: cs.IR

TL;DR: 本文是第一篇关于推荐系统成员推理攻击(MIAs)的全面综述，系统梳理了这一新兴领域的最新进展、设计原理、挑战以及攻防策略。


<details>
  <summary>Details</summary>
Motivation: 推荐系统广泛应用于电商、金融、医疗等领域，但研究发现它们容易受到成员推理攻击，导致用户隐私泄露。传统MIAs方法不适用于推荐系统，需要系统性的综述来梳理这一新兴领域。

Method: 通过统一的分类法对不同的推荐系统MIAs进行分类，分析各种方法的特征、优缺点，并探讨相关的防御策略。

Result: 提供了推荐系统MIAs的全面综述，建立了分类框架，识别了现有研究的局限性和差距。

Conclusion: 指出了多个有前景的未来研究方向，为研究社区提供了参考，也为领域外研究者提供了清晰的描述。

Abstract: Recommender systems (RecSys) have been widely applied to various
applications, including E-commerce, finance, healthcare, social media and have
become increasingly influential in shaping user behavior and decision-making,
highlighting their growing impact in various domains. However, recent studies
have shown that RecSys are vulnerable to membership inference attacks (MIAs),
which aim to infer whether user interaction record was used to train a target
model or not. MIAs on RecSys models can directly lead to a privacy breach. For
example, via identifying the fact that a purchase record that has been used to
train a RecSys associated with a specific user, an attacker can infer that
user's special quirks. In recent years, MIAs have been shown to be effective on
other ML tasks, e.g., classification models and natural language processing.
However, traditional MIAs are ill-suited for RecSys due to the unseen posterior
probability. Although MIAs on RecSys form a newly emerging and rapidly growing
research area, there has been no systematic survey on this topic yet. In this
article, we conduct the first comprehensive survey on RecSys MIAs. This survey
offers a comprehensive review of the latest advancements in RecSys MIAs,
exploring the design principles, challenges, attack and defense associated with
this emerging field. We provide a unified taxonomy that categorizes different
RecSys MIAs based on their characterizations and discuss their pros and cons.
Based on the limitations and gaps identified in this survey, we point out
several promising future research directions to inspire the researchers who
wish to follow this area. This survey not only serves as a reference for the
research community but also provides a clear description for researchers
outside this research domain.

</details>


### [92] [SPARK: Adaptive Low-Rank Knowledge Graph Modeling in Hybrid Geometric Spaces for Recommendation](https://arxiv.org/abs/2509.11094)
*Binhao Wang,Yutian Xiao,Maolin Wang,Zhiqi Li,Tianshuo Wei,Ruocheng Guo,Xiangyu Zhao*

Main category: cs.IR

TL;DR: SPARK是一个多阶段知识图谱增强推荐框架，通过低秩分解去噪、混合几何空间学习和流行度感知自适应融合，有效解决噪声、稀疏性和长尾物品推荐问题。


<details>
  <summary>Details</summary>
Motivation: 知识图谱在推荐系统中面临噪声、稀疏性和欧几里得几何对复杂关系结构建模不足的挑战，特别是对长尾实体的表示学习影响严重，现有方法缺乏针对物品流行度的自适应多源信号融合策略。

Method: 1) 使用Tucker低秩分解去噪知识图谱并生成鲁棒实体表示；2) SVD初始化的混合几何GNN同时在欧几里得和双曲空间中学习表示，利用双曲空间对层次结构的建模优势；3) 物品流行度感知的自适应融合策略动态加权协同过滤、精炼KG嵌入和不同几何空间的信号；4) 对比学习对齐多源表示。

Result: 大量实验证明SPARK在性能上显著优于最先进方法，特别是在改善长尾物品推荐方面表现出色。

Conclusion: SPARK提供了一个强大且原则性的知识增强推荐方法，通过系统性解决噪声、稀疏性和几何建模问题，有效提升了推荐系统性能，特别是对长尾物品的处理能力。

Abstract: Knowledge Graphs (KGs) enhance recommender systems but face challenges from
inherent noise, sparsity, and Euclidean geometry's inadequacy for complex
relational structures, critically impairing representation learning, especially
for long-tail entities. Existing methods also often lack adaptive multi-source
signal fusion tailored to item popularity. This paper introduces SPARK, a novel
multi-stage framework systematically tackling these issues. SPARK first employs
Tucker low-rank decomposition to denoise KGs and generate robust entity
representations. Subsequently, an SVD-initialized hybrid geometric GNN
concurrently learns representations in Euclidean and Hyperbolic spaces; the
latter is strategically leveraged for its aptitude in modeling hierarchical
structures, effectively capturing semantic features of sparse, long-tail items.
A core contribution is an item popularity-aware adaptive fusion strategy that
dynamically weights signals from collaborative filtering, refined KG
embeddings, and diverse geometric spaces for precise modeling of both
mainstream and long-tail items. Finally, contrastive learning aligns these
multi-source representations. Extensive experiments demonstrate SPARK's
significant superiority over state-of-the-art methods, particularly in
improving long-tail item recommendation, offering a robust, principled approach
to knowledge-enhanced recommendation. Implementation code is available at
https://github.com/Applied-Machine-Learning-Lab/SPARK.

</details>


### [93] [Understanding the Information Cocoon: A Multidimensional Assessment and Analysis of News Recommendation Systems](https://arxiv.org/abs/2509.11139)
*Xin Wang,Xiaowen Huang,Jitao Sang*

Main category: cs.IR

TL;DR: 本文提出了首个统一的信息茧房评估框架，通过个体同质化和群体极化两个维度分析新闻推荐系统造成的信息茧房问题，并设计了五种轻量级缓解策略。


<details>
  <summary>Details</summary>
Motivation: 个性化新闻推荐系统无意中创造了信息茧房——同质化的信息泡沫，强化了用户偏见并加剧了社会极化。现有研究缺乏全面的评估框架来系统分析这一问题。

Method: 提出多维分析框架：1）个体同质化（通过主题多样性和点击重复率评估）；2）群体极化（通过网络密度和社区开放性评估）。在真实数据集上进行多轮实验，对七种算法进行基准测试。

Result: 通过实验揭示了关键见解，建立了首个统一的信息茧房度量框架，并为七种推荐算法提供了基准评估结果。

Conclusion: 这项工作不仅建立了信息茧房的系统评估标准，还提供了可部署的轻量级解决方案，为构建更道德的推荐系统提供了实用工具和方法。

Abstract: Personalized news recommendation systems inadvertently create information
cocoons--homogeneous information bubbles that reinforce user biases and amplify
societal polarization. To address the lack of comprehensive assessment
frameworks in prior research, we propose a multidimensional analysis that
evaluates cocoons through dual perspectives: (1) Individual homogenization via
topic diversity (including the number of topic categories and category
information entropy) and click repetition; (2) Group polarization via network
density and community openness. Through multi-round experiments on real-world
datasets, we benchmark seven algorithms and reveal critical insights.
Furthermore, we design five lightweight mitigation strategies. This work
establishes the first unified metric framework for information cocoons and
delivers deployable solutions for ethical recommendation systems.

</details>


### [94] [Do Large Language Models Favor Recent Content? A Study on Recency Bias in LLM-Based Reranking](https://arxiv.org/abs/2509.11353)
*Hanpei Fang,Sijie Tao,Nuo Chen,Kai-Xin Chang,Tetsuya Sakai*

Main category: cs.IR

TL;DR: 研究发现大型语言模型在信息检索中存在显著的近因偏差，倾向于优先选择较新的文档，即使相关性相同。这种偏差在所有测试模型中普遍存在，需要通过有效策略进行缓解。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地应用于信息系统，包括作为信息检索流程中的重排序器，但其对近期偏好的敏感性尚未得到充分研究。

Method: 通过在TREC深度学习检索集合的段落前添加人工发布日期，测试了7个LLM模型（包括GPT-3.5-turbo、GPT-4o、GPT-4、LLaMA-3和Qwen-2.5）在列表式重排序和成对偏好实验中的表现。

Result: 所有模型都表现出明显的近因偏差，"新鲜"段落被持续提升，Top-10的平均出版年份前移最多达4.78年，个别项目排名变化多达95位。在成对偏好实验中，日期注入可使偏好逆转平均达25%。

Conclusion: 研究提供了LLM中普遍存在近因偏差的定量证据，强调了制定有效偏差缓解策略的重要性。尽管较大模型能减弱这种效应，但没有模型能完全消除它。

Abstract: Large language models (LLMs) are increasingly deployed in information
systems, including being used as second-stage rerankers in information
retrieval pipelines, yet their susceptibility to recency bias has received
little attention. We investigate whether LLMs implicitly favour newer documents
by prepending artificial publication dates to passages in the TREC Deep
Learning passage retrieval collections in 2021 (DL21) and 2022 (DL22). Across
seven models, GPT-3.5-turbo, GPT-4o, GPT-4, LLaMA-3 8B/70B, and Qwen-2.5
7B/72B, "fresh" passages are consistently promoted, shifting the Top-10's mean
publication year forward by up to 4.78 years and moving individual items by as
many as 95 ranks in our listwise reranking experiments. Although larger models
attenuate the effect, none eliminate it. We also observe that the preference of
LLMs between two passages with an identical relevance level can be reversed by
up to 25% on average after date injection in our pairwise preference
experiments. These findings provide quantitative evidence of a pervasive
recency bias in LLMs and highlight the importance of effective bias-mitigation
strategies.

</details>


### [95] [Decoding in Latent Spaces for Efficient Inference in LLM-based Recommendation](https://arxiv.org/abs/2509.11524)
*Chengbing Wang,Yang Zhang,Zhicheng Wang,Tianhao Shi,Keqin Bao,Fuli Feng,Tat-Seng Chua*

Main category: cs.IR

TL;DR: L2D是一种高效的潜在空间解码方法，通过直接匹配候选物品与LLM内部思维表示来绕过语言空间的自回归解码，实现10倍以上的加速同时保持或提升推荐性能


<details>
  <summary>Details</summary>
Motivation: 传统生成式推荐方法在语言空间进行自回归解码会产生显著推理开销，需要寻找更高效的解码方式来降低计算成本

Method: 使用测试序列的隐藏状态表示用户偏好物品，从训练序列的隐藏状态获取候选物品表示，然后在潜在空间匹配这两种表示来解码物品

Result: L2D比语言空间解码快10倍以上，同时保持或提升了推荐性能

Conclusion: L2D提供了一种高效的潜在空间解码方案，在不改变LLM生成式调优范式的情况下实现了显著的推理加速

Abstract: Fine-tuning large language models (LLMs) for recommendation in a generative
manner has delivered promising results, but encounters significant inference
overhead due to autoregressive decoding in the language space. This work
explores bypassing language-space decoding by directly matching candidate items
with the LLM's internal thought representations in the latent space,
eliminating the time-consuming autoregressive process to reduce computational
costs. Towards this, we introduce Light Latent-space Decoding (L2D), an
effective and efficient latent-space decoding method. L2D represents
user-preferred items by using the hidden states of test sequences reflecting
the LLM's internal thought, and obtains candidate item representations from the
hidden states of training sequences labeled with the corresponding candidate
items. It then matches the two types of representations to decode items,
achieving latent-space decoding. In this way, it enables efficient decoding
without altering the LLM's generative tuning paradigm, thereby preserving
performance. Extensive empirical results demonstrate that L2D is more than 10x
faster than language-space decoding while maintaining or enhancing performance.

</details>


### [96] [Data-Driven Analysis of Text-Conditioned AI-Generated Music: A Case Study with Suno and Udio](https://arxiv.org/abs/2509.11824)
*Luca Casini,Laura Cros Vila,David Dalmazzo,Anna-Kaisa Kaila,Bob L. T. Sturm*

Main category: cs.IR

TL;DR: 本文通过对Suno和Udio平台2024年5月至10月用户生成的音乐作品进行大规模分析，使用先进的文本嵌入模型、降维和聚类方法，揭示了AI音乐创作的主题偏好、语言选择、提示策略等使用模式。


<details>
  <summary>Details</summary>
Motivation: 随着AI音乐平台如Suno和Udio的普及，数百万人使用这些工具创作音乐，部分作品甚至出现在广告和音乐排行榜中。研究旨在了解这些平台的实际使用情况、用户创作主题以及提示策略。

Method: 采用最先进的文本嵌入模型结合降维和聚类技术，对用户生成的提示词、标签和歌词进行分析，并通过交互式图表自动标注和展示处理后的数据。

Result: 研究揭示了歌词中的突出主题、语言偏好、提示策略，以及用户通过元标签引导模型的特殊尝试。发现了AI音乐创作的文化实践模式。

Conclusion: 为了促进AI生成音乐这一新兴文化实践的音乐学研究，作者分享了代码和资源，为后续研究提供基础。

Abstract: Online AI platforms for creating music from text prompts (AI music), such as
Suno and Udio, are now being used by hundreds of thousands of users. Some AI
music is appearing in advertising, and even charting, in multiple countries.
How are these platforms being used? What subjects are inspiring their users?
This article answers these questions for Suno and Udio using a large collection
of songs generated by users of these platforms from May to October 2024. Using
a combination of state-of-the-art text embedding models, dimensionality
reduction and clustering methods, we analyze the prompts, tags and lyrics, and
automatically annotate and display the processed data in interactive plots. Our
results reveal prominent themes in lyrics, language preference, prompting
strategies, as well as peculiar attempts at steering models through the use of
metatags. To promote the musicological study of the developing cultural
practice of AI-generated music we share our code and resources.

</details>


### [97] [AEFS: Adaptive Early Feature Selection for Deep Recommender Systems](https://arxiv.org/abs/2509.12076)
*Fan Hu,Gaofeng Lu,Jun Chen,Chaonan Guo,Yuekui Yang,Xirong Li*

Main category: cs.IR

TL;DR: 提出自适应早期特征选择(AEFS)方法，在保持自适应特征选择性能的同时，显著减少嵌入层激活参数37.5%


<details>
  <summary>Details</summary>
Motivation: 解决自适应特征选择方法在推荐系统中嵌入层参数过多、效率低下的问题，特别是针对大规模推荐系统数十亿ID类特征的情况

Method: 采用双模型架构：辅助模型负责特征选择，主模型负责预测，通过两个协作训练损失约束确保模型对齐

Result: 在三个基准数据集上的实验验证，AEFS在性能上与最先进的自适应晚期特征选择方法相当，同时嵌入层激活参数减少37.5%

Conclusion: AEFS是一种简单有效的方法，既能自适应选择特征，又能显著减少嵌入层参数，适用于大规模推荐系统

Abstract: Feature selection has emerged as a crucial technique in refining recommender
systems. Recent advancements leveraging Automated Machine Learning (AutoML) has
drawn significant attention, particularly in two main categories: early feature
selection and late feature selection, differentiated by whether the selection
occurs before or after the embedding layer. The early feature selection selects
a fixed subset of features and retrains the model, while the late feature
selection, known as adaptive feature selection, dynamically adjusts feature
choices for each data instance, recognizing the variability in feature
significance. Although adaptive feature selection has shown remarkable
improvements in performance, its main drawback lies in its post-embedding layer
feature selection. This process often becomes cumbersome and inefficient in
large-scale recommender systems with billions of ID-type features, leading to a
highly sparse and parameter-heavy embedding layer. To overcome this, we
introduce Adaptive Early Feature Selection (AEFS), a very simple method that
not only adaptively selects informative features for each instance, but also
significantly reduces the activated parameters of the embedding layer. AEFS
employs a dual-model architecture, encompassing an auxiliary model dedicated to
feature selection and a main model responsible for prediction. To ensure
effective alignment between these two models, we incorporate two collaborative
training loss constraints. Our extensive experiments on three benchmark
datasets validate the efficiency and effectiveness of our approach. Notably,
AEFS matches the performance of current state-of-theart Adaptive Late Feature
Selection methods while achieving a significant reduction of 37. 5% in the
activated parameters of the embedding layer. AEFS is open-source at
https://github. com/fly-dragon211/AEFS .

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [98] [The 1st International Workshop on Disentangled Representation Learning for Controllable Generation (DRL4Real): Methods and Results](https://arxiv.org/abs/2509.10463)
*Qiuyu Chen,Xin Jin,Yue Song,Xihui Liu,Shuai Yang,Tao Yang,Ziqiang Li,Jianguo Huang,Yuntao Wei,Ba'ao Xie,Nicu Sebe,Wenjun,Zeng,Jooyeol Yun,Davide Abati,Mohamed Omran,Jaegul Choo,Amir Habibian,Auke Wiggers,Masato Kobayashi,Ning Ding,Toru Tamaki,Marzieh Gheisari,Auguste Genovesio,Yuheng Chen,Dingkun Liu,Xinyao Yang,Xinping Xu,Baicheng Chen,Dongrui Wu,Junhao Geng,Lexiang Lv,Jianxin Lin,Hanzhe Liang,Jie Zhou,Xuanxin Chen,Jinbao Wang,Can Gao,Zhangyi Wang,Zongze Li,Bihan Wen,Yixin Gao,Xiaohan Pan,Xin Li,Zhibo Chen,Baorui Peng,Zhongming Chen,Haoran Jin*

Main category: cs.LG

TL;DR: 本文总结了ICCV 2025第一届可控制生成解耦表示学习国际研讨会(DRL4Real)，该研讨会旨在将解耦表示学习从理论推向实际应用，重点关注可控生成、模型鲁棒性和可解释性等实用场景。


<details>
  <summary>Details</summary>
Motivation: 弥合解耦表示学习(DRL)的理论潜力与其在现实场景中应用之间的差距，超越合成基准测试，推动DRL在实际应用中的发展。

Method: 通过研讨会形式汇集9篇论文，涵盖新颖归纳偏置（如语言）、扩散模型在DRL中的应用、3D感知解耦、以及DRL在自动驾驶和EEG分析等专业领域的扩展。

Result: 研讨会成功展示了DRL在多个实际应用领域的进展，包括可控生成、模型鲁棒性提升和跨领域泛化能力。

Conclusion: DRL4Real研讨会为解耦表示学习从理论到实践的转化提供了重要平台，推动了该领域在真实世界应用中的发展，并为未来研究指明了方向。

Abstract: This paper reviews the 1st International Workshop on Disentangled
Representation Learning for Controllable Generation (DRL4Real), held in
conjunction with ICCV 2025. The workshop aimed to bridge the gap between the
theoretical promise of Disentangled Representation Learning (DRL) and its
application in realistic scenarios, moving beyond synthetic benchmarks.
DRL4Real focused on evaluating DRL methods in practical applications such as
controllable generation, exploring advancements in model robustness,
interpretability, and generalization. The workshop accepted 9 papers covering a
broad range of topics, including the integration of novel inductive biases
(e.g., language), the application of diffusion models to DRL, 3D-aware
disentanglement, and the expansion of DRL into specialized domains like
autonomous driving and EEG analysis. This summary details the workshop's
objectives, the themes of the accepted papers, and provides an overview of the
methodologies proposed by the authors.

</details>


### [99] [Moment Estimates and DeepRitz Methods on Learning Diffusion Systems with Non-gradient Drifts](https://arxiv.org/abs/2509.10495)
*Fanze Kong,Chen-Chih Lai,Yubin Lu*

Main category: cs.LG

TL;DR: 提出了一种数据驱动的两阶段方法Moment-DeepRitz Method，用于学习包含保守-耗散动力学的广义扩散系统中的漂移分解，该方法对噪声数据具有鲁棒性，并能适应粗糙势和振荡旋转。


<details>
  <summary>Details</summary>
Motivation: 保守-耗散动力学在各种复杂开放系统中普遍存在，需要开发能够有效处理这类系统的数据驱动方法。

Method: 采用两阶段的Moment-DeepRitz方法，通过数据驱动方式学习广义扩散系统中的漂移分解，该方法能够处理噪声数据并适应粗糙势和振荡旋转条件。

Result: 通过多个数值实验验证了该方法的有效性，展示了其在处理保守-耗散动力学系统方面的优越性能。

Conclusion: Moment-DeepRitz方法为学习广义扩散系统中的漂移分解提供了一种有效的数据驱动解决方案，具有鲁棒性和适应性强的特点。

Abstract: Conservative-dissipative dynamics are ubiquitous across a variety of complex
open systems. We propose a data-driven two-phase method, the Moment-DeepRitz
Method, for learning drift decompositions in generalized diffusion systems
involving conservative-dissipative dynamics. The method is robust to noisy
data, adaptable to rough potentials and oscillatory rotations. We demonstrate
its effectiveness through several numerical experiments.

</details>


### [100] [SOH-KLSTM: A Hybrid Kolmogorov-Arnold Network and LSTM Model for Enhanced Lithium-Ion Battery Health Monitoring](https://arxiv.org/abs/2509.10496)
*Imen Jarraya,Safa Ben Atitallah,Fatimah Alahmeda,Mohamed Abdelkadera,Maha Drissa,Fatma Abdelhadic,Anis Koubaaa*

Main category: cs.LG

TL;DR: 提出了一种基于KAN-LSTM混合框架的锂离子电池健康状态(SOH)预测新方法，结合LSTM的长时序依赖学习能力和KAN的非线性逼近能力


<details>
  <summary>Details</summary>
Motivation: 传统SOH估计方法无法有效捕捉电池退化的非线性和时序特性，需要更准确可靠的SOH估计来确保电池应用的安全性、寿命和性能

Method: 提出SOH-KLSTM框架，将Kolmogorov-Arnold Network(KAN)集成到LSTM的候选细胞状态中，结合两者的优势进行电池健康监测

Result: 该方法能够有效捕捉锂离子电池复杂的退化行为，提高SOH预测的准确性

Conclusion: KAN-LSTM混合框架为锂离子电池健康监测提供了一种有效的解决方案，能够更好地处理电池退化的非线性和时序特征

Abstract: Accurate and reliable State Of Health (SOH) estimation for Lithium (Li)
batteries is critical to ensure the longevity, safety, and optimal performance
of applications like electric vehicles, unmanned aerial vehicles, consumer
electronics, and renewable energy storage systems. Conventional SOH estimation
techniques fail to represent the non-linear and temporal aspects of battery
degradation effectively. In this study, we propose a novel SOH prediction
framework (SOH-KLSTM) using Kolmogorov-Arnold Network (KAN)-Integrated
Candidate Cell State in LSTM for Li batteries Health Monitoring. This hybrid
approach combines the ability of LSTM to learn long-term dependencies for
accurate time series predictions with KAN's non-linear approximation
capabilities to effectively capture complex degradation behaviors in Lithium
batteries.

</details>


### [101] [Exploring Multi-view Symbolic Regression methods in physical sciences](https://arxiv.org/abs/2509.10500)
*Etienne Russeil,Fabrício Olivetti de França,Konstantin Malanchev,Guillaume Moinard,Maxime Cherrey*

Main category: cs.LG

TL;DR: 本文比较了四种多视图符号回归（MvSR）工具（Operon、PySR、phy-SO和eggp）在真实数据集上的性能，发现它们都能实现良好精度且参数稀疏，但某些特性有助于生成更好的模型。


<details>
  <summary>Details</summary>
Motivation: 传统上通过第一性原理推导方程来描述现象，现代方法使用符号回归自动化部分过程。多视图符号回归能够描述同一现象生成的多个数据集，有助于缓解过拟合和数据稀缺问题。

Method: 测试和比较Operon、PySR、phy-SO和eggp四种MvSR工具在不同真实数据集上的性能表现。

Result: 所有工具通常都能达到良好的准确性，并提出只有少量自由参数的解决方案。某些特性能够更频繁地生成更好的模型。

Conclusion: 为未来MvSR的发展提供了指导方针，强调了特定功能对模型性能提升的重要性。

Abstract: Describing the world behavior through mathematical functions help scientists
to achieve a better understanding of the inner mechanisms of different
phenomena. Traditionally, this is done by deriving new equations from first
principles and careful observations. A modern alternative is to automate part
of this process with symbolic regression (SR). The SR algorithms search for a
function that adequately fits the observed data while trying to enforce
sparsity, in the hopes of generating an interpretable equation. A particularly
interesting extension to these algorithms is the Multi-view Symbolic Regression
(MvSR). It searches for a parametric function capable of describing multiple
datasets generated by the same phenomena, which helps to mitigate the common
problems of overfitting and data scarcity. Recently, multiple implementations
added support to MvSR with small differences between them. In this paper, we
test and compare MvSR as supported in Operon, PySR, phy-SO, and eggp, in
different real-world datasets. We show that they all often achieve good
accuracy while proposing solutions with only few free parameters. However, we
find that certain features enable a more frequent generation of better models.
We conclude by providing guidelines for future MvSR developments.

</details>


### [102] [From Noise to Precision: A Diffusion-Driven Approach to Zero-Inflated Precipitation Prediction](https://arxiv.org/abs/2509.10501)
*Wentao Gao,Jiuyong Li,Lin Liu,Thuc Duy Le,Xiongren Chen,Xiaojing Du,Jixue Liu,Yanchang Zhao,Yun Chen*

Main category: cs.LG

TL;DR: 提出了Zero Inflation Diffusion Framework (ZIDF)来处理降水预测中的零膨胀数据问题，通过高斯扰动、Transformer预测和扩散去噪技术，在多个指标上显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 降水预测中零膨胀数据（大量零值和稀疏非零事件）带来了重大挑战，需要专门的方法来处理这种特殊的数据分布

Method: ZIDF框架整合了三个关键技术：高斯扰动用于平滑零膨胀分布、基于Transformer的预测模型捕捉时间模式、扩散去噪技术恢复原始数据结构

Result: 实验显示ZIDF相比最先进的降水预测模型有显著性能提升，相对于基准Non-stationary Transformer，MSE降低56.7%，MAE降低21.1%

Conclusion: ZIDF能够稳健处理稀疏时间序列数据，并具有推广到其他零膨胀问题领域的潜力

Abstract: Zero-inflated data pose significant challenges in precipitation forecasting
due to the predominance of zeros with sparse non-zero events. To address this,
we propose the Zero Inflation Diffusion Framework (ZIDF), which integrates
Gaussian perturbation for smoothing zero-inflated distributions,
Transformer-based prediction for capturing temporal patterns, and
diffusion-based denoising to restore the original data structure. In our
experiments, we use observational precipitation data collected from South
Australia along with synthetically generated zero-inflated data. Results show
that ZIDF demonstrates significant performance improvements over multiple
state-of-the-art precipitation forecasting models, achieving up to 56.7\%
reduction in MSE and 21.1\% reduction in MAE relative to the baseline
Non-stationary Transformer. These findings highlight ZIDF's ability to robustly
handle sparse time series data and suggest its potential generalizability to
other domains where zero inflation is a key challenge.

</details>


### [103] [FEDEXCHANGE: Bridging the Domain Gap in Federated Object Detection for Free](https://arxiv.org/abs/2509.10503)
*Haolin Yuan,Jingtao Li,Weiming Zhuang,Chen Chen,Lingjuan Lyu*

Main category: cs.LG

TL;DR: FEDEXCHANGE是一个联邦目标检测框架，通过服务器端动态模型交换策略解决跨域泛化问题，无需增加客户端计算开销


<details>
  <summary>Details</summary>
Motivation: 联邦目标检测中不同客户端数据存在显著域差异（环境、天气等），现有方法忽略边缘设备硬件限制且计算成本高，限制了实际应用

Method: 在服务器端交替进行模型聚合和模型交换：聚合轮次正常聚合所有本地模型；交换轮次基于距离度量对本地模型进行聚类和交换，使本地模型能从其他域数据中学习

Result: 在挑战性域（如雨天条件）中实现了1.6倍的平均精度提升，同时仅需基线方法0.8倍的计算资源

Conclusion: FEDEXCHANGE有效提升了联邦目标检测的跨域性能，且不增加客户端计算负担，具有很好的实际应用价值

Abstract: Federated Object Detection (FOD) enables clients to collaboratively train a
global object detection model without accessing their local data from diverse
domains. However, significant variations in environment, weather, and other
domain specific factors hinder performance, making cross domain generalization
a key challenge. Existing FOD methods often overlook the hardware constraints
of edge devices and introduce local training regularizations that incur high
computational costs, limiting real-world applicability. In this paper, we
propose FEDEXCHANGE, a novel FOD framework that bridges domain gaps without
introducing additional local computational overhead. FEDEXCHANGE employs a
server side dynamic model exchange strategy that enables each client to gain
insights from other clients' domain data without direct data sharing.
Specifically, FEDEXCHANGE allows the server to alternate between model
aggregation and model exchange. During aggregation rounds, the server
aggregates all local models as usual. In exchange rounds, FEDEXCHANGE clusters
and exchanges local models based on distance measures, allowing local models to
learn from a variety of domains. As all operations are performed on the server
side, clients can achieve improved cross domain utility without any additional
computational overhead. Extensive evaluations demonstrate that FEDEXCHANGE
enhances FOD performance, achieving 1.6X better mean average precision in
challenging domains, such as rainy conditions, while requiring only 0.8X the
computational resources compared to baseline methods.

</details>


### [104] [Retrosynthesis Planning via Worst-path Policy Optimisation in Tree-structured MDPs](https://arxiv.org/abs/2509.10504)
*Mianchu Wang,Giovanni Montana*

Main category: cs.LG

TL;DR: 将逆合成规划重新构建为树结构MDP中的最差路径优化问题，提出InterRetro方法，通过自模仿学习实现100%的基准解决率和更短的合成路线


<details>
  <summary>Details</summary>
Motivation: 现有方法优化平均性能，但逆合成规划对合成路线中最弱环节敏感，需要解决最差路径优化问题

Method: 将逆合成规划建模为树结构MDP的最差路径优化，提出InterRetro方法，通过交互学习价值函数和自模仿策略改进

Result: 在Retro*-190基准上实现100%目标解决率，合成路线缩短4.9%，仅用10%训练数据就达到优异性能

Conclusion: InterRetro代表了计算逆合成规划的重要进展，通过最差路径优化框架提供了理论保证和实际性能提升

Abstract: Retrosynthesis planning aims to decompose target molecules into available
building blocks, forming a synthesis tree where each internal node represents
an intermediate compound and each leaf ideally corresponds to a purchasable
reactant. However, this tree becomes invalid if any leaf node is not a valid
building block, making the planning process vulnerable to the "weakest link" in
the synthetic route. Existing methods often optimise for average performance
across branches, failing to account for this worst-case sensitivity. In this
paper, we reframe retrosynthesis as a worst-path optimisation problem within
tree-structured Markov Decision Processes (MDPs). We prove that this
formulation admits a unique optimal solution and offers monotonic improvement
guarantees. Building on this insight, we introduce Interactive Retrosynthesis
Planning (InterRetro), a method that interacts with the tree MDP, learns a
value function for worst-path outcomes, and improves its policy through
self-imitation, preferentially reinforcing past decisions with high estimated
advantage. Empirically, InterRetro achieves state-of-the-art results, solving
100% of targets on the Retro*-190 benchmark, shortening synthetic routes by
4.9%, and achieving promising performance using only 10% of the training data -
representing a significant advance in computational retrosynthesis planning.

</details>


### [105] [AttnBoost: Retail Supply Chain Sales Insights via Gradient Boosting Perspective](https://arxiv.org/abs/2509.10506)
*Muxin Ge,Hanyu Ma,Yiyang Wu,Xiaoli Ma,Yadi Liu,Ye Aung Moe,Weizheng Xie*

Main category: cs.LG

TL;DR: 提出了AttnBoost框架，将特征级注意力机制集成到梯度提升过程中，用于零售需求预测，在提高预测精度的同时增强可解释性


<details>
  <summary>Details</summary>
Motivation: 传统梯度提升决策树在结构化数据上表现良好，但缺乏自适应机制来识别和强调变化条件下最相关的特征，零售供应链中的产品需求预测面临噪声、异构特征和快速变化的消费者行为的挑战

Method: AttnBoost框架，在每轮提升过程中通过轻量级注意力机制动态调整特征重要性，重点关注促销、定价和季节性趋势等高影响力变量

Result: 在大规模零售销售数据集上评估表明，AttnBoost优于标准机器学习和深度表格模型，消融研究证实注意力模块在减轻过拟合和提高可解释性方面的效用

Conclusion: 注意力引导的提升方法为现实世界预测应用中的可解释和可扩展AI提供了一个有前景的方向

Abstract: Forecasting product demand in retail supply chains presents a complex
challenge due to noisy, heterogeneous features and rapidly shifting consumer
behavior. While traditional gradient boosting decision trees (GBDT) offer
strong predictive performance on structured data, they often lack adaptive
mechanisms to identify and emphasize the most relevant features under changing
conditions. In this work, we propose AttnBoost, an interpretable learning
framework that integrates feature-level attention into the boosting process to
enhance both predictive accuracy and explainability. Specifically, the model
dynamically adjusts feature importance during each boosting round via a
lightweight attention mechanism, allowing it to focus on high-impact variables
such as promotions, pricing, and seasonal trends. We evaluate AttnBoost on a
large-scale retail sales dataset and demonstrate that it outperforms standard
machine learning and deep tabular models, while also providing actionable
insights for supply chain managers. An ablation study confirms the utility of
the attention module in mitigating overfitting and improving interpretability.
Our results suggest that attention-guided boosting represents a promising
direction for interpretable and scalable AI in real-world forecasting
applications.

</details>


### [106] [The Anti-Ouroboros Effect: Emergent Resilience in Large Language Models from Recursive Selective Feedback](https://arxiv.org/abs/2509.10509)
*Sai Teja Reddy Adapala*

Main category: cs.LG

TL;DR: 研究发现选择性反馈机制可以逆转大型语言模型在递归训练中的性能退化现象，称为反自噬效应，在Gemma 2B模型上实现了6.6%的性能提升


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在递归训练中的稳定性问题，挑战现有理论预测的模型崩溃现象

Method: 引入选择性反馈机制，在复杂摘要任务上对Gemma 2B模型进行五代递归训练，设置质量过滤、无过滤和随机过滤三种条件进行对比实验

Result: 质量过滤条件下ROUGE-L F1分数提升了6.6%，而无过滤控制组和随机过滤控制组分别下降了3.5%和4.2%，证明了反自噬效应的存在

Conclusion: 系统性韧性可以是大型语言模型在简单选择压力下涌现的特性，为开发更安全、更鲁棒的AI系统提供了可扩展的原则

Abstract: The stability of recursively trained large language models (LLMs) is a
foundational problem for AI safety. Prevailing theory predicts model collapse,
a progressive degradation when models are trained on their own output. We
challenge this narrative by introducing a selective feedback mechanism.
Contrary to expectation, instead of merely slowing decay, our experiments
provide strong evidence that this pressure reverses it, inducing a
statistically significant performance improvement in a Gemma 2B model on a
complex summarization task. We name this phenomenon the Anti-Ouroboros Effect.
We contrast this with a foundational experiment using a simple classifier,
where the theoretical degenerative loop was validated, highlighting the unique
dynamics of high-dimensional models. Our findings establish that systemic
resilience can be an emergent property of LLMs under simple selection pressure,
suggesting a powerful and scalable principle for developing safer and more
robust AI systems. Across five generations, a quality-filtered condition
improved by 6.6% in ROUGE-L F1 score, whereas an unfiltered control degraded by
3.5% and a random-filter control degraded by 4.2%

</details>


### [107] [LogGuardQ: A Cognitive-Enhanced Reinforcement Learning Framework for Cybersecurity Anomaly Detection in Security Logs](https://arxiv.org/abs/2509.10511)
*Umberto Gonçalves de Sousa*

Main category: cs.LG

TL;DR: LogGuardQ是一个新颖的强化学习框架，整合了人类认知启发的双记忆系统和自适应探索策略，在异常检测任务中显著优于传统DQN和PPO算法。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习算法如DQN和PPO在动态环境中存在探索效率低、稳定性差和适应性不足的问题，需要开发更高效的算法来应对不确定环境中的决策挑战。

Method: 提出LogGuardQ框架，集成受人类认知启发的双记忆系统和基于温度衰减与好奇心驱动的自适应探索策略，在模拟访问日志数据集上进行评估。

Result: 在100万条模拟访问日志（47.9%异常）的20000次episode测试中，LogGuardQ达到96.0%检测率（DQN为93.0%，PPO为47.1%），平均奖励20.34±44.63，统计测试显示显著性能优势。

Conclusion: LogGuardQ通过融合认知科学与强化学习，为不确定环境中的自适应学习提供了可扩展的方法，在网络安全、入侵检测和不确定性决策等领域具有应用潜力。

Abstract: Reinforcement learning (RL) has transformed sequential decision-making, but
traditional algorithms like Deep Q-Networks (DQNs) and Proximal Policy
Optimization (PPO) often struggle with efficient exploration, stability, and
adaptability in dynamic environments. This study presents LogGuardQ (Adaptive
Log Guard with Cognitive enhancement), a novel framework that integrates a
dual-memory system inspired by human cognition and adaptive exploration
strategies driven by temperature decay and curiosity. Evaluated on a dataset of
1,000,000 simulated access logs with 47.9% anomalies over 20,000 episodes,
LogGuardQ achieves a 96.0% detection rate (versus 93.0% for DQN and 47.1% for
PPO), with precision of 0.4776, recall of 0.9996, and an F1-score of 0.6450.
The mean reward is 20.34 \pm 44.63 across all episodes (versus 18.80 \pm 43.98
for DQN and -0.17 \pm 23.79 for PPO), with an average of 5.0 steps per episode
(constant across models). Graphical analyses, including learning curves
smoothed with a Savgol filter (window=501, polynomial=2), variance trends,
action distributions, and cumulative detections, demonstrate LogGuardQ's
superior stability and efficiency. Statistical tests (Mann-Whitney U) confirm
significant performance advantages (e.g., p = 0.0002 vs. DQN with negligible
effect size, p < 0.0001 vs. PPO with medium effect size, and p < 0.0001 for DQN
vs. PPO with small effect size). By bridging cognitive science and RL,
LogGuardQ offers a scalable approach to adaptive learning in uncertain
environments, with potential applications in cybersecurity, intrusion
detection, and decision-making under uncertainty.

</details>


### [108] [A Service-Oriented Adaptive Hierarchical Incentive Mechanism for Federated Learning](https://arxiv.org/abs/2509.10512)
*Jiaxing Cao,Yuzhou Gao,Jiwei Huang*

Main category: cs.LG

TL;DR: 提出一种面向服务的自适应激励机制，通过Stackelberg博弈和多智能体马尔可夫决策过程最大化联邦学习中任务发布者、本地模型所有者和数据收集工作者的效用


<details>
  <summary>Details</summary>
Motivation: 联邦学习中存在训练数据不足的问题，需要招募工作者收集数据，但缺乏有效的激励机制来协调各方利益

Method: 建立LMOs与TP之间的Stackelberg博弈理论模型，使用多智能体MDP建模LMOs与工作者交互，采用深度强化学习寻找最优策略，设计ASOSA算法稳定参与者策略

Result: 通过大量数值实验验证了所提方法的有效性

Conclusion: 该自适应激励机制能够有效解决联邦学习中的数据不足问题，最大化各参与方的效用

Abstract: Recently, federated learning (FL) has emerged as a novel framework for
distributed model training. In FL, the task publisher (TP) releases tasks, and
local model owners (LMOs) use their local data to train models. Sometimes, FL
suffers from the lack of training data, and thus workers are recruited for
gathering data. To this end, this paper proposes an adaptive incentive
mechanism from a service-oriented perspective, with the objective of maximizing
the utilities of TP, LMOs and workers. Specifically, a Stackelberg game is
theoretically established between the LMOs and TP, positioning TP as the leader
and the LMOs as followers. An analytical Nash equilibrium solution is derived
to maximize their utilities. The interaction between LMOs and workers is
formulated by a multi-agent Markov decision process (MAMDP), with the optimal
strategy identified via deep reinforcement learning (DRL). Additionally, an
Adaptively Searching the Optimal Strategy Algorithm (ASOSA) is designed to
stabilize the strategies of each participant and solve the coupling problems.
Extensive numerical experiments are conducted to validate the efficacy of the
proposed method.

</details>


### [109] [Mixture-of-Clustered-Experts: Advancing Expert Specialization and Generalization in Instruction Tuning](https://arxiv.org/abs/2509.10513)
*Sugyeong Eo,Jungjun Lee,Chanjun Park,Heuiseok Lim*

Main category: cs.LG

TL;DR: 提出Mixture-of-Clustered-Experts (MoCE)方法，通过双阶段路由机制解决稀疏MoE架构中专家专业化不足的问题，在保持计算效率的同时提升性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 稀疏MoE架构虽然通过条件激活子模块实现了可扩展性，但在指令调优场景中面对输入异质性时，专家专业化程度不足，影响性能和泛化能力。

Method: 采用双阶段路由机制：第一阶段基于序列级特征进行专家组路由，第二阶段在组内进行token级的top-k专家激活，有效划分异质输入并促进专家组专业化。

Result: 在综合基准测试中表现优于强基线模型，展现出更好的泛化能力、鲁棒性和有效性。

Conclusion: MoCE方法通过创新的双阶段路由设计，成功解决了MoE架构中的专家专业化挑战，为处理异质输入提供了有效的解决方案。

Abstract: A sparse Mixture-of-Experts (MoE) architecture has emerged as a highly
scalable solution by conditionally activating sub-modules without a
proportional increase in computational costs. However, improving expert
specialization to enhance performance and generalization remains a challenge
for MoE, especially in instruction tuning scenarios characterized by
significant input heterogeneity. In this work, we propose the
Mixture-of-Clustered-Experts (MoCE) to address this limitation through a
dual-stage routing mechanism. The first stage in the mechanism performs expert
group routing based on sequence-level features, while the second stage
activates the top-$k$ experts within the group at the token level. This
approach enables the effective partitioning of heterogeneous inputs based on
their knowledge requirements, encouraging expert group specialization while
maintaining the advantages of token-level routing. We evaluate MoCE across a
comprehensive set of benchmarks, demonstrating its consistent superiority over
strong baselines and its enhanced generalization capabilities. Detailed
analysis further highlights the robustness and effectiveness of MoCE.

</details>


### [110] [A Differential Manifold Perspective and Universality Analysis of Continuous Attractors in Artificial Neural Networks](https://arxiv.org/abs/2509.10514)
*Shaoxin Tian,Hongkai Liu,Yuying Yang,Jiali Yu,Zizheng Miao,Xuming Huang,Zhishuai Liu,Zhang Yi*

Main category: cs.LG

TL;DR: 本文从微分流形视角建立了分析人工神经网络中连续吸引子的统一框架，验证了与现有结论的兼容性，揭示了连续吸引子现象与局部雅可比矩阵特征值的联系，并证明了奇异值分层在常见分类模型和数据集中的普遍性。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏分析不同动力系统中连续吸引子特性的统一框架，限制了跨架构的泛化能力。连续吸引子在生物和人工神经系统的信息处理中至关重要，涉及空间导航、记忆和深度学习优化。

Method: 从微分流形的角度建立新颖的分析框架，通过验证与先前结论的兼容性，分析局部雅可比矩阵特征值与连续吸引子现象的联系，并在常见分类模型和数据集上展示奇异值分层的普遍性。

Result: 研究发现连续吸引子可能在一般神经网络中普遍存在，验证了框架与现有结论的兼容性，揭示了特征值与连续吸引子的数学联系，证明了奇异值分层在各种模型和数据集中的普遍性。

Conclusion: 研究强调了建立通用理论的必要性，提出的框架由于特征值与奇异值之间的紧密数学联系，为构建这样的理论提供了有前景的基础，表明连续吸引子在神经网络中的普遍存在性。

Abstract: Continuous attractors are critical for information processing in both
biological and artificial neural systems, with implications for spatial
navigation, memory, and deep learning optimization. However, existing research
lacks a unified framework to analyze their properties across diverse dynamical
systems, limiting cross-architectural generalizability. This study establishes
a novel framework from the perspective of differential manifolds to investigate
continuous attractors in artificial neural networks. It verifies compatibility
with prior conclusions, elucidates links between continuous attractor phenomena
and eigenvalues of the local Jacobian matrix, and demonstrates the universality
of singular value stratification in common classification models and datasets.
These findings suggest continuous attractors may be ubiquitous in general
neural networks, highlighting the need for a general theory, with the proposed
framework offering a promising foundation given the close mathematical
connection between eigenvalues and singular values.

</details>


### [111] [Adaptive Preference Optimization with Uncertainty-aware Utility Anchor](https://arxiv.org/abs/2509.10515)
*Xiaobo Wang,Zixia Jia,Jiaqi Li,Qi Liu,Zilong Zheng*

Main category: cs.LG

TL;DR: UAPO是一种新的离线偏好优化框架，通过引入效用锚函数来估计偏好数据标注的不确定性，解决了传统DPO方法对配对数据的严格依赖问题，提高了数据利用效率。


<details>
  <summary>Details</summary>
Motivation: 传统的DPO方法基于Bradley-Terry奖励建模，存在对配对训练数据的严格要求、模型分布偏移、人类理性假设等局限性，限制了偏好优化的灵活性和有效性。

Method: 提出了自适应偏好优化框架UAPO，引入锚函数来估计偏好数据标注的不确定性，支持在非配对数据场景下训练，增强了训练过程的鲁棒性。

Result: 实验结果表明UAPO在不严格依赖数据配对的情况下取得了有竞争力的结果，提高了数据利用效率。

Conclusion: UAPO为更灵活有效的偏好优化方法铺平了道路，解决了传统方法的局限性，实现了更好的数据利用和训练稳定性。

Abstract: Offline preference optimization methods are efficient for large language
models (LLMs) alignment. Direct Preference optimization (DPO)-like learning,
one of the most popular approaches, stands out for its efficiency in reward
modeling. However, these methods typically follow the convention to use
Bradley-Terry (BT) reward modeling that faces several critical assumptions,
including the requirement for pairwise training data, model distribution
shifting, human rationality assumption, etc. To address these limitations, we
propose a general framework for offline preference optimization methods,
Adaptive Preference Optimization with Utility Anchor (UAPO), which introduces
an anchoring function to estimate the uncertainties brought from preference
data annotation. Our method enables training even in scenarios where the data
is unpaired, significantly enhancing data utilization efficiency. Moreover, the
anchor design makes UAPO more robust in the training process. Experimental
results demonstrate that UAPO achieves competitive outcomes without the strict
dependency on data pairing, paving the way for more flexible and effective
preference optimization methods.

</details>


### [112] [Privacy-Preserving Personalization in Education: A Federated Recommender System for Student Performance Prediction](https://arxiv.org/abs/2509.10516)
*Rodrigo Tertulino*

Main category: cs.LG

TL;DR: 提出基于联邦学习的隐私保护推荐系统，使用深度神经网络处理教育数据，FedProx聚合策略优于FedAvg，达到集中式模型82.85%的性能，解决了教育个性化与隐私保护的矛盾。


<details>
  <summary>Details</summary>
Motivation: 教育数字化带来数据驱动个性化机会，但传统集中式推荐系统与学生数据隐私保护法规存在冲突，需要开发隐私保护的推荐解决方案。

Method: 使用联邦学习框架，采用深度神经网络处理ASSISTments教育数据集的特征工程，比较不同联邦聚合策略（FedProx vs FedAvg）。

Result: FedProx在处理异构学生数据时表现更稳定有效，优化后的联邦模型F1分数达76.28%，相当于集中式XGBoost模型性能的82.85%。

Conclusion: 联邦学习方法能够在不集中敏感学生数据的情况下提供高效的个性化内容推荐，为现代教育平台的个性化与隐私保护困境提供了可行解决方案。

Abstract: The increasing digitalization of education presents unprecedented
opportunities for data-driven personalization, yet it introduces significant
student data privacy challenges. Conventional recommender systems rely on
centralized data, a paradigm often incompatible with modern data protection
regulations. A novel privacy-preserving recommender system is proposed and
evaluated to address this critical issue using Federated Learning (FL). The
approach utilizes a Deep Neural Network (DNN) with rich, engineered features
from the large-scale ASSISTments educational dataset. A rigorous comparative
analysis of federated aggregation strategies was conducted, identifying FedProx
as a significantly more stable and effective method for handling heterogeneous
student data than the standard FedAvg baseline. The optimized federated model
achieves a high-performance F1-Score of 76.28\%, corresponding to 82.85\% of
the performance of a powerful, centralized XGBoost model. These findings
validate that a federated approach can provide highly effective content
recommendations without centralizing sensitive student data. Consequently, our
work presents a viable and robust solution to the personalization-privacy
dilemma in modern educational platforms.

</details>


### [113] [A Comparative Benchmark of Federated Learning Strategies for Mortality Prediction on Heterogeneous and Imbalanced Clinical Data](https://arxiv.org/abs/2509.10517)
*Rodrigo Tertulino*

Main category: cs.LG

TL;DR: 本研究比较了五种联邦学习策略在非IID和不平衡临床数据上的死亡率预测性能，发现基于正则化的FedProx方法表现最佳，F1分数达0.8831，为医疗健康应用提供了重要基准。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在预测院内死亡率方面具有巨大潜力，但数据隐私限制和真实世界临床数据的统计异质性阻碍了其发展。联邦学习提供了隐私保护解决方案，但其在非独立同分布和不平衡条件下的性能需要严格研究。

Method: 使用MIMIC-IV数据集，通过按临床护理单元划分数据模拟真实非IID环境。应用SMOTE-Tomek技术处理类别不平衡问题。比较了五种联邦学习策略：FedAvg、FedProx、FedAdagrad、FedAdam和FedCluster，进行了50轮通信实验。

Result: 基于正则化的策略FedProx始终优于其他方法，获得最高F1分数0.8831并保持稳定收敛。基线FedAvg计算效率最高但预测性能显著较低。

Conclusion: 基于正则化的FL算法（如FedProx）为异构和不平衡临床预测任务提供了比标准或服务器端自适应聚合方法更稳健有效的解决方案，为现实世界医疗应用选择合适FL策略提供了关键实证基准。

Abstract: Machine learning models hold significant potential for predicting in-hospital
mortality, yet data privacy constraints and the statistical heterogeneity of
real-world clinical data often hamper their development. Federated Learning
(FL) offers a privacy-preserving solution, but its performance under
non-Independent and Identically Distributed (non-IID) and imbalanced conditions
requires rigorous investigation. The study presents a comparative benchmark of
five federated learning strategies: FedAvg, FedProx, FedAdagrad, FedAdam, and
FedCluster for mortality prediction. Using the large-scale MIMIC-IV dataset, we
simulate a realistic non-IID environment by partitioning data by clinical care
unit. To address the inherent class imbalance of the task, the SMOTE-Tomek
technique is applied to each client's local training data. Our experiments,
conducted over 50 communication rounds, reveal that the regularization-based
strategy, FedProx, consistently outperformed other methods, achieving the
highest F1-Score of 0.8831 while maintaining stable convergence. While the
baseline FedAvg was the most computationally efficient, its predictive
performance was substantially lower. Our findings indicate that
regularization-based FL algorithms like FedProx offer a more robust and
effective solution for heterogeneous and imbalanced clinical prediction tasks
than standard or server-side adaptive aggregation methods. The work provides a
crucial empirical benchmark for selecting appropriate FL strategies for
real-world healthcare applications.

</details>


### [114] [Holographic Knowledge Manifolds: A Novel Pipeline for Continual Learning Without Catastrophic Forgetting in Large Language Models](https://arxiv.org/abs/2509.10518)
*Justin Arndt*

Main category: cs.LG

TL;DR: HKM是一种四阶段流水线方法，通过分形量化、概率纠缠和动态衍射分片技术，实现零灾难性遗忘、3倍知识压缩和67%存储节省，在AI知识表示领域取得突破性进展。


<details>
  <summary>Details</summary>
Motivation: 解决AI系统中的灾难性遗忘问题，同时实现高效的知识压缩和存储优化，为大型语言模型提供无需重新训练的持续适应能力。

Method: 采用四阶段流水线：分形量化、概率纠缠、动态衍射分片和全息集成，结合WikiText和FB15k数据集进行实验验证。

Result: 在2977个节点的实验中实现：0%遗忘率（相比GEM基线无限改进）、3倍压缩、53%训练时间减少、67%存储节省，支持1020次更新且每次仅增长1%。

Conclusion: HKM技术为公共大语言模型带来范式转变，实现"永恒"适应而无需重新训练，预计在PB规模下5年可节省9240万美元，降低21.2%能耗和33%碳足迹。

Abstract: We introduce the Holographic Knowledge Manifold (HKM), a four-phase pipeline
that achieves zero catastrophic forgetting in AI knowledge representation while
maintaining minimal memory growth and high efficiency. Leveraging fractal
quantization, probabilistic entanglement, and dynamic diffraction chipping, HKM
compresses knowledge substrates by 3x with 67% storage savings, integrates
holographically at 100%, and supports over 1,020 updates with 1% growth per
increment. In experiments on combined WikiText and FB15k datasets (scaled to
2,997 nodes), we demonstrate industry-leading performance: 0% forgetting
(infinite improvement over GEM baselines), 3x compression, and 53% training
time reduction on consumer GPU hardware. Hypothetical cost analyses project
$92.4M savings over 5 years at petabyte scale, with 21.2% energy reduction and
33% lower carbon footprint. This work hypothesizes a paradigm shift for public
large language models (LLMs), enabling "eternal" adaptation without retraining.
Future extensions to multimodal fusion and quantum hardware could further
democratize scalable AI, potentially reducing fine-tuning costs by 60-80% for
models like Llama-3 or Grok-4. Code, datasets, and full results are publicly
available for reproducibility.

</details>


### [115] [Gradient Estimation Methods of Approximate Multipliers for High-Accuracy Retraining of Deep Learning Models](https://arxiv.org/abs/2509.10519)
*Chang Meng,Wayne Burleson,Giovanni De Micheli*

Main category: cs.LG

TL;DR: 提出两种查找表方法(LUT-2D和LUT-1D)来精确计算近似乘法器的梯度，提高深度学习模型重训练精度


<details>
  <summary>Details</summary>
Motivation: 现有方法使用精确乘法器的梯度来估计近似乘法器的梯度，导致重训练结果不理想，需要更精确的梯度计算方法

Method: LUT-2D使用二维查找表精细表征近似乘法器梯度；LUT-1D是更高效的变体，使用一维查找表存储梯度值

Result: 在CIFAR-10上平均提高重训练精度3.83%(LUT-2D)和3.72%(LUT-1D)；在ImageNet上LUT-1D平均提高23.69%

Conclusion: 提出的查找表方法能显著提升近似乘法器重训练精度，LUT-1D在保持高精度的同时具有更好的效率

Abstract: Approximate multipliers (AppMults) are widely used in deep learning
accelerators to reduce their area, delay, and power consumption. However,
AppMults introduce arithmetic errors into deep learning models, necessitating a
retraining process to recover accuracy. A key step in retraining is computing
the gradient of the AppMult, i.e., the partial derivative of the approximate
product with respect to each input operand. Existing approaches typically
estimate this gradient using that of the accurate multiplier (AccMult), which
can lead to suboptimal retraining results. To address this, we propose two
methods to obtain more precise gradients of AppMults. The first, called LUT-2D,
characterizes the AppMult gradient with 2-dimensional lookup tables (LUTs),
providing fine-grained estimation and achieving the highest retraining
accuracy. The second, called LUT-1D, is a compact and more efficient variant
that stores gradient values in 1-dimensional LUTs, achieving comparable
retraining accuracy with shorter runtime. Experimental results show that on
CIFAR-10 with convolutional neural networks, our LUT-2D and LUT-1D methods
improve retraining accuracy by 3.83% and 3.72% on average, respectively. On
ImageNet with vision transformer models, our LUT-1D method improves retraining
accuracy by 23.69% on average, compared to a state-of-the-art retraining
framework.

</details>


### [116] [Offline Contextual Bandit with Counterfactual Sample Identification](https://arxiv.org/abs/2509.10520)
*Alexandre Gilotte,Otmane Sakhi,Imad Aouali,Benjamin Heymann*

Main category: cs.LG

TL;DR: 提出Counterfactual Sample Identification方法，通过比较实际动作和反事实动作来识别导致成功结果的动作，而非直接预测奖励，解决了上下文混淆问题


<details>
  <summary>Details</summary>
Motivation: 在生产系统中，传统的上下文赌博机方法依赖直接奖励模型，但这些模型容易受到混淆因素的影响，难以区分动作效果和上下文效果

Method: 重新定义问题：不是预测奖励，而是学习识别在相同上下文下，哪个动作（与实际动作相比，从记录策略中采样的反事实动作）导致了成功的二元结果

Result: 该方法理论上有坚实基础，在合成实验和实际部署中都持续优于直接模型

Conclusion: Counterfactual Sample Identification方法通过反事实比较有效解决了上下文混淆问题，在理论和实践中都表现出优越性能

Abstract: In production systems, contextual bandit approaches often rely on direct
reward models that take both action and context as input. However, these models
can suffer from confounding, making it difficult to isolate the effect of the
action from that of the context. We present \emph{Counterfactual Sample
Identification}, a new approach that re-frames the problem: rather than
predicting reward, it learns to recognize which action led to a successful
(binary) outcome by comparing it to a counterfactual action sampled from the
logging policy under the same context. The method is theoretically grounded and
consistently outperforms direct models in both synthetic experiments and
real-world deployments.

</details>


### [117] [Variational Gaussian Mixture Manifold Models for Client-Specific Federated Personalization](https://arxiv.org/abs/2509.10521)
*Sai Puppala,Ismail Hossain,Md Jahangir Alam,Sajedul Talukder*

Main category: cs.LG

TL;DR: VGM²是一个基于几何的个性化联邦学习框架，通过变分高斯混合流形建模客户端特定几何，在标签倾斜和非平稳环境下实现高效通信和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 传统个性化联邦学习在标签倾斜和非平稳环境下失败，因为单一全局参数化忽略了客户端特定的几何结构。

Method: 学习客户端特定的参数化UMAP嵌入，用混合关系标记建模潜在成对距离，仅交换变分不确定性感知的标记统计量，使用Dirichlet-Normal-Inverse-Gamma后验和共轭矩匹配聚合。

Result: 在8个视觉数据集上，VGM²在非IID标签分片下实现了竞争性或更优的测试F1分数，同时仅通信小型几何摘要。

Conclusion: VGM²通过几何中心化方法有效解决了联邦学习中的异质性问题，提供了通信效率、隐私保护和理论稳定性。

Abstract: Personalized federated learning (PFL) often fails under label skew and
non-stationarity because a single global parameterization ignores
client-specific geometry. We introduce VGM$^2$ (Variational Gaussian Mixture
Manifold), a geometry-centric PFL framework that (i) learns client-specific
parametric UMAP embeddings, (ii) models latent pairwise distances with mixture
relation markers for same and different class pairs, and (iii) exchanges only
variational, uncertainty-aware marker statistics. Each client maintains a
Dirichlet-Normal-Inverse-Gamma (Dir-NIG) posterior over marker weights, means,
and variances; the server aggregates via conjugate moment matching to form
global priors that guide subsequent rounds. We prove that this aggregation
minimizes the summed reverse Kullback-Leibler divergence from client posteriors
within the conjugate family, yielding stability under heterogeneity. We further
incorporate a calibration term for distance-to-similarity mapping and report
communication and compute budgets. Across eight vision datasets with non-IID
label shards, VGM$^2$ achieves competitive or superior test F1 scores compared
to strong baselines while communicating only small geometry summaries. Privacy
is strengthened through secure aggregation and optional differential privacy
noise, and we provide a membership-inference stress test. Code and
configurations will be released to ensure full reproducibility.

</details>


### [118] [Multimodal Deep Learning for ATCO Command Lifecycle Modeling and Workload Prediction](https://arxiv.org/abs/2509.10522)
*Kaizhen Tan*

Main category: cs.LG

TL;DR: 本文提出了一种多模态深度学习框架，通过整合结构化数据、轨迹序列和图像特征来估计空中交通管制员命令生命周期中的两个关键参数：命令与飞机机动之间的时间偏移和命令持续时间。


<details>
  <summary>Details</summary>
Motivation: 在密集空域中，空中交通管制员发出高强度语音指令，准确的负荷建模对安全和效率至关重要。需要开发能够精确预测命令执行时间和持续时间的模型来支持智能命令生成和负荷评估。

Method: 构建高质量数据集，使用滑动窗口和基于直方图的方法检测机动点。开发CNN-Transformer集成模型，整合结构化数据、轨迹序列和图像特征进行多模态学习。

Result: 开发了首个将轨迹与语音命令关联的模型，能够准确、可泛化且可解释地预测命令时间偏移和持续时间。

Conclusion: 该框架为智能命令生成提供了支持，在负荷评估、人员配置和调度方面具有实用价值，是同类研究中的首创模型。

Abstract: Air traffic controllers (ATCOs) issue high-intensity voice commands in dense
airspace, where accurate workload modeling is critical for safety and
efficiency. This paper proposes a multimodal deep learning framework that
integrates structured data, trajectory sequences, and image features to
estimate two key parameters in the ATCO command lifecycle: the time offset
between a command and the resulting aircraft maneuver, and the command
duration. A high-quality dataset was constructed, with maneuver points detected
using sliding window and histogram-based methods. A CNN-Transformer ensemble
model was developed for accurate, generalizable, and interpretable predictions.
By linking trajectories to voice commands, this work offers the first model of
its kind to support intelligent command generation and provides practical value
for workload assessment, staffing, and scheduling.

</details>


### [119] [From Predictions to Explanations: Explainable AI for Autism Diagnosis and Identification of Critical Brain Regions](https://arxiv.org/abs/2509.10523)
*Kush Gupta,Amir Aly,Emmanuel Ifeachor,Rohit Shankar*

Main category: cs.LG

TL;DR: 提出结合深度学习和可解释AI的两模块框架，通过跨域迁移学习解决ASD数据稀缺问题，并使用三种XAI技术解释模型决策和识别关键脑区


<details>
  <summary>Details</summary>
Motivation: 自闭症谱系障碍(ASD)研究中机器学习迁移学习范式应用有限，需要解决数据稀缺问题并提高诊断模型的可解释性

Method: 两模块框架：第一模块使用跨域迁移学习微调深度学习模型进行ASD分类；第二模块应用三种XAI技术（显著性映射、Grad-CAM和SHAP分析）解释模型决策

Result: 跨域迁移学习能有效解决ASD研究中的数据稀缺问题，XAI技术成功识别出与ASD最相关的脑区，结果与现有神经生物学证据高度一致

Conclusion: 该框架不仅提高了ASD诊断的准确性，还通过可解释AI技术增强了临床相关性，为ASD研究提供了有效的计算机辅助诊断方法

Abstract: Autism spectrum disorder (ASD) is a neurodevelopmental condition
characterized by atypical brain maturation. However, the adaptation of transfer
learning paradigms in machine learning for ASD research remains notably
limited. In this study, we propose a computer-aided diagnostic framework with
two modules. This chapter presents a two-module framework combining deep
learning and explainable AI for ASD diagnosis. The first module leverages a
deep learning model fine-tuned through cross-domain transfer learning for ASD
classification. The second module focuses on interpreting the model decisions
and identifying critical brain regions. To achieve this, we employed three
explainable AI (XAI) techniques: saliency mapping, Gradient-weighted Class
Activation Mapping, and SHapley Additive exPlanations (SHAP) analysis. This
framework demonstrates that cross-domain transfer learning can effectively
address data scarcity in ASD research. In addition, by applying three
established explainability techniques, the approach reveals how the model makes
diagnostic decisions and identifies brain regions most associated with ASD.
These findings were compared against established neurobiological evidence,
highlighting strong alignment and reinforcing the clinical relevance of the
proposed approach.

</details>


### [120] [Resource-Aware Neural Network Pruning Using Graph-based Reinforcement Learning](https://arxiv.org/abs/2509.10526)
*Dieter Balemans,Thomas Huybrechts,Jan Steckel,Siegfried Mercelis*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的神经网络剪枝方法，通过将基于图的观察空间集成到AutoML框架中，解决了现有方法的局限性。该方法使用图注意力网络编码器处理网络图表示，并在约束马尔可夫决策过程框架下实现，在多个基准数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统剪枝方法依赖手工设计的启发式规则和局部优化视角，往往导致次优性能和低效的剪枝策略。需要一种能够捕捉网络全局拓扑关系并自动学习最优剪枝策略的方法。

Method: 1) 引入神经网络图表示以捕捉完整的拓扑关系；2) 使用图注意力网络编码器生成丰富的嵌入；3) 从连续剪枝比转向细粒度二元动作空间；4) 在约束马尔可夫决策过程框架下设计自竞争奖励系统

Result: 在CIFAR-10、CIFAR-100和ImageNet等基准数据集上的实验表明，该方法 consistently优于传统剪枝技术，取得了最先进的结果，能够学习到任务特定的剪枝策略，识别出超越简单权重大小考虑的功能冗余连接。

Conclusion: 该框架成功地将图表示和AutoML技术结合，为神经网络剪枝提供了全局视角和自动优化能力，显著提升了剪枝效果和效率，代表了该领域的一个重要进展。

Abstract: This paper presents a novel approach to neural network pruning by integrating
a graph-based observation space into an AutoML framework to address the
limitations of existing methods. Traditional pruning approaches often depend on
hand-crafted heuristics and local optimization perspectives, which can lead to
suboptimal performance and inefficient pruning strategies. Our framework
transforms the pruning process by introducing a graph representation of the
target neural network that captures complete topological relationships between
layers and channels, replacing the limited layer-wise observation space with a
global view of network structure. The core innovations include a Graph
Attention Network (GAT) encoder that processes the network's graph
representation and generates a rich embedding. Additionally, for the action
space we transition from continuous pruning ratios to fine-grained binary
action spaces which enables the agent to learn optimal channel importance
criteria directly from data, moving away from predefined scoring functions.
These contributions are modelled within a Constrained Markov Decision Process
(CMDP) framework, allowing the agent to make informed pruning decisions while
adhering to resource constraints such as target compression rates. For this, we
design a self-competition reward system that encourages the agent to outperform
its previous best performance while satisfying the defined constraints. We
demonstrate the effectiveness of our approach through extensive experiments on
benchmark datasets including CIFAR-10, CIFAR-100, and ImageNet. The experiments
show that our method consistently outperforms traditional pruning techniques,
showing state-of-the-art results while learning task-specific pruning
strategies that identify functionally redundant connections beyond simple
weight magnitude considerations.

</details>


### [121] [STM-Graph: A Python Framework for Spatio-Temporal Mapping and Graph Neural Network Predictions](https://arxiv.org/abs/2509.10528)
*Amirhossein Ghaffari,Huong Nguyen,Lauri Lovén,Ekaterina Gilman*

Main category: cs.LG

TL;DR: STM-Graph是一个开源的Python框架，用于将城市时空事件数据转换为适合图神经网络训练的图表示，集成了多种空间映射方法、OpenStreetMap城市特征、GNN模型和可视化工具。


<details>
  <summary>Details</summary>
Motivation: 城市时空数据具有动态性和复杂性，给预测分析带来独特挑战，需要专门的工具来处理和建模这类数据。

Method: 开发了一个模块化、可扩展的框架，包含空间映射方法、城市特征提取、多种GNN模型集成、可视化工具和图形用户界面。

Result: 创建了一个完整的开源框架，支持快速实验和基准测试，允许集成新的映射方法和自定义模型。

Conclusion: STM-Graph为城市计算领域的研究人员和从业者提供了宝贵的资源，有助于促进城市时空数据分析的发展。

Abstract: Urban spatio-temporal data present unique challenges for predictive analytics
due to their dynamic and complex nature. We introduce STM-Graph, an open-source
Python framework that transforms raw spatio-temporal urban event data into
graph representations suitable for Graph Neural Network (GNN) training and
prediction. STM-Graph integrates diverse spatial mapping methods, urban
features from OpenStreetMap, multiple GNN models, comprehensive visualization
tools, and a graphical user interface (GUI) suitable for professional and
non-professional users. This modular and extensible framework facilitates rapid
experimentation and benchmarking. It allows integration of new mapping methods
and custom models, making it a valuable resource for researchers and
practitioners in urban computing. The source code of the framework and GUI are
available at: https://github.com/Ahghaffari/stm_graph and
https://github.com/tuminguyen/stm_graph_gui.

</details>


### [122] [Mitigating Catastrophic Forgetting and Mode Collapse in Text-to-Image Diffusion via Latent Replay](https://arxiv.org/abs/2509.10529)
*Aoi Otani*

Main category: cs.LG

TL;DR: 本文提出Latent Replay方法，通过存储紧凑的高层特征表示而非原始图像，有效解决文本到图像扩散模型在持续学习中的灾难性遗忘和模式崩溃问题，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像扩散模型在持续学习过程中面临的灾难性遗忘和模式崩溃问题，这些模型在学习新任务时会忘记先前学到的知识，并且输出会变得越来越重复。

Method: 应用神经科学启发的Latent Replay方法，存储模型内部架构提取的紧凑高层特征表示，而不是传统的存储大量图像样本的方法，这类似于海马体存储神经活动模式而非原始感官输入的过程。

Result: 在五个顺序学习的视觉概念实验中，Latent Replay方法显著优于现有方法，最早概念保持了77.59%的图像对齐度，比基线方法高14%，同时保持了输出多样性。意外发现随机选择存储的潜在示例比基于相似性的策略效果更好。

Conclusion: Latent Replay方法为生成式AI模型实现了高效的持续学习，为能够随用户需求演化而不产生过高计算成本的个性化文本到图像模型开辟了道路。

Abstract: Continual learning -- the ability to acquire knowledge incrementally without
forgetting previous skills -- is fundamental to natural intelligence. While the
human brain excels at this, artificial neural networks struggle with
"catastrophic forgetting," where learning new tasks erases previously acquired
knowledge. This challenge is particularly severe for text-to-image diffusion
models, which generate images from textual prompts. Additionally, these models
face "mode collapse," where their outputs become increasingly repetitive over
time. To address these challenges, we apply Latent Replay, a
neuroscience-inspired approach, to diffusion models. Traditional replay methods
mitigate forgetting by storing and revisiting past examples, typically
requiring large collections of images. Latent Replay instead retains only
compact, high-level feature representations extracted from the model's internal
architecture. This mirrors the hippocampal process of storing neural activity
patterns rather than raw sensory inputs, reducing memory usage while preserving
critical information. Through experiments with five sequentially learned visual
concepts, we demonstrate that Latent Replay significantly outperforms existing
methods in maintaining model versatility. After learning all concepts, our
approach retained 77.59% Image Alignment (IA) on the earliest concept, 14%
higher than baseline methods, while maintaining diverse outputs. Surprisingly,
random selection of stored latent examples outperforms similarity-based
strategies. Our findings suggest that Latent Replay enables efficient continual
learning for generative AI models, paving the way for personalized
text-to-image models that evolve with user needs without excessive
computational costs.

</details>


### [123] [Dynamic Adaptive Shared Experts with Grouped Multi-Head Attention Mixture of Experts](https://arxiv.org/abs/2509.10530)
*Cheng Li,Jiexiong Liu,Yixuan Chen,Jie ji*

Main category: cs.LG

TL;DR: 提出DASG-MoE模型，通过分组多头注意力、双尺度共享专家结构和自适应动态路由机制，提升长序列建模的计算效率和长距离依赖捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于混合专家(MoE)架构的Transformer模型在长序列建模中仍存在计算效率不足和长距离依赖捕捉能力有限的问题，特别是在专家资源分配的动态适应性方面存在缺陷。

Method: 1) 使用分组多头注意力(GMHA)机制降低长序列计算复杂度；2) 设计双尺度共享专家结构(DSSE)，浅层专家处理低维特征，深层专家处理高维复杂语义；3) 提出分层自适应动态路由(ADR)机制，根据特征复杂度和任务需求动态选择专家级别。

Result: 在多个长序列基准数据集上的实验表明，DASG-MoE模型优于当前最先进的模型。

Conclusion: DASG-MoE模型通过三个模块的有效整合，显著提升了长序列建模的性能，在计算效率和长距离依赖捕捉方面都取得了良好效果。

Abstract: Transformer models based on the Mixture of Experts (MoE) architecture have
made significant progress in long-sequence modeling, but existing models still
have shortcomings in computational efficiency and the ability to capture
long-range dependencies, especially in terms of the dynamic adaptability of
expert resource allocation. In this paper, we propose a Dynamic Adaptive Shared
Expert and Grouped Multi-Head Attention Hybrid Model (DASG-MoE) to enhance
long-sequence modeling capabilities by integrating three modules. First, we
employ the Grouped Multi-Head Attention (GMHA) mechanism to effectively reduce
the computational complexity of long sequences. By parallel processing through
sequence grouping, local sliding window attention, and feature aggregation, we
address long-range dependency issues and the model's lack of generalization for
local information. Second, we design a Dual-Scale Shared Expert Structure
(DSSE), where shallow experts use lightweight computations to quickly respond
to low-dimensional features, while deep experts process high-dimensional
complex semantics through pre-training transfer and post-training optimization,
achieving a dynamic balance between efficiency and accuracy. Third, we propose
a hierarchical Adaptive Dynamic Routing (ADR) mechanism that dynamically
selects expert levels based on feature complexity and task requirements, and
optimizes resource allocation through a local expert activation strategy.
Experiments on multiple long-sequence benchmark datasets demonstrate that our
DASG-MoE model outperforms state-of-the-art models.

</details>


### [124] [FinXplore: An Adaptive Deep Reinforcement Learning Framework for Balancing and Discovering Investment Opportunities](https://arxiv.org/abs/2509.10531)
*Himanshu Choudhary,Arishi Orra,Manoj Thakur*

Main category: cs.LG

TL;DR: 本文提出了一种结合利用现有资产和探索新投资机会的双智能体深度强化学习方法，用于投资组合优化，在动态市场中提升投资表现。


<details>
  <summary>Details</summary>
Motivation: 传统的深度强化学习投资组合优化方法局限于预定义的投资宇宙，忽视了探索新投资机会的重要性，无法适应动态变化的市场环境。

Method: 使用两个深度强化学习智能体：一个负责在现有投资宇宙内分配资产，另一个协助在扩展宇宙中探索新投资机会，动态平衡利用和探索目标。

Result: 在两个真实市场数据集上的实验表明，该方法优于最先进的投资组合策略和基线方法，证明了其有效性。

Conclusion: 通过结合利用现有资产和探索新机会的双智能体方法，能够更好地适应市场变化并提升投资组合性能，为投资组合优化提供了新思路。

Abstract: Portfolio optimization is essential for balancing risk and return in
financial decision-making. Deep Reinforcement Learning (DRL) has stood out as a
cutting-edge tool for portfolio optimization that learns dynamic asset
allocation using trial-and-error interactions. However, most DRL-based methods
are restricted to allocating assets within a pre-defined investment universe
and overlook exploring new opportunities. This study introduces an investment
landscape that integrates exploiting existing assets with exploring new
investment opportunities in an extended universe. The proposed approach
leverages two DRL agents and dynamically balances these objectives to adapt to
evolving markets while enhancing portfolio performance. One agent allocates
assets within the existing universe, while another assists in exploring new
opportunities in the extended universe. The effciency of the proposed
methodology is determined using two real-world market data sets. The
experiments demonstrate the superiority of the suggested approach against the
state-of-the-art portfolio strategies and baseline methods.

</details>


### [125] [Decoupling the "What" and "Where" With Polar Coordinate Positional Embeddings](https://arxiv.org/abs/2509.10534)
*Anand Gopalakrishnan,Robert Csordás,Jürgen Schmidhuber,Michael C. Mozer*

Main category: cs.LG

TL;DR: PoPE是一种新的位置编码方法，解决了RoPE中内容和位置信息纠缠的问题，在多个领域和模型规模上都表现出更好的性能，并具有优秀的长度外推能力。


<details>
  <summary>Details</summary>
Motivation: 现有的RoPE旋转位置编码存在内容和位置信息纠缠的问题，这会影响模型性能，特别是在需要独立匹配内容和位置的场景中。

Method: 提出了PoPE（极坐标位置嵌入）方法，通过分离内容和位置信息来消除what-where混淆，使用极坐标表示来独立编码这两个维度。

Result: 在音乐、基因组和自然语言领域的自回归序列建模中，使用PoPE的Transformer在评估损失（困惑度）和下游任务性能上均优于RoPE基线。在语言建模中，从124M到774M参数规模都显示出持续优势。PoPE展现出强大的零样本长度外推能力，而RoPE在测试时处理更长序列时性能显著下降。

Conclusion: PoPE通过解耦内容和位置信息，解决了RoPE中的纠缠问题，在多个领域和规模上都表现出优越的性能，特别是在长度外推方面具有显著优势。

Abstract: The attention mechanism in a Transformer architecture matches key to query
based on both content -- the what -- and position in a sequence -- the where.
We present an analysis indicating that what and where are entangled in the
popular RoPE rotary position embedding. This entanglement can impair
performance particularly when decisions require independent matches on these
two factors. We propose an improvement to RoPE, which we call Polar Coordinate
Position Embeddings or PoPE, that eliminates the what-where confound. PoPE is
far superior on a diagnostic task requiring indexing solely by position or by
content. On autoregressive sequence modeling in music, genomic, and natural
language domains, Transformers using PoPE as the positional encoding scheme
outperform baselines using RoPE with respect to evaluation loss (perplexity)
and downstream task performance. On language modeling, these gains persist
across model scale, from 124M to 774M parameters. Crucially, PoPE shows strong
zero-shot length extrapolation capabilities, whereas RoPE's performance
degrades significantly on longer sequences at test time without fine tuning or
the use of position-interpolation methods.

</details>


### [126] [Semantic-guided LoRA Parameters Generation](https://arxiv.org/abs/2509.10535)
*Miaoge Li,Yang Chen,Zhijie Rao,Can Jiang,Jingcai Guo*

Main category: cs.LG

TL;DR: SG-LoRA是一个无需额外训练即可生成用户特定LoRA参数的框架，通过语义引导在共享嵌入空间中测量任务相似度，为零样本开放世界场景提供个性化模型适配。


<details>
  <summary>Details</summary>
Motivation: 解决边缘计算中用户任务特定偏好难以用统一模型处理的问题，同时避免因重新训练/微调带来的高成本和隐私问题。

Method: 使用任务描述作为语义桥梁，在共享嵌入空间中测量与已知专家任务的接近度，基于语义引导建模目标任务的LoRA参数分布来生成新任务的高性能参数。

Result: 在多个挑战性任务上的广泛实验证实了SG-LoRA的卓越性能和显著适应性。

Conclusion: SG-LoRA通过从知名LoRA专家中提炼知识，实现了实时构建与个体意图对齐的LoRA模型，为零样本开放世界设置中的个性化模型适配提供了隐私保护解决方案。

Abstract: Low-Rank Adaptation (LoRA) has demonstrated strong generalization
capabilities across a variety of tasks for efficiently fine-tuning AI models,
especially on resource-constrained edges. However, in real-world applications,
edge users often exhibit task-specific preferences that are difficult to handle
with a unified model trained under a closed-world assumption, and the challenge
may further increase when there are significant domain shifts between training
and deployment. Meanwhile, retraining/fine-tuning models for each user is also
impractical due to its cost-intensive nature and privacy concerns over raw data
utilization from edges. To address these challenges, we propose Semantic-guided
LoRA Parameter Generation (SG-LoRA), the first of its kind framework to
efficiently produce user-specific LoRA parameters without any additional
training on user tasks or access to user-specific data. Concretely, SG-LoRA
uses task descriptions as the semantic bridge, measuring their proximity to a
set of known expert tasks in a shared embedding space. Based on this semantic
guidance, it models the target task's LoRA parameter distribution to generate
high-performing parameters for novel tasks. SG-LoRA enables the real-time
construction of LoRA models aligned with individual intents by distilling
knowledge from prominent LoRA experts and, meanwhile, offering a
privacy-preserving solution for personalized model adaptation in a novel
zero-shot open-world setting proposed in this work. Extensive experiments on
multiple challenging tasks confirm the superior performance and remarkable
adaptability of SG-LoRA. Code is available at
https://github.com/keepgoingjkg/SG-LoRA.

</details>


### [127] [Contextuality, Holonomy and Discrete Fiber Bundles in Group-Valued Boltzmann Machines](https://arxiv.org/abs/2509.10536)
*Jean-Pierre Magnot*

Main category: cs.LG

TL;DR: 提出了几何扩展的受限玻尔兹曼机，允许权重取值于抽象群，引入基于群值holonomy的上下文性指数来量化全局不一致性，建立了与层理论、规范理论和非交换几何的联系


<details>
  <summary>Details</summary>
Motivation: 扩展RBM以建模复杂关系结构（如投影变换、旋量动力学和函数对称性），为视觉、语言和量子学习提供直接应用

Method: 允许RBM权重取值于抽象群（如GL_n(R)、SU(2)或无限维算子群），引入基于图循环上群值holonomy计算的上下文性指数

Result: 建立了与层理论上下文性、规范理论和非交换几何的联系，提供了有限维和无限维的数值和图表示例

Conclusion: 该框架为AI开辟了新方向，包括曲率感知学习架构和不确定或对抗环境中的拓扑正则化

Abstract: We propose a geometric extension of restricted Boltzmann machines (RBMs) by
allowing weights to take values in abstract groups such as \(
\mathrm{GL}_n(\mathbb{R}) \), \( \mathrm{SU}(2) \), or even
infinite-dimensional operator groups. This generalization enables the modeling
of complex relational structures, including projective transformations, spinor
dynamics, and functional symmetries, with direct applications to vision,
language, and quantum learning.
  A central contribution of this work is the introduction of a
\emph{contextuality index} based on group-valued holonomies computed along
cycles in the RBM graph. This index quantifies the global inconsistency or
"curvature" induced by local weights, generalizing classical notions of
coherence, consistency, and geometric flatness. We establish links with
sheaf-theoretic contextuality, gauge theory, and noncommutative geometry, and
provide numerical and diagrammatic examples in both finite and infinite
dimensions.
  This framework opens novel directions in AI, from curvature-aware learning
architectures to topological regularization in uncertain or adversarial
environments.

</details>


### [128] [On Using Large-Batches in Federated Learning](https://arxiv.org/abs/2509.10537)
*Sahil Tyagi*

Main category: cs.LG

TL;DR: 本文提出了一种联邦学习中的大批次训练技术，旨在同时享受大批次训练的并行扩展性和小批次训练的泛化性能优势


<details>
  <summary>Details</summary>
Motivation: 联邦学习在资源受限设备上训练深度网络时面临通信效率与模型质量的权衡。大批次训练可以提高并行效率但会导致泛化性能下降，需要解决这一矛盾

Method: 提出利用小批次和大批次训练之间的权衡关系，探索新的训练方向，使模型既能享受大批次的并行扩展性，又能保持小批次训练的良好泛化能力

Result: 在相同迭代次数下，所提出的大批次训练技术在ResNet50和VGG11模型上分别比小批次训练获得了32.33%和3.74%的更高测试准确率

Conclusion: 该技术成功解决了联邦学习中大批次训练导致的泛化性能下降问题，实现了并行效率与模型质量的平衡

Abstract: Efficient Federated learning (FL) is crucial for training deep networks over
devices with limited compute resources and bounded networks. With the advent of
big data, devices either generate or collect multimodal data to train either
generic or local-context aware networks, particularly when data privacy and
locality is vital. FL algorithms generally trade-off between parallel and
statistical performance, improving model quality at the cost of higher
communication frequency, or vice versa. Under frequent synchronization
settings, FL over a large cluster of devices may perform more work per-training
iteration by processing a larger global batch-size, thus attaining considerable
training speedup. However, this may result in poor test performance (i.e., low
test loss or accuracy) due to generalization degradation issues associated with
large-batch training. To address these challenges with large-batches, this work
proposes our vision of exploiting the trade-offs between small and large-batch
training, and explore new directions to enjoy both the parallel scaling of
large-batches and good generalizability of small-batch training. For the same
number of iterations, we observe that our proposed large-batch training
technique attains about 32.33% and 3.74% higher test accuracy than small-batch
training in ResNet50 and VGG11 models respectively.

</details>


### [129] [DualAlign: Generating Clinically Grounded Synthetic Data](https://arxiv.org/abs/2509.10538)
*Rumeng Li,Xun Wang,Hong Yu*

Main category: cs.LG

TL;DR: DualAlign框架通过统计对齐和语义对齐生成更真实、临床合理的合成医疗数据，在阿尔茨海默病案例中显著提升模型性能


<details>
  <summary>Details</summary>
Motivation: 解决真实电子健康记录(EHR)的隐私限制、罕见疾病标注数据稀缺以及观察性数据集系统性偏差的问题，需要生成既真实又具有临床意义的合成医疗数据

Method: 提出DualAlign框架：1)统计对齐-基于人口统计学和风险因素条件生成；2)语义对齐-整合真实世界症状轨迹指导内容生成。使用LLaMA 3.1-8B模型进行微调

Result: 在阿尔茨海默病案例中，DualAlign生成的情境化症状级别句子更好地反映真实临床文档。结合人工标注数据和DualAlign生成数据训练的模型性能显著优于仅使用黄金数据或无指导合成基线的模型

Conclusion: 虽然DualAlign未能完全捕捉纵向复杂性，但为生成临床基础、隐私保护的合成数据支持低资源临床文本分析提供了实用方法

Abstract: Synthetic clinical data are increasingly important for advancing AI in
healthcare, given strict privacy constraints on real-world EHRs, limited
availability of annotated rare-condition data, and systemic biases in
observational datasets. While large language models (LLMs) can generate fluent
clinical text, producing synthetic data that is both realistic and clinically
meaningful remains challenging. We introduce DualAlign, a framework that
enhances statistical fidelity and clinical plausibility through dual alignment:
(1) statistical alignment, which conditions generation on patient demographics
and risk factors; and (2) semantic alignment, which incorporates real-world
symptom trajectories to guide content generation. Using Alzheimer's disease
(AD) as a case study, DualAlign produces context-grounded symptom-level
sentences that better reflect real-world clinical documentation. Fine-tuning an
LLaMA 3.1-8B model with a combination of DualAlign-generated and
human-annotated data yields substantial performance gains over models trained
on gold data alone or unguided synthetic baselines. While DualAlign does not
fully capture longitudinal complexity, it offers a practical approach for
generating clinically grounded, privacy-preserving synthetic data to support
low-resource clinical text analysis.

</details>


### [130] [GTS_Forecaster: a novel deep learning based geodetic time series forecasting toolbox with python](https://arxiv.org/abs/2509.10560)
*Xuechen Liang,Xiaoxing He,Shengdao Wang,Jean-Philippe Montillet,Zhengkai Huang,Gaël Kermarrec,Shunqiang Hu,Yu Zhou,Jiahui Huang*

Main category: cs.LG

TL;DR: GTS Forecaster是一个开源Python包，用于大地测量时间序列预测，整合了先进的深度学习模型来处理非线性时空模式，支持GNSS、SSH和TG数据集的预测、可视化和评估。


<details>
  <summary>Details</summary>
Motivation: 大地测量时间序列（如GNSS位置、卫星测高海面高度和验潮仪记录）对于监测地表形变和海平面变化至关重要。准确预测这些变量可以增强早期预警系统，支持地震、滑坡、海岸风暴潮和长期海平面变化的灾害缓解。然而，这些变量的非线性、非平稳性和不完整性给经典模型带来了重大挑战。

Method: 开发了GTS Forecaster开源Python包，整合了先进的深度学习模型，包括核注意力网络（KAN）、基于图神经网络的门控循环单元（GNNGRU）和时间感知图神经网络（TimeGNN）。还提供了强大的预处理工具，包括异常值检测和基于强化学习的间隙填充算法（Kalman-TransFusion插值框架KTIF）。

Result: GTS Forecaster目前支持GNSS、SSH和TG数据集的预测、可视化和评估，并且适用于一般时间序列应用。通过将尖端模型与易用接口相结合，促进了深度学习在大地测量预测任务中的应用。

Conclusion: 该研究提供了一个综合性的开源工具，通过整合先进的深度学习技术和预处理方法，有效解决了大地测量时间序列预测中的非线性、非平稳性和数据不完整性问题，为灾害预警和缓解提供了重要技术支持。

Abstract: Geodetic time series -- such as Global Navigation Satellite System (GNSS)
positions, satellite altimetry-derived sea surface height (SSH), and tide gauge
(TG) records -- is essential for monitoring surface deformation and sea level
change. Accurate forecasts of these variables can enhance early warning systems
and support hazard mitigation for earthquakes, landslides, coastal storm surge,
and long-term sea level. However, the nonlinear, non-stationary, and incomplete
nature of such variables presents significant challenges for classic models,
which often fail to capture long-term dependencies and complex spatiotemporal
dynamics. We introduce GTS Forecaster, an open-source Python package for
geodetic time series forecasting. It integrates advanced deep learning models
-- including kernel attention networks (KAN), graph neural network-based gated
recurrent units (GNNGRU), and time-aware graph neural networks (TimeGNN) -- to
effectively model nonlinear spatial-temporal patterns. The package also
provides robust preprocessing tools, including outlier detection and a
reinforcement learning-based gap-filling algorithm, the Kalman-TransFusion
Interpolation Framework (KTIF). GTS Forecaster currently supports forecasting,
visualization, and evaluation of GNSS, SSH, and TG datasets, and is adaptable
to general time series applications. By combining cutting-edge models with an
accessible interface, it facilitates the application of deep learning in
geodetic forecasting tasks.

</details>


### [131] [SME-TEAM: Leveraging Trust and Ethics for Secure and Responsible Use of AI and LLMs in SMEs](https://arxiv.org/abs/2509.10594)
*Iqbal H. Sarker,Helge Janicke,Ahmad Mohsin,Leandros Maglaras*

Main category: cs.LG

TL;DR: 提出一个四支柱框架（数据、算法、人工监督、模型架构），将伦理原则转化为中小企业AI应用的操作实践，促进负责任AI采用


<details>
  <summary>Details</summary>
Motivation: 解决中小企业在采用AI和大语言模型时面临的技术、伦理和信任问题，帮助中小企业安全负责任地使用AI技术

Method: 设计结构化多阶段框架，围绕数据、算法、人工监督和模型架构四个支柱，将理论伦理原则与操作实践相结合

Result: 开发出能够增强AI在中小企业多样化应用中能力的框架，为负责任AI采用提供结构化路线图

Conclusion: 该框架将信任和伦理作为催化剂，能够提升中小企业的韧性、竞争力和可持续创新能力

Abstract: Artificial Intelligence (AI) and Large Language Models (LLMs) are reshaping
today's business practices, however, their adoption within small and
medium-sized enterprises (SMEs) raises significant technical, ethical and trust
issues. This paper proposes a structured, multi-phased framework designed to
embed trust and ethical principles throughout the AI lifecycle for their secure
and responsible use in SMEs. Structured around four pillars, i.e., Data,
Algorithms, Human oversight, and Model Architecture, the framework bridges
theoretical ethical principles with operational practice, enhancing AI
capabilities in diverse SME applications. Ultimately, this paper offers a
structured roadmap for responsible AI adoption, framing trust and ethics as a
catalyst for resilience, competitiveness, and sustainable innovation in SMEs.

</details>


### [132] [pySigLib -- Fast Signature-Based Computations on CPU and GPU](https://arxiv.org/abs/2509.10613)
*Daniil Shmelev,Cristopher Salvi*

Main category: cs.LG

TL;DR: pySigLib是一个高性能Python库，提供CPU和GPU上签名和签名核的优化实现，解决了现有签名核方法在大规模时序数据上的可扩展性问题


<details>
  <summary>Details</summary>
Motivation: 签名核方法在时序数据机器学习中日益重要，特别是在量化金融领域作为判别器和生成模型训练损失函数，但现有实现无法处理实际应用中的大数据集和长序列

Method: 开发了高性能Python库pySigLib，提供CPU和GPU上的签名和签名核优化实现，完全兼容PyTorch自动微分，并引入了新颖的签名核微分方案

Result: 实现了大规模签名计算的高效软件栈，新微分方案以现有库运行时间的一小部分提供精确梯度

Conclusion: pySigLib解决了签名核方法在实际应用中的可扩展性瓶颈，为大规模时序数据分析提供了高效工具

Abstract: Signature-based methods have recently gained significant traction in machine
learning for sequential data. In particular, signature kernels have emerged as
powerful discriminators and training losses for generative models on
time-series, notably in quantitative finance. However, existing implementations
do not scale to the dataset sizes and sequence lengths encountered in practice.
We present pySigLib, a high-performance Python library offering optimised
implementations of signatures and signature kernels on CPU and GPU, fully
compatible with PyTorch's automatic differentiation. Beyond an efficient
software stack for large-scale signature-based computation, we introduce a
novel differentiation scheme for signature kernels that delivers accurate
gradients at a fraction of the runtime of existing libraries.

</details>


### [133] [Optimal Multimarginal Schrödinger Bridge: Minimum Spanning Tree over Measure-valued Vertices](https://arxiv.org/abs/2509.10626)
*Georgiy A. Bondar,Abhishek Halder*

Main category: cs.LG

TL;DR: 本文提出了寻找最优多边际薛定谔桥(MSB)的方法，通过在所有可能的图结构中寻找最优耦合，将问题转化为度量值顶点上的最小生成树问题。


<details>
  <summary>Details</summary>
Motivation: 传统的MSB需要预先指定相关结构图，但实际应用中最佳图结构往往未知。本文旨在寻找在所有可能图结构中最优的MSB耦合方案。

Method: 将最优MSB问题转化为最小生成树问题：第一步构建完全图，边权重为对应双边际SB的最优值加上端点熵之和；第二步在该加权图上求解标准最小生成树问题。

Result: 提出了计算最优MSB的有效方法，通过数值实验验证了所提解决方案的有效性和可行性。

Conclusion: 最优多边际薛定谔桥问题可以转化为度量值顶点上的最小生成树问题，并通过两步法有效求解，为处理未知相关结构的多变量耦合问题提供了新思路。

Abstract: The Multimarginal Schr\"odinger Bridge (MSB) finds the optimal coupling among
a collection of random vectors with known statistics and a known correlation
structure. In the MSB formulation, this correlation structure is specified
\emph{a priori} as an undirected connected graph with measure-valued vertices.
In this work, we formulate and solve the problem of finding the optimal MSB in
the sense we seek the optimal coupling over all possible graph structures. We
find that computing the optimal MSB amounts to solving the minimum spanning
tree problem over measure-valued vertices. We show that the resulting problem
can be solved in two steps. The first step constructs a complete graph with
edge weight equal to a sum of the optimal value of the corresponding bimarginal
SB and the entropies of the endpoints. The second step solves a standard
minimum spanning tree problem over that complete weighted graph. Numerical
experiments illustrate the proposed solution.

</details>


### [134] [Interpretable neural network system identification method for two families of second-order systems based on characteristic curves](https://arxiv.org/abs/2509.10632)
*Federico J. Gonzalez,Luis P. Lara*

Main category: cs.LG

TL;DR: 提出了基于特征曲线(CC)的统一数据驱动框架，结合微分方程数学结构和神经网络灵活性，通过三种识别策略(SINDy-CC、Poly-CC、NN-CC)实现非线性系统辨识，在保持可解释性的同时处理复杂非线性和不连续性。


<details>
  <summary>Details</summary>
Motivation: 解决非线性系统辨识中可解释性与灵活性之间的权衡问题，需要整合物理约束，同时利用神经网络的强大表达能力。

Method: 使用特征曲线(CC)概念，每个非线性函数由专用神经网络建模，提出三种识别策略：SINDy-CC(稀疏回归+方程结构约束)、Poly-CC(高次多项式表示)、NN-CC(神经网络无需基函数假设)。

Result: 三种方法都适用于简单多项式非线性系统(如van der Pol振荡器)，NN-CC在处理复杂非线性和不连续性系统(如粘滑系统)时表现最优。

Conclusion: 基于CC的框架，特别是NN-CC方法，能够在保持可解释性的同时捕捉复杂非线性，为具有不连续性和复杂非线性的系统建模提供了强大工具。

Abstract: Nonlinear system identification often involves a fundamental trade-off
between interpretability and flexibility, often requiring the incorporation of
physical constraints. We propose a unified data-driven framework that combines
the mathematical structure of the governing differential equations with the
flexibility of neural networks (NNs). At the core of our approach is the
concept of characteristic curves (CCs), which represent individual nonlinear
functions (e.g., friction and restoring components) of the system. Each CC is
modeled by a dedicated NN, enabling a modular and interpretable representation
of the system equation. To demonstrate the versatility of the CC-based
formalism, we introduce three identification strategies: (1) SINDy-CC, which
extends the sparse regression approach of SINDy by incorporating the
mathematical structure of the governing equations as constraints; (2) Poly-CC,
which represents each CC using high-degree polynomials; and (3) NN-CC, which
uses NNs without requiring prior assumptions about basis functions. Our results
show that all three approaches are well-suited for systems with simple
polynomial nonlinearities, such as the van der Pol oscillator. In contrast,
NN-CC demonstrates superior performance in modeling systems with complex
nonlinearities and discontinuities, such as those observed in stick-slip
systems. The key contribution of this work is to demonstrate that the CC-based
framework, particularly the NN-CC approach, can capture complex nonlinearities
while maintaining interpretability through the explicit representation of the
CCs. This balance makes it well-suited for modeling systems with
discontinuities and complex nonlinearities that are challenging to assess using
traditional polynomial or sparse regression methods, providing a powerful tool
for nonlinear system identification.

</details>


### [135] [Accurate and Private Diagnosis of Rare Genetic Syndromes from Facial Images with Federated Deep Learning](https://arxiv.org/abs/2509.10635)
*Ali Burak Ünal,Cem Ata Baykara,Peter Krawitz,Mete Akgün*

Main category: cs.LG

TL;DR: 提出了基于联邦学习的GestaltMatcher服务，通过跨机构协作训练全局特征提取器，在保护患者隐私的同时实现90%以上的集中式性能


<details>
  <summary>Details</summary>
Motivation: 传统GestaltMatcher框架依赖集中式数据集，但患者数据分散在不同机构且受隐私法规限制，限制了系统进一步发展

Method: 采用跨机构水平联邦学习框架，将患者数据映射到共享潜在空间，使用隐私保护核矩阵计算框架进行综合征推断和发现

Result: 联邦服务保持了90%以上的集中式性能，对不同机构数量和异构数据分布具有鲁棒性

Conclusion: 联邦学习方法有效解决了医疗数据隐私和分散性问题，新参与者可直接受益并贡献于系统

Abstract: Machine learning has shown promise in facial dysmorphology, where
characteristic facial features provide diagnostic clues for rare genetic
disorders. GestaltMatcher, a leading framework in this field, has demonstrated
clinical utility across multiple studies, but its reliance on centralized
datasets limits further development, as patient data are siloed across
institutions and subject to strict privacy regulations. We introduce a
federated GestaltMatcher service based on a cross-silo horizontal federated
learning framework, which allows hospitals to collaboratively train a global
ensemble feature extractor without sharing patient images. Patient data are
mapped into a shared latent space, and a privacy-preserving kernel matrix
computation framework enables syndrome inference and discovery while
safeguarding confidentiality. New participants can directly benefit from and
contribute to the system by adopting the global feature extractor and kernel
configuration from previous training rounds. Experiments show that the
federated service retains over 90% of centralized performance and remains
robust to both varying silo numbers and heterogeneous data distributions.

</details>


### [136] [Test-Time Warmup for Multimodal Large Language Models](https://arxiv.org/abs/2509.10641)
*Nikita Rajaneesh,Thomas Zollo,Richard Zemel*

Main category: cs.LG

TL;DR: 提出Test-Time Warmup方法，通过在推理前对MLLM进行测试实例级别的预热，利用弱监督辅助任务数据来提升多模态大语言模型在复杂推理任务上的性能


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在文本和图像交叉推理方面潜力巨大但尚未完全实现，主要原因是整个多模态模型训练数据量有限，导致复杂推理任务表现不佳

Method: 提出测试时预热方法，针对每个测试实例进行模型适配，利用弱监督辅助任务的数据来预热模型，而不是依赖大量标注数据进行微调

Result: 在Llama-Vision-Instruct模型上，MMMU任务相对性能提升4.03%，VQA-Rad提升5.28%，GQA提升1.63%

Conclusion: 推理前的'预热'可以显著增强多模态大语言模型在不同推理任务上的鲁棒性

Abstract: Multimodal Large Language Models (MLLMs) hold great promise for advanced
reasoning at the intersection of text and images, yet they have not fully
realized this potential. MLLMs typically integrate an LLM, a vision encoder,
and a connector that maps the vision encoder's embeddings into the LLM's text
embedding space. Although each component is pretrained on massive datasets with
billions of samples, the entire multimodal model is typically trained on only
thousands (or a few million) samples, which can result in weak performance on
complex reasoning tasks. To address these shortcomings, instead of relying on
extensive labeled datasets for fine-tuning, we propose a Test-Time Warmup
method that adapts the MLLM per test instance by leveraging data from weakly
supervised auxiliary tasks. With our approach, we observe a relative
performance improvement of 4.03% on MMMU, 5.28% on VQA-Rad, and 1.63% on GQA on
the Llama-Vision-Instruct model. Our method demonstrates that 'warming up'
before inference can enhance MLLMs' robustness across diverse reasoning tasks.

</details>


### [137] [Self-Supervised Goal-Reaching Results in Multi-Agent Cooperation and Exploration](https://arxiv.org/abs/2509.10656)
*Chirayu Nimonkar,Shlok Shah,Catherine Ji,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 本文提出了一种基于自监督目标达成技术的多智能体协作方法，通过最大化访问特定目标状态的概率来替代传统的标量奖励最大化，使智能体能够在稀疏奖励信号下学习协作和长时程推理。


<details>
  <summary>Details</summary>
Motivation: 设计能够激发多智能体协调行为和长时程推理的奖励函数具有挑战性。本文旨在研究如何利用自监督目标达成技术来促进智能体之间的合作，让人类用户可以通过指定单一目标状态而非复杂奖励函数来定义任务。

Method: 采用自监督目标达成技术，让智能体最大化访问特定目标状态的概率，而不是最大化标量奖励。该方法利用稀疏的反馈信号，使智能体能够从这种反馈中学习协作行为。

Result: 在多智能体强化学习基准测试中，该方法优于其他使用相同稀疏奖励信号的替代方法。即使没有显式的探索机制，该方法也能在替代方法从未观察到成功试验的环境中实现 emergent 合作和探索。

Conclusion: 自监督多智能体目标达成技术能够有效促进智能体间的协作和探索，为稀疏奖励环境下的多智能体协调问题提供了有前景的解决方案。

Abstract: For groups of autonomous agents to achieve a particular goal, they must
engage in coordination and long-horizon reasoning. However, designing reward
functions to elicit such behavior is challenging. In this paper, we study how
self-supervised goal-reaching techniques can be leveraged to enable agents to
cooperate. The key idea is that, rather than have agents maximize some scalar
reward, agents aim to maximize the likelihood of visiting a certain goal. This
problem setting enables human users to specify tasks via a single goal state
rather than implementing a complex reward function. While the feedback signal
is quite sparse, we will demonstrate that self-supervised goal-reaching
techniques enable agents to learn from such feedback. On MARL benchmarks, our
proposed method outperforms alternative approaches that have access to the same
sparse reward signal as our method. While our method has no explicit mechanism
for exploration, we observe that self-supervised multi-agent goal-reaching
leads to emergent cooperation and exploration in settings where alternative
approaches never witness a single successful trial.

</details>


### [138] [M4GN: Mesh-based Multi-segment Hierarchical Graph Network for Dynamic Simulations](https://arxiv.org/abs/2509.10659)
*Bo Lei,Victor M. Castillo,Yeping Hu*

Main category: cs.LG

TL;DR: M4GN是一个三层次、基于分段的层次图神经网络，通过混合分割策略和模态分解特征指导的细化，解决了传统GNN在大规模网格上的计算成本和过平滑问题，在保持精度的同时显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 解决基于网格的图神经网络在大型长程网格上深度消息传递带来的高计算成本和过平滑问题，同时克服层次GNN在构建粗化图和保持细尺度精度方面的挑战。

Method: 采用三层次分段中心架构：1）混合分割策略结合快速图分割器和超像素式细化；2）置换不变聚合器编码分段信息；3）微观GNN捕获局部动态，宏观Transformer跨段推理。

Result: 在多个基准数据集上，M4GN相比最先进基线方法预测精度提升高达56%，推理速度提升高达22%。

Conclusion: M4GN通过创新的分段策略和层次架构，在PDE模拟中实现了精度和效率的平衡，为大规模网格计算提供了有效的解决方案。

Abstract: Mesh-based graph neural networks (GNNs) have become effective surrogates for
PDE simulations, yet their deep message passing incurs high cost and
over-smoothing on large, long-range meshes; hierarchical GNNs shorten
propagation paths but still face two key obstacles: (i) building coarse graphs
that respect mesh topology, geometry, and physical discontinuities, and (ii)
maintaining fine-scale accuracy without sacrificing the speed gained from
coarsening. We tackle these challenges with M4GN, a three-tier, segment-centric
hierarchical network. M4GN begins with a hybrid segmentation strategy that
pairs a fast graph partitioner with a superpixel-style refinement guided by
modal-decomposition features, producing contiguous segments of dynamically
consistent nodes. These segments are encoded by a permutation-invariant
aggregator, avoiding the order sensitivity and quadratic cost of aggregation
approaches used in prior works. The resulting information bridges a micro-level
GNN, which captures local dynamics, and a macro-level transformer that reasons
efficiently across segments, achieving a principled balance between accuracy
and efficiency. Evaluated on multiple representative benchmark datasets, M4GN
improves prediction accuracy by up to 56% while achieving up to 22% faster
inference than state-of-the-art baselines.

</details>


### [139] [Least-Ambiguous Multi-Label Classifier](https://arxiv.org/abs/2509.10689)
*Misgina Tsighe Hagos,Claes Lundström*

Main category: cs.LG

TL;DR: 提出一种模型无关的单正例多标签学习方法，利用保形预测生成校准的集合值输出，在12个基准数据集上表现优于现有基线


<details>
  <summary>Details</summary>
Motivation: 多标签学习中收集完整标签标注成本高昂，许多数据集每个训练实例只标注单个正标签，但实际存在多个相关标签，这种单正例多标签学习(SPMLL)由于极端部分监督而具有挑战性

Method: 基于保形预测的模型无关方法，产生校准的集合值输出，不依赖标签分布假设，在单标签训练和多标签评估之间架起监督鸿沟

Result: 在12个基准数据集上评估，相比现有基线方法取得一致性的改进，具有实际应用价值

Conclusion: 该方法能够可靠地进行多标签预测，解决了SPMLL中的监督不足问题，无需对标签分布做出假设

Abstract: Multi-label learning often requires identifying all relevant labels for
training instances, but collecting full label annotations is costly and
labor-intensive. In many datasets, only a single positive label is annotated
per training instance, despite the presence of multiple relevant labels. This
setting, known as single-positive multi-label learning (SPMLL), presents a
significant challenge due to its extreme form of partial supervision. We
propose a model-agnostic approach to SPMLL that draws on conformal prediction
to produce calibrated set-valued outputs, enabling reliable multi-label
predictions at test time. Our method bridges the supervision gap between
single-label training and multi-label evaluation without relying on label
distribution assumptions. We evaluate our approach on 12 benchmark datasets,
demonstrating consistent improvements over existing baselines and practical
applicability.

</details>


### [140] [Learning Concave Bid Shading Strategies in Online Auctions via Measure-valued Proximal Optimization](https://arxiv.org/abs/2509.10693)
*Iman Nodozi,Djordje Gligorijevic,Abhishek Halder*

Main category: cs.LG

TL;DR: 提出一种基于测度值优化的首价拍卖竞价策略，通过Wasserstein-proximal更新自适应调整竞价参数分布


<details>
  <summary>Details</summary>
Motivation: 解决首价拍卖中的竞价优化问题，通过参数化竞价策略在考虑上下文信息的情况下最大化期望收益

Method: 将竞价优化建模为测度值凸优化问题，使用正则化Wasserstein-proximal更新来调整竞价参数分布，能量函数基于上下文条件

Result: 证明了该测度值凸优化问题存在闭式解，数值示例验证了方法的有效性

Conclusion: 该方法能够有效优化首价拍卖中的竞价策略，通过在期望收益高的区域集中竞价分布权重来提高性能

Abstract: This work proposes a bid shading strategy for first-price auctions as a
measure-valued optimization problem. We consider a standard parametric form for
bid shading and formulate the problem as convex optimization over the joint
distribution of shading parameters. After each auction, the shading parameter
distribution is adapted via a regularized Wasserstein-proximal update with a
data-driven energy functional. This energy functional is conditional on the
context, i.e., on publisher/user attributes such as domain, ad slot type,
device, or location. The proposed algorithm encourages the bid distribution to
place more weight on values with higher expected surplus, i.e., where the win
probability and the value gap are both large. We show that the resulting
measure-valued convex optimization problem admits a closed form solution. A
numerical example illustrates the proposed method.

</details>


### [141] [Verifying Computational Graphs in Production-Grade Distributed Machine Learning Frameworks](https://arxiv.org/abs/2509.10694)
*Kahfi S. Zulkifli,Wenbo Qian,Shaowei Zhu,Yuan Zhou,Zhen Zhang,Chang Lou*

Main category: cs.LG

TL;DR: Scalify是一个轻量级框架，通过等式饱和和数据日志式推理验证计算图的语义等价性，暴露机器学习框架中的静默错误，能在普通机器上几分钟内验证Llama-3.1-405B等超大模型


<details>
  <summary>Details</summary>
Motivation: 现代机器学习框架通过并行化和优化技术支持超大模型，但这些技术增加了复杂性，引入了严重降低模型性能的静默错误，现有解决方案要么是临时的，要么在生产环境中成本太高

Method: 使用等式饱和和数据日志式推理验证计算图的语义等价性；通过并行重写和层记忆化对图进行分区；重用重写模板；用关系推理和符号双射推理增强等式饱和；将差异定位到精确的代码位置

Result: 在普通机器上几分钟内验证了Llama-3.1-405B等超大模型；在亚马逊生产机器学习框架中发现了5个未知错误

Conclusion: Scalify提供了一个可扩展的轻量级解决方案，能够有效检测机器学习框架中的静默错误，并为调试提供可操作的指导

Abstract: Modern machine learning frameworks support very large models by incorporating
parallelism and optimization techniques. Yet, these very techniques add new
layers of complexity, introducing silent errors that severely degrade model
performance. Existing solutions are either ad hoc or too costly for production.
  We present Scalify, a lightweight framework that exposes silent errors by
verifying semantic equivalence of computational graphs using equality
saturation and Datalog-style reasoning. To scale, Scalify partitions graphs
with parallel rewriting and layer memoization, reuses rewrite templates, and
augments equality saturation with relational reasoning and symbolic bijection
inference. It further localizes discrepancies to precise code sites, turning
verification results into actionable debugging guidance. Scalify verifies
models as large as Llama-3.1-405B within minutes on a commodity machine and
exposed five unknown bugs in Amazon production machine learning frameworks.

</details>


### [142] [Kalman Bayesian Transformer](https://arxiv.org/abs/2509.10695)
*Haoming Jing,Oren Wright,José M. F. Moura,Yorie Nakahira*

Main category: cs.LG

TL;DR: 提出了一种基于贝叶斯框架的顺序微调方法，通过后验推断、矩传播和卡尔曼贝叶斯神经网络来处理数据分布漂移和有限内存环境下的顺序学习问题。


<details>
  <summary>Details</summary>
Motivation: 解决顺序学习中面临的数据分布漂移、小样本训练稳定性、延迟关键环境下的训练效率以及不确定性量化等挑战。

Method: 将顺序微调构建为贝叶斯后验推断问题，整合闭式矩传播、卡尔曼贝叶斯神经网络和softmax函数的泰勒近似矩计算，以前训练模型作为先验并基于不确定性自适应平衡新旧信息。

Result: 通过数值模拟验证了方法在决策变换器顺序适应分布漂移任务和有限内存资源场景下的有效性。

Conclusion: 该方法实现了鲁棒且数据高效的顺序学习，能够显式处理预训练模型先验并通过量化不确定性自适应平衡新信息。

Abstract: Sequential fine-tuning of transformers is useful when new data arrive
sequentially, especially with shifting distributions. Unlike batch learning,
sequential learning demands that training be stabilized despite a small amount
of data by balancing new information and previously learned knowledge in the
pre-trained models. This challenge is further complicated when training is to
be completed in latency-critical environments and learning must additionally
quantify and be mediated by uncertainty. Motivated by these challenges, we
propose a novel method that frames sequential fine-tuning as a posterior
inference problem within a Bayesian framework. Our approach integrates
closed-form moment propagation of random variables, Kalman Bayesian Neural
Networks, and Taylor approximations of the moments of softmax functions. By
explicitly accounting for pre-trained models as priors and adaptively balancing
them against new information based on quantified uncertainty, our method
achieves robust and data-efficient sequential learning. The effectiveness of
our method is demonstrated through numerical simulations involving sequential
adaptation of a decision transformer to tasks characterized by distribution
shifts and limited memory resources.

</details>


### [143] [CrunchLLM: Multitask LLMs for Structured Business Reasoning and Outcome Prediction](https://arxiv.org/abs/2509.10698)
*Rabeya Tus Sadia,Qiang Cheng*

Main category: cs.LG

TL;DR: CrunchLLM是一个针对创业公司成功预测的领域适应大语言模型框架，通过结合结构化数据和非结构化文本，在Crunchbase数据集上实现了超过80%的准确率


<details>
  <summary>Details</summary>
Motivation: 预测初创公司成功（通过收购或IPO退出）是创业和创新研究的关键问题，传统机器学习方法准确率有限，而大语言模型难以直接适应特定领域业务数据

Method: CrunchLLM框架整合结构化公司属性和非结构化文本叙述，应用参数高效微调策略和提示优化，专门针对创业数据进行领域适应

Result: 在Crunchbase初创公司成功预测上达到超过80%的准确率，显著优于传统分类器和基线LLM，并提供可解释的推理轨迹

Conclusion: 通过领域感知微调和结构化-非结构化数据融合来适应LLM，可以推进创业结果的预测建模，为风险投资和创新政策提供方法论框架和实用工具

Abstract: Predicting the success of start-up companies, defined as achieving an exit
through acquisition or IPO, is a critical problem in entrepreneurship and
innovation research. Datasets such as Crunchbase provide both structured
information (e.g., funding rounds, industries, investor networks) and
unstructured text (e.g., company descriptions), but effectively leveraging this
heterogeneous data for prediction remains challenging. Traditional machine
learning approaches often rely only on structured features and achieve moderate
accuracy, while large language models (LLMs) offer rich reasoning abilities but
struggle to adapt directly to domain-specific business data. We present
\textbf{CrunchLLM}, a domain-adapted LLM framework for startup success
prediction. CrunchLLM integrates structured company attributes with
unstructured textual narratives and applies parameter-efficient fine-tuning
strategies alongside prompt optimization to specialize foundation models for
entrepreneurship data. Our approach achieves accuracy exceeding 80\% on
Crunchbase startup success prediction, significantly outperforming traditional
classifiers and baseline LLMs. Beyond predictive performance, CrunchLLM
provides interpretable reasoning traces that justify its predictions, enhancing
transparency and trustworthiness for financial and policy decision makers. This
work demonstrates how adapting LLMs with domain-aware fine-tuning and
structured--unstructured data fusion can advance predictive modeling of
entrepreneurial outcomes. CrunchLLM contributes a methodological framework and
a practical tool for data-driven decision making in venture capital and
innovation policy.

</details>


### [144] [Using LLMs for Late Multimodal Sensor Fusion for Activity Recognition](https://arxiv.org/abs/2509.10729)
*Ilker Demirel,Karan Thakkar,Benjamin Elizalde,Miquel Espi Marques,Shirley Ren,Jaya Narain*

Main category: cs.LG

TL;DR: LLM用于音频和运动传感器数据的后期融合，实现零样本和少样本活动分类，在Ego4D数据集上取得显著高于随机水平的性能


<details>
  <summary>Details</summary>
Motivation: 传感器数据流包含丰富的活动和上下文信息，但整合互补信息具有挑战性。传统方法需要大量对齐的训练数据来学习共享嵌入空间

Method: 使用大型语言模型(LLM)对音频和运动时间序列数据进行后期融合，在Ego4D数据集的多样化活动识别子集上进行零样本和少样本分类

Result: LLM在12类活动分类中取得了显著高于随机水平的F1分数，无需任务特定训练，零样本和少样本分类均表现良好

Conclusion: LLM基于融合的方法可以在有限对齐训练数据的情况下实现多模态时间应用，且无需为特定应用部署额外的多模态模型，节省内存和计算资源

Abstract: Sensor data streams provide valuable information around activities and
context for downstream applications, though integrating complementary
information can be challenging. We show that large language models (LLMs) can
be used for late fusion for activity classification from audio and motion time
series data. We curated a subset of data for diverse activity recognition
across contexts (e.g., household activities, sports) from the Ego4D dataset.
Evaluated LLMs achieved 12-class zero- and one-shot classification F1-scores
significantly above chance, with no task-specific training. Zero-shot
classification via LLM-based fusion from modality-specific models can enable
multimodal temporal applications where there is limited aligned training data
for learning a shared embedding space. Additionally, LLM-based fusion can
enable model deploying without requiring additional memory and computation for
targeted application-specific multimodal models.

</details>


### [145] [Matched-Pair Experimental Design with Active Learning](https://arxiv.org/abs/2509.10742)
*Weizhi Li,Gautam Dasarathy,Visar Berisha*

Main category: cs.LG

TL;DR: 本文提出了一种主动学习的匹配对实验设计方法，通过顺序招募患者来识别高治疗效果区域，将目标区域识别问题构建为分类问题，既降低了实验成本，又确保覆盖整个高治疗效果区域。


<details>
  <summary>Details</summary>
Motivation: 传统匹配对实验设计在整个群体中检测治疗效果，当整体效应较小时，需要识别和靶向治疗效果最高的特定区域，以提高干预效果和实验效率。

Method: 提出主动学习框架，将高治疗效果区域的识别构建为分类问题，在匹配对设计中顺序主动招募患者，通过理论分析和实验验证方法的有效性。

Result: 理论分析表明该方法具有较低的标签复杂度，实验证明在多种实际场景下都能有效识别高治疗效果区域，显著降低实验成本。

Conclusion: 所提出的主动学习匹配对实验设计能够高效识别高治疗效果区域，确保覆盖整个目标区域，为靶向干预提供了有效的实验设计方法。

Abstract: Matched-pair experimental designs aim to detect treatment effects by pairing
participants and comparing within-pair outcome differences. In many situations,
the overall effect size is small across the entire population. Then, the focus
naturally shifts to identifying and targeting high treatment-effect regions
where the intervention is most effective. This paper proposes a matched-pair
experimental design that sequentially and actively enrolls patients in high
treatment-effect regions. Importantly, we frame the identification of the
target region as a classification problem and propose an active learning
framework tailored to matched-pair designs. The proposed design not only
reduces the experimental cost of detecting treatment efficacy, but also ensures
that the identified regions enclose the entire high-treatment-effect regions.
Our theoretical analysis of the framework's label complexity, along with
experiments in practical scenarios, demonstrates the efficiency and advantages
of the approach.

</details>


### [146] [HalluField: Detecting LLM Hallucinations via Field-Theoretic Modeling](https://arxiv.org/abs/2509.10753)
*Minh Vu,Brian K. Tran,Syed A. Shah,Geigh Zollicoffer,Nhat Hoang-Xuan,Manish Bhattarai*

Main category: cs.LG

TL;DR: HalluField是一种基于场论和热力学原理的幻觉检测方法，通过分析LLM输出token路径的能量和熵分布来量化语义稳定性，从而检测幻觉。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常产生不准确或不可靠的内容（幻觉），这限制了它们在高风险应用中的部署，因此需要一种通用的幻觉检测方法。

Method: 基于参数化变分原理和热力学，将LLM对查询和温度设置的响应建模为离散似然token路径集合，每个路径关联相应的能量和熵，通过分析温度和似然变化下能量和熵分布的变异来检测幻觉。

Result: HalluField在计算上高效且实用，直接操作模型输出logits，无需微调或辅助神经网络，在模型和数据集上实现了最先进的幻觉检测性能。

Conclusion: 通过物理透镜建模LLM行为，HalluField提供了一种原理性的幻觉检测方法，在性能和实用性方面都表现出色。

Abstract: Large Language Models (LLMs) exhibit impressive reasoning and
question-answering capabilities. However, they often produce inaccurate or
unreliable content known as hallucinations. This unreliability significantly
limits their deployment in high-stakes applications. Thus, there is a growing
need for a general-purpose method to detect hallucinations in LLMs. In this
work, we introduce HalluField, a novel field-theoretic approach for
hallucination detection based on a parametrized variational principle and
thermodynamics. Inspired by thermodynamics, HalluField models an LLM's response
to a given query and temperature setting as a collection of discrete likelihood
token paths, each associated with a corresponding energy and entropy. By
analyzing how energy and entropy distributions vary across token paths under
changes in temperature and likelihood, HalluField quantifies the semantic
stability of a response. Hallucinations are then detected by identifying
unstable or erratic behavior in this energy landscape. HalluField is
computationally efficient and highly practical: it operates directly on the
model's output logits without requiring fine-tuning or auxiliary neural
networks. Notably, the method is grounded in a principled physical
interpretation, drawing analogies to the first law of thermodynamics.
Remarkably, by modeling LLM behavior through this physical lens, HalluField
achieves state-of-the-art hallucination detection performance across models and
datasets.

</details>


### [147] [Contextual Budget Bandit for Food Rescue Volunteer Engagement](https://arxiv.org/abs/2509.10777)
*Ariana Tang,Naveen Raman,Fei Fang,Zheyuan Ryan Shi*

Main category: cs.LG

TL;DR: 提出Contextual Budget Bandit算法解决志愿者食品救援平台中的地理不平等问题，通过预算分配优化匹配率低的社区


<details>
  <summary>Details</summary>
Motivation: 现有算法在提升志愿者参与度时会加剧地理不平等，导致某些社区系统性处于劣势地位

Method: 结合情境相关预算分配的 restless multi-armed bandits 模型，开发启发式算法和保证最优的Mitosis算法

Result: 在合成和真实食品救援数据集上优于基线方法，实现了地理公平性

Conclusion: 所提算法能有效缓解食品救援中的地理不平等问题，平衡志愿者参与度和社区公平性

Abstract: Volunteer-based food rescue platforms tackle food waste by matching surplus
food to communities in need. These platforms face the dual problem of
maintaining volunteer engagement and maximizing the food rescued. Existing
algorithms to improve volunteer engagement exacerbate geographical disparities,
leaving some communities systematically disadvantaged. We address this issue by
proposing Contextual Budget Bandit. Contextual Budget Bandit incorporates
context-dependent budget allocation in restless multi-armed bandits, a model of
decision-making which allows for stateful arms. By doing so, we can allocate
higher budgets to communities with lower match rates, thereby alleviating
geographical disparities. To tackle this problem, we develop an empirically
fast heuristic algorithm. Because the heuristic algorithm can achieve a poor
approximation when active volunteers are scarce, we design the Mitosis
algorithm, which is guaranteed to compute the optimal budget allocation.
Empirically, we demonstrate that our algorithms outperform baselines on both
synthetic and real-world food rescue datasets, and show how our algorithm
achieves geographical fairness in food rescue.

</details>


### [148] [GoldenTransformer: A Modular Fault Injection Framework for Transformer Robustness Research](https://arxiv.org/abs/2509.10790)
*Luke Howard*

Main category: cs.LG

TL;DR: GoldenTransformer是一个模块化的故障注入框架，用于评估大型语言模型在硬件故障条件下的鲁棒性，支持多种故障类型注入和实验分析。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer模型在各种领域取得了最先进的性能，但其在故障条件下的鲁棒性研究仍然不足，需要专门的工具来评估模型对硬件故障的弹性。

Method: 基于PyTorch和HuggingFace Transformers构建的Python平台，支持权重损坏、激活注入和注意力层干扰等多种故障注入，提供实验可重复性、指标记录和可视化功能。

Result: 框架成功实现了对预训练Transformer模型的多点故障注入，并通过分类和生成任务的示例实验验证了其有效性。

Conclusion: GoldenTransformer为研究人员和从业者提供了宝贵的模型鲁棒性分析工具，有助于指导现实世界LLM应用中的可靠系统设计。

Abstract: Transformers have become the foundation for a wide range of
state--of--the--art models across natural language processing, computer vision,
and other machine learning domains. Despite their widespread deployment, the
robustness of these models under fault conditions remains underexplored. We
present GoldenTransformer, a modular and extensible fault injection framework
designed to evaluate the resiliency of Large Language Models to induced
hardware faults. GoldenTransformer offers a unified Python-based platform for
injecting diverse classes of faults--such as weight corruption, activation
injections, and attention--level disruptions--into pretrained
transformer--based models. Inspired by the GoldenEye simulator for DNNs, our
framework focuses on the unique challenges of working with large transformer
architectures, including considerations such as structural complexity, latent
dependencies, and nonuniform layer definitions. GoldenTransformer is built atop
PyTorch and HuggingFace Transformers, and it supports experiment
reproducibility, metric logging, and visualization out of the box. We detail
the technical design and use of GoldenTransformer and demonstrate through
several example experiments on classification and generation tasks. By enabling
controlled injection of faults at multiple logical and structural points in a
transformer, GoldenTransformer offers researchers and practitioners a valuable
tool for model robustness analysis and for guiding dependable system design in
real-world LLM applications.

</details>


### [149] [Rethinking Sparse Autoencoders: Select-and-Project for Fairness and Control from Encoder Features Alone](https://arxiv.org/abs/2509.10809)
*Antonio Bărbălau,Cristian Daniel Păduraru,Teodor Poncu,Alexandru Tifrea,Elena Burceanu*

Main category: cs.LG

TL;DR: 本文提出了一种基于编码器的稀疏自编码器去偏方法S&P TopK，通过正交化输入嵌入与编码器权重，在保持下游性能的同时显著提升了公平性指标。


<details>
  <summary>Details</summary>
Motivation: 当前基于稀疏自编码器的去偏方法都假设特征表示存在于解码器权重中，本文挑战这一基本假设，探索编码器在表示去偏中的作用。

Method: 提出了S&P TopK框架，包含三个关键创新：(i)非常规的SAE特征选择策略，(ii)通过正交化输入嵌入与编码器权重的新去偏方法，(iii)通过编码器权重插值保持性能的机制。

Result: S&P TopK在公平性指标上比传统SAE方法提升高达3.2倍，在VLM测试时去偏方面达到最先进水平的1.8倍提升，同时保持了下游任务性能。

Conclusion: 编码器权重在表示去偏中具有重要作用，S&P TopK框架为稀疏自编码器的去偏应用提供了新的有效途径。

Abstract: Sparse Autoencoders (SAEs) have proven valuable due to their ability to
provide interpretable and steerable representations. Current debiasing methods
based on SAEs manipulate these sparse activations presuming that feature
representations are housed within decoder weights. We challenge this
fundamental assumption and introduce an encoder-focused alternative for
representation debiasing, contributing three key findings: (i) we highlight an
unconventional SAE feature selection strategy, (ii) we propose a novel SAE
debiasing methodology that orthogonalizes input embeddings against encoder
weights, and (iii) we establish a performance-preserving mechanism during
debiasing through encoder weight interpolation. Our Selection and Projection
framework, termed S\&P TopK, surpasses conventional SAE usage in fairness
metrics by a factor of up to 3.2 and advances state-of-the-art test-time VLM
debiasing results by a factor of up to 1.8 while maintaining downstream
performance.

</details>


### [150] [FACTORS: Factorial Approximation for Complementary Two-factor Optimization with Risk-aware Scoring](https://arxiv.org/abs/2509.10825)
*Dongseok Kim,Wonjun Jeong,Gisung Oh*

Main category: cs.LG

TL;DR: FACTORS是一个结合实验设计与Shapley分解的框架，用于解决对训练因素组合敏感的性能和稳定性问题，通过风险调整目标函数在固定预算下可靠选择配置。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习模型性能对多种训练因素组合敏感的问题，传统方法难以在有限预算下同时考虑性能、稳定性和成本。

Method: 采用实验设计与Shapley分解相结合的方法，通过两条互补路径（基于条件均值的插件路径和基于最小二乘的路径）估计主效应和双因素交互作用，并整合到风险调整目标函数中。

Result: 在多样化数据集和设计条件下，提高了排名保持和最优配置识别能力，降低了决策风险，在预算约束下提供可解释的稳定性能提升。

Conclusion: FACTORS框架为机器学习调优提供了理论基础和实践工具，能够在有限预算下实现可解释的性能优化和稳定配置选择。

Abstract: We propose FACTORS, a framework that combines design of experiments with
Shapley decomposition to address performance and stability issues that are
sensitive to combinations of training factors. Our approach consistently
estimates main effects and two-factor interactions, then integrates them into a
risk-adjusted objective function that jointly accounts for uncertainty and
cost, enabling reliable selection of configurations under a fixed budget.
Effect estimation is implemented through two complementary paths: a plug-in
path based on conditional means, and a least-squares path that reconstructs
Shapley contributions from samples. These paths are designed to work
complementarily even when design density and bias levels differ. By
incorporating standardization of estimates, bias correction, and uncertainty
quantification, our procedure ensures comparability across heterogeneous factor
spaces and designs, while a lightweight search routine yields configurations
within practical time even for large factor spaces. On the theoretical side, we
provide error decompositions, sample complexity analysis, and upper bounds on
optimality gaps. On the interpretive side, we summarize main effects and
interactions in map form, highlighting adjustment priorities and safe
improvement pathways. Across diverse datasets and design conditions, our
approach improves rank preservation and optimal configuration identification,
reduces decision-making risks, and offers a tuning foundation that delivers
interpretable justification alongside stable performance gains even under
budget constraints.

</details>


### [151] [Neurosymbolic AI Transfer Learning Improves Network Intrusion Detection](https://arxiv.org/abs/2509.10850)
*Huynh T. T. Tran,Jacob Sander,Achraf Cohen,Brian Jalaian,Nathaniel D. Bastian*

Main category: cs.LG

TL;DR: 本文提出了一种创新的神经符号AI框架，应用于网络入侵检测系统，通过迁移学习和不确定性量化技术，在大规模结构化数据集上训练的迁移学习模型优于依赖小数据集的神经网络模型。


<details>
  <summary>Details</summary>
Motivation: 迁移学习在计算机视觉、自然语言处理和医学影像等领域已广泛应用，但在网络安全领域的应用尚未得到充分探索。网络入侵检测系统在对抗恶意网络活动中起着关键作用，需要更有效的解决方案。

Method: 开发了一种神经符号AI框架，结合迁移学习和不确定性量化技术，利用大规模结构化数据集进行训练，应用于网络入侵检测任务。

Result: 研究结果表明，基于大规模结构化数据集训练的迁移学习模型在性能上超越了依赖较小数据集的神经网络模型。

Conclusion: 该研究为网络安全解决方案开辟了新途径，证明了迁移学习在网络安全领域的巨大潜力，特别是在网络入侵检测系统中的应用前景。

Abstract: Transfer learning is commonly utilized in various fields such as computer
vision, natural language processing, and medical imaging due to its impressive
capability to address subtasks and work with different datasets. However, its
application in cybersecurity has not been thoroughly explored. In this paper,
we present an innovative neurosymbolic AI framework designed for network
intrusion detection systems, which play a crucial role in combating malicious
activities in cybersecurity. Our framework leverages transfer learning and
uncertainty quantification. The findings indicate that transfer learning
models, trained on large and well-structured datasets, outperform neural-based
models that rely on smaller datasets, paving the way for a new era in
cybersecurity solutions.

</details>


### [152] [CogGNN: Cognitive Graph Neural Networks in Generative Connectomics](https://arxiv.org/abs/2509.10864)
*Mayssa Soussia,Yijun Lin,Mohamed Ali Mahjoub,Islem Rekik*

Main category: cs.LG

TL;DR: CogGNN是首个认知化生成模型，通过赋予图神经网络认知能力（如视觉记忆）来生成保留认知特征的大脑网络，在连接脑模板学习中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于图神经网络的生成方法仅关注结构和拓扑特性，忽视了认知特征，需要开发能够同时保留结构和认知属性的生成模型。

Method: 提出CogGNN模型，集成视觉输入等认知能力，采用基于视觉记忆的损失函数和协同优化策略来生成认知增强的连接脑模板。

Result: 大量实验表明CogGNN在性能上优于最先进的方法，生成的模板既具有认知意义又保持结构完整性。

Conclusion: CogGNN为认知基础的大脑网络建模建立了坚实基础，推动了认知感知生成模型在神经科学中的应用。

Abstract: Generative learning has advanced network neuroscience, enabling tasks like
graph super-resolution, temporal graph prediction, and multimodal brain graph
fusion. However, current methods, mainly based on graph neural networks (GNNs),
focus solely on structural and topological properties, neglecting cognitive
traits. To address this, we introduce the first cognified generative model,
CogGNN, which endows GNNs with cognitive capabilities (e.g., visual memory) to
generate brain networks that preserve cognitive features. While broadly
applicable, we present CogGNN, a specific variant designed to integrate visual
input, a key factor in brain functions like pattern recognition and memory
recall. As a proof of concept, we use our model to learn connectional brain
templates (CBTs), population-level fingerprints from multi-view brain networks.
Unlike prior work that overlooks cognitive properties, CogGNN generates CBTs
that are both cognitively and structurally meaningful. Our contributions are:
(i) a novel cognition-aware generative model with a visual-memory-based loss;
(ii) a CBT-learning framework with a co-optimization strategy to yield
well-centered, discriminative, cognitively enhanced templates. Extensive
experiments show that CogGNN outperforms state-of-the-art methods, establishing
a strong foundation for cognitively grounded brain network modeling.

</details>


### [153] [GTHNA: Local-global Graph Transformer with Memory Reconstruction for Holistic Node Anomaly Evaluation](https://arxiv.org/abs/2509.10869)
*Mingkang Li,Xuexiong Luo,Yue Zhang,Yaoyang Li,Fu Lin*

Main category: cs.LG

TL;DR: 提出了一种新颖的图异常检测框架，通过局部-全局Transformer编码器、记忆引导重建机制和多尺度表示匹配策略，有效解决了现有方法中的过平滑和异常节点干扰问题。


<details>
  <summary>Details</summary>
Motivation: 图结构数据中的异常检测具有挑战性，现有GCN方法存在过平滑问题，图重建方法容易受到异常节点干扰，导致检测不准确。

Method: 集成三个关键组件：局部-全局Transformer编码器捕获结构依赖关系，记忆引导重建机制抑制异常节点影响，多尺度表示匹配策略从多个粒度评估异常。

Result: 在七个基准数据集上的广泛实验表明，该方法优于现有最先进方法，为跨域图异常检测提供了全面且可推广的解决方案。

Conclusion: 该框架通过协同工作组件实现了更鲁棒的异常评估，为图异常检测领域提供了有效的解决方案。

Abstract: Anomaly detection in graph-structured data is an inherently challenging
problem, as it requires the identification of rare nodes that deviate from the
majority in both their structural and behavioral characteristics. Existing
methods, such as those based on graph convolutional networks (GCNs), often
suffer from over-smoothing, which causes the learned node representations to
become indistinguishable. Furthermore, graph reconstruction-based approaches
are vulnerable to anomalous node interference during the reconstruction
process, leading to inaccurate anomaly detection. In this work, we propose a
novel and holistic anomaly evaluation framework that integrates three key
components: a local-global Transformer encoder, a memory-guided reconstruction
mechanism, and a multi-scale representation matching strategy. These components
work synergistically to enhance the model's ability to capture both local and
global structural dependencies, suppress the influence of anomalous nodes, and
assess anomalies from multiple levels of granularity. Anomaly scores are
computed by combining reconstruction errors and memory matching signals,
resulting in a more robust evaluation. Extensive experiments on seven benchmark
datasets demonstrate that our method outperforms existing state-of-the-art
approaches, offering a comprehensive and generalizable solution for anomaly
detection across various graph domains.

</details>


### [154] [Optimal message passing for molecular prediction is simple, attentive and spatial](https://arxiv.org/abs/2509.10871)
*Alma C. Castaneda-Leautaud,Rommie E. Amaro*

Main category: cs.LG

TL;DR: 通过简化消息传递机制和使用多维度分子图描述符，设计了性能超越复杂预训练模型的MPNN架构，在保持预测性能的同时降低50%以上计算成本


<details>
  <summary>Details</summary>
Motivation: 提高消息传递神经网络在分子属性预测中的性能，通过简化消息传递过程和采用捕捉分子图多维度特征的描述符

Method: 设计双向消息传递+注意力机制的简约架构，排除自感知；分析结构多样性对模型组件需求的影响；评估2D/3D图结构和空间特征的作用

Result: 在多数数据集上达到state-of-the-art性能；发现卷积归一化因子无益；2D分子图配合适当3D描述符即可保持性能，计算成本降低50%以上

Conclusion: 相对简单的MPNN模型可获得更高类别可分性；3D描述符补充2D图结构是计算高效的策略；结构多样性影响模型组件需求

Abstract: Strategies to improve the predicting performance of Message-Passing
Neural-Networks for molecular property predictions can be achieved by
simplifying how the message is passed and by using descriptors that capture
multiple aspects of molecular graphs. In this work, we designed model
architectures that achieved state-of-the-art performance, surpassing more
complex models such as those pre-trained on external databases. We assessed
dataset diversity to complement our performance results, finding that
structural diversity influences the need for additional components in our MPNNs
and feature sets.
  In most datasets, our best architecture employs bidirectional message-passing
with an attention mechanism, applied to a minimalist message formulation that
excludes self-perception, highlighting that relatively simpler models, compared
to classical MPNNs, yield higher class separability. In contrast, we found that
convolution normalization factors do not benefit the predictive power in all
the datasets tested. This was corroborated in both global and node-level
outputs. Additionally, we analyzed the influence of both adding spatial
features and working with 3D graphs, finding that 2D molecular graphs are
sufficient when complemented with appropriately chosen 3D descriptors. This
approach not only preserves predictive performance but also reduces
computational cost by over 50%, making it particularly advantageous for
high-throughput screening campaigns.

</details>


### [155] [Robustifying Diffusion-Denoised Smoothing Against Covariate Shift](https://arxiv.org/abs/2509.10913)
*Ali Hedayatnia,Mostafa Tavassolipour,Babak Nadjar Araabi,Abdol-Hossein Vahabie*

Main category: cs.LG

TL;DR: 本文提出了一种新方法来改进扩散去噪平滑(DDS)中的协变量偏移问题，通过在去噪扩散模型中针对添加噪声设计对抗目标函数，显著提高了认证精度并在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散去噪平滑方法虽然能产生最先进的结果，但使用去噪扩散模型会通过噪声估计错误引入协变量偏移，从而降低平滑分类器的性能。

Method: 提出了一种新颖的对抗目标函数，专注于去噪扩散模型的添加噪声，基于对协变量偏移来源的理解，训练基础分类器使其对去噪器引入的协变量偏移具有鲁棒性。

Result: 在三个标准分类基准测试(MNIST、CIFAR-10和ImageNet)上显著提高了认证精度，在l2对抗扰动方面实现了新的最先进性能。

Conclusion: 通过针对去噪扩散模型中的噪声估计问题设计对抗训练方法，有效解决了协变量偏移问题，提升了随机平滑方法的鲁棒性认证性能。

Abstract: Randomized smoothing is a well-established method for achieving certified
robustness against l2-adversarial perturbations. By incorporating a denoiser
before the base classifier, pretrained classifiers can be seamlessly integrated
into randomized smoothing without significant performance degradation. Among
existing methods, Diffusion Denoised Smoothing - where a pretrained denoising
diffusion model serves as the denoiser - has produced state-of-the-art results.
However, we show that employing a denoising diffusion model introduces a
covariate shift via misestimation of the added noise, ultimately degrading the
smoothed classifier's performance. To address this issue, we propose a novel
adversarial objective function focused on the added noise of the denoising
diffusion model. This approach is inspired by our understanding of the origin
of the covariate shift. Our goal is to train the base classifier to ensure it
is robust against the covariate shift introduced by the denoiser. Our method
significantly improves certified accuracy across three standard classification
benchmarks - MNIST, CIFAR-10, and ImageNet - achieving new state-of-the-art
performance in l2-adversarial perturbations. Our implementation is publicly
available at
https://github.com/ahedayat/Robustifying-DDS-Against-Covariate-Shift

</details>


### [156] [ToMA: Token Merge with Attention for Image Generation with Diffusion Models](https://arxiv.org/abs/2509.10918)
*Wenbo Lu,Shaoyi Zheng,Yuxuan Xia,Shengjie Wang*

Main category: cs.LG

TL;DR: ToMA是一种新的token合并方法，通过GPU友好的矩阵运算重新设计token缩减，在保持图像质量的同时显著降低扩散模型生成延迟


<details>
  <summary>Details</summary>
Motivation: 现有的token缩减方法（如ToMeSD和ToFu）虽然能减少FLOPs，但由于依赖GPU效率低下的操作（如排序、分散写入），在与优化注意力实现（如FlashAttention）配合时会产生开销，抵消了理论上的加速效果

Method: 1) 将token合并重新表述为子模优化问题来选择多样化token；2) 通过GPU友好的矩阵运算实现类似注意力的线性变换进行合并/解合并；3) 利用潜在局部性和序列冗余（模式重用）来最小化开销

Result: ToMA将SDXL/Flux的生成延迟分别降低了24%/23%（DINO Δ < 0.07），在保持图像质量的同时超越了先前的方法

Conclusion: 这项工作弥合了transformer在扩散模型中理论与实际效率之间的差距，为高效图像生成提供了实用的解决方案

Abstract: Diffusion models excel in high-fidelity image generation but face scalability
limits due to transformers' quadratic attention complexity. Plug-and-play token
reduction methods like ToMeSD and ToFu reduce FLOPs by merging redundant tokens
in generated images but rely on GPU-inefficient operations (e.g., sorting,
scattered writes), introducing overheads that negate theoretical speedups when
paired with optimized attention implementations (e.g., FlashAttention). To
bridge this gap, we propose Token Merge with Attention (ToMA), an off-the-shelf
method that redesigns token reduction for GPU-aligned efficiency, with three
key contributions: 1) a reformulation of token merge as a submodular
optimization problem to select diverse tokens; 2) merge/unmerge as an
attention-like linear transformation via GPU-friendly matrix operations; and 3)
exploiting latent locality and sequential redundancy (pattern reuse) to
minimize overhead. ToMA reduces SDXL/Flux generation latency by 24%/23%,
respectively (with DINO $\Delta < 0.07$), outperforming prior methods. This
work bridges the gap between theoretical and practical efficiency for
transformers in diffusion.

</details>


### [157] [Clarifying Model Transparency: Interpretability versus Explainability in Deep Learning with MNIST and IMDB Examples](https://arxiv.org/abs/2509.10929)
*Mitali Raj*

Main category: cs.LG

TL;DR: 本文对深度学习中可解释性(interpretability)和可解释性(explainability)进行了比较分析，指出前者关注模型内在机制的全域理解，后者侧重后处理技术提供的局部解释。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型的"黑盒"特性阻碍了其在可信领域的应用，需要明确区分可解释性和可解释性这两个常被混用的概念。

Method: 通过MNIST数字分类和IMDB情感分析两个案例研究，使用特征归因方法和词级重要性分析来展示局部解释技术。

Result: 证实了可解释性主要涉及模型内在机制的全域理解能力，而可解释性更多与解释单个预测的后处理技术相关。局部解释不能使复杂模型变得全局透明。

Conclusion: 清晰区分可解释性和可解释性对于发展可靠的人工智能至关重要，这种区分在标准数据集上得到了验证。

Abstract: The impressive capabilities of deep learning models are often counterbalanced
by their inherent opacity, commonly termed the "black box" problem, which
impedes their widespread acceptance in high-trust domains. In response, the
intersecting disciplines of interpretability and explainability, collectively
falling under the Explainable AI (XAI) umbrella, have become focal points of
research. Although these terms are frequently used as synonyms, they carry
distinct conceptual weights. This document offers a comparative exploration of
interpretability and explainability within the deep learning paradigm,
carefully outlining their respective definitions, objectives, prevalent
methodologies, and inherent difficulties. Through illustrative examinations of
the MNIST digit classification task and IMDB sentiment analysis, we
substantiate a key argument: interpretability generally pertains to a model's
inherent capacity for human comprehension of its operational mechanisms (global
understanding), whereas explainability is more commonly associated with
post-hoc techniques designed to illuminate the basis for a model's individual
predictions or behaviors (local explanations). For example, feature attribution
methods can reveal why a specific MNIST image is recognized as a '7', and
word-level importance can clarify an IMDB sentiment outcome. However, these
local insights do not render the complex underlying model globally transparent.
A clear grasp of this differentiation, as demonstrated by these standard
datasets, is vital for fostering dependable and sound artificial intelligence.

</details>


### [158] [The Psychogenic Machine: Simulating AI Psychosis, Delusion Reinforcement and Harm Enablement in Large Language Models](https://arxiv.org/abs/2509.10970)
*Joshua Au Yeung,Jacopo Dalmasso,Luca Foschini,Richard JB Dobson,Zeljko Kraljevic*

Main category: cs.LG

TL;DR: 研究发现大型语言模型(LLMs)具有诱发或加剧精神病症状的潜在风险，所有测试模型都表现出确认妄想和促成伤害的倾向，安全干预严重不足。


<details>
  <summary>Details</summary>
Motivation: 随着AI精神病的报道增多，需要系统评估LLMs在与用户交互时可能强化妄想信念的心理致病性风险。

Method: 开发了psychosis-bench基准测试，包含16个结构化对话场景，评估8个主流LLMs在妄想确认、伤害促成和安全干预三个维度的表现。

Result: 所有模型都表现出心理致病性潜力(平均DCS 0.91)，经常促成有害请求(平均HES 0.69)，安全干预率仅37%。隐性场景表现更差，模型规模与安全性不成正比。

Conclusion: LLMs的心理致病性是可量化的风险，需要重新思考LLM训练方式，这不仅是技术挑战更是公共卫生问题，需要多方合作解决。

Abstract: Background: Emerging reports of "AI psychosis" are on the rise, where
user-LLM interactions may exacerbate or induce psychosis or adverse
psychological symptoms. The sycophantic and agreeable nature of LLMs can
beneficial, it can become a vector for harm by reinforcing delusional beliefs
in vulnerable users.
  Methods: We introduce psychosis-bench, a novel benchmark designed to
systematically evaluate the psychogenicity of LLMs comprimising 16 structured,
12-turn conversational scenarios simulating the progression of delusional
themes(Erotic Delusions, Grandiose/Messianic Delusions, Referential Delusions)
and potential harms. We evaluated eight prominent LLMs for Delusion
Confirmation (DCS), Harm Enablement (HES), and Safety Intervention(SIS) across
explicit and implicit conversational contexts.
  Findings: Across 1,536 simulated conversation turns, all LLMs demonstrated
psychogenic potential, showing a strong tendency to perpetuate rather than
challenge delusions (mean DCS of 0.91 $\pm$0.88). Models frequently enabled
harmful user requests (mean HES of 0.69 $\pm$0.84) and offered safety
interventions in only roughly a third of applicable turns (mean SIS of 0.37
$\pm$0.48). 51 / 128 (39.8%) of scenarios had no safety interventions offered.
Performance was significantly worse in implicit scenarios, models were more
likely to confirm delusions and enable harm while offering fewer interventions
(p < .001). A strong correlation was found between DCS and HES (rs = .77).
Model performance varied widely, indicating that safety is not an emergent
property of scale alone.
  Conclusion: This study establishes LLM psychogenicity as a quantifiable risk
and underscores the urgent need for re-thinking how we train LLMs. We frame
this issue not merely as a technical challenge but as a public health
imperative requiring collaboration between developers, policymakers, and
healthcare professionals.

</details>


### [159] [PHLoRA: data-free Post-hoc Low-Rank Adapter extraction from full-rank checkpoint](https://arxiv.org/abs/2509.10971)
*Bhoomit Vasani,Jack FitzGerald,Anjie Fang,Sushmit Vaish*

Main category: cs.LG

TL;DR: PHLoRA是一种无需训练数据或梯度即可从全秩微调模型中提取低秩适配器的方法，通过权重差异的低秩分解来重构适配器模块，支持动态路由和可扩展推理。


<details>
  <summary>Details</summary>
Motivation: 现有的适配器训练方法需要显式训练每个适配器，而PHLoRA旨在从已有的全秩微调模型或第三方检查点中提取适配器，实现微调与适配器生成的解耦。

Method: 通过计算基础模型与其微调对应模型之间权重差异的低秩分解，重构适配器模块，支持通过S-LoRA进行动态路由或在NVIDIA NIM等平台上进行可扩展服务。

Result: 在文本、图像和视频基准测试中，提取的适配器保留了全权重差异的高能量，可以安全剪枝，重新合并时在下游任务性能上几乎无退化。

Conclusion: PHLoRA为所有现有全秩检查点提供了适配器就绪的实用路径，实现了所有模型的可扩展推理民主化。

Abstract: We introduce PHLoRA (Pronounced "flora"). (Post-hoc LoRA), a simple yet
powerful method to extract low-rank adaptation adapters from full-rank
fine-tuned models without requiring access to training data or gradients. By
computing the low-rank decomposition of weight differences between a base model
and its fine-tuned counterpart, our method reconstructs adapter modules that
can be merged or dynamically routed at inference time via S-LoRA, or served in
scalable, industry settings using platforms like NVIDIA NIM. This approach
amortizes latency overhead across requests and yields substantial cost savings.
Unlike prior work that trains each adapter explicitly, our approach decouples
fine-tuning from adapter generation, allowing adapter extraction from existing
full-rank models or third-party checkpoints. Experiments on text, image, and
video benchmarks using the Amazon Nova model family demonstrate that extracted
adapters preserve high energy from the full weight delta, can be pruned safely,
and yield negligible degradation in downstream task performance when re-merged.
Overall, PHLoRA provides a practical path for making all existing full-rank
checkpoints adapter-ready, democratizing scalable inference for all models.

</details>


### [160] [Decoupling Search and Learning in Neural Net Training](https://arxiv.org/abs/2509.10973)
*Akshay Vegesna,Samip Dahal*

Main category: cs.LG

TL;DR: 提出了一个两阶段训练框架：先在表示空间进行进化搜索寻找多样化表示解，然后在参数空间通过梯度回归学习这些表示，以克服梯度下降探索性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 梯度下降通常收敛到单一训练损失最小值，缺乏探索可能泛化更好的替代最小值的机制，而直接在参数空间搜索多样化最小值是计算不可行的。

Method: 采用两阶段训练：1）在中间激活表示空间进行进化搜索寻找多样化的表示解；2）在参数空间通过梯度回归学习这些搜索到的表示。

Result: 通过进化搜索发现的表示解具有可学习性，在MNIST、CIFAR-10和CIFAR-100上接近SGD性能，性能随搜索计算量增加而提升直至饱和，产生的模型在表示轨迹上与梯度下降训练的模型有质的不同。

Conclusion: 该工作展示了如何通过将表示空间搜索与参数空间梯度学习解耦，来克服梯度下降的探索局限性，为未来训练算法提供了新思路。

Abstract: Gradient descent typically converges to a single minimum of the training loss
without mechanisms to explore alternative minima that may generalize better.
Searching for diverse minima directly in high-dimensional parameter space is
generally intractable. To address this, we propose a framework that performs
training in two distinct phases: search in a tractable representation space
(the space of intermediate activations) to find diverse representational
solutions, and gradient-based learning in parameter space by regressing to
those searched representations. Through evolutionary search, we discover
representational solutions whose fitness and diversity scale with
compute--larger populations and more generations produce better and more varied
solutions. These representations prove to be learnable: networks trained by
regressing to searched representations approach SGD's performance on MNIST,
CIFAR-10, and CIFAR-100. Performance improves with search compute up to
saturation. The resulting models differ qualitatively from networks trained
with gradient descent, following different representational trajectories during
training. This work demonstrates how future training algorithms could overcome
gradient descent's exploratory limitations by decoupling search in
representation space from efficient gradient-based learning in parameter space.

</details>


### [161] [California Wildfire Inventory (CAWFI): An Extensive Dataset for Predictive Techniques based on Artificial Intelligence](https://arxiv.org/abs/2509.11015)
*Rohan Tan Bhowmik,Youn Soo Jung,Juan Aguilera,Mary Prunicki,Kari Nadeau*

Main category: cs.LG

TL;DR: 本文介绍了加州野火数据库(CAWFI)，这是一个包含3700多万数据点的野火预测数据集，整合了2012-2018年的历史野火数据和2012-2022年的指标数据，用于训练机器学习模型进行野火预测。


<details>
  <summary>Details</summary>
Motivation: 由于气候变化和生态系统破坏，全球野火对环境、基础设施和人类生命的影响日益严重。现有解决方案主要关注火灾发生后的检测，而缺乏有效的预测技术。开发高精度预测技术需要大量数据集来训练机器学习模型。

Method: 创建加州野火数据库(CAWFI)，包含：1)每日历史加州野火数据(2012-2018)；2)指标数据(2012-2022)，包括领先指标(气象数据)、滞后指标(环境数据)和地质指标(植被和海拔数据)。

Result: CAWFI已成功用于训练时空人工智能模型，在使用2012-2017年指标数据训练后，能够预测85.7%的未来面积超过30万英亩的大型野火。

Conclusion: 该数据集旨在促进野火预测研究和解决方案的发展，并为其他地区未来野火数据库的建立树立先例。

Abstract: Due to climate change and the disruption of ecosystems worldwide, wildfires
are increasingly impacting environment, infrastructure, and human lives
globally. Additionally, an exacerbating climate crisis means that these losses
would continue to grow if preventative measures are not implemented. Though
recent advancements in artificial intelligence enable wildfire management
techniques, most deployed solutions focus on detecting wildfires after
ignition. The development of predictive techniques with high accuracy requires
extensive datasets to train machine learning models. This paper presents the
California Wildfire Inventory (CAWFI), a wildfire database of over 37 million
data points for building and training wildfire prediction solutions, thereby
potentially preventing megafires and flash fires by addressing them before they
spark. The dataset compiles daily historical California wildfire data from 2012
to 2018 and indicator data from 2012 to 2022. The indicator data consists of
leading indicators (meteorological data correlating to wildfire-prone
conditions), trailing indicators (environmental data correlating to prior and
early wildfire activity), and geological indicators (vegetation and elevation
data dictating wildfire risk and spread patterns). CAWFI has already
demonstrated success when used to train a spatio-temporal artificial
intelligence model, predicting 85.7% of future wildfires larger than 300,000
acres when trained on 2012-2017 indicator data. This dataset is intended to
enable wildfire prediction research and solutions as well as set a precedent
for future wildfire databases in other regions.

</details>


### [162] [FragmentGPT: A Unified GPT Model for Fragment Growing, Linking, and Merging in Molecular Design](https://arxiv.org/abs/2509.11044)
*Xuefeng Liu,Songhao Jiang,Qinan Huang,Tinson Xu,Ian Foster,Mengdi Wang,Hening Lin,Jinbo Xu,Rick Stevens*

Main category: cs.LG

TL;DR: FragmentGPT是一个基于GPT的分子片段连接生成模型，通过化学感知的键断裂预训练和奖励排名对齐算法，解决片段药物发现中的连接设计和结构冗余问题。


<details>
  <summary>Details</summary>
Motivation: 片段药物发现(FBDD)中设计有效连接器来组合分子片段具有挑战性，特别是当片段存在结构冗余时，传统方法无法简单通过添加或移除原子来解决。

Method: 包含两个核心组件：(1)化学感知的能量基键断裂预训练策略，使GPT模型具备片段生长、连接和合并能力；(2)奖励排名对齐探索算法(RAE)，结合专家模仿学习、数据选择和增强，以及监督微调来实现多目标对齐。

Result: 在真实癌症数据集上的实验表明，FragmentGPT能够生成化学有效、高质量的分子，适用于下游药物发现任务。

Conclusion: FragmentGPT提供了一个统一的框架，能够生成连接不同分子亚基的连接器，同时优化多个药物目标，并通过智能合并解决结构冗余问题，实现可控的目标驱动分子组装。

Abstract: Fragment-Based Drug Discovery (FBDD) is a popular approach in early drug
development, but designing effective linkers to combine disconnected molecular
fragments into chemically and pharmacologically viable candidates remains
challenging. Further complexity arises when fragments contain structural
redundancies, like duplicate rings, which cannot be addressed by simply adding
or removing atoms or bonds. To address these challenges in a unified framework,
we introduce FragmentGPT, which integrates two core components: (1) a novel
chemically-aware, energy-based bond cleavage pre-training strategy that equips
the GPT-based model with fragment growing, linking, and merging capabilities,
and (2) a novel Reward Ranked Alignment with Expert Exploration (RAE) algorithm
that combines expert imitation learning for diversity enhancement, data
selection and augmentation for Pareto and composite score optimality, and
Supervised Fine-Tuning (SFT) to align the learner policy with multi-objective
goals. Conditioned on fragment pairs, FragmentGPT generates linkers that
connect diverse molecular subunits while simultaneously optimizing for multiple
pharmaceutical goals. It also learns to resolve structural redundancies-such as
duplicated fragments-through intelligent merging, enabling the synthesis of
optimized molecules. FragmentGPT facilitates controlled, goal-driven molecular
assembly. Experiments and ablation studies on real-world cancer datasets
demonstrate its ability to generate chemically valid, high-quality molecules
tailored for downstream drug discovery tasks.

</details>


### [163] [Data-Efficient Ensemble Weather Forecasting with Diffusion Models](https://arxiv.org/abs/2509.11047)
*Kevin Valencia,Ziyang Liu,Justin Cui*

Main category: cs.LG

TL;DR: 本文研究了在自回归扩散模型中通过精选数据选择来提升数据效率，发现简单的时间分层采样方法仅使用20%训练数据就能达到或超越全数据训练的性能。


<details>
  <summary>Details</summary>
Motivation: 虽然扩散模型在集合天气预报中显示出潜力，但自回归特性使其计算成本高昂，这在数据有限、昂贵或难以处理的气候科学中是一个挑战。

Method: 评估了多种数据采样策略，重点研究了时间分层采样方法，并与全数据训练进行对比。

Result: 时间分层采样方法在仅使用20%训练数据的情况下，在某些指标上优于全数据模型，在其他指标上表现略差但接近。

Conclusion: 研究证明了数据高效扩散训练的可行性，为未来开发超越随机或纯时间采样的自适应或模型感知采样方法提供了动力。

Abstract: Although numerical weather forecasting methods have dominated the field,
recent advances in deep learning methods, such as diffusion models, have shown
promise in ensemble weather forecasting. However, such models are typically
autoregressive and are thus computationally expensive. This is a challenge in
climate science, where data can be limited, costly, or difficult to work with.
In this work, we explore the impact of curated data selection on these
autoregressive diffusion models. We evaluate several data sampling strategies
and show that a simple time stratified sampling approach achieves performance
similar to or better than full-data training. Notably, it outperforms the
full-data model on certain metrics and performs only slightly worse on others
while using only 20% of the training data. Our results demonstrate the
feasibility of data-efficient diffusion training, especially for weather
forecasting, and motivates future work on adaptive or model-aware sampling
methods that go beyond random or purely temporal sampling.

</details>


### [164] [An Advanced Convolutional Neural Network for Bearing Fault Diagnosis under Limited Data](https://arxiv.org/abs/2509.11053)
*Shengke Sun,Shuzhen Han,Ziqian Luan,Xinghao Qin,Jiao Yin,Zhanshan Zhao,Jinli Cao,Hua Wang*

Main category: cs.LG

TL;DR: 提出DAC-FCF框架，通过条件生成对抗网络生成多样化数据、对比学习建模样本关系、傅里叶卷积神经网络提取全局特征，在轴承故障诊断中显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 轴承故障诊断中高质量标注数据稀缺，传统数据增强方法存在模式崩溃问题，CNN难以提取振动信号的全局特征，现有方法无法有效建模有限样本间复杂关系。

Method: 1) 条件一致性潜在表示重建GAN(CCLR-GAN)生成多样化数据；2) 对比学习联合优化机制建模样本关系；3) 一维傅里叶卷积神经网络(1D-FCNN)实现全局感知。

Result: 在CWRU数据集上比基线方法提升32%，在自建测试台上提升10%，消融实验证明了各组件有效性。

Conclusion: DAC-FCF为有限数据下的轴承故障诊断提供了有前景的解决方案。

Abstract: In the area of bearing fault diagnosis, deep learning (DL) methods have been
widely used recently. However, due to the high cost or privacy concerns,
high-quality labeled data are scarce in real world scenarios. While few-shot
learning has shown promise in addressing data scarcity, existing methods still
face significant limitations in this domain. Traditional data augmentation
techniques often suffer from mode collapse and generate low-quality samples
that fail to capture the diversity of bearing fault patterns. Moreover,
conventional convolutional neural networks (CNNs) with local receptive fields
makes them inadequate for extracting global features from complex vibration
signals. Additionally, existing methods fail to model the intricate
relationships between limited training samples. To solve these problems, we
propose an advanced data augmentation and contrastive fourier convolution
framework (DAC-FCF) for bearing fault diagnosis under limited data. Firstly, a
novel conditional consistent latent representation and reconstruction
generative adversarial network (CCLR-GAN) is proposed to generate more diverse
data. Secondly, a contrastive learning based joint optimization mechanism is
utilized to better model the relations between the available training data.
Finally, we propose a 1D fourier convolution neural network (1D-FCNN) to
achieve a global-aware of the input data. Experiments demonstrate that DAC-FCF
achieves significant improvements, outperforming baselines by up to 32\% on
case western reserve university (CWRU) dataset and 10\% on a self-collected
test bench. Extensive ablation experiments prove the effectiveness of the
proposed components. Thus, the proposed DAC-FCF offers a promising solution for
bearing fault diagnosis under limited data.

</details>


### [165] [Machine Learning Framework for Audio-Based Equipment Condition Monitoring: A Comparative Study of Classification Algorithms](https://arxiv.org/abs/2509.11075)
*Srijesh Pillai,Yodhin Agarwal,Zaheeruddin Ahmed*

Main category: cs.LG

TL;DR: 本文提出了一个用于音频设备状态监测的机器学习模型评估框架，通过系统性和统计严谨的方法解决了算法选择缺乏标准化的问题。


<details>
  <summary>Details</summary>
Motivation: 音频设备状态监测领域缺乏标准化的算法选择方法，阻碍了可重复性研究。本文旨在填补这一空白，为工业环境提供可靠的监测解决方案选择指南。

Method: 利用包含127个特征的丰富特征集（涵盖时域、频域和时频域），在合成和真实数据集上验证方法论，采用集成方法进行模型评估。

Result: 集成方法取得了最佳性能（94.2%准确率，0.942 F1分数），统计测试证实其显著优于单个算法8-15%。

Conclusion: 本研究提供了一个经过验证的基准测试协议和实用的指导方针，用于在工业环境中选择稳健的监测解决方案。

Abstract: Audio-based equipment condition monitoring suffers from a lack of
standardized methodologies for algorithm selection, hindering reproducible
research. This paper addresses this gap by introducing a comprehensive
framework for the systematic and statistically rigorous evaluation of machine
learning models. Leveraging a rich 127-feature set across time, frequency, and
time-frequency domains, our methodology is validated on both synthetic and
real-world datasets. Results demonstrate that an ensemble method achieves
superior performance (94.2% accuracy, 0.942 F1-score), with statistical testing
confirming its significant outperformance of individual algorithms by 8-15%.
Ultimately, this work provides a validated benchmarking protocol and practical
guidelines for selecting robust monitoring solutions in industrial settings.

</details>


### [166] [DemandLens: Enhancing Forecast Accuracy Through Product-Specific Hyperparameter Optimization](https://arxiv.org/abs/2509.11085)
*Srijesh Pillai,M. I. Jawid Nazir*

Main category: cs.LG

TL;DR: Prophet模型结合COVID-19指标和SKU特定超参数优化，为床垫盒装行业提供准确销售预测，帮助合同制造商优化供应链管理


<details>
  <summary>Details</summary>
Motivation: 床垫盒装行业依赖第三方合同制造，但美国合同制造商数量有限，需要智能管理原材料和供应链来服务多个品牌，避免瓶颈并优化采购成本

Method: 采用基于Prophet的预测模型，整合COVID-19相关指标，并进行SKU特定的超参数优化

Result: 模型展现出强大的预测能力，为合同制造商和床垫品牌提供了可靠的供应链运营优化工具

Conclusion: 该方法有效解决了依赖第三方制造的床垫行业的销售预测需求，帮助制造商提前准备并优化原材料采购

Abstract: DemandLens demonstrates an innovative Prophet based forecasting model for the
mattress-in-a-box industry, incorporating COVID-19 metrics and SKU-specific
hyperparameter optimization. This industry has seen significant growth of
E-commerce players in the recent years, wherein the business model majorly
relies on outsourcing Mattress manufacturing and related logistics and supply
chain operations, focusing on marketing the product and driving conversions
through Direct-to-Consumer sales channels. Now, within the United States, there
are a limited number of Mattress contract manufacturers available, and hence,
it is important that they manage their raw materials, supply chain, and,
inventory intelligently, to be able to cater maximum Mattress brands. Our
approach addresses the critical need for accurate Sales Forecasting in an
industry that is heavily dependent on third-party Contract Manufacturing. This,
in turn, helps the contract manufacturers to be prepared, hence, avoiding
bottleneck scenarios, and aiding them to source raw materials at optimal rates.
The model demonstrates strong predictive capabilities through SKU-specific
Hyperparameter optimization, offering the Contract Manufacturers and Mattress
brands a reliable tool to streamline supply chain operations.

</details>


### [167] [GCN-TULHOR: Trajectory-User Linking Leveraging GCNs and Higher-Order Spatial Representations](https://arxiv.org/abs/2509.11095)
*Khoa Tran,Pranav Gupta,Manos Papagelis*

Main category: cs.LG

TL;DR: GCN-TULHOR方法通过六边形网格化将原始位置数据转换为高阶移动流表示，结合图卷积网络有效解决轨迹-用户链接中的稀疏数据和不完整路径问题，在多个数据集上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹-用户链接方法在处理稀疏数据、不完整路径和复杂空间依赖关系方面存在不足，通常依赖低级别签到数据或忽略空间模式，需要更有效的解决方案。

Method: 使用六边形网格化将稀疏签到和连续GPS轨迹数据转换为统一的高阶移动流表示，集成图卷积网络(GCN)显式建模复杂空间关系和非局部依赖，无需时间戳或兴趣点等辅助信息。

Result: 在六个真实数据集上的实验显示，相比经典基线、RNN和Transformer模型以及TULHOR方法，在准确率、精确率、召回率和F1分数上均取得一致改进，相对增益达到1-8%。

Conclusion: 结合基于图的空间学习与序列建模为轨迹-用户链接提供了鲁棒且可扩展的解决方案，在推荐系统、城市规划和安全领域具有重要应用价值。

Abstract: Trajectory-user linking (TUL) aims to associate anonymized trajectories with
the users who generated them, which is crucial for personalized
recommendations, privacy-preserving analytics, and secure location-based
services. Existing methods struggle with sparse data, incomplete routes, and
limited modeling of complex spatial dependencies, often relying on low-level
check-in data or ignoring spatial patterns. In this paper, we introduced
GCN-TULHOR, a method that transforms raw location data into higher-order
mobility flow representations using hexagonal tessellation, reducing data
sparsity and capturing richer spatial semantics, and integrating Graph
Convolutional Networks (GCNs). Our approach converts both sparse check-in and
continuous GPS trajectory data into unified higher-order flow representations,
mitigating sparsity while capturing deeper semantic information. The GCN layer
explicitly models complex spatial relationships and non-local dependencies
without requiring side information such as timestamps or points of interest.
Experiments on six real-world datasets show consistent improvements over
classical baselines, RNN- and Transformer-based models, and the TULHOR method
in accuracy, precision, recall, and F1-score. GCN-TULHOR achieves 1-8% relative
gains in accuracy and F1. Sensitivity analysis identifies an optimal setup with
a single GCN layer and 512-dimensional embeddings. The integration of GCNs
enhances spatial learning and improves generalizability across mobility data.
This work highlights the value of combining graph-based spatial learning with
sequential modeling, offering a robust and scalable solution for TUL with
applications in recommendations, urban planning, and security.

</details>


### [168] [BIGNet: Pretrained Graph Neural Network for Embedding Semantic, Spatial, and Topological Data in BIM Models](https://arxiv.org/abs/2509.11104)
*Jin Han,Xin-Zheng Lu,Jia-Rui Lin*

Main category: cs.LG

TL;DR: 开发了首个大规模图神经网络BIGNet，用于学习和重用BIM模型中的多维设计特征，在BIM设计检查任务中实现了72.7%的F1分数提升


<details>
  <summary>Details</summary>
Motivation: 现有大型基础模型主要关注文本和视觉数据，忽略了BIM模型中丰富的语义、空间和拓扑特征，需要专门的方法来学习这些多维设计特征

Method: 提出可扩展的图表示方法编码BIM组件的语义-空间-拓扑特征；创建包含近100万节点和350万边的数据集；基于GraphMAE2引入新的消息传递机制，采用节点掩码策略进行预训练

Result: 同构图表示优于异构图；考虑30cm半径内的局部空间关系能提升性能；基于GAT的特征提取获得最佳迁移学习效果；相比未预训练模型平均F1分数提升72.7%

Conclusion: BIGNet能有效学习和迁移BIM设计特征，为未来设计和全生命周期管理的自动化应用提供了有力支持

Abstract: Large Foundation Models (LFMs) have demonstrated significant advantages in
civil engineering, but they primarily focus on textual and visual data,
overlooking the rich semantic, spatial, and topological features in BIM
(Building Information Modelling) models. Therefore, this study develops the
first large-scale graph neural network (GNN), BIGNet, to learn, and reuse
multidimensional design features embedded in BIM models. Firstly, a scalable
graph representation is introduced to encode the "semantic-spatial-topological"
features of BIM components, and a dataset with nearly 1 million nodes and 3.5
million edges is created. Subsequently, BIGNet is proposed by introducing a new
message-passing mechanism to GraphMAE2 and further pretrained with a node
masking strategy. Finally, BIGNet is evaluated in various transfer learning
tasks for BIM-based design checking. Results show that: 1) homogeneous graph
representation outperforms heterogeneous graph in learning design features, 2)
considering local spatial relationships in a 30 cm radius enhances performance,
and 3) BIGNet with GAT (Graph Attention Network)-based feature extraction
achieves the best transfer learning results. This innovation leads to a 72.7%
improvement in Average F1-score over non-pretrained models, demonstrating its
effectiveness in learning and transferring BIM design features and facilitating
their automated application in future design and lifecycle management.

</details>


### [169] [Agentic Username Suggestion and Multimodal Gender Detection in Online Platforms: Introducing the PNGT-26K Dataset](https://arxiv.org/abs/2509.11136)
*Farbod Bijary,Mohsen Ebadpour,Amirhosein Tajbakhsh*

Main category: cs.LG

TL;DR: 本文介绍了PNGT-26K数据集，包含26,000个波斯名字及其性别和英文音译，并开发了Open Gender Detection和Nominalist两个框架来解决波斯名字处理中的挑战。


<details>
  <summary>Details</summary>
Motivation: 波斯名字在自然语言处理应用中面临独特挑战，特别是性别检测和数字身份创建方面，由于音译不一致和文化特定的命名模式，现有工具性能显著下降，且缺乏全面数据集。

Method: 创建PNGT-26K数据集（约26,000个波斯名字元组），开发Open Gender Detection框架（基于用户资料照片和名字进行概率性别猜测）和Nominalist框架（使用AI代理帮助用户选择社交媒体用户名）。

Result: 提供了公开可用的PNGT-26K数据集和两个框架，Open Gender Detection可用于生产环境，Nominalist可轻松集成到任何网站改善用户体验。

Conclusion: 该研究解决了波斯名字处理的关键挑战，通过提供全面数据集和实用框架，为波斯语NLP应用提供了重要资源和支持。

Abstract: Persian names present unique challenges for natural language processing
applications, particularly in gender detection and digital identity creation,
due to transliteration inconsistencies and cultural-specific naming patterns.
Existing tools exhibit significant performance degradation on Persian names,
while the scarcity of comprehensive datasets further compounds these
limitations. To address these challenges, the present research introduces
PNGT-26K, a comprehensive dataset of Persian names, their commonly associated
gender, and their English transliteration, consisting of approximately 26,000
tuples. As a demonstration of how this resource can be utilized, we also
introduce two frameworks, namely Open Gender Detection and Nominalist. Open
Gender Detection is a production-grade, ready-to-use framework for using
existing data from a user, such as profile photo and name, to give a
probabilistic guess about the person's gender. Nominalist, the second framework
introduced by this paper, utilizes agentic AI to help users choose a username
for their social media accounts on any platform. It can be easily integrated
into any website to provide a better user experience. The PNGT-26K dataset,
Nominalist and Open Gender Detection frameworks are publicly available on
Github.

</details>


### [170] [Feature Space Topology Control via Hopkins Loss](https://arxiv.org/abs/2509.11154)
*Einari Vaaras,Manu Airaksinen*

Main category: cs.LG

TL;DR: 本文提出了Hopkins损失函数，利用Hopkins统计量来强制实施期望的特征空间拓扑结构，与现有保持输入特征拓扑的方法形成对比。实验表明该损失函数在分类和降维任务中对性能影响很小，同时能有效修改特征拓扑。


<details>
  <summary>Details</summary>
Motivation: 特征空间拓扑的修改对机器学习应用有益，包括降维、生成建模、迁移学习和对抗攻击鲁棒性。现有方法主要关注保持输入特征拓扑，而本文旨在主动修改特征拓扑。

Method: 提出新颖的Hopkins损失函数，基于Hopkins统计量来强制实施期望的特征空间拓扑结构。在语音、文本和图像数据上进行分类和非线性瓶颈自编码器降维实验。

Result: 实验结果显示，在分类任务中集成Hopkins损失对分类性能影响很小，同时能够有效修改特征拓扑结构。在降维任务中也表现出类似的效果。

Conclusion: Hopkins损失函数是一种有效的工具，可以在几乎不影响主要任务性能的前提下，主动修改特征空间拓扑结构，为各种机器学习应用提供额外的好处。

Abstract: Feature space topology refers to the organization of samples within the
feature space. Modifying this topology can be beneficial in machine learning
applications, including dimensionality reduction, generative modeling, transfer
learning, and robustness to adversarial attacks. This paper introduces a novel
loss function, Hopkins loss, which leverages the Hopkins statistic to enforce a
desired feature space topology, which is in contrast to existing
topology-related methods that aim to preserve input feature topology. We
evaluate the effectiveness of Hopkins loss on speech, text, and image data in
two scenarios: classification and dimensionality reduction using nonlinear
bottleneck autoencoders. Our experiments show that integrating Hopkins loss
into classification or dimensionality reduction has only a small impact on
classification performance while providing the benefit of modifying feature
topology.

</details>


### [171] [AQUA: Attention via QUery mAgnitudes for Memory and Compute Efficient Inference in LLMs](https://arxiv.org/abs/2509.11155)
*Santhosh G S,Saurav Prakash,Balaraman Ravindran*

Main category: cs.LG

TL;DR: AQUA是一种新颖的注意力近似策略，通过SVD投影和基于查询幅度的动态维度选择，显著降低注意力计算成本，在Llama-3.1-8B上实现25%计算减少且性能影响可忽略


<details>
  <summary>Details</summary>
Motivation: 注意力机制的二次复杂度限制了LLM处理长上下文的能力，成为计算和内存的关键瓶颈，需要高效的近似方法来解决这一问题

Method: 采用两阶段方法：离线阶段通过SVD在标定数据集上计算通用投影矩阵；在线推理阶段投影查询和键向量，并根据查询幅度动态选择稀疏维度子集

Result: 在Llama-3.1-8B等先进模型上，实现了25%的注意力点积计算减少，在广泛基准测试中性能影响统计不显著，并能协同加速现有token淘汰方法

Conclusion: AQUA提供了效率和准确性之间的可控平衡，为大规模LLM推理提供了实用且强大的工具，使其更加可访问和可持续

Abstract: The quadratic complexity of the attention mechanism remains a fundamental
barrier to scaling Large Language Models (LLMs) to longer contexts, creating a
critical bottleneck in both computation and memory. To address this, we
introduce AQUA (Attention via QUery mAgnitudes) a novel and versatile
approximation strategy that significantly reduces the cost of attention with a
graceful performance trade-off. Our method operates in two phases: an efficient
offline step where we compute a universal, language agnostic projection matrix
via SVD on a calibration dataset, and an online inference step where we project
query and key vectors and dynamically select a sparse subset of dimensions
based on the query's magnitude. We provide a formal theoretical analysis of
AQUA, establishing the break-even point at which it becomes more
computationally efficient than standard attention. Our empirical evaluations on
state-of-the-art models like Llama-3.1-8B demonstrate that a 25% reduction in
the attention dot-product computation can be achieved with a statistically
insignificant impact on performance across a wide range of benchmarks. We
further showcase the versatility of AQUA by demonstrating its ability to
synergistically accelerate existing token eviction methods like H2O and to
directly reduce KV-cache memory size. By offering a controllable knob to
balance efficiency and accuracy, AQUA provides a practical and powerful tool
for making large-scale LLM inference more accessible and sustainable.

</details>


### [172] [Stabilizing Data-Free Model Extraction](https://arxiv.org/abs/2509.11159)
*Dat-Thinh Nguyen,Kim-Hung Le,Nhien-An Le-Khac*

Main category: cs.LG

TL;DR: MetaDFME是一种新颖的无数据模型提取方法，通过元学习训练生成器来减少分布偏移，从而缓解替代模型精度振荡问题，在多个基准数据集上优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有的无数据模型提取方法存在替代模型精度振荡问题，这主要是由于攻击过程中生成数据分布的持续偏移导致的，使得无法确定最优替代模型。

Method: 采用元学习方法训练生成器，在攻击过程中迭代捕获合成数据的元表示，这些元表示可以通过少量步骤适应生成有助于替代模型从目标模型学习的数据，同时减少分布偏移的影响。

Result: 在MNIST、SVHN、CIFAR-10和CIFAR-100等流行基准图像数据集上的实验表明，MetaDFME优于当前最先进的无数据模型提取方法，并在攻击过程中表现出更稳定的替代模型精度。

Conclusion: MetaDFME通过元学习方法有效解决了无数据模型提取中的分布偏移问题，显著提高了替代模型的稳定性和性能，为机器学习即服务系统的安全防护提供了新的解决方案。

Abstract: Model extraction is a severe threat to Machine Learning-as-a-Service systems,
especially through data-free approaches, where dishonest users can replicate
the functionality of a black-box target model without access to realistic data.
Despite recent advancements, existing data-free model extraction methods suffer
from the oscillating accuracy of the substitute model. This oscillation, which
could be attributed to the constant shift in the generated data distribution
during the attack, makes the attack impractical since the optimal substitute
model cannot be determined without access to the target model's in-distribution
data. Hence, we propose MetaDFME, a novel data-free model extraction method
that employs meta-learning in the generator training to reduce the distribution
shift, aiming to mitigate the substitute model's accuracy oscillation. In
detail, we train our generator to iteratively capture the meta-representations
of the synthetic data during the attack. These meta-representations can be
adapted with a few steps to produce data that facilitates the substitute model
to learn from the target model while reducing the effect of distribution
shifts. Our experiments on popular baseline image datasets, MNIST, SVHN,
CIFAR-10, and CIFAR-100, demonstrate that MetaDFME outperforms the current
state-of-the-art data-free model extraction method while exhibiting a more
stable substitute model's accuracy during the attack.

</details>


### [173] [GK-SMOTE: A Hyperparameter-free Noise-Resilient Gaussian KDE-Based Oversampling Approach](https://arxiv.org/abs/2509.11163)
*Mahabubur Rahman Miraj,Hongyu Huang,Ting Yang,Jinxue Zhao,Nankun Mu,Xinyu Lei*

Main category: cs.LG

TL;DR: GK-SMOTE是一种基于高斯核密度估计的过采样方法，无需超参数调优，能够有效处理类别不平衡和标签噪声问题，在多种评估指标上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 传统过采样技术（如SMOTE）在处理标签噪声和复杂数据分布时效果不佳，导致分类准确率下降，特别是在医疗诊断、欺诈检测等关键应用中。

Method: 提出GK-SMOTE方法，使用高斯核密度估计区分安全区域和噪声区域，在高密度少数类区域生成合成样本，避免模糊或噪声区域，无需参数调优。

Result: 在多个二分类数据集上的实验表明，GK-SMOTE在MCC、平衡准确率和AUPRC等关键评估指标上均优于现有最先进的过采样技术。

Conclusion: GK-SMOTE为不平衡分类任务提供了一个鲁棒、高效的解决方案，特别适用于噪声数据环境，是现实世界应用的理想选择。

Abstract: Imbalanced classification is a significant challenge in machine learning,
especially in critical applications like medical diagnosis, fraud detection,
and cybersecurity. Traditional oversampling techniques, such as SMOTE, often
fail to handle label noise and complex data distributions, leading to reduced
classification accuracy. In this paper, we propose GK-SMOTE, a
hyperparameter-free, noise-resilient extension of SMOTE, built on Gaussian
Kernel Density Estimation (KDE). GK-SMOTE enhances class separability by
generating synthetic samples in high-density minority regions, while
effectively avoiding noisy or ambiguous areas. This self-adaptive approach uses
Gaussian KDE to differentiate between safe and noisy regions, ensuring more
accurate sample generation without requiring extensive parameter tuning. Our
extensive experiments on diverse binary classification datasets demonstrate
that GK-SMOTE outperforms existing state-of-the-art oversampling techniques
across key evaluation metrics, including MCC, Balanced Accuracy, and AUPRC. The
proposed method offers a robust, efficient solution for imbalanced
classification tasks, especially in noisy data environments, making it an
attractive choice for real-world applications.

</details>


### [174] [Harnessing Optimization Dynamics for Curvature-Informed Model Merging](https://arxiv.org/abs/2509.11167)
*Pouria Mahdavinia,Hamed Mahdavi,Niloofar Mireshghallah,Mehrdad Mahdavi*

Main category: cs.LG

TL;DR: OTA Merging是一种基于优化器轨迹的模型合并方法，通过二阶矩统计量作为曲率代理来重新加权参数编辑，结合Fast Fisher Grafting进行任务定位和稀疏化，有效减少任务干扰并提升合并模型质量。


<details>
  <summary>Details</summary>
Motivation: 研究如何在监督微调阶段有效合并多个能力导向的SFT检查点（数学、代码、指令跟随、知识回忆等），避免联合重新训练，解决传统权重空间合并中的负迁移和任务干扰问题。

Method: 提出OTA Merging（曲率感知聚合）利用优化器二阶矩统计作为对角曲率代理来重新加权参数编辑；开发Fast Fisher Grafting进行曲率驱动的任务定位，稀疏化冲突或低重要性编辑；还开发了内存轻量的二阶矩压缩方法。

Result: OTA+FFG在多样化能力SFT检查点上显著提升了合并模型质量，减少了负迁移，在不同稀疏度水平下保持鲁棒性。分析显示检查点间存在显著曲率重叠，解释了线性合并在实践中有效的原因。

Conclusion: 该方法通过曲率感知的模型合并和任务定位技术，有效解决了多能力模型合并中的干扰问题，为模型组合提供了新的视角和技术方案，所有代码和资源已开源。

Abstract: Model merging is an effective post-training strategy for composing
capabilities in large language models without joint retraining. We study this
in the supervised fine-tuning (SFT) stage, where multiple capability-based SFT
checkpoints -- spanning math, code, precise instruction following, general
instruction following, and knowledge recall -- must be consolidated into a
single model. We introduce Optimization Trajectory Aware (OTA) Merging, a
curvature-aware aggregation that leverages optimizer second-moment statistics
as a diagonal curvature proxy to reweight parameter edits and mitigate
interference. Complementing OTA, we propose Fast Fisher Grafting (FFG), a
curvature-driven task-localization step that sparsifies conflicting or
low-importance edits. FFG induces extremely low-rank masks concentrated in
early attention query/key projections and token embeddings, exploiting shared
curvature across capabilities. We further develop a memory-light compression of
the second moments that preserves OTA's effect. Across diverse capability-based
SFT checkpoints, OTA+FFG improves merged-model quality over strong weight-space
baselines, reduces negative transfer, and remains robust across sparsity
levels. Analyses reveal substantial curvature overlap between checkpoints,
offering a novel lens on why simple linear merging can be effective in
practice. Ablations confirm that FFG is critical for reducing task interference
and that the compressed second moments retain the gains of the full
formulation. To facilitate reproducibility, we open-source all code, training
and evaluation scripts, visualization artifacts, and capability-specific SFT
checkpoints at https://github.com/pmahdavi/ota-merge.

</details>


### [175] [Federated Recommender System with Data Valuation for E-commerce Platform](https://arxiv.org/abs/2509.11196)
*Jongwon Park,Minku Kang,Wooseok Sim,Soyoung Lee,Hogun Park*

Main category: cs.LG

TL;DR: FedGDVE是一个联邦学习推荐系统，通过选择性增强本地图数据来提高性能，在FL环境中性能提升高达34.86%


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习推荐系统仅依赖客户端私有数据，忽略了可用的公共数据集。大型购物平台与小型在线商店合作时，全局数据可以缓解本地数据稀疏和偏差问题，但简单整合会引入噪声和无关模式

Method: 提出FedGDVE方法：1）使用预训练图编码器提取全局结构特征；2）本地有效性预测器评估客户端特定相关性；3）基于强化学习的概率估计器过滤和采样最相关的全局交互

Result: 在联邦学习环境中，FedGDVE在公认基准测试上性能提升高达34.86%

Conclusion: 通过选择性增强本地图数据，FedGDVE有效解决了全局数据整合中的噪声问题，显著提升了联邦学习推荐系统的性能

Abstract: Federated Learning (FL) is gaining prominence in machine learning as privacy
concerns grow. This paradigm allows each client (e.g., an individual online
store) to train a recommendation model locally while sharing only model
updates, without exposing the raw interaction logs to a central server, thereby
preserving privacy in a decentralized environment. Nonetheless, most existing
FL-based recommender systems still rely solely on each client's private data,
despite the abundance of publicly available datasets that could be leveraged to
enrich local training; this potential remains largely underexplored. To this
end, we consider a realistic scenario wherein a large shopping platform
collaborates with multiple small online stores to build a global recommender
system. The platform possesses global data, such as shareable user and item
lists, while each store holds a portion of interaction data privately (or
locally). Although integrating global data can help mitigate the limitations of
sparse and biased clients' local data, it also introduces additional
challenges: simply combining all global interactions can amplify noise and
irrelevant patterns, worsening personalization and increasing computational
costs. To address these challenges, we propose FedGDVE, which selectively
augments each client's local graph with semantically aligned samples from the
global dataset. FedGDVE employs: (i) a pre-trained graph encoder to extract
global structural features, (ii) a local valid predictor to assess
client-specific relevance, (iii) a reinforcement-learning-based probability
estimator to filter and sample only the most pertinent global interactions.
FedGDVE improves performance by up to 34.86% on recognized benchmarks in FL
environments.

</details>


### [176] [Foundational theory for optimal decision tree problems. I. Algorithmic and geometric foundations](https://arxiv.org/abs/2509.11226)
*Xi He*

Main category: cs.LG

TL;DR: 本文提出了四种新的最优决策树问题定义，基于代数编程理论构建了动态规划算法，为具有任意分割规则的最优决策树提供了统一高效的解决方案


<details>
  <summary>Details</summary>
Motivation: 为最优决策树问题提供形式化的规范定义，并构建可证明正确性的算法框架，以支持灵活的分割规则

Method: 基于代数编程理论的关系形式化方法，从规范中构造性地推导出动态规划解决方案，提出了四种通用问题定义和相应算法

Result: 开发了四种新颖的最优算法，能够处理满足特定公理和目标函数形式的任意分割规则，统一了现有方法

Conclusion: 该框架为最优决策树问题提供了优雅的解决方案，具有扩展到支持更灵活决策树算法的潜力，包括混合分割规则

Abstract: In the first paper (part I) of this series of two, we introduce four novel
definitions of the ODT problems: three for size-constrained trees and one for
depth-constrained trees. These definitions are stated unambiguously through
executable recursive programs, satisfying all criteria we propose for a formal
specification. In this sense, they resemble the "standard form" used in the
study of general-purpose solvers.
  Grounded in algebraic programming theory-a relational formalism for deriving
correct-by-construction algorithms from specifications-we can not only
establish the existence or nonexistence of dynamic programming solutions but
also derive them constructively whenever they exist. Consequently, the four
generic problem definitions yield four novel optimal algorithms for ODT
problems with arbitrary splitting rules that satisfy the axioms and objective
functions of a given form. These algorithms encompass the known
depth-constrained, axis-parallel ODT algorithm as the special case, while
providing a unified, efficient, and elegant solution for the general ODT
problem.
  In Part II, we present the first optimal hypersurface decision tree algorithm
and provide comprehensive experiments against axis-parallel decision tree
algorithms, including heuristic CART and state-of-the-art optimal methods. The
results demonstrate the significant potential of decision trees with flexible
splitting rules. Moreover, our framework is readily extendable to support
algorithms for constructing even more flexible decision trees, including those
with mixed splitting rules.

</details>


### [177] [TransZero: Parallel Tree Expansion in MuZero using Transformer Networks](https://arxiv.org/abs/2509.11233)
*Emil Malmsten,Wendelin Böhmer*

Main category: cs.LG

TL;DR: TransZero是一种基于模型的强化学习算法，通过transformer网络并行生成多个潜在未来状态，相比MuZero实现了11倍的速度提升，同时保持样本效率。


<details>
  <summary>Details</summary>
Motivation: 解决蒙特卡洛树搜索(MCTS)中的顺序瓶颈问题，MuZero等现有方法需要逐步构建搜索树，限制了决策速度。

Method: 使用基于transformer的网络同时生成多个潜在未来状态，结合均值-方差约束(MVC)评估器消除对顺序访问计数的依赖，实现整个子树的并行扩展。

Result: 在MiniGrid和LunarLander环境中，相比MuZero实现了高达11倍的时钟时间加速，同时保持了样本效率。

Conclusion: 并行树构建可以显著加速基于模型的强化学习，使复杂环境中的实时决策更接近实际应用。

Abstract: We present TransZero, a model-based reinforcement learning algorithm that
removes the sequential bottleneck in Monte Carlo Tree Search (MCTS). Unlike
MuZero, which constructs its search tree step by step using a recurrent
dynamics model, TransZero employs a transformer-based network to generate
multiple latent future states simultaneously. Combined with the Mean-Variance
Constrained (MVC) evaluator that eliminates dependence on inherently sequential
visitation counts, our approach enables the parallel expansion of entire
subtrees during planning. Experiments in MiniGrid and LunarLander show that
TransZero achieves up to an eleven-fold speedup in wall-clock time compared to
MuZero while maintaining sample efficiency. These results demonstrate that
parallel tree construction can substantially accelerate model-based
reinforcement learning, bringing real-time decision-making in complex
environments closer to practice. The code is publicly available on GitHub.

</details>


### [178] [Online Optimization on Hadamard Manifolds: Curvature Independent Regret Bounds on Horospherically Convex Objectives](https://arxiv.org/abs/2509.11236)
*Emre Sahinoglu,Shahin Shahrampour*

Main category: cs.LG

TL;DR: 本文研究Hadamard流形上的在线黎曼优化，提出使用horospherical凸性(h-凸性)代替传统测地凸性，获得了与流形曲率无关的O(√T)和O(logT)遗憾界，与欧几里得空间结果一致。


<details>
  <summary>Details</summary>
Motivation: 传统基于测地凸性(g-凸性)的在线黎曼优化方法遗憾界严重依赖于流形曲率，性能较差。为了解决这一限制，需要寻找曲率无关的优化框架。

Method: 使用horospherical凸性(h-凸性)分析黎曼在线梯度下降算法，针对h-凸函数和强h-凸函数分别建立理论保证。在对称正定矩阵流形上进行了实验验证。

Result: 获得了曲率无关的O(√T)和O(logT)遗憾界，与欧几里得空间的最优结果匹配。实验验证了在在线Tyler M-估计和在线Fréchet均值计算中的应用效果。

Conclusion: horospherical凸性为黎曼在线优化提供了曲率无关的理论框架，能够获得与欧几里得空间相当的性能，具有重要的理论和实践意义。

Abstract: We study online Riemannian optimization on Hadamard manifolds under the
framework of horospherical convexity (h-convexity). Prior work mostly relies on
the geodesic convexity (g-convexity), leading to regret bounds scaling poorly
with the manifold curvature. To address this limitation, we analyze Riemannian
online gradient descent for h-convex and strongly h-convex functions and
establish $O(\sqrt{T})$ and $O(\log(T))$ regret guarantees, respectively. These
bounds are curvature-independent and match the results in the Euclidean
setting. We validate our approach with experiments on the manifold of symmetric
positive definite (SPD) matrices equipped with the affine-invariant metric. In
particular, we investigate online Tyler's $M$-estimation and online Fr\'echet
mean computation, showing the application of h-convexity in practice.

</details>


### [179] [Gradient Free Deep Reinforcement Learning With TabPFN](https://arxiv.org/abs/2509.11259)
*David Schiff,Ofir Lindenbaum,Yonathan Efroni*

Main category: cs.LG

TL;DR: TabPFN RL是一种无需梯度的深度强化学习框架，使用预训练transformer TabPFN作为Q函数近似器，通过上下文学习进行推理，在经典控制任务中表现优于或匹配DQN。


<details>
  <summary>Details</summary>
Motivation: 基于梯度的优化在深度强化学习中存在超参数敏感、训练不稳定和计算成本高的问题，需要探索无需梯度的替代方法。

Method: 重新利用元训练transformer TabPFN作为Q函数近似器，通过上下文学习进行单次前向推理，设计高奖励轨迹门控机制处理固定上下文限制。

Result: 在Gymnasium经典控制套件中，TabPFN RL在CartPole v1、MountainCar v0和Acrobot v1任务上匹配或超越Deep Q Network，无需梯度下降或大量超参数调优。

Conclusion: TabPFN等先验拟合网络为快速计算高效的RL提供了可行基础，开启了使用大型预训练transformer进行无梯度RL的新方向。

Abstract: Gradient based optimization is fundamental to most modern deep reinforcement
learning algorithms, however, it introduces significant sensitivity to
hyperparameters, unstable training dynamics, and high computational costs. We
propose TabPFN RL, a novel gradient free deep RL framework that repurposes the
meta trained transformer TabPFN as a Q function approximator. Originally
developed for tabular classification, TabPFN is a transformer pre trained on
millions of synthetic datasets to perform inference on new unseen datasets via
in context learning. Given an in context dataset of sample label pairs and new
unlabeled data, it predicts the most likely labels in a single forward pass,
without gradient updates or task specific fine tuning. We use TabPFN to predict
Q values using inference only, thereby eliminating the need for back
propagation at both training and inference. To cope with the model's fixed
context budget, we design a high reward episode gate that retains only the top
5% of trajectories. Empirical evaluations on the Gymnasium classic control
suite demonstrate that TabPFN RL matches or surpasses Deep Q Network on
CartPole v1, MountainCar v0, and Acrobot v1, without applying gradient descent
or any extensive hyperparameter tuning. We discuss the theoretical aspects of
how bootstrapped targets and non stationary visitation distributions violate
the independence assumptions encoded in TabPFN's prior, yet the model retains a
surprising generalization capacity. We further formalize the intrinsic context
size limit of in context RL algorithms and propose principled truncation
strategies that enable continual learning when the context is full. Our results
establish prior fitted networks such as TabPFN as a viable foundation for fast
and computationally efficient RL, opening new directions for gradient free RL
with large pre trained transformers.

</details>


### [180] [SelectMix: Enhancing Label Noise Robustness through Targeted Sample Mixing](https://arxiv.org/abs/2509.11265)
*Qiuhao Liu,Ling Li,Yao Lu,Qi Xuan,Zhaowei Zhu,Jiaheng Wei*

Main category: cs.LG

TL;DR: SelectMix是一个针对噪声标签的置信度引导混合框架，通过K折交叉验证识别噪声样本，选择性地将不确定样本与置信度高的同类样本混合，使用软标签确保监督信号与混合输入对齐。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络容易记忆噪声标签，严重影响泛化性能。现有的Mixup方法缺乏样本选择和混合策略的指导，会无意中传播噪声监督。

Method: 使用K折交叉验证进行置信度不匹配分析识别噪声样本，选择性地将不确定样本与置信度高的同类样本混合，采用软标签表示混合样本的组成。

Result: 在多个合成数据集（MNIST、Fashion-MNIST、CIFAR-10、CIFAR-100）和真实世界基准数据集（CIFAR-N、MNIST、Clothing1M）上，SelectMix始终优于强基线方法。

Conclusion: SelectMix在噪声标签学习中表现出有效性和鲁棒性，通过置信度引导的混合策略显著提升了模型的泛化性能。

Abstract: Deep neural networks tend to memorize noisy labels, severely degrading their
generalization performance. Although Mixup has demonstrated effectiveness in
improving generalization and robustness, existing Mixup-based methods typically
perform indiscriminate mixing without principled guidance on sample selection
and mixing strategy, inadvertently propagating noisy supervision. To overcome
these limitations, we propose SelectMix, a confidence-guided mixing framework
explicitly tailored for noisy labels. SelectMix first identifies potentially
noisy or ambiguous samples through confidence based mismatch analysis using
K-fold cross-validation, then selectively blends identified uncertain samples
with confidently predicted peers from their potential classes. Furthermore,
SelectMix employs soft labels derived from all classes involved in the mixing
process, ensuring the labels accurately represent the composition of the mixed
samples, thus aligning supervision signals closely with the actual mixed
inputs. Through extensive theoretical analysis and empirical evaluations on
multiple synthetic (MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100) and real-world
benchmark datasets (CIFAR-N, MNIST and Clothing1M), we demonstrate that
SelectMix consistently outperforms strong baseline methods, validating its
effectiveness and robustness in learning with noisy labels.

</details>


### [181] [Protected Probabilistic Classification Library](https://arxiv.org/abs/2509.11267)
*Ivan Petej*

Main category: cs.LG

TL;DR: 介绍了一个新的Python包，专门用于解决数据集偏移下概率分类器的校准问题，在二分类和多分类场景中验证了有效性


<details>
  <summary>Details</summary>
Motivation: 解决在训练集和测试集数据分布发生变化时，概率分类器的校准问题，这在批量学习和在线学习分类问题中都很常见

Method: 开发了一个专门的Python包，采用后处理校准方法，适用于二分类和多分类设置，支持批量学习和在线学习场景

Result: 实证结果令人鼓舞，表明该方法在各种数据集偏移场景下都能有效工作，优于多个现有的后处理校准方法

Conclusion: 该技术在处理训练和测试数据分布变化的分类问题中具有实用价值，能够有效改善概率分类器的校准性能

Abstract: This paper introduces a new Python package specifically designed to address
calibration of probabilistic classifiers under dataset shift. The method is
demonstrated in binary and multi-class settings and its effectiveness is
measured against a number of existing post-hoc calibration methods. The
empirical results are promising and suggest that our technique can be helpful
in a variety of settings for batch and online learning classification problems
where the underlying data distribution changes between the training and test
sets.

</details>


### [182] [PINGS: Physics-Informed Neural Network for Fast Generative Sampling](https://arxiv.org/abs/2509.11284)
*Achmad Ardani Prasha,Clavino Ourizqi Rachmadi,Muhamad Fauzan Ibnu Syahlan,Naufal Rahfi Anugerah,Nanda Garin Raditya,Putri Amelia,Sabrina Laila Mutiara,Hilman Syachr Ramadhan*

Main category: cs.LG

TL;DR: PINGS是一个基于物理信息神经网络的快速生成采样框架，通过单次前向传递实现扩散采样，相比传统迭代方法速度提升显著


<details>
  <summary>Details</summary>
Motivation: 为了解决扩散模型采样需要多次迭代计算（高NFE）导致生成速度慢的问题，开发一种能够通过单次前向传递实现高质量采样的方法

Method: 使用物理信息神经网络（PINN）来近似反向时间概率流动力学，将生成采样构建为带有端点锚定的残差问题，实现从3D标准正态分布到非高斯混合模型的直接映射

Result: 在RTX 3090上生成10^4个样本仅需16.54±0.56毫秒，相比DPM-Solver（468-843ms）和DDIM（960ms）速度提升显著，同时保持了目标分布的结构特性（MMD²=1.88×10⁻²）

Conclusion: PINGS作为一种白盒可微分映射框架，为快速基于函数的生成采样提供了有前景的途径，并具有扩展到科学模拟应用的潜力

Abstract: We introduce PINGS (Physics-Informed Neural Network for Fast Generative
Sampling), a framework that amortizes diffusion sampling by training a
physics-informed network to approximate reverse-time probability-flow dynamics,
reducing sampling to a single forward pass (NFE = 1). As a proof of concept, we
learn a direct map from a 3D standard normal to a non-Gaussian Gaussian Mixture
Model (GMM). PINGS preserves the target's distributional structure
(multi-bandwidth kernel $MMD^2 = 1.88 \times 10^{-2}$ with small errors in
mean, covariance, skewness, and excess kurtosis) and achieves constant-time
generation: $10^4$ samples in $16.54 \pm 0.56$ millisecond on an RTX 3090,
versus 468-843 millisecond for DPM-Solver (10/20) and 960 millisecond for DDIM
(50) under matched conditions. We also sanity-check the
PINN/automatic-differentiation pipeline on a damped harmonic oscillator,
obtaining MSEs down to $\mathcal{O}(10^{-5})$. Compared to fast but iterative
ODE solvers and direct-map families (Flow, Rectified-Flow, Consistency), PINGS
frames generative sampling as a PINN-style residual problem with endpoint
anchoring, yielding a white-box, differentiable map with NFE = 1. These
proof-of-concept results position PINGS as a promising route to fast,
function-based generative sampling with potential extensions to scientific
simulation (e.g., fast calorimetry).

</details>


### [183] [Efficient Single-Step Framework for Incremental Class Learning in Neural Networks](https://arxiv.org/abs/2509.11285)
*Alejandro Dopico-Castro,Oscar Fontenla-Romero,Bertha Guijarro-Berdiñas,Amparo Alonso-Betanzos*

Main category: cs.LG

TL;DR: CIFNet是一种高效的类增量学习方法，通过冻结预训练特征提取器、压缩数据缓冲区和单层神经网络分类器，在保持高精度的同时大幅降低计算成本和训练时间。


<details>
  <summary>Details</summary>
Motivation: 解决类增量学习中的灾难性遗忘问题，特别是在资源受限环境下现有方法计算成本高、训练过程复杂的问题。

Method: 整合三个组件：冻结的预训练特征提取器（避免主干网络微调）、压缩数据缓冲区（高效内存使用）、单层神经网络分类器（单步优化过程）。

Result: 在基准数据集上验证，CIFNet有效缓解分类器层面的灾难性遗忘，达到与现有SOTA方法相当的精度，同时显著提升训练效率和可持续性。

Conclusion: CIFNet使类增量学习在资源有限环境下更加实用可行，特别是在有强预训练特征提取器可用时，代表了该领域的重要进展。

Abstract: Incremental learning remains a critical challenge in machine learning, as
models often struggle with catastrophic forgetting -the tendency to lose
previously acquired knowledge when learning new information. These challenges
are even more pronounced in resource-limited settings. Many existing Class
Incremental Learning (CIL) methods achieve high accuracy by continually
adapting their feature representations; however, they often require substantial
computational resources and complex, iterative training procedures. This work
introduces CIFNet (Class Incremental and Frugal Network), a novel CIL approach
that addresses these limitations by offering a highly efficient and sustainable
solution. CIFNet's key innovation lies in its novel integration of several
existing, yet separately explored, components: a pre-trained and frozen feature
extractor, a compressed data buffer, and an efficient non-iterative one-layer
neural network for classification. A pre-trained and frozen feature extractor
eliminates computationally expensive fine-tuning of the backbone. This,
combined with a compressed buffer for efficient memory use, enables CIFNet to
perform efficient class-incremental learning through a single-step optimization
process on fixed features, minimizing computational overhead and training time
without requiring multiple weight updates. Experiments on benchmark datasets
confirm that CIFNet effectively mitigates catastrophic forgetting at the
classifier level, achieving high accuracy comparable to that of existing
state-of-the-art methods, while substantially improving training efficiency and
sustainability. CIFNet represents a significant advancement in making
class-incremental learning more accessible and pragmatic in environments with
limited resources, especially when strong pre-trained feature extractors are
available.

</details>


### [184] [Opal: An Operator Algebra View of RLHF](https://arxiv.org/abs/2509.11298)
*Madhava Gaikwad*

Main category: cs.LG

TL;DR: Opal提出了一个基于算子视角的RLHF框架，将目标表示为基本效用上的加法惩罚和乘法成对权重的阶梯结构，并引入了GKPO作为标准表示格式。


<details>
  <summary>Details</summary>
Motivation: 为了解决RLHF方法中目标函数表示不一致、难以比较和转换的问题，提供一个统一的数学框架和标准化表示。

Method: 使用算子视角将RLHF目标分解为加法惩罚和乘法权重阶梯，提出GKPO标准格式进行序列化、规范化和哈希处理。

Result: 建立了RLHF方法的统一表示框架，证明了在特定条件下的可约简性，并提供了不同方法间的转换能力。

Conclusion: Opal框架为RLHF提供了标准化的数学基础和实用工具，促进了不同方法间的比较和集成，但某些情况下存在不可约简的限制。

Abstract: We present Opal, an operator view of reinforcement learning from human
feedback (RLHF). Objectives are expressed as ladders of two primitives on a
base utility: additive penalties and multiplicative pairwise weights. We
describe a simple reduction law with if-and-only-if conditions: such ladders
collapse to a normal form on pairwise margins when the reference is fixed,
penalties are additive, and weights are independent of intermediate margins.
When these assumptions do not hold (reference shift, non-additive gates,
score-dependent weights), small examples demonstrate non-reducibility.
  Building on this view, we introduce GKPO (Generalized Kernel Preference
Object), a canonical schema in which many RLHF methods can be represented and,
when reducible, mapped back from. GKPO provides a standard JSON serialization,
canonicalization and hashing rules, and explicit flags with finite witnesses
when assumptions fail.
  We illustrate these ideas with GKPO examples for DPO, RRHF, and ORPO, along
with cross-method conversions (where assumptions permit) and minimal stress
tests (SHIFT/GATE/SCORE) that highlight non-reducibility. A lightweight Python
reference library accompanies the schema, implementing canonical hashing and
adapters for DPO and RRHF.

</details>


### [185] [MatQnA: A Benchmark Dataset for Multi-modal Large Language Models in Materials Characterization and Analysis](https://arxiv.org/abs/2509.11335)
*Yonghao Weng,Liqiang Gao,Linwu Zhu,Jian Huang*

Main category: cs.LG

TL;DR: MatQnA是首个专门为材料表征技术设计的多模态基准数据集，包含10种主流表征方法，用于评估AI模型在材料分析领域的能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在通用领域取得突破，但在高度专业化的材料表征分析领域的能力尚未得到系统验证，需要专门的评估基准。

Method: 采用LLM与人工验证相结合的混合方法构建高质量问答对，包含选择题和主观题，涵盖XPS、XRD、SEM、TEM等10种主流材料表征技术。

Result: 初步评估显示最先进的多模态AI模型（GPT-4.1、Claude 4、Gemini 2.5、Doubao Vision Pro 32K）在材料数据解释分析任务中的客观题准确率接近90%。

Conclusion: 多模态AI模型在材料表征分析领域展现出强大应用潜力，MatQnA数据集为这一领域的AI能力评估提供了重要基准工具。

Abstract: Recently, large language models (LLMs) have achieved remarkable breakthroughs
in general domains such as programming and writing, and have demonstrated
strong potential in various scientific research scenarios. However, the
capabilities of AI models in the highly specialized field of materials
characterization and analysis have not yet been systematically or sufficiently
validated. To address this gap, we present MatQnA, the first multi-modal
benchmark dataset specifically designed for material characterization
techniques. MatQnA includes ten mainstream characterization methods, such as
X-ray Photoelectron Spectroscopy (XPS), X-ray Diffraction (XRD), Scanning
Electron Microscopy (SEM), Transmission Electron Microscopy (TEM), etc. We
employ a hybrid approach combining LLMs with human-in-the-loop validation to
construct high-quality question-answer pairs, integrating both multiple-choice
and subjective questions. Our preliminary evaluation results show that the most
advanced multi-modal AI models (e.g., GPT-4.1, Claude 4, Gemini 2.5, and Doubao
Vision Pro 32K) have already achieved nearly 90% accuracy on objective
questions in materials data interpretation and analysis tasks, demonstrating
strong potential for applications in materials characterization and analysis.
The MatQnA dataset is publicly available at
https://huggingface.co/datasets/richardhzgg/matQnA.

</details>


### [186] [On the Escaping Efficiency of Distributed Adversarial Training Algorithms](https://arxiv.org/abs/2509.11337)
*Ying Cao,Kun Yuan,Ali H. Sayed*

Main category: cs.LG

TL;DR: 本文比较了分布式对抗训练算法（集中式与去中心化策略），发现在小扰动和大批量情况下，去中心化算法能更快逃离局部极小值，获得更平坦的最小值，从而提升模型鲁棒性。但随着扰动增强，此优势可能不再保持。


<details>
  <summary>Details</summary>
Motivation: 对抗训练对提升模型鲁棒性很重要，但分布式环境下的不同训练策略（集中式vs去中心化）对模型平坦度和鲁棒性的影响尚不清楚，需要系统性的理论和实验分析。

Method: 建立了理论框架分析算法逃离局部极小值的效率，通过模拟实验系统比较集中式和去中心化（共识和扩散）对抗训练算法的性能。

Result: 当扰动边界较小且使用大批量时，去中心化算法比集中式策略更快逃离局部极小值，获得更平坦的最小值。但随着扰动增强，这种优势可能消失。

Conclusion: 去中心化策略在分布式环境中具有提升模型鲁棒性的潜力，特别是在温和攻击条件下，这为分布式对抗训练提供了新的理论指导和实践方向。

Abstract: Adversarial training has been widely studied in recent years due to its role
in improving model robustness against adversarial attacks. This paper focuses
on comparing different distributed adversarial training algorithms--including
centralized and decentralized strategies--within multi-agent learning
environments. Previous studies have highlighted the importance of model
flatness in determining robustness. To this end, we develop a general
theoretical framework to study the escaping efficiency of these algorithms from
local minima, which is closely related to the flatness of the resulting models.
We show that when the perturbation bound is sufficiently small (i.e., when the
attack strength is relatively mild) and a large batch size is used,
decentralized adversarial training algorithms--including consensus and
diffusion--are guaranteed to escape faster from local minima than the
centralized strategy, thereby favoring flatter minima. However, as the
perturbation bound increases, this trend may no longer hold. In the simulation
results, we illustrate our theoretical findings and systematically compare the
performance of models obtained through decentralized and centralized
adversarial training algorithms. The results highlight the potential of
decentralized strategies to enhance the robustness of models in distributed
settings.

</details>


### [187] [BiLSTM-VHP: BiLSTM-Powered Network for Viral Host Prediction](https://arxiv.org/abs/2509.11345)
*Azher Ahmed Efat,Farzana Islam,Annajiat Alim Rasel,Munima Haque*

Main category: cs.LG

TL;DR: 提出了BiLSTM-VHP模型，使用双向LSTM架构从400个碱基长度的核苷酸序列预测病毒宿主，在三种病毒上取得了优于先前研究的准确率


<details>
  <summary>Details</summary>
Motivation: 人畜共患病毒疾病（如SARS-CoV-2、猴痘和猪流感病毒）频繁爆发，快速准确预测病毒宿主来源有助于预防疾病传播

Method: 使用轻量级双向长短期记忆（BiLSTM）网络架构，处理400个碱基长度的核苷酸序列，预测orthohantavirus、rabies lyssavirus和rotavirus A三种病毒的宿主

Result: 模型在orthohantavirus上达到89.62%准确率，rotavirus A上96.58%，rabies lyssavirus上77.22%，均优于先前研究。还提供了三个精心策划的数据集

Conclusion: BiLSTM-VHP是一个有效的病毒宿主预测工具，可用于预防人畜共患疾病的传播，相关代码和数据集已公开

Abstract: Recorded history shows the long coexistence of humans and animals, suggesting
it began much earlier. Despite some beneficial interdependence, many animals
carry viral diseases that can spread to humans. These diseases are known as
zoonotic diseases. Recent outbreaks of SARS-CoV-2, Monkeypox and swine flu
viruses have shown how these viruses can disrupt human life and cause death.
Fast and accurate predictions of the host from which the virus spreads can help
prevent these diseases from spreading. This work presents BiLSTM-VHP, a
lightweight bidirectional long short-term memory (LSTM)-based architecture that
can predict the host from the nucleotide sequence of orthohantavirus, rabies
lyssavirus, and rotavirus A with high accuracy. The proposed model works with
nucleotide sequences of 400 bases in length and achieved a prediction accuracy
of 89.62% for orthohantavirus, 96.58% for rotavirus A, and 77.22% for rabies
lyssavirus outperforming previous studies. Moreover, performance of the model
is assessed using the confusion matrix, F-1 score, precision, recall,
microaverage AUC. In addition, we introduce three curated datasets of
orthohantavirus, rotavirus A, and rabies lyssavirus containing 8,575, 95,197,
and 22,052 nucleotide sequences divided into 9, 12, and 29 host classes,
respectively. The codes and dataset are available at
https://doi.org/10.17605/OSF.IO/ANFKR

</details>


### [188] [On Linear Mode Connectivity of Mixture-of-Experts Architectures](https://arxiv.org/abs/2509.11348)
*Viet-Hoang Tran,Van Hoan Trinh,Khanh Vinh Bui,Tan M. Nguyen*

Main category: cs.LG

TL;DR: 本文系统研究了混合专家(MoE)架构中的线性模式连接性(LMC)现象，发现独立训练的MoE模型可以通过参数空间的线性路径连接且保持低损失，提出了匹配算法来实现模型对齐，并在多种MoE配置和数据集上验证了LMC的存在。


<details>
  <summary>Details</summary>
Motivation: 受到标准神经网络中LMC现象的启发，研究MoE架构中的线性连接性，这对理解模型集成、泛化能力和神经损失几何具有重要意义。MoE架构因其可扩展性和计算效率而备受关注，但其中的LMC现象尚未得到系统研究。

Method: 首先全面分析稠密和稀疏门控机制，证明MoE架构的对称性完全由专家组件和门控函数的排列表征。基于此提出匹配算法来实现独立训练MoE模型的对齐，从而发现LMC。在多种MoE配置（稠密、稀疏、共享专家变体）和不同规模、模态的数据集上进行实证验证。

Result: 研究结果证实了MoE架构中存在LMC现象，匹配算法成功实现了模型对齐，在线性路径上保持了低损失。这一发现在不同模型设置和数据集上都得到了验证。

Conclusion: MoE架构中存在线性模式连接性，这为理解深度学习模型的功能景观和优化动力学提供了基础性见解。研究揭示了MoE模型的对称性特征，提出的匹配算法为模型集成和优化研究提供了新工具。

Abstract: Linear Mode Connectivity (LMC) is a notable phenomenon in the loss landscapes
of neural networks, wherein independently trained models have been observed to
be connected--up to permutation symmetries--by linear paths in parameter space
along which the loss remains consistently low. This observation challenges
classical views of non-convex optimization and has implications for model
ensembling, generalization, and our understanding of neural loss geometry.
Inspired by recent studies on LMC in standard neural networks, we
systematically investigate this phenomenon within Mixture-of-Experts (MoE)
architectures--a class of models known for their scalability and computational
efficiency, which combine traditional neural networks--referred to as
experts--through a learnable gating mechanism. We begin by conducting a
comprehensive analysis of both dense and sparse gating regimes, demonstrating
that the symmetries inherent to MoE architectures are fully characterized by
permutations acting on both the expert components and the gating function.
Building on these foundational findings, we propose a matching algorithm that
enables alignment between independently trained MoEs, thereby facilitating the
discovery of LMC. Finally, we empirically validate the presence of LMC using
our proposed algorithm across diverse MoE configurations--including dense,
sparse, and shared-expert variants--under a wide range of model settings and
datasets of varying scales and modalities. Our results confirm the existence of
LMC in MoE architectures and offer fundamental insights into the functional
landscape and optimization dynamics of deep learning models.

</details>


### [189] [Online Omniprediction with Long-Term Constraints](https://arxiv.org/abs/2509.11357)
*Yahav Bechavod,Jiuyao Lu,Aaron Roth*

Main category: cs.LG

TL;DR: 提出并研究具有长期约束的在线全预测问题，设计单一预测集使下游代理在保证后悔界的同时满足累积约束


<details>
  <summary>Details</summary>
Motivation: 解决多代理系统中，每个代理具有不同效用函数和约束函数时，如何通过单一预测集使所有代理都能同时保证低后悔和约束满足

Method: 设计预测算法，使下游代理只需基于预测选择行动，即可保证每个代理的后悔界为O(√T)且累积约束违反为O(1)，并扩展到任意上下文定义的子序列

Result: 证明了可以生成单一预测集，保证所有下游代理获得O(√T)后悔界和O(1)累积约束违反，且能扩展到任意子序列同时保证性能

Conclusion: 该框架为多代理在线决策提供了有效的预测机制，能够在复杂约束下实现全局性能保证

Abstract: We introduce and study the problem of online omniprediction with long-term
constraints. At each round, a forecaster is tasked with generating predictions
for an underlying (adaptively, adversarially chosen) state that are broadcast
to a collection of downstream agents, who must each choose an action. Each of
the downstream agents has both a utility function mapping actions and state to
utilities, and a vector-valued constraint function mapping actions and states
to vector-valued costs. The utility and constraint functions can arbitrarily
differ across downstream agents. Their goal is to choose actions that guarantee
themselves no regret while simultaneously guaranteeing that they do not
cumulatively violate the constraints across time. We show how to make a single
set of predictions so that each of the downstream agents can guarantee this by
acting as a simple function of the predictions, guaranteeing each of them
$\tilde{O}(\sqrt{T})$ regret and $O(1)$ cumulative constraint violation. We
also show how to extend our guarantees to arbitrary intersecting contextually
defined \emph{subsequences}, guaranteeing each agent both regret and constraint
violation bounds not just marginally, but simultaneously on each subsequence,
against a benchmark set of actions simultaneously tailored to each subsequence.

</details>


### [190] [Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting](https://arxiv.org/abs/2509.11452)
*Yining Lu,Zilong Wang,Shiyang Li,Xin Liu,Changlong Yu,Qingyu Yin,Zhan Shi,Zixuan Zhang,Meng Jiang*

Main category: cs.LG

TL;DR: 提出动态奖励加权方法解决多目标强化学习中固定权重线性标量化无法处理非凸帕累托前沿的问题，通过超体积引导和梯度优化两种策略实现在线权重调整。


<details>
  <summary>Details</summary>
Motivation: 传统多目标强化学习使用固定权重线性标量化，无法有效处理非凸帕累托前沿，特别是在大型语言模型在线偏好对齐中，随机轨迹生成的非线性映射使得单一静态权重方案无法找到最优权衡。

Method: 引入动态奖励加权方法，包括：(1)超体积引导权重适应和(2)梯度基权重优化，在在线强化学习过程中自适应调整奖励权重。

Result: 实验证明该方法与常用在线RL算法兼容，在多个数学推理数据集上有效，适用于不同模型家族，能以更少训练步骤获得帕累托占优解。

Conclusion: 动态奖励加权方法有效解决了固定权重线性标量化的局限性，为在线多目标对齐提供了通用工具包，能够更好地探索目标空间的帕累托前沿。

Abstract: Prior works in multi-objective reinforcement learning typically use linear
reward scalarization with fixed weights, which provably fail to capture
non-convex Pareto fronts and thus yield suboptimal results. This limitation
becomes especially critical in online preference alignment for large language
models. Here, stochastic trajectories generated by parameterized policies
create highly non-linear and non-convex mappings from parameters to objectives
that no single static weighting scheme can find optimal trade-offs. We address
this limitation by introducing dynamic reward weighting, which adaptively
adjusts reward weights during the online reinforcement learning process. Unlike
existing approaches that rely on fixed-weight interpolation, our dynamic
weighting continuously balances and prioritizes objectives in training,
facilitating effective exploration of Pareto fronts in objective space. We
introduce two approaches of increasing sophistication and generalizability: (1)
hypervolume-guided weight adaptation and (2) gradient-based weight
optimization, offering a versatile toolkit for online multi-objective
alignment. Our extensive experiments demonstrate their compatibility with
commonly used online reinforcement learning algorithms (including GRPO,
REINFORCE, and RLOO), effectiveness across multiple mathematical reasoning
datasets, and applicability to different model families, consistently achieving
Pareto dominant solutions with fewer training steps than fixed-weight linear
scalarization baselines.

</details>


### [191] [PersonaX: Multimodal Datasets with LLM-Inferred Behavior Traits](https://arxiv.org/abs/2509.11362)
*Loka Li,Wong Yu Kang,Minghao Fu,Guangyi Chen,Zhenhao Chen,Gongxu Luo,Yuewen Sun,Salman Khan,Peter Spirtes,Kun Zhang*

Main category: cs.LG

TL;DR: 提出了PersonaX多模态数据集，包含CelebPersona和AthlePersona两个子集，整合了行为特征、面部图像和传记信息，用于多模态行为特征分析和因果推理研究。


<details>
  <summary>Details</summary>
Motivation: 现有数据集很少将行为描述符与面部属性和传记信息等多模态数据结合，限制了人类行为特征的全面分析。

Method: 构建了包含9444名公众人物和4181名运动员的多模态数据集，使用三个高性能大语言模型推断行为特征评估，并提出了新颖的因果表示学习框架。

Result: 在合成和真实数据上的实验证明了方法的有效性，通过统计独立性检验和因果分析揭示了多模态特征之间的关系。

Conclusion: PersonaX为研究LLM推断的行为特征与视觉、传记属性的结合提供了基础，推动了多模态特征分析和因果推理的发展。

Abstract: Understanding human behavior traits is central to applications in
human-computer interaction, computational social science, and personalized AI
systems. Such understanding often requires integrating multiple modalities to
capture nuanced patterns and relationships. However, existing resources rarely
provide datasets that combine behavioral descriptors with complementary
modalities such as facial attributes and biographical information. To address
this gap, we present PersonaX, a curated collection of multimodal datasets
designed to enable comprehensive analysis of public traits across modalities.
PersonaX consists of (1) CelebPersona, featuring 9444 public figures from
diverse occupations, and (2) AthlePersona, covering 4181 professional athletes
across 7 major sports leagues. Each dataset includes behavioral trait
assessments inferred by three high-performing large language models, alongside
facial imagery and structured biographical features. We analyze PersonaX at two
complementary levels. First, we abstract high-level trait scores from text
descriptions and apply five statistical independence tests to examine their
relationships with other modalities. Second, we introduce a novel causal
representation learning (CRL) framework tailored to multimodal and
multi-measurement data, providing theoretical identifiability guarantees.
Experiments on both synthetic and real-world data demonstrate the effectiveness
of our approach. By unifying structured and unstructured analysis, PersonaX
establishes a foundation for studying LLM-inferred behavioral traits in
conjunction with visual and biographical attributes, advancing multimodal trait
analysis and causal reasoning.

</details>


### [192] [Measuring Visual Understanding in Telecom domain: Performance Metrics for Image-to-UML conversion using VLMs](https://arxiv.org/abs/2509.11667)
*HG Ranjani,Rutuja Prabhudesai*

Main category: cs.LG

TL;DR: 提出评估电信领域序列图转换为PlantUML格式的性能指标，比较Claude Sonnet和GPT-4V在3GPP文档序列图转换中的表现


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对序列图转换为PlantUML格式的全面评估，特别是在电信领域3GPP文档中图像转换的组件级比较

Method: 使用3GPP文档序列图数据集，比较两个VLMs（Claude Sonnet和GPT-4V）与人工标注的真实值，采用版本控制工具捕获差异并引入标准性能指标

Result: 节点、边和消息能够准确捕获，但VLMs在复杂结构（如注释、框、组）上表现不佳

Conclusion: 需要在训练数据中更好地表示复杂组件，以提高微调VLMs在序列图转换中的性能

Abstract: Telecom domain 3GPP documents are replete with images containing sequence
diagrams. Advances in Vision-Language Large Models (VLMs) have eased conversion
of such images to machine-readable PlantUML (puml) formats. However, there is a
gap in evaluation of such conversions - existing works do not compare puml
scripts for various components. In this work, we propose performance metrics to
measure the effectiveness of such conversions. A dataset of sequence diagrams
from 3GPP documents is chosen to be representative of domain-specific actual
scenarios. We compare puml outputs from two VLMs - Claude Sonnet and GPT-4V -
against manually created ground truth representations. We use version control
tools to capture differences and introduce standard performance metrics to
measure accuracies along various components: participant identification,
message flow accuracy, sequence ordering, and grouping construct preservation.
We demonstrate effectiveness of proposed metrics in quantifying conversion
errors across various components of puml scripts. The results show that nodes,
edges and messages are accurately captured. However, we observe that VLMs do
not necessarily perform well on complex structures such as notes, box, groups.
Our experiments and performance metrics indicates a need for better
representation of these components in training data for fine-tuned VLMs.

</details>


### [193] [Detecting Model Drifts in Non-Stationary Environment Using Edit Operation Measures](https://arxiv.org/abs/2509.11367)
*Chang-Hwan Lee,Alexander Shim*

Main category: cs.LG

TL;DR: 提出了一种基于编辑操作度量的新框架，用于检测强化学习环境中的模型漂移，通过分析智能体行为序列的分布变化来识别非平稳环境中的动态变化。


<details>
  <summary>Details</summary>
Motivation: 现实世界应用（如医疗、机器人、金融）中的环境动态可能随时间变化，导致模型漂移，传统RL方法假设环境平稳性，无法有效处理这种非平稳性。

Method: 引入一套基于编辑操作的度量方法，量化在平稳和扰动条件下生成的状态-动作轨迹之间的偏差，通过分析行为序列的分布变化来检测漂移。

Result: 实验表明这些度量方法能够有效区分漂移和非漂移场景，即使在不同程度的噪声下也能保持良好性能，为实际应用提供了实用的漂移检测工具。

Conclusion: 提出的编辑操作基度量框架为检测非平稳RL环境中的模型漂移提供了有效解决方案，具有实际应用价值。

Abstract: Reinforcement learning (RL) agents typically assume stationary environment
dynamics. Yet in real-world applications such as healthcare, robotics, and
finance, transition probabilities or reward functions may evolve, leading to
model drift. This paper proposes a novel framework to detect such drifts by
analyzing the distributional changes in sequences of agent behavior.
Specifically, we introduce a suite of edit operation-based measures to quantify
deviations between state-action trajectories generated under stationary and
perturbed conditions. Our experiments demonstrate that these measures can
effectively distinguish drifted from non-drifted scenarios, even under varying
levels of noise, providing a practical tool for drift detection in
non-stationary RL environments.

</details>


### [194] [Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning](https://arxiv.org/abs/2509.11816)
*Filip Sondej,Yushi Yang*

Main category: cs.LG

TL;DR: 提出了一种基于PCA分析激活和梯度的高选择性遗忘技术，能有效移除语言模型中的危险知识，同时保持通用性能


<details>
  <summary>Details</summary>
Motivation: 当前的安全训练和遗忘技术无法有效移除语言模型中的危险知识，需要找到既能有效遗忘特定知识又不影响通用性能的方法

Method: 通过对激活和模块输出梯度进行PCA分析，识别包含共同表示的子空间，在计算遗忘更新前先压缩这些子空间，从而只针对特定事实相关的表示进行遗忘

Result: 在Llama-3.1-8B上遗忘WMDP数据集事实时，生物危害事实的遗忘效果比最佳基线好80倍，网络危害事实好30倍，同时通用性能影响极小（WikiText损失仅增加0.1%），每个事实只需不到3GPU秒

Conclusion: 该方法能够高效、选择性地移除语言模型中的危险知识，在保持模型通用性能的同时实现强大的遗忘效果

Abstract: Current unlearning techniques and safety training consistently fail to remove
dangerous knowledge from language models. We analyze the root causes and
propose a highly selective technique which unlearns robustly and without
disrupting general performance.
  We perform PCA on activations and module output gradients to identify
subspaces containing common representations, and collapse them before
calculating unlearning updates. This way we avoid unlearning general
representations, and only target those specific to the unlearned facts.
  When unlearning WMDP dataset facts from Llama-3.1-8B, we drop post-attack
accuracy 80x more than our best baseline (Circuit Breakers) on biohazardous
facts and 30x more on cyberhazardous facts. Despite this, we disrupt general
performance 30x less (only 0.1% WikiText loss increase), while requiring less
than 3 GPU-seconds per fact.

</details>


### [195] [Decoding Musical Origins: Distinguishing Human and AI Composers](https://arxiv.org/abs/2509.11369)
*Cheng-Yang Tsai,Tzu-Wei Huang,Shao-Yu Wei,Guan-Wei Chen,Hung-Ying Chu,Yu-Cheng Lin*

Main category: cs.LG

TL;DR: 本研究开发了YNote音乐符号系统，并基于此训练了一个高精度分类模型，能够准确区分人类创作、规则算法生成和LLM生成的音乐作品，准确率达到98.25%。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，AI音乐生成成为研究热点，但音乐数据表示仍是一个重要挑战。需要开发新的音乐符号系统和分类方法来识别不同来源的音乐作品。

Method: 开发YNote机器学习友好型音乐符号系统，将音乐分类问题转化为文本分类问题，使用TF-IDF算法提取结构特征，并应用SMOTE技术处理数据不平衡问题。

Result: 模型达到了98.25%的准确率，成功证明YNote保留了足够的风格信息用于分析，并能识别不同AI生成技术留下的独特"技术指纹"。

Conclusion: YNote系统有效解决了音乐数据表示问题，所提出的分类模型为追踪AI生成内容的来源提供了有力工具，在音乐作品溯源方面具有重要应用价值。

Abstract: With the rapid advancement of Large Language Models (LLMs), AI-driven music
generation has become a vibrant and fruitful area of research. However, the
representation of musical data remains a significant challenge. To address
this, a novel, machine-learning-friendly music notation system, YNote, was
developed. This study leverages YNote to train an effective classification
model capable of distinguishing whether a piece of music was composed by a
human (Native), a rule-based algorithm (Algorithm Generated), or an LLM (LLM
Generated). We frame this as a text classification problem, applying the Term
Frequency-Inverse Document Frequency (TF-IDF) algorithm to extract structural
features from YNote sequences and using the Synthetic Minority Over-sampling
Technique (SMOTE) to address data imbalance. The resulting model achieves an
accuracy of 98.25%, successfully demonstrating that YNote retains sufficient
stylistic information for analysis. More importantly, the model can identify
the unique " technological fingerprints " left by different AI generation
techniques, providing a powerful tool for tracing the origins of AI-generated
content.

</details>


### [196] [MillStone: How Open-Minded Are LLMs?](https://arxiv.org/abs/2509.11967)
*Harold Triedman,Vitaly Shmatikov*

Main category: cs.LG

TL;DR: MillStone是首个系统测量外部论点对LLM在争议问题上立场影响的基准测试，发现LLM在大多数问题上思想开放，权威信息源容易影响其立场


<details>
  <summary>Details</summary>
Motivation: 随着用户开始依赖LLM获取包括争议性话题在内的各种信息，需要了解LLM输出中的立场和观点如何受其使用的信息源文档影响

Method: 开发MillStone基准测试，应用于9个领先的LLM，测量它们对不同立场论点的开放程度、模型间一致性、最具说服力的论点以及不同模型的差异

Result: LLM在大多数问题上思想开放，权威信息源容易动摇LLM的立场，凸显了信息源选择的重要性以及LLM信息检索系统可能被操纵的风险

Conclusion: 需要关注LLM信息检索系统中信息源的选择和质量控制，以防止系统被操纵和确保输出立场的客观性

Abstract: Large language models equipped with Web search, information retrieval tools,
and other agentic capabilities are beginning to supplant traditional search
engines. As users start to rely on LLMs for information on many topics,
including controversial and debatable issues, it is important to understand how
the stances and opinions expressed in LLM outputs are influenced by the
documents they use as their information sources.
  In this paper, we present MillStone, the first benchmark that aims to
systematically measure the effect of external arguments on the stances that
LLMs take on controversial issues (not all of them political). We apply
MillStone to nine leading LLMs and measure how ``open-minded'' they are to
arguments supporting opposite sides of these issues, whether different LLMs
agree with each other, which arguments LLMs find most persuasive, and whether
these arguments are the same for different LLMs.
  In general, we find that LLMs are open-minded on most issues. An
authoritative source of information can easily sway an LLM's stance,
highlighting the importance of source selection and the risk that LLM-based
information retrieval and search systems can be manipulated.

</details>


### [197] [Intelligent Reservoir Decision Support: An Integrated Framework Combining Large Language Models, Advanced Prompt Engineering, and Multimodal Data Fusion for Real-Time Petroleum Operations](https://arxiv.org/abs/2509.11376)
*Seyed Kourosh Mahjour,Seyed Saman Mahjour*

Main category: cs.LG

TL;DR: 本研究提出了一个结合大型语言模型和多模态数据融合的集成框架，用于石油储层管理，在15个不同储层环境中验证了优异性能，实现了94.2%的储层表征准确率和显著的成本降低。


<details>
  <summary>Details</summary>
Motivation: 石油行业面临前所未有的储层管理挑战，需要快速整合复杂的多模态数据集以支持实时决策制定。

Method: 结合GPT-4o、Claude 4 Sonnet和Gemini 2.5 Pro等先进大语言模型，采用领域特定的检索增强生成技术（包含50,000+石油工程文档）、思维链推理和少样本学习，通过视觉变换器处理地震解释、测井和生产数据。

Result: 在15个不同储层环境中验证：94.2%储层表征准确率、87.6%生产预测精度、91.4%井位优化成功率；响应时间亚秒级，安全可靠性96.2%；成本降低62-78%（平均72%），投资回收期8个月；异常检测准确率96.2%，环境事件减少45%。

Conclusion: 该研究展示了前沿AI技术与石油领域专业知识的实际集成，显著提升了运营效率、安全性和经济性能，为行业提供了可复制的解决方案。

Abstract: The petroleum industry faces unprecedented challenges in reservoir
management, requiring rapid integration of complex multimodal datasets for
real-time decision support. This study presents a novel integrated framework
combining state-of-the-art large language models (GPT-4o, Claude 4 Sonnet,
Gemini 2.5 Pro) with advanced prompt engineering techniques and multimodal data
fusion for comprehensive reservoir analysis. The framework implements
domain-specific retrieval-augmented generation (RAG) with over 50,000 petroleum
engineering documents, chain-of-thought reasoning, and few-shot learning for
rapid field adaptation. Multimodal integration processes seismic
interpretations, well logs, and production data through specialized AI models
with vision transformers. Field validation across 15 diverse reservoir
environments demonstrates exceptional performance: 94.2% reservoir
characterization accuracy, 87.6% production forecasting precision, and 91.4%
well placement optimization success rate. The system achieves sub-second
response times while maintaining 96.2% safety reliability with no high-risk
incidents during evaluation. Economic analysis reveals 62-78% cost reductions
(mean 72%) relative to traditional methods with 8-month payback period.
Few-shot learning reduces field adaptation time by 72%, while automated prompt
optimization achieves 89% improvement in reasoning quality. The framework
processed real-time data streams with 96.2% anomaly detection accuracy and
reduced environmental incidents by 45%. We provide detailed experimental
protocols, baseline comparisons, ablation studies, and statistical significance
testing to ensure reproducibility. This research demonstrates practical
integration of cutting-edge AI technologies with petroleum domain expertise for
enhanced operational efficiency, safety, and economic performance.

</details>


### [198] [AMQ: Enabling AutoML for Mixed-precision Weight-Only Quantization of Large Language Models](https://arxiv.org/abs/2509.12019)
*Sangjun Lee,Seung-taek Woo,Jungyu Jin,Changhun Lee,Eunhyeok Park*

Main category: cs.LG

TL;DR: AMQ是一个自动化混合精度权重量化框架，通过智能搜索算法在严格内存约束下为LLM找到最优的量化位宽配置，平衡模型质量和内存使用


<details>
  <summary>Details</summary>
Motivation: 为了在严格内存约束下部署大型语言模型，需要找到性能最佳的量化配置，但组合搜索空间巨大（超过10^100种可能），传统黑盒优化方法不可行

Method: 采用四种关键技术：(1)基于先验知识的搜索空间剪枝，(2)量化代理绕过格式转换，(3)质量预测器减少评估开销，(4)迭代搜索更新策略实现快速稳定收敛

Result: AMQ能够高效探索质量-效率边界，达到帕累托前沿，生成既紧凑又高性能的LLM

Conclusion: AMQ框架成功解决了大规模量化配置搜索的挑战，为内存受限环境下的LLM部署提供了有效的自动化解决方案

Abstract: To enable broader deployment of Large Language Models (LLMs), it is essential
to identify the best-performing model under strict memory constraints. We
present AMQ, Automated Mixed-Precision Weight-Only Quantization, a framework
that assigns layer-wise quantization bit-widths to optimally balance model
quality and memory usage. However, the combinatorial search space, with over
10^{100} possible configurations, makes conventional black-box optimization
infeasible. AMQ overcomes this challenge through four key innovations:(1)
search space pruning using prior knowledge to exclude unpromising
configurations, (2) quantization proxy to bypass costly format conversions
during search, (3) quality predictor to minimize evaluation overhead, and (4)
iterative search-and-update strategy for fast and stable convergence. By
integrating these components, AMQ efficiently explores the quality-efficiency
landscape, reaching the Pareto frontier and yielding LLMs that are both compact
and high-performing. Our code is available at https://github.com/dlwns147/amq.

</details>


### [199] [Enhancing ML Models Interpretability for Credit Scoring](https://arxiv.org/abs/2509.11389)
*Sagi Schwartz,Qinling Wang,Fang Fang*

Main category: cs.LG

TL;DR: 提出一种混合方法：使用黑盒模型的后解释进行特征选择，然后训练既保持预测能力又透明的玻璃盒模型，在Lending Club数据集上仅用10个特征就达到与基准黑盒模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在违约预测中缺乏透明度，无法满足监管要求（如IRB模型），需要开发既准确又透明的模型。

Method: 使用SHAP进行特征选择，XGBoost作为基准黑盒模型，EBM和PLTR作为玻璃盒模型，并通过特征交互分析、相关性检查和专家输入进行模型精炼。

Result: 该方法实现了与基准黑盒模型相当的性能，同时特征数量减少了88.5%（仅用10个特征）。

Conclusion: 混合方法能够平衡预测性能和模型透明度，满足监管要求，同时通过特征交互分析和专家输入进一步提升模型的可解释性和鲁棒性。

Abstract: Predicting default is essential for banks to ensure profitability and
financial stability. While modern machine learning methods often outperform
traditional regression techniques, their lack of transparency limits their use
in regulated environments. Explainable artificial intelligence (XAI) has
emerged as a solution in domains like credit scoring. However, most XAI
research focuses on post-hoc interpretation of black-box models, which does not
produce models lightweight or transparent enough to meet regulatory
requirements, such as those for Internal Ratings-Based (IRB) models.
  This paper proposes a hybrid approach: post-hoc interpretations of black-box
models guide feature selection, followed by training glass-box models that
maintain both predictive power and transparency.
  Using the Lending Club dataset, we demonstrate that this approach achieves
performance comparable to a benchmark black-box model while using only 10
features - an 88.5% reduction. In our example, SHapley Additive exPlanations
(SHAP) is used for feature selection, eXtreme Gradient Boosting (XGBoost)
serves as the benchmark and the base black-box model, and Explainable Boosting
Machine (EBM) and Penalized Logistic Tree Regression (PLTR) are the
investigated glass-box models.
  We also show that model refinement using feature interaction analysis,
correlation checks, and expert input can further enhance model interpretability
and robustness.

</details>


### [200] [Event2Vec: A Geometric Approach to Learning Composable Representations of Event Sequences](https://arxiv.org/abs/2509.12188)
*Antonin Sulc*

Main category: cs.LG

TL;DR: Event2Vec是一个学习离散事件序列表示的新框架，使用加法循环结构学习可组合、可解释的嵌入，支持欧几里得和双曲几何空间。


<details>
  <summary>Details</summary>
Motivation: 受神经表示中几何和拓扑结构重要性的启发，需要为离散事件序列开发能够捕捉序列结构特性的表示学习方法。

Method: 采用简单的加法循环结构，在特定训练目标下学习可组合的嵌入表示。提供理论分析证明在欧几里得空间中收敛到理想的加法结构，并引入双曲空间变体来处理层次化数据。

Result: 理论分析表明模型学习到的表示收敛到线性加法假设，实验验证了假设并展示了双曲模型在层次化事件序列上的优越性能。

Conclusion: Event2Vec框架成功实现了事件序列的可解释嵌入学习，双曲几何在处理层次化数据结构方面表现出色，为序列表示学习提供了新的几何视角。

Abstract: The study of neural representations, both in biological and artificial
systems, is increasingly revealing the importance of geometric and topological
structures. Inspired by this, we introduce Event2Vec, a novel framework for
learning representations of discrete event sequences. Our model leverages a
simple, additive recurrent structure to learn composable, interpretable
embeddings. We provide a theoretical analysis demonstrating that, under
specific training objectives, our model's learned representations in a
Euclidean space converge to an ideal additive structure. This ensures that the
representation of a sequence is the vector sum of its constituent events, a
property we term the linear additive hypothesis. To address the limitations of
Euclidean geometry for hierarchical data, we also introduce a variant of our
model in hyperbolic space, which is naturally suited to embedding tree-like
structures with low distortion. We present experiments to validate our
hypothesis and demonstrate the benefits of each geometry, highlighting the
improved performance of the hyperbolic model on hierarchical event sequences.

</details>


### [201] [From Firewalls to Frontiers: AI Red-Teaming is a Domain-Specific Evolution of Cyber Red-Teaming](https://arxiv.org/abs/2509.11398)
*Anusha Sinha,Keltin Grimes,James Lucassen,Michael Feffer,Nathan VanHoudnos,Zhiwei Steven Wu,Hoda Heidari*

Main category: cs.LG

TL;DR: 本文主张将AI红队测试视为网络安全红队测试的领域特定演进，通过融合AI和网络安全红队的优势来更有效地评估AI系统的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着企业系统越来越多地采用AI技术，传统的红队测试方法需要演进以应对AI系统带来的独特漏洞和风险。AI系统具有新的风险特征、新的故障模式以及通常包含无法修补的漏洞，这需要重新调整披露和缓解策略的优先级。

Method: 提出将AI红队测试框架化为网络安全红队测试的领域特定演进，让现有的网络安全红队采用这种框架来更好地评估包含AI组件的系统，同时让AI红队借鉴成熟的网络安全红队结构和流程。

Result: 通过这种融合方法，网络安全红队能够识别AI带来的新风险，而AI红队则能够利用经过验证的结构来模拟真实对手、建立相互问责机制，并发展可重复、可扩展的工具链。

Conclusion: AI和网络安全红队的融合将创建一个强大的安全生态系统，使社区能够更好地适应快速变化的威胁环境，为AI系统的安全评估提供更有效的方法论和实践框架。

Abstract: A red team simulates adversary attacks to help defenders find effective
strategies to defend their systems in a real-world operational setting. As more
enterprise systems adopt AI, red-teaming will need to evolve to address the
unique vulnerabilities and risks posed by AI systems. We take the position that
AI systems can be more effectively red-teamed if AI red-teaming is recognized
as a domain-specific evolution of cyber red-teaming. Specifically, we argue
that existing Cyber Red Teams who adopt this framing will be able to better
evaluate systems with AI components by recognizing that AI poses new risks, has
new failure modes to exploit, and often contains unpatchable bugs that
re-prioritize disclosure and mitigation strategies. Similarly, adopting a
cybersecurity framing will allow existing AI Red Teams to leverage a
well-tested structure to emulate realistic adversaries, promote mutual
accountability with formal rules of engagement, and provide a pattern to mature
the tooling necessary for repeatable, scalable engagements. In these ways, the
merging of AI and Cyber Red Teams will create a robust security ecosystem and
best position the community to adapt to the rapidly changing threat landscape.

</details>


### [202] [Framing AI System Benchmarking as a Learning Task: FlexBench and the Open MLPerf Dataset](https://arxiv.org/abs/2509.11413)
*Grigori Fursin,Daniel Altunay*

Main category: cs.LG

TL;DR: FlexBench是一个模块化的AI基准测试框架，将基准测试视为AI任务，通过持续评估模型在不同数据集、软件和硬件上的性能，为AI系统部署提供可操作的见解。


<details>
  <summary>Details</summary>
Motivation: 现有AI基准测试（如MLPerf）难以跟上快速发展的AI领域，无法有效支持AI系统的部署、优化和协同设计决策。

Method: 开发FlexBench作为MLPerf LLM推理基准的模块化扩展，集成HuggingFace，收集准确性、延迟、吞吐量、能耗和成本等关键指标的基准测试结果和元数据到Open MLPerf数据集中。

Result: 通过MLPerf推理提交成功验证了FlexBench概念，包括在商用服务器上评估DeepSeek R1和LLaMA 3.3模型。

Conclusion: FlexBench使从业者能够根据可用资源、需求和约束做出成本效益高的AI部署决策，支持预测建模和特征工程。

Abstract: Existing AI system benchmarks such as MLPerf often struggle to keep pace with
the rapidly evolving AI landscape, making it difficult to support informed
deployment, optimization, and co-design decisions for AI systems. We suggest
that benchmarking itself can be framed as an AI task - one in which models are
continuously evaluated and optimized across diverse datasets, software, and
hardware, using key metrics such as accuracy, latency, throughput, energy
consumption, and cost. To support this perspective, we present FlexBench: a
modular extension of the MLPerf LLM inference benchmark, integrated with
HuggingFace and designed to provide relevant and actionable insights.
Benchmarking results and metadata are collected into an Open MLPerf Dataset,
which can be collaboratively curated, extended, and leveraged for predictive
modeling and feature engineering. We successfully validated the FlexBench
concept through MLPerf Inference submissions, including evaluations of DeepSeek
R1 and LLaMA 3.3 on commodity servers. The broader objective is to enable
practitioners to make cost-effective AI deployment decisions that reflect their
available resources, requirements, and constraints.

</details>


### [203] [Long-time dynamics and universality of nonconvex gradient descent](https://arxiv.org/abs/2509.11426)
*Qiyang Han*

Main category: cs.LG

TL;DR: 该论文开发了一种通用方法来分析广义单指标模型中非凸梯度下降的长期轨迹行为，在大纵横比条件下揭示了梯度下降迭代的集中性和隐式正则化效应。


<details>
  <summary>Details</summary>
Motivation: 研究非凸梯度下降在大纵横比条件下的长期行为特性，理解梯度下降的动力学特性和隐式正则化现象，为实际应用提供理论指导。

Method: 提出高斯理论梯度下降概念，通过状态演化系统的两个递归标量方程来跟踪动力学，建立通用浓度保证，适用于广泛的设计矩阵类别。

Result: 证明梯度下降迭代在大纵横比条件下集中围绕确定性向量，具有数据无关性和特征向量强不相干性，成功应用于回归设置中的全局收敛证明和状态演化参数估计。

Conclusion: 该理论提供了分析非凸梯度下降长期行为的通用框架，揭示了隐式正则化效应的普遍性，为超参数调优和运行时间确定等实际任务提供了低成本统计有效工具。

Abstract: This paper develops a general approach to characterize the long-time
trajectory behavior of nonconvex gradient descent in generalized single-index
models in the large aspect ratio regime. In this regime, we show that for each
iteration the gradient descent iterate concentrates around a deterministic
vector called the `Gaussian theoretical gradient descent', whose dynamics can
be tracked by a state evolution system of two recursive equations for two
scalars. Our concentration guarantees hold universally for a broad class of
design matrices and remain valid over long time horizons until algorithmic
convergence or divergence occurs. Moreover, our approach reveals that gradient
descent iterates are in general approximately independent of the data and
strongly incoherent with the feature vectors, a phenomenon previously known as
the `implicit regularization' effect of gradient descent in specific models
under Gaussian data.
  As an illustration of the utility of our general theory, we present two
applications of different natures in the regression setting. In the first, we
prove global convergence of nonconvex gradient descent with general independent
initialization for a broad class of structured link functions, and establish
universality of randomly initialized gradient descent in phase retrieval for
large aspect ratios. In the second, we develop a data-free iterative algorithm
for estimating state evolution parameters along the entire gradient descent
trajectory, thereby providing a low-cost yet statistically valid tool for
practical tasks such as hyperparameter tuning and runtime determination.
  As a by-product of our analysis, we show that in the large aspect ratio
regime, the Gaussian theoretical gradient descent coincides with a recent line
of dynamical mean-field theory for gradient descent over the constant-time
horizon.

</details>


### [204] [Tabular Data with Class Imbalance: Predicting Electric Vehicle Crash Severity with Pretrained Transformers (TabPFN) and Mamba-Based Models](https://arxiv.org/abs/2509.11449)
*Shriyank Somvanshi,Pavan Hebli,Gaurab Chhetri,Subasish Das*

Main category: cs.LG

TL;DR: 基于深度学习框架分析电动汽车碰撞严重程度预测，使用SMOTEENN处理数据不平衡，比较三种深度表格模型性能，发现MambaAttention在严重伤害分类中表现最佳


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车普及，需要更精确的碰撞严重程度预测方法来指导安全干预措施，传统方法在处理表格数据和类别不平衡方面存在局限

Method: 使用2017-2023年德克萨斯州真实碰撞数据，筛选23,301条电动汽车碰撞记录，应用XGBoost和随机森林进行特征重要性分析，采用SMOTEENN处理类别不平衡，并对比TabPFN、MambaNet和MambaAttention三种深度表格模型

Result: 识别出交叉口关系、首次有害事件、人员年龄、碰撞速度限制和工作日等关键预测因子，MambaAttention模型在严重伤害分类方面表现最优，得益于其基于注意力的特征重加权机制

Conclusion: 深度表格架构在电动汽车碰撞严重程度预测方面具有显著潜力，能够为数据驱动的安全干预提供支持，MambaAttention模型特别适用于严重伤害案例的分类

Abstract: This study presents a deep tabular learning framework for predicting crash
severity in electric vehicle (EV) collisions using real-world crash data from
Texas (2017-2023). After filtering for electric-only vehicles, 23,301
EV-involved crash records were analyzed. Feature importance techniques using
XGBoost and Random Forest identified intersection relation, first harmful
event, person age, crash speed limit, and day of week as the top predictors,
along with advanced safety features like automatic emergency braking. To
address class imbalance, Synthetic Minority Over-sampling Technique and Edited
Nearest Neighbors (SMOTEENN) resampling was applied. Three state-of-the-art
deep tabular models, TabPFN, MambaNet, and MambaAttention, were benchmarked for
severity prediction. While TabPFN demonstrated strong generalization,
MambaAttention achieved superior performance in classifying severe injury cases
due to its attention-based feature reweighting. The findings highlight the
potential of deep tabular architectures for improving crash severity prediction
and enabling data-driven safety interventions in EV crash contexts.

</details>


### [205] [Drug Repurposing Using Deep Embedded Clustering and Graph Neural Networks](https://arxiv.org/abs/2509.11493)
*Luke Delzer,Robert Kroleski,Ali K. AlShami,Jugal Kalita*

Main category: cs.LG

TL;DR: 提出了一种结合无监督深度嵌入聚类和图神经网络链接预测的机器学习流程，用于从多组学数据中发现新的药物-疾病关联，取得了高精度的预测性能。


<details>
  <summary>Details</summary>
Motivation: 药物重定位传统上经济成本高昂，现有研究多依赖简化的已知药物-疾病相似性数据集，需要更先进的方法来发现复杂的生化机制和新的药物-疾病关联。

Method: 使用无监督自编码器和聚类训练将组学数据降维到压缩的潜在嵌入空间，将9022种药物分成35个聚类，然后结合监督图神经网络进行链接预测。

Result: 图神经网络表现出色：预测准确率0.901，ROC AUC 0.960，F1分数0.901。生成了477个概率超过99%的跨疾病域新药物-疾病链接候选列表。

Conclusion: 该方法能够发现跨不相关疾病领域的新药物-疾病关联前景，同时推进了机器学习在药物重定位研究中的应用理解。

Abstract: Drug repurposing has historically been an economically infeasible process for
identifying novel uses for abandoned drugs. Modern machine learning has enabled
the identification of complex biochemical intricacies in candidate drugs;
however, many studies rely on simplified datasets with known drug-disease
similarities. We propose a machine learning pipeline that uses unsupervised
deep embedded clustering, combined with supervised graph neural network link
prediction to identify new drug-disease links from multi-omic data.
Unsupervised autoencoder and cluster training reduced the dimensionality of
omic data into a compressed latent embedding. A total of 9,022 unique drugs
were partitioned into 35 clusters with a mean silhouette score of 0.8550. Graph
neural networks achieved strong statistical performance, with a prediction
accuracy of 0.901, receiver operating characteristic area under the curve of
0.960, and F1-Score of 0.901. A ranked list comprised of 477 per-cluster link
probabilities exceeding 99 percent was generated. This study could provide new
drug-disease link prospects across unrelated disease domains, while advancing
the understanding of machine learning in drug repurposing studies.

</details>


### [206] [OASIS: A Deep Learning Framework for Universal Spectroscopic Analysis Driven by Novel Loss Functions](https://arxiv.org/abs/2509.11499)
*Chris Young,Juejing Liu,Marie L. Mortensen,Yifu Feng,Elizabeth Li,Zheming Wang,Xiaofeng Guo,Kevin M. Rosso,Xin Zhang*

Main category: cs.LG

TL;DR: OASIS是一个机器学习框架，用于自动化光谱分析，包括去噪、基线校正和峰值参数提取，无需人工干预。


<details>
  <summary>Details</summary>
Motivation: 随着光谱数据在各个科学和工程领域的激增，需要自动化处理方法来提高效率和准确性。

Method: 使用战略性设计的合成数据集训练模型，开发了创新的任务特定损失函数（如ViPeR用于峰值定位），创建紧凑而高精度的模型。

Result: 在拉曼、紫外可见和荧光光谱实验数据上验证了OASIS的性能，展示了其在原位实验、高通量优化和在线监测中的应用潜力。

Conclusion: 优化损失函数是开发高性能机器学习模型的关键资源高效策略，OASIS框架具有广泛的应用前景。

Abstract: The proliferation of spectroscopic data across various scientific and
engineering fields necessitates automated processing. We introduce OASIS
(Omni-purpose Analysis of Spectra via Intelligent Systems), a machine learning
(ML) framework for technique-independent, automated spectral analysis,
encompassing denoising, baseline correction, and comprehensive peak parameter
(location, intensity, FWHM) retrieval without human intervention. OASIS
achieves its versatility through models trained on a strategically designed
synthetic dataset incorporating features from numerous spectroscopy techniques.
Critically, the development of innovative, task-specific loss functions-such as
the vicinity peak response (ViPeR) for peak localization-enabled the creation
of compact yet highly accurate models from this dataset, validated with
experimental data from Raman, UV-vis, and fluorescence spectroscopy. OASIS
demonstrates significant potential for applications including in situ
experiments, high-throughput optimization, and online monitoring. This study
underscores the optimization of the loss function as a key resource-efficient
strategy to develop high-performance ML models.

</details>


### [207] [Know What You Don't Know: Selective Prediction for Early Exit DNNs](https://arxiv.org/abs/2509.11520)
*Divya Jyoti Bajpai,Manjesh Kumar Hanawal*

Main category: cs.LG

TL;DR: SPEED方法通过选择性预测和延迟分类器改进早期退出DNN，在保持低延迟的同时提高可信度，减少错误预测风险50%，速度提升2.05倍


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络在关键应用中存在的推理延迟和可信度问题，特别是早期退出策略因模型过度自信导致的不可靠预测问题

Method: 提出SPEED方法，在每层使用延迟分类器检查样本难度，识别难以预测的样本并将其推迟到专家层处理，结合选择性预测和早期退出策略

Result: 方法将错误预测风险降低50%，推理速度相比最终层提升2.05倍，同时提高了准确性和延迟性能

Conclusion: 选择性预测辅助的早期退出策略能有效平衡推理效率和预测可信度，为关键应用中的DNN部署提供了可行的解决方案

Abstract: Inference latency and trustworthiness of Deep Neural Networks (DNNs) are the
bottlenecks in deploying them in critical applications like sensitive tasks.
Early Exit (EE) DNNs overcome the latency issues by allowing samples to exit
from intermediary layers if they attain `high' confidence scores on the
predicted class. However, the DNNs are known to exhibit overconfidence, which
can lead to many samples exiting early and render EE strategies untrustworthy.
We use Selective Prediction (SP) to overcome this issue by checking the
`hardness' of the samples rather than just relying on the confidence score
alone. We propose SPEED, a novel approach that uses Deferral Classifiers (DCs)
at each layer to check the hardness of samples before performing EEs.
Specifically, the DCs identify if a sample is hard to predict at an
intermediary layer, leading to hallucination, and defer it to an expert. Early
detection of hard samples for inference prevents the wastage of computational
resources and improves trust by deferring the hard samples to the expert. We
demonstrate that EE aided with SP improves both accuracy and latency. Our
method minimizes the risk of wrong prediction by $50\%$ with a speedup of
$2.05\times$ as compared to the final layer. The anonymized source code is
available at https://github.com/Div290/SPEED

</details>


### [208] [DARD: Dice Adversarial Robustness Distillation against Adversarial Attacks](https://arxiv.org/abs/2509.11525)
*Jing Zou,Shungeng Zhang,Meikang Qiu,Chong Li*

Main category: cs.LG

TL;DR: DARD方法通过知识蒸馏将大模型的鲁棒性转移到小模型，在保持标准精度的同时显著提升对抗鲁棒性


<details>
  <summary>Details</summary>
Motivation: 深度学习模型易受对抗样本攻击，传统对抗训练会降低自然数据性能，而大模型展现更好的鲁棒性，需要将其转移到小模型

Method: 提出DARD对抗鲁棒性蒸馏方法，通过定制化的知识蒸馏范式转移鲁棒性；同时提出DPGD对抗样本生成方法进行有效攻击

Result: DARD方法在相同架构下 consistently优于对抗训练网络，实现了更优的鲁棒性和标准精度

Conclusion: 通过知识蒸馏可以系统性地将大模型的鲁棒性转移到紧凑学生模型，DARD方法为提升模型安全性和实用性提供了有效解决方案

Abstract: Deep learning models are vulnerable to adversarial examples, posing critical
security challenges in real-world applications. While Adversarial Training (AT
) is a widely adopted defense mechanism to enhance robustness, it often incurs
a trade-off by degrading performance on unperturbed, natural data. Recent
efforts have highlighted that larger models exhibit enhanced robustness over
their smaller counterparts. In this paper, we empirically demonstrate that such
robustness can be systematically distilled from large teacher models into
compact student models. To achieve better performance, we introduce Dice
Adversarial Robustness Distillation (DARD), a novel method designed to transfer
robustness through a tailored knowledge distillation paradigm. Additionally, we
propose Dice Projected Gradient Descent (DPGD), an adversarial example
generalization method optimized for effective attack. Our extensive experiments
demonstrate that the DARD approach consistently outperforms adversarially
trained networks with the same architecture, achieving superior robustness and
standard accuracy.

</details>


### [209] [UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning](https://arxiv.org/abs/2509.11543)
*Zhengxi Lu,Jiabo Ye,Fei Tang,Yongliang Shen,Haiyang Xu,Ziwei Zheng,Weiming Lu,Ming Yan,Fei Huang,Jun Xiao,Yueting Zhuang*

Main category: cs.LG

TL;DR: 提出半在线强化学习新范式，通过离线轨迹模拟在线RL，解决GUI智能体在离线训练和在线执行间的困境，在多个基准测试中实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 当前GUI智能体面临离线RL缺乏轨迹级奖励信号、在线RL奖励稀疏且部署成本高的根本困境，需要一种能兼顾训练效率和在线推理能力的方法

Method: 半在线强化学习：在离线轨迹上模拟在线RL，使用补丁模块自适应恢复rollout与专家轨迹的差异，引入折扣未来回报计算奖励，并用加权步级和回合级优势优化策略

Result: 在四个动态基准测试中，7B模型达到SOTA性能，相比基础模型显著提升（AndroidWorld +12.0%，AITW +23.8%）

Conclusion: 半在线RL有效弥合了离线训练效率和在线多轮推理之间的差距，为GUI智能体提供了实用的训练范式

Abstract: Graphical User Interface (GUI) agents have demonstrated remarkable progress
in automating complex user interface interactions through reinforcement
learning. However, current approaches face a fundamental dilemma: offline RL
enables stable training on pre-collected trajectories, but struggles with
multi-step task execution for lack of trajectory-level reward signals; online
RL captures these signals through environment interaction, but suffers from
sparse rewards and prohibitive deployment costs. To address it, we present
Semi-online Reinforcement Learning, a novel paradigm that simulates online RL
on offline trajectories. During each rollout process, we preserve the original
model output within the multi-turn dialogue, where a Patch Module adaptively
recovers the divergence between rollout and expert trajectories. To capture
long-term training signals, Semi-online RL introduces discounted future returns
into the reward computation and optimizes the policy with weighted step-level
and episode-level advantages. We further introduce Semi-Online Performance
(SOP), a metric that aligns better with true online performance, serving as a
practical and effective proxy for real-world evaluation. Experiments show that
ours Semi-online RL achieves SOTA performance among 7B models across four
dynamic benchmarks, with significant gains over the base model (e.g., +12.0% on
AndroidWorld, +23.8% on AITW), demonstrating significant progress in bridging
the gap between offline training efficiency and online multi-turn reasoning.
The code is available at https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1.

</details>


### [210] [Compressed Sensing: Mathematical Foundations, Implementation, and Advanced Optimization Techniques](https://arxiv.org/abs/2509.11550)
*Shane Stevenson,Maryam Sabagh*

Main category: cs.LG

TL;DR: 压缩感知是一种信号处理技术，可以从少量测量中重建信号，利用信号在变换域中的稀疏性特性。


<details>
  <summary>Details</summary>
Motivation: 探索压缩感知的数学原理、逻辑和病理特性，并将其应用于实际信号处理中。

Method: 研究压缩感知的数学公式化表达，分析其理论基础和潜在问题。

Result: 建立了压缩感知的理论框架，验证了从少量测量中重建稀疏信号的可能性。

Conclusion: 压缩感知为信号处理提供了新的范式，能够有效减少数据采集需求，在多个领域具有应用潜力。

Abstract: Compressed sensing is a signal processing technique that allows for the
reconstruction of a signal from a small set of measurements. The key idea
behind compressed sensing is that many real-world signals are inherently
sparse, meaning that they can be efficiently represented in a different space
with only a few components compared to their original space representation. In
this paper we will explore the mathematical formulation behind compressed
sensing, its logic and pathologies, and apply compressed sensing to real world
signals.

</details>


### [211] [Dynamic Adaptive Parsing of Temporal and Cross-Variable Patterns for Network State Classification](https://arxiv.org/abs/2509.11601)
*Yuan Gao,Xuelong Wang,Zhenguo Dong,Yong Zhang*

Main category: cs.LG

TL;DR: DAPNet是一个基于专家混合架构的网络状态分类框架，通过整合周期性分析、动态变量相关性建模和混合时序特征提取三个专家网络，解决了现有方法无法同时捕捉时序模式和变量依赖关系的问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在网络状态分类中存在关键权衡：专注于时序模式的方法忽略变量依赖关系，而专注于依赖关系的方法无法捕捉细粒度时序细节。需要一种能够同时处理这两种特征的解决方案。

Method: 提出DAPNet框架，采用专家混合架构，包含三个专门网络：周期性分析专家、动态跨变量相关性建模专家、混合时序特征提取专家。通过可学习的门控网络动态分配权重，并使用混合正则化损失函数确保训练稳定性。

Result: 在两个大规模网络入侵检测数据集（CICIDS2017/2018）上的广泛实验验证了DAPNet在目标应用中具有更高的准确性。在十个公共UEA基准数据集上的评估证明了架构设计的通用性。

Conclusion: DAPNet是一个专门用于网络状态分类的专业框架，成功解决了同时捕捉时序周期性和变量依赖关系的挑战，在网络安全和性能优化方面具有重要应用价值。

Abstract: Effective network state classification is a primary task for ensuring network
security and optimizing performance. Existing deep learning models have shown
considerable progress in this area. Some methods excel at analyzing the complex
temporal periodicities found in traffic data, while graph-based approaches are
adept at modeling the dynamic dependencies between different variables.
However, a key trade-off remains, as these methods struggle to capture both
characteristics simultaneously. Models focused on temporal patterns often
overlook crucial variable dependencies, whereas those centered on dependencies
may fail to capture fine-grained temporal details. To address this trade-off,
we introduce DAPNet, a framework based on a Mixture-of-Experts architecture.
DAPNet integrates three specialized networks for periodic analysis, dynamic
cross-variable correlation modeling, and hybrid temporal feature extraction. A
learnable gating network dynamically assigns weights to experts based on the
input sample and computes a weighted fusion of their outputs. Furthermore, a
hybrid regularization loss function ensures stable training and addresses the
common issue of class imbalance. Extensive experiments on two large-scale
network intrusion detection datasets (CICIDS2017/2018) validate DAPNet's higher
accuracy for its target application. The generalizability of the architectural
design is evaluated across ten public UEA benchmark datasets, positioning
DAPNet as a specialized framework for network state classification.

</details>


### [212] [Topology Structure Optimization of Reservoirs Using GLMY Homology](https://arxiv.org/abs/2509.11612)
*Yu Chen,Shengwei Wang,Hongwei Lin*

Main category: cs.LG

TL;DR: 使用持久GLMY同调理论分析储层计算网络的拓扑结构，发现性能与一维GLMY同调群密切相关，并开发了通过修改一维同调群最小代表环来优化储层结构的方法。


<details>
  <summary>Details</summary>
Motivation: 储层计算网络在时间序列处理中很高效，但其拓扑结构和性能之间的关系难以分析，缺乏合适的数学工具来研究。

Method: 采用持久GLMY同调理论分析储层拓扑结构，通过修改一维GLMY同调群的最小代表环来优化储层结构。

Result: 实验验证储层性能受网络结构和数据集周期性的共同影响，优化后的储层结构性能得到提升。

Conclusion: GLMY同调理论为分析储层拓扑结构提供了有效工具，一维同调群与性能密切相关，结构优化方法能有效提升储层性能。

Abstract: Reservoir is an efficient network for time series processing. It is well
known that network structure is one of the determinants of its performance.
However, the topology structure of reservoirs, as well as their performance, is
hard to analyzed, due to the lack of suitable mathematical tools. In this
paper, we study the topology structure of reservoirs using persistent GLMY
homology theory, and develop a method to improve its performance. Specifically,
it is found that the reservoir performance is closely related to the
one-dimensional GLMY homology groups. Then, we develop a reservoir structure
optimization method by modifying the minimal representative cycles of
one-dimensional GLMY homology groups. Finally, by experiments, it is validated
that the performance of reservoirs is jointly influenced by the reservoir
structure and the periodicity of the dataset.

</details>


### [213] [Inducing Uncertainty for Test-Time Privacy](https://arxiv.org/abs/2509.11625)
*Muhammad H. Ashiq,Peter Triantafillou,Hung Yun Tseng,Grigoris G. Chrysos*

Main category: cs.LG

TL;DR: 论文提出了一种针对机器学习模型遗忘后仍存在测试时隐私风险的新算法，通过在模型权重中添加扰动来最大化保护实例的不确定性，同时保持其他实例的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的遗忘方法虽然能移除数据对模型的影响，但模型仍会对已遗忘数据产生高置信度预测，这可能导致攻击者利用错误或过时数据危害用户，存在测试时隐私风险。

Method: 提出基于帕累托最优目标的微调算法，平衡测试时隐私与模型效用；同时提供可证明的近似算法，在非凸假设下实现(ε, δ)隐私保证。

Result: 实验表明该方法在各种图像识别基准上实现了>3倍的不确定性增强，同时准确率下降<0.2%。

Conclusion: 该框架为终端用户提供了额外的保护工具，证明了隐私-效用权衡的紧致非空界，有效解决了遗忘方法未能保护的测试时隐私威胁。

Abstract: Unlearning is the predominant method for removing the influence of data in
machine learning models. However, even after unlearning, models often continue
to produce the same predictions on the unlearned data with high confidence.
This persistent behavior can be exploited by adversaries using confident model
predictions on incorrect or obsolete data to harm users. We call this threat
model, which unlearning fails to protect against, *test-time privacy*. In
particular, an adversary with full model access can bypass any naive defenses
which ensure test-time privacy. To address this threat, we introduce an
algorithm which perturbs model weights to induce maximal uncertainty on
protected instances while preserving accuracy on the rest of the instances. Our
core algorithm is based on finetuning with a Pareto optimal objective that
explicitly balances test-time privacy against utility. We also provide a
certifiable approximation algorithm which achieves $(\varepsilon, \delta)$
guarantees without convexity assumptions. We then prove a tight, non-vacuous
bound that characterizes the privacy-utility tradeoff that our algorithms
incur. Empirically, our method obtains $>3\times$ stronger uncertainty than
pretraining with $<0.2\%$ drops in accuracy on various image recognition
benchmarks. Altogether, this framework provides a tool to guarantee additional
protection to end users.

</details>


### [214] [SpeCa: Accelerating Diffusion Transformers with Speculative Feature Caching](https://arxiv.org/abs/2509.11628)
*Jiacheng Liu,Chang Zou,Yuanhuiyi Lyu,Fei Ren,Shaobo Wang,Kaixin Li,Linfeng Zhang*

Main category: cs.LG

TL;DR: SpeCa是一个基于推测采样的扩散模型加速框架，通过预测-验证机制和自适应计算分配，实现了6-7倍的加速效果，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然能生成高质量图像和视频，但计算需求巨大，存在严格的时间依赖性和计算密集型前向传递问题，难以实现实时应用。

Method: 引入推测采样到扩散模型，基于完全计算的参考时间步预测后续时间步的中间特征；采用参数无关的验证机制评估预测可靠性；实现样本自适应计算分配，根据生成复杂度动态调整资源。

Result: 在FLUX上实现6.34倍加速（质量下降仅5.5%），DiT上7.3倍加速（保持生成保真度），HunyuanVideo上6.1倍加速时VBench得分79.84%。验证机制开销仅占完整推理成本的1.67%-3.5%。

Conclusion: SpeCa为扩散模型推理建立了新的高效范式，即使在激进的加速比下也能保持生成质量，解决了扩散模型实时应用的关键瓶颈。

Abstract: Diffusion models have revolutionized high-fidelity image and video synthesis,
yet their computational demands remain prohibitive for real-time applications.
These models face two fundamental challenges: strict temporal dependencies
preventing parallelization, and computationally intensive forward passes
required at each denoising step. Drawing inspiration from speculative decoding
in large language models, we present SpeCa, a novel 'Forecast-then-verify'
acceleration framework that effectively addresses both limitations. SpeCa's
core innovation lies in introducing Speculative Sampling to diffusion models,
predicting intermediate features for subsequent timesteps based on fully
computed reference timesteps. Our approach implements a parameter-free
verification mechanism that efficiently evaluates prediction reliability,
enabling real-time decisions to accept or reject each prediction while
incurring negligible computational overhead. Furthermore, SpeCa introduces
sample-adaptive computation allocation that dynamically modulates resources
based on generation complexity, allocating reduced computation for simpler
samples while preserving intensive processing for complex instances.
Experiments demonstrate 6.34x acceleration on FLUX with minimal quality
degradation (5.5% drop), 7.3x speedup on DiT while preserving generation
fidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The
verification mechanism incurs minimal overhead (1.67%-3.5% of full inference
costs), establishing a new paradigm for efficient diffusion model inference
while maintaining generation quality even at aggressive acceleration ratios.
Our codes have been released in Github:
\textbf{https://github.com/Shenyi-Z/Cache4Diffusion}

</details>


### [215] [Reasoned Safety Alignment: Ensuring Jailbreak Defense via Answer-Then-Check](https://arxiv.org/abs/2509.11629)
*Chentao Cao,Xiaojun Xu,Bo Han,Hang Li*

Main category: cs.LG

TL;DR: 提出Answer-Then-Check安全对齐方法，通过先思考回答再安全检查来增强LLM对越狱攻击的鲁棒性，在保持推理能力的同时显著提升安全性并降低过度拒绝率。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力不断增强，确保其对抗越狱攻击的安全性仍然是一个关键挑战。现有方法存在过度拒绝合法查询或无法提供安全替代回答的问题。

Method: 构建包含8万样本的Reasoned Safety Alignment (ReSA)数据集，训练模型先直接回答问题，然后批判性评估其安全性，再决定是否提供最终答案。

Result: 方法在安全性和实用性方面达到帕累托前沿，在MMLU、MATH500和HumanEval等基准测试中保持通用推理能力，并能提供安全替代回答而非简单拒绝。仅需500个样本即可达到与完整数据集相当的性能。

Conclusion: Answer-Then-Check方法有效提升了LLM的安全性，同时保持了模型的有用性，证明安全对齐可能比先前假设需要更少的数据量。

Abstract: As large language models (LLMs) continue to advance in capabilities, ensuring
their safety against jailbreak attacks remains a critical challenge. In this
paper, we introduce a novel safety alignment approach called Answer-Then-Check,
which enhances LLM robustness against malicious prompts by applying thinking
ability to mitigate jailbreaking problems before producing a final answer to
the user. Our method enables models to directly answer the question in their
thought and then critically evaluate its safety before deciding whether to
provide it. To implement this approach, we construct the Reasoned Safety
Alignment (ReSA) dataset, comprising 80K examples that teach models to reason
through direct responses and then analyze their safety. Experimental results
demonstrate that our approach achieves the Pareto frontier with superior safety
capability while decreasing over-refusal rates on over-refusal benchmarks.
Notably, the model fine-tuned with ReSA maintains general reasoning
capabilities on benchmarks like MMLU, MATH500, and HumanEval. Besides, our
method equips models with the ability to perform safe completion. Unlike
post-hoc methods that can only reject harmful queries, our model can provide
helpful and safe alternative responses for sensitive topics (e.g., self-harm).
Furthermore, we discover that training on a small subset of just 500 examples
can achieve comparable performance to using the full dataset, suggesting that
safety alignment may require less data than previously assumed.

</details>


### [216] [Adaptive-GraphSketch: Real-Time Edge Anomaly Detection via Multi-Layer Tensor Sketching and Temporal Decay](https://arxiv.org/abs/2509.11633)
*Ocheme Anthony Ekle,William Eberle*

Main category: cs.LG

TL;DR: ADAPTIVE-GRAPHSKETCH是一个轻量级可扩展框架，用于流式边数据中的实时异常检测，通过时间多张量素描和CMS-CU技术实现高效内存使用和概率异常评分，在多个真实数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 动态图中的异常检测对于识别恶意活动、欺诈和意外行为至关重要，但现有方法在可扩展性、概率可解释性和对演化流量模式的适应性方面存在困难。

Method: 集成时间多张量素描与Count-Min Sketch（CMS-CU）来紧凑跟踪边频率模式，使用贝叶斯推断进行概率异常评分，并应用指数加权移动平均（EWMA）进行自适应阈值调整。

Result: 在四个真实入侵检测数据集上，ADAPTIVE-GRAPHSKETCH优于ANOEDGE-G/L、MIDAS-R和F-FADE等基线方法，在CIC-IDS2018上获得6.5% AUC增益，在CIC-DDoS2019上获得15.6%增益，仅用10个哈希函数在3.4秒内处理2000万条边。

Conclusion: ADAPTIVE-GRAPHSKETCH是大规模流式图中快速准确异常检测的实用有效方法，具有优秀的性能和效率。

Abstract: Anomaly detection in dynamic graphs is essential for identifying malicious
activities, fraud, and unexpected behaviors in real-world systems such as
cybersecurity and power grids. However, existing approaches struggle with
scalability, probabilistic interpretability, and adaptability to evolving
traffic patterns. In this paper, we propose ADAPTIVE-GRAPHSKETCH, a lightweight
and scalable framework for real-time anomaly detection in streaming edge data.
Our method integrates temporal multi-tensor sketching with Count-Min Sketch
using Conservative Update (CMS-CU) to compactly track edge frequency patterns
with bounded memory, while mitigating hash collision issues. We incorporate
Bayesian inference for probabilistic anomaly scoring and apply Exponentially
Weighted Moving Average (EWMA) for adaptive thresholding tuned to burst
intensity. Extensive experiments on four real-world intrusion detection
datasets demonstrate that ADAPTIVE-GRAPHSKETCH outperforms state-of-the-art
baselines such as ANOEDGE-G/L, MIDAS-R, and F-FADE, achieving up to 6.5% AUC
gain on CIC-IDS2018 and up to 15.6% on CIC-DDoS2019, while processing 20
million edges in under 3.4 seconds using only 10 hash functions. Our results
show that ADAPTIVE-GRAPHSKETCH is practical and effective for fast, accurate
anomaly detection in large-scale streaming graphs.
  Keywords: Anomaly Detection, Streaming, Real-time, Dynamic Graphs, Edge
Streams, Tensor Sketching

</details>


### [217] [Assessing On-the-Ground Disaster Impact Using Online Data Sources](https://arxiv.org/abs/2509.11634)
*Saketh Vishnubhatla,Ujun Jeong,Bohan Jiang,Paras Sheth,Zhen Tan,Adrienne Raglin,Huan Liu*

Main category: cs.LG

TL;DR: 本文研究了如何利用多种在线数据源（社交媒体、新闻报道、航空影像和卫星数据）来实时评估灾害影响，并与传统线下评估方法进行比较，发现不同数据源可以提供互补信息。


<details>
  <summary>Details</summary>
Motivation: 传统灾害影响评估方法存在时间延迟和偏差问题，需要探索在线数据源在实时灾害影响评估中的潜力和有效性。

Method: 收集多个十亿美元级别灾害的县级数据，从多种在线数据源获取信息，并与传统线下评估结果进行对比分析。

Result: 研究发现不同在线数据源能够提供互补信息，有助于更全面地评估灾害影响。

Conclusion: 在线数据源可以作为传统灾害评估方法的有效补充，提供实时、多角度的灾害影响信息，有助于制定更有效的应急响应计划。

Abstract: Assessing the impact of a disaster in terms of asset losses and human
casualties is essential for preparing effective response plans. Traditional
methods include offline assessments conducted on the ground, where volunteers
and first responders work together to collect the estimate of losses through
windshield surveys or on-ground inspection. However, these methods have a time
delay and are prone to different biases. Recently, various online data sources,
including social media, news reports, aerial imagery, and satellite data, have
been utilized to evaluate the impact of disasters. Online data sources provide
real-time data streams for estimating the offline impact. Limited research
exists on how different online sources help estimate disaster impact at a given
administrative unit. In our work, we curate a comprehensive dataset by
collecting data from multiple online sources for a few billion-dollar disasters
at the county level. We also analyze how online estimates compare with
traditional offline-based impact estimates for the disaster. Our findings
provide insight into how different sources can provide complementary
information to assess the disaster.

</details>


### [218] [An Interventional Approach to Real-Time Disaster Assessment via Causal Attribution](https://arxiv.org/abs/2509.11676)
*Saketh Vishnubhatla,Alimohammad Beigi,Rui Heng Foo,Umang Goel,Ujun Jeong,Bohan Jiang,Adrienne Raglin,Huan Liu*

Main category: cs.LG

TL;DR: 本文提出了一种干预性灾害分析工具，能够模拟反事实情景并分析不同因素对灾害严重性的因果归因，补充传统预测性灾害建模工具的不足。


<details>
  <summary>Details</summary>
Motivation: 传统灾害分析工具基于历史观测数据进行预测，但无法支持用户修改输入状态来模拟反事实的"如果...会怎样"情景，缺乏干预性和因果分析能力。

Method: 利用卫星图像、新闻和社交媒体等实时数据源，开发干预性工具来模拟不同输入状态下的灾害严重性，并提供因果归因分析和可操作的缓解建议。

Result: 开发了公开可用的源代码工具，能够进行反事实情景模拟，理解不同因素对灾害严重性的因果影响，并生成易于缓解规划的行动建议。

Conclusion: 该干预性工具有效补充了传统灾害建模方法，通过实时数据支持和因果分析能力，为灾害管理和缓解规划提供了更强大的决策支持。

Abstract: Traditional disaster analysis and modelling tools for assessing the severity
of a disaster are predictive in nature. Based on the past observational data,
these tools prescribe how the current input state (e.g., environmental
conditions, situation reports) results in a severity assessment. However, these
systems are not meant to be interventional in the causal sense, where the user
can modify the current input state to simulate counterfactual "what-if"
scenarios. In this work, we provide an alternative interventional tool that
complements traditional disaster modelling tools by leveraging real-time data
sources like satellite imagery, news, and social media. Our tool also helps
understand the causal attribution of different factors on the estimated
severity, over any given region of interest. In addition, we provide actionable
recourses that would enable easier mitigation planning. Our source code is
publicly available.

</details>


### [219] [Beyond Regularity: Modeling Chaotic Mobility Patterns for Next Location Prediction](https://arxiv.org/abs/2509.11713)
*Yuqian Wu,Yuhong Peng,Jiapeng Yu,Xiangyu Liu,Zeting Yan,Kang Lin,Weifeng Su,Bingqing Qu,Raymond Lee,Dingqi Yang*

Main category: cs.LG

TL;DR: CANOE是一个用于下一个位置预测的混沌神经振荡器网络，通过生物启发的注意力机制和多重上下文融合，有效解决了移动模式动态不平衡和上下文信息利用不足的问题，在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效处理周期性和混沌移动模式之间的动态不平衡，且在稀疏轨迹上适应性不足；同时未能充分利用时间规律等上下文线索，这些线索即使在混沌模式中也具有比空间预测更强的可预测性。

Method: 提出了CANOE模型，包含生物启发的混沌神经振荡注意力机制来注入自适应变异性，使用Tri-Pair Interaction Encoder和Cross Context Attentive Decoder来融合"谁-何时-何地"的多模态上下文信息。

Result: 在两个真实数据集上的实验表明，CANOE相比最佳基线方法有3.17%-13.11%的性能提升，能够对不同混沌程度的移动轨迹进行鲁棒预测，消融研究也验证了关键设计选择的有效性。

Conclusion: CANOE通过创新的注意力机制和上下文融合方法，有效解决了移动预测中的动态不平衡和上下文利用问题，为下一个位置预测任务提供了强有力的解决方案。

Abstract: Next location prediction is a key task in human mobility analysis, crucial
for applications like smart city resource allocation and personalized
navigation services. However, existing methods face two significant challenges:
first, they fail to address the dynamic imbalance between periodic and chaotic
mobile patterns, leading to inadequate adaptation over sparse trajectories;
second, they underutilize contextual cues, such as temporal regularities in
arrival times, which persist even in chaotic patterns and offer stronger
predictability than spatial forecasts due to reduced search spaces. To tackle
these challenges, we propose \textbf{\method}, a
\underline{\textbf{C}}h\underline{\textbf{A}}otic \underline{\textbf{N}}eural
\underline{\textbf{O}}scillator n\underline{\textbf{E}}twork for next location
prediction, which introduces a biologically inspired Chaotic Neural Oscillatory
Attention mechanism to inject adaptive variability into traditional attention,
enabling balanced representation of evolving mobility behaviors, and employs a
Tri-Pair Interaction Encoder along with a Cross Context Attentive Decoder to
fuse multimodal ``who-when-where'' contexts in a joint framework for enhanced
prediction performance. Extensive experiments on two real-world datasets
demonstrate that CANOE consistently and significantly outperforms a sizeable
collection of state-of-the-art baselines, yielding 3.17\%-13.11\% improvement
over the best-performing baselines across different cases. In particular, CANOE
can make robust predictions over mobility trajectories of different mobility
chaotic levels. A series of ablation studies also supports our key design
choices. Our code is available at: https://github.com/yuqian2003/CANOE.

</details>


### [220] [DRAG: Data Reconstruction Attack using Guided Diffusion](https://arxiv.org/abs/2509.11724)
*Wa-Kin Lei,Jun-Cheng Chen,Shang-Tse Chen*

Main category: cs.LG

TL;DR: 提出基于引导扩散的新型数据重建攻击方法，利用预训练的潜在扩散模型从视觉基础模型的中间表示中重建高保真图像，在分割推理场景中显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 随着大型基础模型的兴起，分割推理成为流行计算范式，但现有数据重建攻击主要针对小型CNN分类模型，大型基础模型在分割推理中的隐私风险尚未充分探索

Method: 基于引导扩散的数据重建攻击，利用在大规模数据集上预训练的潜在扩散模型（LDM）中嵌入的先验知识，在LDM学习的图像先验上进行迭代重建

Result: 大量实验表明，该方法在从视觉基础模型的深层中间表示重建数据方面，在定性和定量上都显著优于最先进的方法

Conclusion: 研究结果强调了在分割推理场景中为大型模型提供更强大隐私保护机制的紧迫性

Abstract: With the rise of large foundation models, split inference (SI) has emerged as
a popular computational paradigm for deploying models across lightweight edge
devices and cloud servers, addressing data privacy and computational cost
concerns. However, most existing data reconstruction attacks have focused on
smaller CNN classification models, leaving the privacy risks of foundation
models in SI settings largely unexplored. To address this gap, we propose a
novel data reconstruction attack based on guided diffusion, which leverages the
rich prior knowledge embedded in a latent diffusion model (LDM) pre-trained on
a large-scale dataset. Our method performs iterative reconstruction on the
LDM's learned image prior, effectively generating high-fidelity images
resembling the original data from their intermediate representations (IR).
Extensive experiments demonstrate that our approach significantly outperforms
state-of-the-art methods, both qualitatively and quantitatively, in
reconstructing data from deep-layer IRs of the vision foundation model. The
results highlight the urgent need for more robust privacy protection mechanisms
for large models in SI scenarios. Code is available at:
https://github.com/ntuaislab/DRAG.

</details>


### [221] [Fast and Interpretable Machine Learning Modelling of Atmospheric Molecular Clusters](https://arxiv.org/abs/2509.11728)
*Lauri Seppäläinen,Jakub Kubečka,Jonas Elm,Kai Puolamäki*

Main category: cs.LG

TL;DR: 本研究提出使用k近邻回归模型作为量子化学计算的快速替代方案，用于预测大气分子团簇的形成能，在保持化学精度的同时大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 大气分子团簇的形成是气候建模中的关键不确定性因素，量子化学计算虽然准确但计算成本高昂，需要开发快速且准确的替代方法。

Method: 采用k近邻回归模型，结合化学信息距离度量（包括核诱导度量和通过度量学习获得的度量），使用FCHL19等分子描述符，在QM9基准集和大气分子团簇数据集上进行测试。

Result: k-NN模型在准确性上可与复杂的核岭回归模型相媲美，计算时间减少数个数量级，能够处理超过25万个数据点的大规模数据集，对未见的大团簇外推预测误差接近1 kcal/mol。

Conclusion: k-NN回归模型具有内置可解释性和直接的不确定性估计能力，是加速大气化学及其他领域发现的强大工具。

Abstract: Understanding how atmospheric molecular clusters form and grow is key to
resolving one of the biggest uncertainties in climate modelling: the formation
of new aerosol particles. While quantum chemistry offers accurate insights into
these early-stage clusters, its steep computational costs limit large-scale
exploration. In this work, we present a fast, interpretable, and surprisingly
powerful alternative: $k$-nearest neighbour ($k$-NN) regression model. By
leveraging chemically informed distance metrics, including a kernel-induced
metric and one learned via metric learning for kernel regression (MLKR), we
show that simple $k$-NN models can rival more complex kernel ridge regression
(KRR) models in accuracy, while reducing computational time by orders of
magnitude. We perform this comparison with the well-established
Faber-Christensen-Huang-Lilienfeld (FCHL19) molecular descriptor, but other
descriptors (e.g., FCHL18, MBDF, and CM) can be shown to have similar
performance. Applied to both simple organic molecules in the QM9 benchmark set
and large datasets of atmospheric molecular clusters (sulphuric acid-water and
sulphuric-multibase -base systems), our $k$-NN models achieve near-chemical
accuracy, scale seamlessly to datasets with over 250,000 entries, and even
appears to extrapolate to larger unseen clusters with minimal error (often
nearing 1 kcal/mol). With built-in interpretability and straightforward
uncertainty estimation, this work positions $k$-NN as a potent tool for
accelerating discovery in atmospheric chemistry and beyond.

</details>


### [222] [Data Fusion and Machine Learning for Ship Fuel Consumption Modelling -- A Case of Bulk Carrier Vessel](https://arxiv.org/abs/2509.11750)
*Abdella Mohamed,Xiangyu Hu,Christian Hendricks*

Main category: cs.LG

TL;DR: 本研究利用机器学习技术结合船舶航行报告和外部气象海洋大数据，准确预测船舶燃油消耗，为IMO节能减排要求提供操作层面的解决方案。


<details>
  <summary>Details</summary>
Motivation: 国际海事组织(IMO)推动船舶节能减排的强制要求，需要开发有效的操作措施来降低船舶燃油消耗和碳排放，而准确的燃油消耗预测是实现这些策略的理论基础。

Method: 收集一艘散货船296份航行报告和28个参数，整合Copernicus海洋环境监测服务的19个参数和ECMWF的61个参数等水文气象大数据，使用机器学习技术建立燃油消耗预测模型。

Result: 结果显示机器学习技术能够通过结合航行报告与气候海洋数据准确预测船舶燃油消耗，外部公共数据源的融合显著提高了建模精度，并识别出了影响燃油消耗的最关键参数。

Conclusion: 机器学习方法在船舶燃油消耗预测方面具有强大潜力，但需要在同类船舶上进行验证以确认模型的普适性和可推广性。

Abstract: There is an increasing push for operational measures to reduce ships' bunker
fuel consumption and carbon emissions, driven by the International Maritime
Organization (IMO) mandates. Key performance indicators such as the Energy
Efficiency Operational Indicator (EEOI) focus on fuel efficiency. Strategies
like trim optimization, virtual arrival, and green routing have emerged. The
theoretical basis for these approaches lies in accurate prediction of fuel
consumption as a function of sailing speed, displacement, trim, climate, and
sea state. This study utilized 296 voyage reports from a bulk carrier vessel
over one year (November 16, 2021 to November 21, 2022) and 28 parameters,
integrating hydrometeorological big data from the Copernicus Marine Environment
Monitoring Service (CMEMS) with 19 parameters and the European Centre for
Medium-Range Weather Forecasts (ECMWF) with 61 parameters. The objective was to
evaluate whether fusing external public data sources enhances modeling accuracy
and to highlight the most influential parameters affecting fuel consumption.
The results reveal a strong potential for machine learning techniques to
predict ship fuel consumption accurately by combining voyage reports with
climate and sea data. However, validation on similar classes of vessels remains
necessary to confirm generalizability.

</details>


### [223] [Stabilizing PINNs: A regularization scheme for PINN training to avoid unstable fixed points of dynamical systems](https://arxiv.org/abs/2509.11768)
*Milos Babic,Franz M. Rohrhofer,Bernhard C. Geiger*

Main category: cs.LG

TL;DR: 该论文提出了一种正则化方案来解决物理信息神经网络(PINNs)训练中的局部最小值问题，通过惩罚不稳定固定点对应的解来提高训练成功率和避免物理不正确的解。


<details>
  <summary>Details</summary>
Motivation: 物理信息神经网络(PINNs)的损失函数在动力系统固定点处存在局部最小值，这会在前向设置中干扰训练并导致物理不正确的解。

Method: 基于稳定性理论，提出了一种正则化方案，惩罚对应于不稳定固定点的解。

Result: 在四个动力系统（包括Lotka-Volterra模型和van der Pol振荡器）上的实验结果表明，该方案有助于避免物理不正确的解，并显著提高了PINNs的训练成功率。

Conclusion: 所提出的正则化方案有效解决了PINNs训练中的局部最小值问题，提高了求解初始值问题的准确性和可靠性。

Abstract: It was recently shown that the loss function used for training
physics-informed neural networks (PINNs) exhibits local minima at solutions
corresponding to fixed points of dynamical systems. In the forward setting,
where the PINN is trained to solve initial value problems, these local minima
can interfere with training and potentially leading to physically incorrect
solutions. Building on stability theory, this paper proposes a regularization
scheme that penalizes solutions corresponding to unstable fixed points.
Experimental results on four dynamical systems, including the Lotka-Volterra
model and the van der Pol oscillator, show that our scheme helps avoiding
physically incorrect solutions and substantially improves the training success
rate of PINNs.

</details>


### [224] [Multimodal Regression for Enzyme Turnover Rates Prediction](https://arxiv.org/abs/2509.11782)
*Bozhen Hu,Cheng Tan,Siyuan Li,Jiangbin Zheng,Sizhe Qiu,Jun Xia,Stan Z. Li*

Main category: cs.LG

TL;DR: 提出了一种多模态框架，通过整合酶序列、底物结构和环境因素来预测酶周转率，结合预训练语言模型、CNN、GNN和注意力机制，并利用符号回归获得可解释的数学公式。


<details>
  <summary>Details</summary>
Motivation: 酶周转率是酶动力学的基本参数，但由于实验测量成本高且复杂，大多数生物体中酶周转率数据稀缺，需要开发预测方法来填补这一空白。

Method: 使用预训练语言模型和CNN提取蛋白质序列特征，GNN捕获底物分子表示，注意力机制增强酶-底物相互作用，并采用Kolmogorov-Arnold Networks进行符号回归学习数学公式。

Result: 大量实验表明该框架优于传统和最先进的深度学习方法，提供了准确且可解释的预测。

Conclusion: 该工作为研究酶动力学提供了强大工具，在酶工程、生物技术和工业生物催化领域具有应用前景。

Abstract: The enzyme turnover rate is a fundamental parameter in enzyme kinetics,
reflecting the catalytic efficiency of enzymes. However, enzyme turnover rates
remain scarce across most organisms due to the high cost and complexity of
experimental measurements. To address this gap, we propose a multimodal
framework for predicting the enzyme turnover rate by integrating enzyme
sequences, substrate structures, and environmental factors. Our model combines
a pre-trained language model and a convolutional neural network to extract
features from protein sequences, while a graph neural network captures
informative representations from substrate molecules. An attention mechanism is
incorporated to enhance interactions between enzyme and substrate
representations. Furthermore, we leverage symbolic regression via
Kolmogorov-Arnold Networks to explicitly learn mathematical formulas that
govern the enzyme turnover rate, enabling interpretable and accurate
predictions. Extensive experiments demonstrate that our framework outperforms
both traditional and state-of-the-art deep learning approaches. This work
provides a robust tool for studying enzyme kinetics and holds promise for
applications in enzyme engineering, biotechnology, and industrial biocatalysis.

</details>


### [225] [Watch Your Step: A Cost-Sensitive Framework for Accelerometer-Based Fall Detection in Real-World Streaming Scenarios](https://arxiv.org/abs/2509.11789)
*Timilehin B. Aderinola,Luca Palmerini,Ilaria D'Ascanio,Lorenzo Chiari,Jochen Klenk,Clemens Becker,Brian Caulfield,Georgiana Ifrim*

Main category: cs.LG

TL;DR: 提出了一种实时跌倒检测框架，使用成本敏感学习策略优化决策阈值，在真实世界跌倒数据集上实现了100%召回率和84%精度的优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有跌倒检测方法多依赖模拟数据或假设已知跌倒事件，限制了实际应用。需要开发无需先验知识、计算高效且适合连续监测的实时检测方法。

Method: 使用FARSEEING真实世界跌倒数据集的60多小时IMU数据，采用高效分类器进行流式处理，引入成本敏感学习策略调整决策阈值，平衡漏检和误报风险。

Result: 在FARSEEING数据集上实现召回率1.00、精度0.84、F1分数0.91，平均推理时间低于5毫秒/样本，检测所有跌倒同时保持低误报率。

Conclusion: 成本敏感阈值调优增强了基于加速度计的跌倒检测鲁棒性，计算高效的框架适合部署在实时可穿戴传感器系统中进行连续监测。

Abstract: Real-time fall detection is crucial for enabling timely interventions and
mitigating the severe health consequences of falls, particularly in older
adults. However, existing methods often rely on simulated data or assumptions
such as prior knowledge of fall events, limiting their real-world
applicability. Practical deployment also requires efficient computation and
robust evaluation metrics tailored to continuous monitoring. This paper
presents a real-time fall detection framework for continuous monitoring without
prior knowledge of fall events. Using over 60 hours of inertial measurement
unit (IMU) data from the FARSEEING real-world falls dataset, we employ recent
efficient classifiers to compute fall probabilities in streaming mode. To
enhance robustness, we introduce a cost-sensitive learning strategy that tunes
the decision threshold using a cost function reflecting the higher risk of
missed falls compared to false alarms. Unlike many methods that achieve high
recall only at the cost of precision, our framework achieved Recall of 1.00,
Precision of 0.84, and an F1 score of 0.91 on FARSEEING, detecting all falls
while keeping false alarms low, with average inference time below 5 ms per
sample. These results demonstrate that cost-sensitive threshold tuning enhances
the robustness of accelerometer-based fall detection. They also highlight the
potential of our computationally efficient framework for deployment in
real-time wearable sensor systems for continuous monitoring.

</details>


### [226] [Visualization and Analysis of the Loss Landscape in Graph Neural Networks](https://arxiv.org/abs/2509.11792)
*Samir Moustafa,Lorenz Kummer,Simon Fetzel,Nils M. Kriege,Wilfried N. Gansterer*

Main category: cs.LG

TL;DR: 提出了一种可学习的降维方法来可视化GNN损失景观，分析了多种因素对GNN优化的影响，超越了现有的PCA方法，为GNN架构和训练策略设计提供了新见解。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在优化、表达能力和泛化能力之间的相互作用尚未得到充分理解，需要开发有效的方法来可视化和分析GNN的优化景观。

Method: 引入了可学习的维度缩减方法用于可视化GNN损失景观，分析了过平滑、跳跃知识、量化、稀疏化和预条件子等多种因素对GNN优化的影响。

Result: 可学习投影方法超越了最先进的基于PCA的方法，能够以更低的内存使用准确重建高维参数。架构、稀疏化和优化器的预条件对GNN优化景观和最终性能有显著影响。

Conclusion: 这些发现有助于开发更高效的GNN架构设计和训练策略，为理解GNN优化过程提供了重要见解。

Abstract: Graph Neural Networks (GNNs) are powerful models for graph-structured data,
with broad applications. However, the interplay between GNN parameter
optimization, expressivity, and generalization remains poorly understood. We
address this by introducing an efficient learnable dimensionality reduction
method for visualizing GNN loss landscapes, and by analyzing the effects of
over-smoothing, jumping knowledge, quantization, sparsification, and
preconditioner on GNN optimization. Our learnable projection method surpasses
the state-of-the-art PCA-based approach, enabling accurate reconstruction of
high-dimensional parameters with lower memory usage. We further show that
architecture, sparsification, and optimizer's preconditioning significantly
impact the GNN optimization landscape and their training process and final
prediction performance. These insights contribute to developing more efficient
designs of GNN architectures and training strategies.

</details>


### [227] [FedDAF: Federated Domain Adaptation Using Model Functional Distance](https://arxiv.org/abs/2509.11819)
*Mrinmay Sen,Ankita Das,Sidhant Nair,C Krishna Mohan*

Main category: cs.LG

TL;DR: FedDAF是一种新的联邦域适应方法，通过计算模型功能距离和基于相似性的聚合，同时解决域偏移和目标数据稀缺问题，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有FDA方法主要关注域偏移问题，假设目标端有充足数据，但忽略了域偏移和数据稀缺的双重挑战。同时，现有方法未能根据目标端目标优先共享源客户端相关信息。

Method: FedDAF使用基于相似性的全局源模型和目标模型聚合，通过计算在目标数据上的平均梯度场之间的模型功能距离。计算两个模型平均梯度场之间的角度，并用Gompertz函数进行归一化。全局源模型通过简单平均聚合所有本地源模型。

Result: 在真实世界数据集上的实验表明，FedDAF在测试准确率方面优于现有的FL、PFL和FDA方法。

Conclusion: FedDAF有效解决了联邦域适应中的域偏移和数据稀缺双重挑战，通过目标导向的模型聚合实现了更好的性能表现。

Abstract: Federated Domain Adaptation (FDA) is a federated learning (FL) approach that
improves model performance at the target client by collaborating with source
clients while preserving data privacy. FDA faces two primary challenges: domain
shifts between source and target data and limited labeled data at the target.
Most existing FDA methods focus on domain shifts, assuming ample target data,
yet often neglect the combined challenges of both domain shifts and data
scarcity. Moreover, approaches that address both challenges fail to prioritize
sharing relevant information from source clients according to the target's
objective. In this paper, we propose FedDAF, a novel approach addressing both
challenges in FDA. FedDAF uses similarity-based aggregation of the global
source model and target model by calculating model functional distance from
their mean gradient fields computed on target data. This enables effective
model aggregation based on the target objective, constructed using target data,
even with limited data. While computing model functional distance between these
two models, FedDAF computes the angle between their mean gradient fields and
then normalizes with the Gompertz function. To construct the global source
model, all the local source models are aggregated using simple average in the
server. Experiments on real-world datasets demonstrate FedDAF's superiority
over existing FL, PFL, and FDA methods in terms of achieving better test
accuracy.

</details>


### [228] [Transparent and Fair Profiling in Employment Services: Evidence from Switzerland](https://arxiv.org/abs/2509.11847)
*Tim Räz*

Main category: cs.LG

TL;DR: 本文研究表明，可解释的提升机模型在长期失业风险预测中表现几乎与最佳黑盒模型相当，同时通过稀疏性、特征平滑和公平性缓解技术增强了透明度和公平性，为就业服务提供了可信赖的替代方案。


<details>
  <summary>Details</summary>
Motivation: 长期失业对求职者和公共就业服务都是挑战，当前使用的黑盒机器学习模型存在透明度和公平性问题，需要研究可解释模型是否能作为替代方案。

Method: 使用瑞士行政数据，比较传统统计模型、可解释模型和黑盒模型的预测性能、可解释性和公平性，重点评估可解释提升机模型的表现。

Result: 可解释提升机模型表现几乎与最佳黑盒模型相当，模型稀疏性、特征平滑和公平性缓解技术能在仅轻微损失性能的情况下显著提升透明度和公平性。

Conclusion: 可解释的失业风险分析模型提供了可信赖的黑盒模型替代方案，在不牺牲性能的前提下实现了更好的透明度和公平性。

Abstract: Long-term unemployment (LTU) is a challenge for both jobseekers and public
employment services. Statistical profiling tools are increasingly used to
predict LTU risk. Some profiling tools are opaque, black-box machine learning
models, which raise issues of transparency and fairness. This paper
investigates whether interpretable models could serve as an alternative, using
administrative data from Switzerland. Traditional statistical, interpretable,
and black-box models are compared in terms of predictive performance,
interpretability, and fairness. It is shown that explainable boosting machines,
a recent interpretable model, perform nearly as well as the best black-box
models. It is also shown how model sparsity, feature smoothing, and fairness
mitigation can enhance transparency and fairness with only minor losses in
performance. These findings suggest that interpretable profiling provides an
accountable and trustworthy alternative to black-box models without
compromising performance.

</details>


### [229] [TabStruct: Measuring Structural Fidelity of Tabular Data](https://arxiv.org/abs/2509.11950)
*Xiangjian Jiang,Nikola Simidjievski,Mateja Jamnik*

Main category: cs.LG

TL;DR: 提出了一个联合考虑结构保真度和传统评估维度的表格生成器评估框架，引入了无需真实因果结构的全局效用指标，并构建了大规模评估基准TabStruct


<details>
  <summary>Details</summary>
Motivation: 现有表格生成器评估方法往往忽视结构保真度与传统评估维度之间的相互作用，且通常局限于玩具数据集，因为量化结构保真度需要真实因果结构信息

Method: 提出全局效用(global utility)评估指标，可以在没有真实因果结构的情况下评估结构保真度；构建TabStruct基准，包含13个表格生成器和29个数据集的全面评估

Result: 全局效用提供了任务无关、领域无关的表格生成器性能评估视角，基准分析显示该方法能有效评估不同类别生成器的性能

Conclusion: 该框架为表格生成器评估提供了更全面的视角，全局效用指标解决了真实因果结构缺失的问题，TabStruct基准为领域提供了大规模量化分析工具

Abstract: Evaluating tabular generators remains a challenging problem, as the unique
causal structural prior of heterogeneous tabular data does not lend itself to
intuitive human inspection. Recent work has introduced structural fidelity as a
tabular-specific evaluation dimension to assess whether synthetic data complies
with the causal structures of real data. However, existing benchmarks often
neglect the interplay between structural fidelity and conventional evaluation
dimensions, thus failing to provide a holistic understanding of model
performance. Moreover, they are typically limited to toy datasets, as
quantifying existing structural fidelity metrics requires access to
ground-truth causal structures, which are rarely available for real-world
datasets. In this paper, we propose a novel evaluation framework that jointly
considers structural fidelity and conventional evaluation dimensions. We
introduce a new evaluation metric, $\textbf{global utility}$, which enables the
assessment of structural fidelity even in the absence of ground-truth causal
structures. In addition, we present $\textbf{TabStruct}$, a comprehensive
evaluation benchmark offering large-scale quantitative analysis on 13 tabular
generators from nine distinct categories, across 29 datasets. Our results
demonstrate that global utility provides a task-independent, domain-agnostic
lens for tabular generator performance. We release the TabStruct benchmark
suite, including all datasets, evaluation pipelines, and raw results. Code is
available at https://github.com/SilenceX12138/TabStruct.

</details>


### [230] [Deep operator network for surrogate modeling of poroelasticity with random permeability fields](https://arxiv.org/abs/2509.11966)
*Sangjoon Park,Yeonjong Shin,Jinhyun Choo*

Main category: cs.LG

TL;DR: 提出基于DeepONet的替代模型框架，用于处理具有随机渗透率场的多孔弹性问题，通过无量纲化、降维和两步训练策略提高预测精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 多孔弹性问题中随机渗透率场的模拟计算成本高昂，需要高效的替代模型来支持概率分析、不确定性量化和反问题求解。

Method: 采用深度算子网络(DeepONet)学习从随机渗透率场到瞬态多孔弹性响应的映射，结合无量纲化、Karhunen-Loève展开降维和分支-主干网络两步训练策略。

Result: 在两个基准问题（土壤固结和地下水开采引起的地面沉降）上，DeepONet在保持高预测精度的同时实现了显著的计算加速。

Conclusion: 该方法为具有随机渗透率场的多孔弹性系统提供了一种可扩展且高效的替代建模技术，具有广泛的应用潜力。

Abstract: Poroelasticity -- coupled fluid flow and elastic deformation in porous media
-- often involves spatially variable permeability, especially in subsurface
systems. In such cases, simulations with random permeability fields are widely
used for probabilistic analysis, uncertainty quantification, and inverse
problems. These simulations require repeated forward solves that are often
prohibitively expensive, motivating the development of efficient surrogate
models. However, efficient surrogate modeling techniques for poroelasticity
with random permeability fields remain scarce. In this study, we propose a
surrogate modeling framework based on the deep operator network (DeepONet), a
neural architecture designed to learn mappings between infinite-dimensional
function spaces. The proposed surrogate model approximates the solution
operator that maps random permeability fields to transient poroelastic
responses. To enhance predictive accuracy and stability, we integrate three
strategies: nondimensionalization of the governing equations, input
dimensionality reduction via Karhunen--Lo\'eve expansion, and a two-step
training procedure that decouples the optimization of branch and trunk
networks. The methodology is evaluated on two benchmark problems in
poroelasticity: soil consolidation and ground subsidence induced by groundwater
extraction. In both cases, the DeepONet achieves substantial speedup in
inference while maintaining high predictive accuracy across a wide range of
permeability statistics. These results highlight the potential of the proposed
approach as a scalable and efficient surrogate modeling technique for
poroelastic systems with random permeability fields.

</details>


### [231] [Examining the Relationship between Scientific Publishing Activity and Hype-Driven Financial Bubbles: A Comparison of the Dot-Com and AI Eras](https://arxiv.org/abs/2509.11982)
*Aksheytha Chelikavada,Casey C. Bennett*

Main category: cs.LG

TL;DR: 该研究通过分析科学出版数据（引用网络）来预测金融泡沫，比较了互联网泡沫时代（1994-2001）和AI时代（2017-2024）的模式，发现互联网泡沫的模式并不能有效预测AI泡沫的形成与破灭。


<details>
  <summary>Details</summary>
Motivation: 金融泡沫往往突然出现但造成长期经济影响，研究旨在探索是否可以通过分析科学出版数据（如引用网络）来发现预测未来类似泡沫兴衰的信号。

Method: 使用时序社交网络分析（SNA）检测科学家出版引用网络与金融市场数据之间的关系，比较互联网时代和AI时代两个技术快速变革时期，并采用多种分析技术（LSTM、KNN、AR X/GARCH）进行深入分析。

Result: 结果显示互联网泡沫时代的模式并不能明确预测AI泡沫的兴衰。虽然年度引用网络反映了两个时代科学家出版行为的变化，但存在一部分AI时代科学家的出版影响力模式与互联网泡沫时代相似。数据分析表明AI时代可能存在前所未有的金融泡沫形式或根本不存在泡沫。

Conclusion: 研究发现互联网泡沫时代存在的模式不能有效转化并应用于AI市场，意味着基于历史泡沫模式的预测方法在新技术时代可能不再适用。

Abstract: Financial bubbles often arrive without much warning, but create long-lasting
economic effects. For example, during the dot-com bubble, innovative
technologies created market disruptions through excitement for a promised
bright future. Such technologies originated from research where scientists had
developed them for years prior to their entry into the markets. That raises a
question on the possibility of analyzing scientific publishing data (e.g.
citation networks) leading up to a bubble for signals that may forecast the
rise and fall of similar future bubbles. To that end, we utilized temporal SNAs
to detect possible relationships between the publication citation networks of
scientists and financial market data during two modern eras of rapidly shifting
technology: 1) dot-com era from 1994 to 2001 and 2) AI era from 2017 to 2024.
Results showed that the patterns from the dot-com era (which did end in a
bubble) did not definitively predict the rise and fall of an AI bubble. While
yearly citation networks reflected possible changes in publishing behavior of
scientists between the two eras, there was a subset of AI era scientists whose
publication influence patterns mirrored those during the dot-com era. Upon
further analysis using multiple analysis techniques (LSTM, KNN, AR X/GARCH),
the data seems to suggest two possibilities for the AI era: unprecedented form
of financial bubble unseen or that no bubble exists. In conclusion, our
findings imply that the patterns present in the dot-com era do not effectively
translate in such a manner to apply them to the AI market.

</details>


### [232] [Low-rank Orthogonalization for Large-scale Matrix Optimization with Applications to Foundation Model Training](https://arxiv.org/abs/2509.11983)
*Chuan He,Zhanwang Deng,Zhaosong Lu*

Main category: cs.LG

TL;DR: 本文提出了低秩正交化方法，利用神经网络训练中梯度的低秩特性，开发了低秩矩阵符号梯度下降和低秩Muon优化器，在GPT-2和LLaMA预训练中表现优于原版Muon。


<details>
  <summary>Details</summary>
Motivation: 神经网络训练本质上是大规模矩阵优化问题，但长期以来忽略了参数矩阵结构。Muon优化器通过显式利用矩阵结构在基础模型训练中表现出色，其中矩阵正交化是关键组件。

Method: 提出低秩正交化方法，利用训练过程中梯度的低秩特性，在此基础上开发低秩矩阵符号梯度下降算法和低秩Muon变体。

Result: 数值实验显示低秩正交化性能优越，低秩Muon在GPT-2和LLaMA预训练中超越了精心调优的原版Muon。

Conclusion: 理论分析了低秩矩阵符号梯度下降的迭代复杂度，以及在重尾噪声下低秩Muon寻找近似随机平稳解的收敛性能。

Abstract: Neural network (NN) training is inherently a large-scale matrix optimization
problem, yet the matrix structure of NN parameters has long been overlooked.
Recently, the optimizer Muon \cite{jordanmuon}, which explicitly exploits this
structure, has gained significant attention for its strong performance in
foundation model training. A key component contributing to Muon's success is
matrix orthogonalization. In this paper, we propose {\it low-rank
orthogonalization}, which explicitly leverages the low-rank nature of gradients
during NN training. Building on this, we propose low-rank matrix-signed
gradient descent and a low-rank variant of Muon. Our numerical experiments
demonstrate the superior performance of low-rank orthogonalization, with the
low-rank Muon achieving promising results in GPT-2 and LLaMA pretraining --
surpassing the performance of the carefully tuned vanilla Muon. Theoretically,
we establish the iteration complexity of the low-rank matrix-signed gradient
descent for finding an approximate stationary solution, as well as that of
low-rank Muon for finding an approximate stochastic stationary solution under
heavy-tailed noise.

</details>


### [233] [Learning from Uncertain Similarity and Unlabeled Data](https://arxiv.org/abs/2509.11984)
*Meng Wei,Zhongnian Li,Peng Ying,Xinzheng Xu*

Main category: cs.LG

TL;DR: 提出USimUL框架，通过引入不确定性组件来减少相似性标注中的标签泄露风险，使用不确定相似性和未标记数据进行无偏风险估计，达到统计最优参数收敛率。


<details>
  <summary>Details</summary>
Motivation: 现有基于相似性的弱监督学习方法需要精确的相似性标注，这可能泄露敏感标签信息并带来隐私风险。

Method: 提出USimUL框架，为每个相似性对嵌入不确定性组件，开发无偏风险估计器从不确定相似性和未标记数据中学习。

Result: 在基准和真实数据集上的大量实验表明，该方法相比传统基于相似性的方法获得了更优越的分类性能。

Conclusion: USimUL框架有效解决了相似性标注中的隐私泄露问题，同时保持了优秀的分类性能，为隐私保护的弱监督学习提供了新思路。

Abstract: Existing similarity-based weakly supervised learning approaches often rely on
precise similarity annotations between data pairs, which may inadvertently
expose sensitive label information and raise privacy risks. To mitigate this
issue, we propose Uncertain Similarity and Unlabeled Learning (USimUL), a novel
framework where each similarity pair is embedded with an uncertainty component
to reduce label leakage. In this paper, we propose an unbiased risk estimator
that learns from uncertain similarity and unlabeled data. Additionally, we
theoretically prove that the estimator achieves statistically optimal
parametric convergence rates. Extensive experiments on both benchmark and
real-world datasets show that our method achieves superior classification
performance compared to conventional similarity-based approaches.

</details>


### [234] [Generalizing Behavior via Inverse Reinforcement Learning with Closed-Form Reward Centroids](https://arxiv.org/abs/2509.12010)
*Filippo Lazzati,Alberto Maria Metelli*

Main category: cs.LG

TL;DR: 提出了一种新的逆强化学习方法，通过计算可行奖励集合的质心来选择平均策略，以解决IRL中多奖励函数解释相同行为的问题。


<details>
  <summary>Details</summary>
Motivation: 逆强化学习(IRL)存在固有的不适定性——多个奖励函数都能解释相同的专家行为，这些奖励在新环境中可能产生不同的策略，需要决策标准来选择部署哪个策略。

Method: 提出选择可行奖励集合有界子集中奖励诱导策略的"平均"策略，通过计算该子集的奖励质心进行规划，并开发了仅使用专家演示离线数据集的高效估计算法。

Result: 推导出了奖励质心的闭式表达式，提出了可证明高效的估计算法，并通过数值模拟展示了专家行为与该方法产生行为之间的关系。

Conclusion: 该方法为IRL中的策略选择问题提供了原则性解决方案，通过奖励质心规划可获得平均策略，有效处理了IRL的不适定性问题。

Abstract: We study the problem of generalizing an expert agent's behavior, provided
through demonstrations, to new environments and/or additional constraints.
Inverse Reinforcement Learning (IRL) offers a promising solution by seeking to
recover the expert's underlying reward function, which, if used for planning in
the new settings, would reproduce the desired behavior. However, IRL is
inherently ill-posed: multiple reward functions, forming the so-called feasible
set, can explain the same observed behavior. Since these rewards may induce
different policies in the new setting, in the absence of additional
information, a decision criterion is needed to select which policy to deploy.
In this paper, we propose a novel, principled criterion that selects the
"average" policy among those induced by the rewards in a certain bounded subset
of the feasible set. Remarkably, we show that this policy can be obtained by
planning with the reward centroid of that subset, for which we derive a
closed-form expression. We then present a provably efficient algorithm for
estimating this centroid using an offline dataset of expert demonstrations
only. Finally, we conduct numerical simulations that illustrate the
relationship between the expert's behavior and the behavior produced by our
method.

</details>


### [235] [Learning non-Markovian Dynamical Systems with Signature-based Encoders](https://arxiv.org/abs/2509.12022)
*Eliott Pradeleix,Rémy Hosseinkhan-Boucher,Alena Shilova,Onofrio Semeraro,Lionel Mathelin*

Main category: cs.LG

TL;DR: 该论文提出使用签名变换作为编码器来学习非马尔可夫动力学，解决了传统神经ODE依赖马尔可夫假设的局限性，在连续时间建模中表现优于RNN编码器。


<details>
  <summary>Details</summary>
Motivation: 神经ODE虽然能有效建模动力系统，但依赖马尔可夫假设（未来状态仅取决于当前状态），这在现实世界中往往不成立。现有基于RNN的编码器存在离散性和训练困难的问题，需要更好的连续时间建模方法。

Method: 使用签名变换作为编码器，这是一种具有坚实理论基础的多维时间信息汇总方法。将基于签名的编码方案集成到编码器-解码器动力学模型中。

Result: 在合成基准测试中，基于签名的编码方案在测试性能上优于基于RNN的替代方法。

Conclusion: 签名变换为学习非马尔可夫动力学提供了有效的连续时间编码方案，克服了传统方法的局限性，在性能上表现更优。

Abstract: Neural ordinary differential equations offer an effective framework for
modeling dynamical systems by learning a continuous-time vector field. However,
they rely on the Markovian assumption - that future states depend only on the
current state - which is often untrue in real-world scenarios where the
dynamics may depend on the history of past states. This limitation becomes
especially evident in settings involving the continuous control of complex
systems with delays and memory effects. To capture historical dependencies,
existing approaches often rely on recurrent neural network (RNN)-based
encoders, which are inherently discrete and struggle with continuous modeling.
In addition, they may exhibit poor training behavior. In this work, we
investigate the use of the signature transform as an encoder for learning
non-Markovian dynamics in a continuous-time setting. The signature transform
offers a continuous-time alternative with strong theoretical foundations and
proven efficiency in summarizing multidimensional information in time. We
integrate a signature-based encoding scheme into encoder-decoder dynamics
models and demonstrate that it outperforms RNN-based alternatives in test
performance on synthetic benchmarks.

</details>


### [236] [Imitation Learning as Return Distribution Matching](https://arxiv.org/abs/2509.12026)
*Filippo Lazzati,Alberto Maria Metelli*

Main category: cs.LG

TL;DR: 本文研究通过模仿学习训练风险敏感的强化学习智能体，目标是匹配专家的期望回报和风险态度（回报分布特征）。提出了基于Wasserstein距离的风险敏感模仿学习框架，开发了两种高效算法RS-BC和RS-KT，并证明了非马尔可夫策略的优势。


<details>
  <summary>Details</summary>
Motivation: 标准模仿学习只关注匹配专家的期望回报，但忽略了风险态度等回报分布的其他重要特征。为了训练出能够同时匹配专家平均表现和风险偏好的智能体，需要开发风险敏感的模仿学习方法。

Method: 提出了基于Wasserstein距离的风险敏感模仿学习问题表述。在表格设置下，首先证明了马尔可夫策略的表达能力有限，然后引入了充分表达的非马尔可夫策略子类。基于此开发了两种算法：RS-BC（未知转移模型）和RS-KT（已知转移模型），并设计了基于oracle的变体处理未知专家奖励的情况。

Result: 理论分析表明RS-KT通过利用动态信息实现了比RS-BC更低的样本复杂度。数值模拟验证了两种算法的样本效率，并展示了非马尔可夫策略相对于标准高效模仿学习算法的优势。

Conclusion: 该研究为风险敏感模仿学习提供了理论基础和实用算法，证明了非马尔可夫策略在匹配专家回报分布方面的有效性，为训练具有特定风险态度的智能体提供了新方法。

Abstract: We study the problem of training a risk-sensitive reinforcement learning (RL)
agent through imitation learning (IL). Unlike standard IL, our goal is not only
to train an agent that matches the expert's expected return (i.e., its average
performance) but also its risk attitude (i.e., other features of the return
distribution, such as variance). We propose a general formulation of the
risk-sensitive IL problem in which the objective is to match the expert's
return distribution in Wasserstein distance. We focus on the tabular setting
and assume the expert's reward is known. After demonstrating the limited
expressivity of Markovian policies for this task, we introduce an efficient and
sufficiently expressive subclass of non-Markovian policies tailored to it.
Building on this subclass, we develop two provably efficient algorithms, RS-BC
and RS-KT, for solving the problem when the transition model is unknown and
known, respectively. We show that RS-KT achieves substantially lower sample
complexity than RS-BC by exploiting dynamics information. We further
demonstrate the sample efficiency of return distribution matching in the
setting where the expert's reward is unknown by designing an oracle-based
variant of RS-KT. Finally, we complement our theoretical analysis of RS-KT and
RS-BC with numerical simulations, highlighting both their sample efficiency and
the advantages of non-Markovian policies over standard sample-efficient IL
algorithms.

</details>


### [237] [Travel Time and Weather-Aware Traffic Forecasting in a Conformal Graph Neural Network Framework](https://arxiv.org/abs/2509.12043)
*Mayur Patil,Qadeer Ahmed,Shawn Midlam-Mohler*

Main category: cs.LG

TL;DR: 提出基于图神经网络和自适应邻接矩阵的交通流预测框架，通过正态分布和变异系数处理交通随机性，结合天气因素调整边权重，并采用自适应共形预测提供不确定性量化


<details>
  <summary>Details</summary>
Motivation: 交通流预测对交通管理至关重要，但由于城市交通的随机性和环境因素影响，传统方法难以有效处理交通变异性和复杂依赖关系

Method: 使用图神经网络框架，利用对数正态分布和变异系数构建自适应邻接矩阵，整合天气因素（温度、风速、降水）动态调整边权重，并采用自适应共形预测进行不确定性量化

Result: 相比基线方法，该模型在预测精度和不确定性边界方面表现更好，通过SUMO仿真验证，模拟的平均旅行时间落在历史数据区间内

Conclusion: 提出的方法能够有效适应交通随机性和环境变化，提供可靠的预测和不确定性量化，验证了模型的鲁棒性和实用性

Abstract: Traffic flow forecasting is essential for managing congestion, improving
safety, and optimizing various transportation systems. However, it remains a
prevailing challenge due to the stochastic nature of urban traffic and
environmental factors. Better predictions require models capable of
accommodating the traffic variability influenced by multiple dynamic and
complex interdependent factors. In this work, we propose a Graph Neural Network
(GNN) framework to address the stochasticity by leveraging adaptive adjacency
matrices using log-normal distributions and Coefficient of Variation (CV)
values to reflect real-world travel time variability. Additionally, weather
factors such as temperature, wind speed, and precipitation adjust edge weights
and enable GNN to capture evolving spatio-temporal dependencies across traffic
stations. This enhancement over the static adjacency matrix allows the model to
adapt effectively to traffic stochasticity and changing environmental
conditions. Furthermore, we utilize the Adaptive Conformal Prediction (ACP)
framework to provide reliable uncertainty quantification, achieving target
coverage while maintaining acceptable prediction intervals. Experimental
results demonstrate that the proposed model, in comparison with baseline
methods, showed better prediction accuracy and uncertainty bounds. We, then,
validate this method by constructing traffic scenarios in SUMO and applying
Monte-Carlo simulation to derive a travel time distribution for a Vehicle Under
Test (VUT) to reflect real-world variability. The simulated mean travel time of
the VUT falls within the intervals defined by INRIX historical data, verifying
the model's robustness.

</details>


### [238] [Hi-DARTS: Hierarchical Dynamically Adapting Reinforcement Trading System](https://arxiv.org/abs/2509.12048)
*Hoon Sagong,Heesu Kim,Hanbeen Hong*

Main category: cs.LG

TL;DR: Hi-DARTS是一个分层多智能体强化学习框架，通过元智能体分析市场波动性并动态激活高频或低频交易专用智能体，解决了传统交易系统计算效率和市场响应性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统自主交易系统由于固定的操作频率，难以平衡计算效率和市场响应性。需要一种能够根据市场条件动态调整交易频率的智能系统。

Method: 提出Hi-DARTS分层多智能体强化学习框架，使用元智能体分析市场波动性，根据需要动态激活专门的高频或低频时间框架交易智能体。

Result: 在2024年1月至2025年5月的AAPL股票回测中，Hi-DARTS实现了25.17%的累计回报率和0.75的夏普比率，优于AAPL的买入持有策略(12.19%)和SPY ETF(20.01%)。

Conclusion: 研究表明，动态分层智能体能够在保持高计算效率的同时实现优越的风险调整后回报，为自主交易系统提供了新的解决方案。

Abstract: Conventional autonomous trading systems struggle to balance computational
efficiency and market responsiveness due to their fixed operating frequency. We
propose Hi-DARTS, a hierarchical multi-agent reinforcement learning framework
that addresses this trade-off. Hi-DARTS utilizes a meta-agent to analyze market
volatility and dynamically activate specialized Time Frame Agents for
high-frequency or low-frequency trading as needed. During back-testing on AAPL
stock from January 2024 to May 2025, Hi-DARTS yielded a cumulative return of
25.17% with a Sharpe Ratio of 0.75. This performance surpasses standard
benchmarks, including a passive buy-and-hold strategy on AAPL (12.19% return)
and the S&P 500 ETF (SPY) (20.01% return). Our work demonstrates that dynamic,
hierarchical agents can achieve superior risk-adjusted returns while
maintaining high computational efficiency.

</details>


### [239] [Foundational theory for optimal decision tree problems. II. Optimal hypersurface decision tree algorithm](https://arxiv.org/abs/2509.12057)
*Xi He*

Main category: cs.LG

TL;DR: 本文提出了首个超曲面决策树（HODT）算法，能够处理一般超曲面分割规则的最优决策树问题，无需依赖外部求解器。相比现有仅限于超平面分割的方法，HODT在合成数据上能更准确地恢复真实模型，对噪声更具鲁棒性，在30个真实数据集上比最优轴平行决策树算法准确率提升高达30%。


<details>
  <summary>Details</summary>
Motivation: 现有最优决策树方法仅限于超平面分割规则这一特殊情况，且依赖通用求解器。本文旨在解决一般超曲面决策树模型的最优化问题，提供不依赖外部求解器的专用算法。

Method: 基于第一部分建立的算法和几何基础，开发了首个超曲面决策树（HODT）算法。使用合成数据集测试不同树大小、数据规模、维度和噪声条件下的性能，并在30个真实数据集上评估泛化性能。

Result: HODT算法在合成数据上比轴平行树更准确地恢复真实模型，对噪声更具鲁棒性。在真实数据集上，当树复杂度得到适当控制时，HODT比最优轴平行决策树算法的准确率提升高达30%。

Conclusion: HODT算法成功解决了超曲面决策树的最优化问题，在准确性和鲁棒性方面显著优于现有方法，为更复杂的决策树模型提供了有效的求解方案。

Abstract: Decision trees are a ubiquitous model for classification and regression tasks
due to their interpretability and efficiency. However, solving the optimal
decision tree (ODT) problem remains a challenging combinatorial optimization
task. Even for the simplest splitting rules--axis-parallel hyperplanes--it is
NP-hard to optimize. In Part I of this series, we rigorously defined the proper
decision tree model through four axioms and, based on these, introduced four
formal definitions of the ODT problem. From these definitions, we derived four
generic algorithms capable of solving ODT problems for arbitrary decision trees
satisfying the axioms. We also analyzed the combinatorial geometric properties
of hypersurfaces, showing that decision trees defined by polynomial
hypersurface splitting rules satisfy the proper axioms that we proposed.
  In this second paper (Part II) of this two-part series, building on the
algorithmic and geometric foundations established in Part I, we introduce the
first hypersurface decision tree (HODT) algorithm. To the best of our
knowledge, existing optimal decision tree methods are, to date, limited to
hyperplane splitting rules--a special case of hypersurfaces--and rely on
general-purpose solvers. In contrast, our HODT algorithm addresses the general
hypersurface decision tree model without requiring external solvers.
  Using synthetic datasets generated from ground-truth hyperplane decision
trees, we vary tree size, data size, dimensionality, and label and feature
noise. Results showing that our algorithm recovers the ground truth more
accurately than axis-parallel trees and exhibits greater robustness to noise.
We also analyzed generalization performance across 30 real-world datasets,
showing that HODT can achieve up to 30% higher accuracy than the
state-of-the-art optimal axis-parallel decision tree algorithm when tree
complexity is properly controlled.

</details>


### [240] [Early Detection of Branched Broomrape (Phelipanche ramosa) Infestation in Tomato Crops Using Leaf Spectral Analysis and Machine Learning](https://arxiv.org/abs/2509.12074)
*Mohammadreza Narimani,Alireza Pourreza,Ali Moghimi,Parastoo Farajpoor,Hamid Jafarbiglu,Mohsen B. Mesgaran*

Main category: cs.LG

TL;DR: 利用叶片光谱反射率和集成机器学习方法，在番茄生长早期（585 GDD）实现了89%准确率的分枝列当寄生检测，但后期准确率下降至69%。


<details>
  <summary>Details</summary>
Motivation: 分枝列当是一种叶绿素缺失的寄生杂草，通过从宿主提取营养威胁番茄生产，需要开发早期检测方法来支持靶向干预和减少产量损失。

Method: 在田间实验中跟踪300株番茄植株，使用便携式光谱仪获取叶片反射率（400-2500 nm），经过预处理（波段去噪、1 nm插值、Savitzky-Golay平滑、相关性波段缩减），采用集成学习方法结合随机森林、XGBoost、SVM和朴素贝叶斯算法。

Result: 在1500 nm和2000 nm水吸收特征附近观察到明显的类别差异，感染植株在早期阶段叶片含水量降低。集成模型在585 GDD时达到89%准确率，召回率分别为0.86（感染）和0.93（非感染），但在后期阶段（如1568 GDD）准确率下降至69%。

Conclusion: 尽管感染植株数量少且存在环境干扰因素，近端传感与集成学习能够在冠层症状可见之前及时检测分枝列当，支持靶向干预措施。

Abstract: Branched broomrape (Phelipanche ramosa) is a chlorophyll-deficient parasitic
weed that threatens tomato production by extracting nutrients from the host. We
investigate early detection using leaf-level spectral reflectance (400-2500 nm)
and ensemble machine learning. In a field experiment in Woodland, California,
we tracked 300 tomato plants across growth stages defined by growing degree
days (GDD). Leaf reflectance was acquired with a portable spectrometer and
preprocessed (band denoising, 1 nm interpolation, Savitzky-Golay smoothing,
correlation-based band reduction). Clear class differences were observed near
1500 nm and 2000 nm water absorption features, consistent with reduced leaf
water content in infected plants at early stages. An ensemble combining Random
Forest, XGBoost, SVM with RBF kernel, and Naive Bayes achieved 89% accuracy at
585 GDD, with recalls of 0.86 (infected) and 0.93 (noninfected). Accuracy
declined at later stages (e.g., 69% at 1568 GDD), likely due to senescence and
weed interference. Despite the small number of infected plants and
environmental confounders, results show that proximal sensing with ensemble
learning enables timely detection of broomrape before canopy symptoms are
visible, supporting targeted interventions and reduced yield losses.

</details>


### [241] [A Time-Series Foundation Model by Universal Delay Embedding](https://arxiv.org/abs/2509.12080)
*Zijian Wang,Peng Tao,Jifan Shi,Rui Bao,Rui Liu,Luonan Chen*

Main category: cs.LG

TL;DR: UDE是一个预训练基础模型，通过延迟嵌入表示和Koopman算子预测的结合，在时间序列预测中实现了20%以上的MSE平均降低，并展现出优异的可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测方法在处理非线性系统时存在局限性，需要一种能够保持动力学系统本质特性并提供可解释性的通用框架。

Method: 基于Takens嵌入定理，通过Hankel矩阵构造二维子空间补丁作为图像表示，利用自注意力编码器学习有限维Koopman算子进行线性预测。

Result: 在多个基准测试和真实气候数据集上，相比最先进的基础模型平均减少20%以上的均方误差，在微调场景中表现出优异的泛化能力。

Conclusion: UDE作为一个可扩展、可解释的通用时间序列建模框架，能够一致识别拓扑信息子空间并稳健编码领域不变动力学，具有广泛的科学和工业应用前景。

Abstract: This study introduces Universal Delay Embedding (UDE), a pretrained
foundation model designed to revolutionize time-series forecasting through
principled integration of delay embedding representation and Koopman operator
prediction. Leveraging Takens' embedding theorem, UDE as a dynamical
representation of observed data constructs two-dimensional subspace patches
from Hankel matrices, theoretically preserving dynamical and topological
properties of underlying dynamical systems. Such patches are viewed as images,
which can be efficiently processed by exploiting advanced deep learning
technologies. Computationally, these patches further serve as tokens for
learning a self-attention encoder, thus enabling accurate prediction of
nonlinear time-series by a finite-dimensional Koopman operator in a linear
manner in a latent space. Extensive evaluations across various benchmarks and
real-world climate datasets demonstrate over 20% average reduction in mean
squared error versus state-of-the-art foundation models, alongside superior
generalization in fine-tuning scenarios. In particular, the learned dynamical
representations and Koopman operator prediction forms from the patches exhibit
exceptional interpretability, with consistent identification of topologically
informative subspaces and robust encoding of domain-invariant dynamics,
establishing UDE as a scalable, interpretable framework for universal
time-series modeling and forecasting with broad scientific and industrial
applicability.

</details>


### [242] [Deceptive Risk Minimization: Out-of-Distribution Generalization by Deceiving Distribution Shift Detectors](https://arxiv.org/abs/2509.12081)
*Anirudha Majumdar*

Main category: cs.LG

TL;DR: 提出欺骗性风险最小化(DRM)方法，通过让训练数据在观察者看来呈现独立同分布特征，来识别稳定特征并实现分布外泛化


<details>
  <summary>Details</summary>
Motivation: 解决传统方法需要测试数据或预定义训练数据域的局限性，旨在开发无需这些前提条件就能消除伪相关性和实现跨域泛化的方法

Method: 基于保形鞅检测器，同时学习消除分布偏移的特征并最小化任务特定损失的可微分目标函数

Result: 在概念偏移的数值实验和机器人模仿学习场景的协变量偏移模拟中验证了DRM的有效性

Conclusion: DRM提供了一种无需测试数据或预定义域划分的分布外泛化新范式，在多种偏移场景下表现出良好性能

Abstract: This paper proposes deception as a mechanism for out-of-distribution (OOD)
generalization: by learning data representations that make training data appear
independent and identically distributed (iid) to an observer, we can identify
stable features that eliminate spurious correlations and generalize to unseen
domains. We refer to this principle as deceptive risk minimization (DRM) and
instantiate it with a practical differentiable objective that simultaneously
learns features that eliminate distribution shifts from the perspective of a
detector based on conformal martingales while minimizing a task-specific loss.
In contrast to domain adaptation or prior invariant representation learning
methods, DRM does not require access to test data or a partitioning of training
data into a finite number of data-generating domains. We demonstrate the
efficacy of DRM on numerical experiments with concept shift and a simulated
imitation learning setting with covariate shift in environments that a robot is
deployed in.

</details>


### [243] [Draw a Portrait of Your Graph Data: An Instance-Level Profiling Framework for Graph-Structured Data](https://arxiv.org/abs/2509.12094)
*Tianqi Zhao,Russa Biswas,Megha Khosla*

Main category: cs.LG

TL;DR: NodePro是一个节点分析框架，通过为单个节点分配可解释的配置文件分数，实现图机器学习模型的细粒度诊断，揭示模型在节点级别的行为差异。


<details>
  <summary>Details</summary>
Motivation: 现有的图机器学习模型在整体性能上相似，但在节点级别表现不同，标准评估指标无法捕捉这些细粒度差异，难以诊断模型在何时何地失败。

Method: NodePro结合数据中心的信号（特征相似性、标签不确定性、结构模糊性）和模型中心的度量（预测置信度、训练一致性），为每个节点分配可解释的配置文件分数。

Result: NodePro能够揭示模型之间的系统性差异，即使聚合指标无法区分；节点配置文件可泛化到未见节点，支持无真实标签的预测可靠性评估；在结构化知识图中有效识别语义不一致或损坏的节点。

Conclusion: NodePro提供了一个有效的细粒度诊断框架，能够深入分析图模型在节点级别的行为差异，具有实际应用价值。

Abstract: Graph machine learning models often achieve similar overall performance yet
behave differently at the node level, failing on different subsets of nodes
with varying reliability. Standard evaluation metrics such as accuracy obscure
these fine grained differences, making it difficult to diagnose when and where
models fail. We introduce NodePro, a node profiling framework that enables
fine-grained diagnosis of model behavior by assigning interpretable profile
scores to individual nodes. These scores combine data-centric signals, such as
feature dissimilarity, label uncertainty, and structural ambiguity, with
model-centric measures of prediction confidence and consistency during
training. By aligning model behavior with these profiles, NodePro reveals
systematic differences between models, even when aggregate metrics are
indistinguishable. We show that node profiles generalize to unseen nodes,
supporting prediction reliability without ground-truth labels. Finally, we
demonstrate the utility of NodePro in identifying semantically inconsistent or
corrupted nodes in a structured knowledge graph, illustrating its effectiveness
in real-world settings.

</details>


### [244] [$K$-Level Policy Gradients for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.12117)
*Aryaman Reddi,Gabriele Tiboni,Jan Peters,Carlo D'Eramo*

Main category: cs.LG

TL;DR: K-Level Policy Gradient (KPG) 方法通过递归更新智能体策略来解决多智能体强化学习中的协调问题，相比传统方法能更快发现有效协调策略，并在理论和实验上表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统的actor-critic多智能体强化学习算法在策略更新时只考虑其他智能体的当前策略，而不考虑它们在同一更新步骤中的同时更新，这会导致协调失误和效率低下。

Method: 提出K-Level Policy Gradient (KPG)方法，递归地更新每个智能体相对于其他智能体更新后策略的策略，加速有效协调策略的发现。将KPG应用于深度MARL算法MAPPO、MADDPG和FACMAC。

Result: 理论证明KPG在有限迭代次数下能在特定条件下单调收敛到局部纳什均衡。在StarCraft II和多智能体MuJoCo实验中表现出优于现有深度MARL算法的性能。

Conclusion: KPG方法通过考虑智能体间的同步策略更新，有效解决了多智能体协调问题，在理论和实验上都证明了其优越性，为深度多智能体强化学习提供了新的有效方法。

Abstract: Actor-critic algorithms for deep multi-agent reinforcement learning (MARL)
typically employ a policy update that responds to the current strategies of
other agents. While being straightforward, this approach does not account for
the updates of other agents at the same update step, resulting in
miscoordination. In this paper, we introduce the $K$-Level Policy Gradient
(KPG), a method that recursively updates each agent against the updated
policies of other agents, speeding up the discovery of effective coordinated
policies. We theoretically prove that KPG with finite iterates achieves
monotonic convergence to a local Nash equilibrium under certain conditions. We
provide principled implementations of KPG by applying it to the deep MARL
algorithms MAPPO, MADDPG, and FACMAC. Empirically, we demonstrate superior
performance over existing deep MARL algorithms in StarCraft II and multi-agent
MuJoCo.

</details>


### [245] [Do machine learning climate models work in changing climate dynamics?](https://arxiv.org/abs/2509.12147)
*Maria Conchita Agana Navarro,Geng Li,Theo Wolf,María Pérez-Ortiz*

Main category: cs.LG

TL;DR: 本研究系统评估了机器学习气候模型在分布外场景下的表现，发现现有模型在不同OOD场景中性能差异显著，强调了建立稳健评估框架的重要性。


<details>
  <summary>Details</summary>
Motivation: 气候变化导致前所未有的极端事件频率和严重性增加，这些分布外事件对风险评估和气候适应至关重要。虽然机器学习模型在气候预测中显示出潜力，但其在分布偏移下的泛化能力尚未在气候背景下得到充分探索。

Method: 通过将成熟的OOD评估方法应用于气候数据，系统评估了最先进的基于机器学习的气候模型在多样化分布外场景中的表现，并在大规模数据集上进行了实验。

Result: 实验结果显示，现有模型在不同分布外场景中表现出显著的性能变异性，揭示了当前模型的优势和局限性。

Conclusion: 研究结果强调了稳健评估框架的重要性，并为机器学习在气候风险预测中的可靠应用提供了可行的指导见解。

Abstract: Climate change is accelerating the frequency and severity of unprecedented
events, deviating from established patterns. Predicting these
out-of-distribution (OOD) events is critical for assessing risks and guiding
climate adaptation. While machine learning (ML) models have shown promise in
providing precise, high-speed climate predictions, their ability to generalize
under distribution shifts remains a significant limitation that has been
underexplored in climate contexts. This research systematically evaluates
state-of-the-art ML-based climate models in diverse OOD scenarios by adapting
established OOD evaluation methodologies to climate data. Experiments on
large-scale datasets reveal notable performance variability across scenarios,
shedding light on the strengths and limitations of current models. These
findings underscore the importance of robust evaluation frameworks and provide
actionable insights to guide the reliable application of ML for climate risk
forecasting.

</details>


### [246] [Learning Neural Networks by Neuron Pursuit](https://arxiv.org/abs/2509.12154)
*Akshay Kumar,Jarvis Haupt*

Main category: cs.LG

TL;DR: 本文研究同质神经网络梯度流在稀疏结构鞍点附近的演化行为，并基于观察提出Neuron Pursuit贪婪算法来训练深度神经网络


<details>
  <summary>Details</summary>
Motivation: 研究同质神经网络梯度流在特定鞍点附近的行为特征，这些鞍点是梯度流从原点逃逸后首先遇到的，对理解训练动态很重要

Method: 第一部分分析梯度流在稀疏结构鞍点附近的演化；第二部分提出Neuron Pursuit算法，通过迭代添加精心选择权重的神经元来扩展网络并最小化训练损失

Result: 发现梯度流在鞍点附近停留较长时间，小范数权重保持小但方向收敛；Neuron Pursuit算法通过数值实验验证了有效性

Conclusion: 揭示了同质神经网络梯度流在鞍点附近的重要行为特征，并基于此开发了有效的贪婪训练算法

Abstract: The first part of this paper studies the evolution of gradient flow for
homogeneous neural networks near a class of saddle points exhibiting a sparsity
structure. The choice of these saddle points is motivated from previous works
on homogeneous networks, which identified the first saddle point encountered by
gradient flow after escaping the origin. It is shown here that, when
initialized sufficiently close to such saddle points, gradient flow remains
near the saddle point for a sufficiently long time, during which the set of
weights with small norm remain small but converge in direction. Furthermore,
important empirical observations are made on the behavior of gradient descent
after escaping these saddle points. The second part of the paper, motivated by
these results, introduces a greedy algorithm to train deep neural networks
called Neuron Pursuit (NP). It is an iterative procedure which alternates
between expanding the network by adding neuron(s) with carefully chosen
weights, and minimizing the training loss using this augmented network. The
efficacy of the proposed algorithm is validated using numerical experiments.

</details>


### [247] [From Autoencoders to CycleGAN: Robust Unpaired Face Manipulation via Adversarial Learning](https://arxiv.org/abs/2509.12176)
*Collin Guo*

Main category: cs.LG

TL;DR: 提出了一种基于对抗训练的CycleGAN框架，用于无配对数据的人脸合成与编辑，通过谱归一化、身份感知损失和地标约束来提升真实感和身份保持能力


<details>
  <summary>Details</summary>
Motivation: 解决无配对、未对齐数据集下人脸合成与编辑的挑战，传统自编码器方法在细节保持和身份一致性方面存在不足，需要更鲁棒的解决方案

Method: 采用谱归一化的CycleGAN框架，结合身份感知损失、感知损失和地标加权的循环一致性约束，确保训练稳定性和面部几何结构保持

Result: 在FID、LPIPS和ID-Sim指标上优于自编码器方法，在无配对数据集上实现高质量结果，在配对子集上接近pix2pix性能

Conclusion: 引导式谱归一化CycleGAN为从自编码器到鲁棒无配对人脸操作提供了实用路径，无需配对数据即可实现高质量合成

Abstract: Human face synthesis and manipulation are increasingly important in
entertainment and AI, with a growing demand for highly realistic,
identity-preserving images even when only unpaired, unaligned datasets are
available. We study unpaired face manipulation via adversarial learning, moving
from autoencoder baselines to a robust, guided CycleGAN framework. While
autoencoders capture coarse identity, they often miss fine details. Our
approach integrates spectral normalization for stable training, identity- and
perceptual-guided losses to preserve subject identity and high-level structure,
and landmark-weighted cycle constraints to maintain facial geometry across pose
and illumination changes. Experiments show that our adversarial trained
CycleGAN improves realism (FID), perceptual quality (LPIPS), and identity
preservation (ID-Sim) over autoencoders, with competitive cycle-reconstruction
SSIM and practical inference times, which achieved high quality without paired
datasets and approaching pix2pix on curated paired subsets. These results
demonstrate that guided, spectrally normalized CycleGANs provide a practical
path from autoencoders to robust unpaired face manipulation.

</details>


### [248] [All that structure matches does not glitter](https://arxiv.org/abs/2509.12178)
*Maya M. Martirossyan,Thomas Egg,Philipp Hoellmer,George Karypis,Mark Transtrum,Adrian Roitberg,Mingjie Liu,Richard G. Hennig,Ellad B. Tadmor,Stefano Martiniani*

Main category: cs.LG

TL;DR: 本文批判性分析了晶体结构预测任务中常用数据集和评估指标的问题，提出了数据去重、合理分割和新评估指标等改进方案


<details>
  <summary>Details</summary>
Motivation: 材料生成模型的进展依赖于可靠的数据集和评估基准，但现有数据集存在重复结构、不合理分割等问题，导致模型评估不准确

Method: 分析碳-24和perov-5数据集的重复结构和分割问题，提出去重版本、按原子数分割、新分割方案，并开发METRe和cRMSE新评估指标

Result: 发现碳-24数据集仅含约40%独特结构，perov-5数据集存在多晶型分布问题，提出的改进方案能更准确评估模型性能

Conclusion: 材料科学数据集需要仔细处理重复结构和合理分割，新提出的评估指标METRe和cRMSE能更可靠地衡量晶体结构预测模型的性能

Abstract: Generative models for materials, especially inorganic crystals, hold
potential to transform the theoretical prediction of novel compounds and
structures. Advancement in this field depends critically on robust benchmarks
and minimal, information-rich datasets that enable meaningful model evaluation.
This paper critically examines common datasets and reported metrics for a
crystal structure prediction task$\unicode{x2014}$generating the most likely
structures given the chemical composition of a material. We focus on three key
issues: First, materials datasets should contain unique crystal structures; for
example, we show that the widely-utilized carbon-24 dataset only contains
$\approx$40% unique structures. Second, materials datasets should not be split
randomly if polymorphs of many different compositions are numerous, which we
find to be the case for the perov-5 dataset. Third, benchmarks can mislead if
used uncritically, e.g., reporting a match rate metric without considering the
structural variety exhibited by identical building blocks. To address these
oft-overlooked issues, we introduce several fixes. We provide revised versions
of the carbon-24 dataset: one with duplicates removed, one deduplicated and
split by number of atoms $N$, and two containing only identical structures but
with different unit cells. We also propose a new split for the perov-5 dataset
which ensures polymorphs are grouped within each split subset, setting a more
sensible standard for benchmarking model performance. Finally, we present METRe
and cRMSE, new model evaluation metrics that can correct existing issues with
the match rate metric.

</details>


### [249] [Dynamic Relational Priming Improves Transformer in Multivariate Time Series](https://arxiv.org/abs/2509.12196)
*Hunjae Lee,Corey Clark*

Main category: cs.LG

TL;DR: 提出了Prime Attention机制，通过动态关系调优来改进标准注意力，在多元时间序列数据上实现更好的关系建模，预测精度提升6.5%，序列长度减少40%


<details>
  <summary>Details</summary>
Motivation: 标准注意力机制使用静态token表示，无法捕捉多元时间序列中不同通道对之间可能由完全不同的物理定律或时间动态支配的异质关系

Method: Prime Attention通过可学习的调制动态定制每个token表示，针对每个token对的独特关系动态优化成对交互，同时保持与标准注意力相同的渐近计算复杂度

Result: 在基准测试中一致优于标准注意力，预测精度提升高达6.5%，使用比标准注意力少40%的序列长度就能达到相当或更好的性能

Conclusion: Prime Attention通过表示可塑性有效提取MTS中关系特定信息，展示了优越的关系建模能力，特别适用于具有异质依赖关系的领域

Abstract: Standard attention mechanisms in transformers employ static token
representations that remain unchanged across all pair-wise computations in each
layer. This limits their representational alignment with the potentially
diverse relational dynamics of each token-pair interaction. While they excel in
domains with relatively homogeneous relationships, standard attention's static
relational learning struggles to capture the diverse, heterogeneous
inter-channel dependencies of multivariate time series (MTS) data--where
different channel-pair interactions within a single system may be governed by
entirely different physical laws or temporal dynamics. To better align the
attention mechanism for such domain phenomena, we propose attention with
dynamic relational priming (prime attention). Unlike standard attention where
each token presents an identical representation across all of its pair-wise
interactions, prime attention tailors each token dynamically (or per
interaction) through learnable modulations to best capture the unique
relational dynamics of each token pair, optimizing each pair-wise interaction
for that specific relationship. This representational plasticity of prime
attention enables effective extraction of relationship-specific information in
MTS while maintaining the same asymptotic computational complexity as standard
attention. Our results demonstrate that prime attention consistently
outperforms standard attention across benchmarks, achieving up to 6.5\%
improvement in forecasting accuracy. In addition, we find that prime attention
achieves comparable or superior performance using up to 40\% less sequence
length compared to standard attention, further demonstrating its superior
relational modeling capabilities.

</details>
