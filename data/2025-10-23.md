<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 5]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [SBAN: A Framework \& Multi-Dimensional Dataset for Large Language Model Pre-Training and Software Code Mining](https://arxiv.org/abs/2510.18936)
*Hamed Jelodar,Mohammad Meymani,Samita Bai,Roozbeh Razavi-Far,Ali A. Ghorbani*

Main category: cs.IR

TL;DR: SBAN是一个大规模多维数据集，包含300多万个样本，涵盖二进制代码、汇编指令、自然语言描述和源代码四个层次，用于推进LLM在软件代码分析中的预训练和评估。


<details>
  <summary>Details</summary>
Motivation: 为了推进大型语言模型在软件代码分析中的应用，需要能够支持跨表示学习、语义理解和自动化恶意软件检测的多模态数据集。

Method: 构建包含超过300万个样本的多维数据集，每个样本包含二进制代码、汇编指令、自然语言描述和源代码四个互补层次。

Result: 创建了SBAN数据集，包含290万个良性样本和67.2万个恶意软件样本，支持代码翻译、代码解释等软件挖掘任务。

Conclusion: SBAN通过桥接低级机器表示和高级人类语义，为构建智能代码推理系统提供了坚实基础，为软件行为挖掘和安全分析开辟了新机会。

Abstract: This paper introduces SBAN (Source code, Binary, Assembly, and Natural
Language Description), a large-scale, multi-dimensional dataset designed to
advance the pre-training and evaluation of large language models (LLMs) for
software code analysis. SBAN comprises more than 3 million samples, including
2.9 million benign and 672,000 malware respectively, each represented across
four complementary layers: binary code, assembly instructions, natural language
descriptions, and source code. This unique multimodal structure enables
research on cross-representation learning, semantic understanding of software,
and automated malware detection. Beyond security applications, SBAN supports
broader tasks such as code translation, code explanation, and other software
mining tasks involving heterogeneous data. It is particularly suited for
scalable training of deep models, including transformers and other LLM
architectures. By bridging low-level machine representations and high-level
human semantics, SBAN provides a robust foundation for building intelligent
systems that reason about code. We believe that this dataset opens new
opportunities for mining software behavior, improving security analytics, and
enhancing LLM capabilities in pre-training and fine-tuning tasks for software
code mining.

</details>


### [2] [XGen-Q: An Explainable Domain-Adaptive LLM Framework with Retrieval-Augmented Generation for Software Security](https://arxiv.org/abs/2510.19006)
*Hamed Jelodar,Mohammad Meymani,Roozbeh Razavi-Far,Ali A. Ghorbani*

Main category: cs.IR

TL;DR: XGen-Q是基于Qwen-Coder架构的领域适应LLM，专门用于恶意软件检测和分析，通过多阶段提示策略和检索增强生成技术，在复杂代码混淆情况下仍能提供可靠的恶意软件识别和详细取证报告。


<details>
  <summary>Details</summary>
Motivation: 现有恶意软件检测系统对混淆或未知威胁的泛化能力不足，需要更适应性强且可解释的模型来应对网络安全挑战。

Method: 基于Qwen-Coder架构构建领域适应LLM，在超过100万个恶意软件样本的大规模语料库上进行预训练，采用多阶段提示策略和检索增强生成(RAG)，并通过系统暴露不同混淆模式的训练流程增强泛化能力。

Result: XGen-Q相比竞争基线实现了显著更低的困惑度，并在新型恶意软件样本上表现出强大性能。

Conclusion: 基于LLM的方法在可解释和鲁棒的恶意软件分析方面具有广阔前景。

Abstract: Generative AI and large language models (LLMs) have shown strong capabilities
in code understanding, but their use in cybersecurity, particularly for malware
detection and analysis, remains limited. Existing detection systems often fail
to generalize to obfuscated or previously unseen threats, underscoring the need
for more adaptable and explainable models. To address this challenge, we
introduce XGen-Q, a domain-adapted LLM built on the Qwen-Coder architecture and
pretrained on a large-scale corpus of over one million malware samples,
spanning both source and assembly code. XGen-Q uses a multi-stage prompt
strategy combined with retrieval-augmented generation (RAG) to deliver reliable
malware identification and detailed forensic reporting, even in the presence of
complex code obfuscation. To further enhance generalization, we design a
training pipeline that systematically exposes the model to diverse obfuscation
patterns. Experimental results show that XGen-Q achieves significantly lower
perplexity than competitive baselines and exhibits strong performance on novel
malware samples, demonstrating the promise of LLM-based approaches for
interpretable and robust malware analysis.

</details>


### [3] [C2T-ID: Converting Semantic Codebooks to Textual Document Identifiers for Generative Search](https://arxiv.org/abs/2510.19221)
*Yingchen Zhang,Ruqing Zhang,Jiafeng Guo,Wenjun Peng,Sen Li,Fuyu Lv,Xueqi Cheng*

Main category: cs.IR

TL;DR: C2T-ID是一种新的文档标识符设计方法，通过层次聚类构建语义数字docid，然后用高频关键词替换数字标签，平衡语义表达能力和搜索空间约束。


<details>
  <summary>Details</summary>
Motivation: 解决生成式检索中语义信息丰富性与搜索空间可控性之间的权衡问题。数字标识符无法利用大语言模型的自然语言理解能力，而纯文本标识符会导致解码空间过大且对早期错误敏感。

Method: 1) 通过层次聚类构建语义数字docid；2) 提取高频元数据关键词，迭代地将数字标签替换为簇内top-K关键词；3) 可选的两级语义平滑步骤进一步提升C2T-ID的流畅性。

Result: 在Natural Questions和淘宝产品搜索上的实验表明，C2T-ID显著优于原子、语义码本和纯文本docid基线方法。

Conclusion: C2T-ID在平衡语义表达能力和搜索空间约束方面表现出色，为生成式检索提供了有效的文档标识符设计方案。

Abstract: Designing document identifiers (docids) that carry rich semantic information
while maintaining tractable search spaces is a important challenge in
generative retrieval (GR). Popular codebook methods address this by building a
hierarchical semantic tree and constraining generation to its child nodes, yet
their numeric identifiers cannot leverage the large language model's pretrained
natural language understanding. Conversely, using text as docid provides more
semantic expressivity but inflates the decoding space, making the system
brittle to early-step errors. To resolve this trade-off, we propose C2T-ID: (i)
first construct semantic numerical docid via hierarchical clustering; (ii) then
extract high-frequency metadata keywords and iteratively replace each numeric
label with its cluster's top-K keywords; and (iii) an optional two-level
semantic smoothing step further enhances the fluency of C2T-ID. Experiments on
Natural Questions and Taobao's product search demonstrate that C2T-ID
significantly outperforms atomic, semantic codebook, and pure-text docid
baselines, demonstrating its effectiveness in balancing semantic expressiveness
with search space constraints.

</details>


### [4] [CoRECT: A Framework for Evaluating Embedding Compression Techniques at Scale](https://arxiv.org/abs/2510.19340)
*L. Caspari,M. Dinzinger,K. Gosh Dastidar,C. Fellicious,J. Mitrović,M. Granitzer*

Main category: cs.IR

TL;DR: CoRECT框架用于大规模评估嵌入压缩方法，在100M段落上非学习压缩方法能显著减小索引大小且性能损失统计不显著，但最佳压缩方法选择仍具挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有研究常忽略语料库复杂性对密集检索性能的影响，需要系统评估嵌入压缩方法在不同语料条件下的表现。

Method: 引入CoRECT框架和新的数据集集合，对8种代表性压缩方法进行基准测试，评估其在大型语料上的压缩效果和检索质量。

Result: 非学习压缩方法在100M段落上能大幅减小索引大小，性能损失统计不显著，但不同模型间性能差异明显。

Conclusion: 压缩方法选择具有挑战性，需要CoRECT框架实现一致比较和明智选择，所有代码、数据和结果已开源。

Abstract: Dense retrieval systems have proven to be effective across various
benchmarks, but require substantial memory to store large search indices.
Recent advances in embedding compression show that index sizes can be greatly
reduced with minimal loss in ranking quality. However, existing studies often
overlook the role of corpus complexity -- a critical factor, as recent work
shows that both corpus size and document length strongly affect dense retrieval
performance. In this paper, we introduce CoRECT (Controlled Retrieval
Evaluation of Compression Techniques), a framework for large-scale evaluation
of embedding compression methods, supported by a newly curated dataset
collection. To demonstrate its utility, we benchmark eight representative types
of compression methods. Notably, we show that non-learned compression achieves
substantial index size reduction, even on up to 100M passages, with
statistically insignificant performance loss. However, selecting the optimal
compression method remains challenging, as performance varies across models.
Such variability highlights the necessity of CoRECT to enable consistent
comparison and informed selection of compression methods. All code, data, and
results are available on GitHub and HuggingFace.

</details>


### [5] [Top-P Masking for Cross Language Information Retrieval](https://arxiv.org/abs/2510.19758)
*Joseph Casale,Andrew Silverschotz,Joseph DeSimone*

Main category: cs.IR

TL;DR: 提出使用Top-P动态掩码替代Top-K掩码方案，在跨语言信息检索任务中表现更优


<details>
  <summary>Details</summary>
Motivation: Top-K掩码方案被提出作为促进信息检索任务中稀疏表示的简单方法，但现有算法如BLADE仅将其作为后处理阶段使用

Method: 采用类似大语言模型中核采样的Top-P动态掩码方法

Result: 在跨语言信息检索领域评估显示，Top-P动态掩码比Top-K掩码性能更好

Conclusion: Top-P动态掩码在跨语言信息检索任务中优于传统的Top-K掩码方案

Abstract: Top-K masking schemes have been proposed as a method to promote sparse
representations in Information Retrieval (IR) tasks, as a simple alternative to
Floating Point Operations per Second (FLOPS) regularization. Algorithms such as
Bilingual Lexical and Document Expansion Model (BLADE), adopt this approach as
a post-processing stage. We propose using Top-P Dynamic Masking similar to
Nucleus Sampling in Large Language Models, and demonstrate better performance
than Top-K masking. Specifically, we evaluate our methods in the domain of
Cross Language Information Retrieval (CLIR)

</details>
