<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 29]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Frequency-aware Adaptive Contrastive Learning for Sequential Recommendation](https://arxiv.org/abs/2601.17057)
*Zhikai Wang,Weihua Zhang*

Main category: cs.IR

TL;DR: FACL框架通过频率感知的自适应对比学习，解决序列推荐中数据增强对低频物品和稀疏用户行为的偏见问题，显著提升推荐准确性和长尾场景适用性。


<details>
  <summary>Details</summary>
Motivation: 研究发现序列推荐中的对比学习数据增强存在固有偏见，会损害低频物品和稀疏用户行为的完整性，导致推荐系统在长尾场景表现不佳。

Method: 提出FACL框架：1）微观层面的自适应扰动保护稀有物品完整性；2）宏观层面的重加权放大稀疏和稀有交互序列在训练中的影响力。

Result: 在五个公开基准数据集上，FACL持续优于最先进的数据增强和模型增强方法，推荐准确性提升最高达3.8%，显著缓解低频物品和用户的性能下降。

Conclusion: FACL框架具有强大的意图保持能力和对现实世界长尾推荐场景的优越适用性，为解决序列推荐中的频率偏见问题提供了有效方案。

Abstract: In this paper, we revisited the role of data augmentation in contrastive learning for sequential recommendation, revealing its inherent bias against low-frequency items and sparse user behaviors. To address this limitation, we proposed FACL, a frequency-aware adaptive contrastive learning framework that introduces micro-level adaptive perturbation to protect the integrity of rare items, as well as macro-level reweighting to amplify the influence of sparse and rare-interaction sequences during training. Comprehensive experiments on five public benchmark datasets demonstrated that FACL consistently outperforms state-of-the-art data augmentation and model augmentation-based methods, achieving up to 3.8% improvement in recommendation accuracy. Moreover, fine-grained analyses confirm that FACL significantly alleviates the performance drop on low-frequency items and users, highlighting its robust intent-preserving ability and its superior applicability to real-world, long-tail recommendation scenarios.

</details>


### [2] [Evaluation on Entity Matching in Recommender Systems](https://arxiv.org/abs/2601.17218)
*Zihan Huang,Rohan Surana,Zhouhang Xie,Junda Wu,Yu Xia,Julian McAuley*

Main category: cs.IR

TL;DR: 本文提出了Reddit-Amazon-EM数据集，用于评估推荐系统中的跨数据集实体匹配方法，填补了该领域缺乏严格评估框架的空白。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏用于跨数据集实体匹配的严格评估框架，这阻碍了LLM驱动的对话推荐和知识基础数据集构建等领域的进展。特别是在推荐系统（如对话推荐系统和基于知识的推荐系统）中，实体匹配是重要组件，但缺乏统一的评估标准。

Method: 1. 构建Reddit-Amazon-EM数据集：包含来自Reddit和Amazon'23数据集的自然出现项目；2. 通过人工标注识别Reddit-Movies和Amazon'23中对应的电影实体；3. 使用该数据集全面评估多种实体匹配方法：基于规则、基于图、基于词法、基于嵌入和基于LLM的方法。

Result: 1. 创建了包含人工标注实体匹配黄金标准的数据集；2. 通过实验确定了最佳性能的实体匹配方法；3. 提供了两个数据集之间的映射关系，作为未来研究的宝贵资源。

Conclusion: Reddit-Amazon-EM数据集填补了推荐系统实体匹配评估的空白，为未来研究提供了可复现的基础。通过发布人工标注的黄金标准和数据集映射，该工作将促进实体匹配方法在推荐系统中的进一步发展。

Abstract: Entity matching is a crucial component in various recommender systems, including conversational recommender systems (CRS) and knowledge-based recommender systems. However, the lack of rigorous evaluation frameworks for cross-dataset entity matching impedes progress in areas such as LLM-driven conversational recommendations and knowledge-grounded dataset construction.
  In this paper, we introduce Reddit-Amazon-EM, a novel dataset comprising naturally occurring items from Reddit and the Amazon '23 dataset. Through careful manual annotation, we identify corresponding movies across Reddit-Movies and Amazon'23, two existing recommender system datasets with inherently overlapping catalogs. Leveraging Reddit-Amazon-EM, we conduct a comprehensive evaluation of state-of-the-art entity matching methods, including rule-based, graph-based, lexical-based, embedding-based, and LLM-based approaches.
  For reproducible research, we release our manually annotated entity matching gold set and provide the mapping between the two datasets using the best-performing method from our experiments. This serves as a valuable resource for advancing future work on entity matching in recommender systems.

</details>


### [3] [FinMetaMind: A Tech Blueprint on NLQ Systems for Financial Knowledge Search](https://arxiv.org/abs/2601.17333)
*Lalit Pant,Shivang Nagar*

Main category: cs.IR

TL;DR: 本文提出了一个针对金融知识搜索的现代自然语言查询系统技术蓝图，通过结合NLP、搜索工程和向量数据模型，解决了金融数据检索中的发现、相关性排序、数据新鲜度和实体识别等关键挑战。


<details>
  <summary>Details</summary>
Motivation: 传统金融知识搜索方法存在精度和召回率不足的问题，且难以有效连接分散的金融对象、事件和关系。自然语言查询能够使用人类自然语言与信息系统交互，有望显著提升金融知识搜索的效果和深度洞察能力。

Method: 结合自然语言处理、搜索工程和向量数据模型的核心构建模块，设计了包含离线索引和在线检索的架构组件。详细阐述了金融数据集和文档的独特需求，并提供了实验方法论、数据使用和结果分析。

Result: 提出的NLQ系统相比传统方法在金融知识搜索中提高了精度和召回率，能够更有效地连接不同的金融对象、事件和关系，为金融服务中的增强知识搜索提供了实际应用案例。

Conclusion: 该研究为金融领域的自然语言查询系统提供了全面的技术蓝图和理论支持，展示了NLQ在提升金融知识搜索效果方面的潜力，并指出了未来优化的方向。

Abstract: Natural Language Query (NLQ) allows users to search and interact with information systems using plain, human language instead of structured query syntax. This paper presents a technical blueprint on the design of a modern NLQ system tailored to financial knowledge search. The introduction of NLQ not only enhances the precision and recall of the knowledge search compared to traditional methods, but also facilitates deeper insights by efficiently linking disparate financial objects, events, and relationships. Using core constructs from natural language processing, search engineering, and vector data models, the proposed system aims to address key challenges in discovering, relevance ranking, data freshness, and entity recognition intrinsic to financial data retrieval. In this work, we detail the unique requirements of NLQ for financial datasets and documents, outline the architectural components for offline indexing and online retrieval, and discuss the real-world use cases of enhanced knowledge search in financial services. We delve into the theoretical underpinnings and experimental evidence supporting our proposed architecture, ultimately providing a comprehensive analysis on the subject matter. We also provide a detailed elaboration of our experimental methodology, the data used, the results and future optimizations in this study.

</details>


### [4] [Beyond Correlations: A Downstream Evaluation Framework for Query Performance Prediction](https://arxiv.org/abs/2601.17339)
*Payel Santra,Partha Basuchowdhuri,Debasis Ganguly*

Main category: cs.IR

TL;DR: 本文提出一个面向下游应用的QPP评估框架，通过将QPP估计分布作为IR融合的先验来评估预测器质量，发现下游效果与标准相关性评估不匹配。


<details>
  <summary>Details</summary>
Motivation: 传统QPP评估仅测量集合层面的相关性，无法量化单个查询的预测效果，也无法连接到下游应用。高相关性的QPP方法在实际IR管道中可能无法有效支持查询特定决策。

Method: 提出下游聚焦的评估框架：将多个排序器检索到的top文档列表中的QPP估计分布作为IR融合的先验。一方面，这些估计分布与真实检索质量分布的匹配度反映预测器质量；另一方面，作为先验使用表明预测器在IR管道中做出明智决策的能力。

Result: 实验表明：1）QPP估计在加权IR融合中非常重要，相比未加权的CombSUM和RRF融合策略有超过4.5%的显著改进；2）QPP的下游效果与标准相关性评估不相关，揭示了新的见解。

Conclusion: 需要重新思考QPP评估方法，下游应用导向的评估框架能更好地反映QPP方法在实际IR系统中的实用价值，而传统相关性评估可能误导方法选择。

Abstract: The standard practice of query performance prediction (QPP) evaluation is to measure a set-level correlation between the estimated retrieval qualities and the true ones. However, neither this correlation-based evaluation measure quantifies QPP effectiveness at the level of individual queries, nor does this connect to a downstream application, meaning that QPP methods yielding high correlation values may not find a practical application in query-specific decisions in an IR pipeline. In this paper, we propose a downstream-focussed evaluation framework where a distribution of QPP estimates across a list of top-documents retrieved with several rankers is used as priors for IR fusion. While on the one hand, a distribution of these estimates closely matching that of the true retrieval qualities indicates the quality of the predictor, their usage as priors on the other hand indicates a predictor's ability to make informed choices in an IR pipeline. Our experiments firstly establish the importance of QPP estimates in weighted IR fusion, yielding substantial improvements of over 4.5% over unweighted CombSUM and RRF fusion strategies, and secondly, reveal new insights that the downstream effectiveness of QPP does not correlate well with the standard correlation-based QPP evaluation.

</details>


### [5] [Breaking Flat: A Generalised Query Performance Prediction Evaluation Framework](https://arxiv.org/abs/2601.17359)
*Payel Santra,Partha Basuchowdhuri,Debasis Ganguly*

Main category: cs.IR

TL;DR: 本文提出了查询性能预测(QPP)的三个任务设置：单排序器多查询(SRMQ-PP)、多排序器单查询(MRSQ-PP)和多排序器多查询(MRMQ-PP)，发现不同任务中QPP模型效果差异显著，且预测最佳排序器比预测查询难度更具挑战性。


<details>
  <summary>Details</summary>
Motivation: 传统QPP主要用于评估单个排序模型在不同查询上的表现，但更细粒度的扩展任务——为给定查询选择最有效的排序模型——更具挑战性。本文旨在将QPP任务及其评估推广到更全面的设置。

Method: 将QPP任务形式化为三个设置：(1) SRMQ-PP：标准用例，评估单个排序器在多个查询上的性能预测；(2) MRSQ-PP：评估QPP模型为单个查询选择最有效排序器的能力；(3) MRMQ-PP：同时考虑所有查询-排序器对的联合预测。

Result: 研究发现：(a) QPP模型的相对有效性在不同任务(SRMQ-PP vs MRSQ-PP)间存在显著差异；(b) 为查询预测最佳排序器比预测给定排序器下查询的相对难度要困难得多。

Conclusion: QPP任务需要根据具体应用场景进行区分，不同设置下的模型性能不可直接比较。为查询选择最佳排序器是一个更具挑战性的问题，需要专门的研究和评估方法。

Abstract: The traditional use-case of query performance prediction (QPP) is to identify which queries perform well and which perform poorly for a given ranking model. A more fine-grained and arguably more challenging extension of this task is to determine which ranking models are most effective for a given query. In this work, we generalize the QPP task and its evaluation into three settings: (i) SingleRanker MultiQuery (SRMQ-PP), corresponding to the standard use case; (ii) MultiRanker SingleQuery (MRSQ-PP), which evaluates a QPP model's ability to select the most effective ranker for a query; and (iii) MultiRanker MultiQuery (MRMQ-PP), which considers predictions jointly across all query ranker pairs. Our results show that (a) the relative effectiveness of QPP models varies substantially across tasks (SRMQ-PP vs. MRSQ-PP), and (b) predicting the best ranker for a query is considerably more difficult than predicting the relative difficulty of queries for a given ranker.

</details>


### [6] [UniGRec: Unified Generative Recommendation with Soft Identifiers for End-to-End Optimization](https://arxiv.org/abs/2601.17438)
*Jialei Li,Yang Zhang,Yimeng Bai,Shuai Zhu,Ziqi Xue,Xiaoyan Zhao,Dingxian Wang,Frank Yang,Andrew Rabinovich,Xiangnan He*

Main category: cs.IR

TL;DR: UniGRec是一个统一的生成式推荐框架，通过可微软项目标识符将分词器和推荐器统一在最终推荐目标下，解决了训练-推理差异、标识符崩溃和协同信号不足三大挑战。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐方法通常将分词器与推荐器解耦或依赖异步交替优化，限制了端到端对齐。为了解决这一问题，需要将分词器和推荐器统一在最终推荐目标下进行联合端到端训练。

Method: 提出UniGRec框架：1) 使用退火推理对齐平滑连接软训练和硬推理；2) 采用码字均匀正则化防止标识符崩溃并促进码本多样性；3) 设计双重协同蒸馏机制，从轻量级教师模型中提取协同先验来共同指导分词器和推荐器。

Result: 在真实世界数据集上的广泛实验表明，UniGRec始终优于最先进的基线方法。

Conclusion: UniGRec通过统一的分词器和推荐器端到端训练，有效解决了生成式推荐中的关键挑战，实现了更好的推荐性能。

Abstract: Generative recommendation has recently emerged as a transformative paradigm that directly generates target items, surpassing traditional cascaded approaches. It typically involves two components: a tokenizer that learns item identifiers and a recommender trained on them. Existing methods often decouple tokenization from recommendation or rely on asynchronous alternating optimization, limiting full end-to-end alignment. To address this, we unify the tokenizer and recommender under the ultimate recommendation objective via differentiable soft item identifiers, enabling joint end-to-end training. However, this introduces three challenges: training-inference discrepancy due to soft-to-hard mismatch, item identifier collapse from codeword usage imbalance, and collaborative signal deficiency due to an overemphasis on fine-grained token-level semantics.
  To tackle these challenges, we propose UniGRec, a unified generative recommendation framework that addresses them from three perspectives. UniGRec employs Annealed Inference Alignment during tokenization to smoothly bridge soft training and hard inference, a Codeword Uniformity Regularization to prevent identifier collapse and encourage codebook diversity, and a Dual Collaborative Distillation mechanism that distills collaborative priors from a lightweight teacher model to jointly guide both the tokenizer and the recommender. Extensive experiments on real-world datasets demonstrate that UniGRec consistently outperforms state-of-the-art baseline methods. Our codes are available at https://github.com/Jialei-03/UniGRec.

</details>


### [7] [Adversarial Alignment and Disentanglement for Cross-Domain CTR Prediction with Domain-Encompassing Features](https://arxiv.org/abs/2601.17472)
*Junyou He,Lixi Deng,Huichao Guo,Ye Tang,Yong Li,Sulong Xu*

Main category: cs.IR

TL;DR: 提出A²DCDR模型，通过对抗对齐和特征解耦改进跨域推荐，结合域不变特征、非对齐特征和原始上下文数据，提升推荐性能


<details>
  <summary>Details</summary>
Motivation: 现有跨域推荐方法主要依赖域不变特征结合目标域特定特征，忽略了有价值的非对齐特征，导致性能受限

Method: 1) 使用对抗训练改进MMD以增强泛化能力；2) 特征解耦器和重建机制实现域内解耦；3) 融合域不变特征、非对齐特征和原始上下文数据的新表示方法

Result: 在真实数据集和在线A/B测试中，A²DCDR模型优于现有方法，证明了其有效性和实际应用价值

Conclusion: A²DCDR模型通过全面捕捉跨域信息（包括域不变和有价值的非对齐特征），显著提升了跨域推荐性能，具有实际应用潜力

Abstract: Cross-domain recommendation (CDR) has been increasingly explored to address data sparsity and cold-start issues. However, recent approaches typically disentangle domain-invariant features shared between source and target domains, as well as domain-specific features for each domain. However, they often rely solely on domain-invariant features combined with target domain-specific features, which can lead to suboptimal performance. To overcome the limitations, this paper presents the Adversarial Alignment and Disentanglement Cross-Domain Recommendation ($A^2DCDR$ ) model, an innovative approach designed to capture a comprehensive range of cross-domain information, including both domain-invariant and valuable non-aligned features. The $A^2DCDR$ model enhances cross-domain recommendation through three key components: refining MMD with adversarial training for better generalization, employing a feature disentangler and reconstruction mechanism for intra-domain disentanglement, and introducing a novel fused representation combining domain-invariant, non-aligned features with original contextual data. Experiments on real-world datasets and online A/B testing show that $A^2DCDR$ outperforms existing methods, confirming its effectiveness and practical applicability. The code is provided at https://github.com/youzi0925/A-2DCDR/tree/main.

</details>


### [8] [Towards Fair Large Language Model-based Recommender Systems without Costly Retraining](https://arxiv.org/abs/2601.17492)
*Jin Li,Huilin Gu,Shoujin Wang,Qi Zhang,Shui Yu,Chen Wang,Xiwei Xu,Fang Chen*

Main category: cs.IR

TL;DR: FUDLR提出了一种快速统一的LLM推荐系统去偏方法，将去偏问题重新定义为高效的机器遗忘任务，通过偏置无关掩码识别需要遗忘的样本，并高效移除其对模型参数的影响。


<details>
  <summary>Details</summary>
Motivation: LLM推荐系统容易延续训练数据中的偏置，导致严重公平性问题。现有去偏方法面临两大挑战：1) 针对特定偏置类型设计，缺乏处理多样或新兴偏置的通用性；2) 依赖重训练，对大规模LLM计算不可行。

Method: FUDLR采用两阶段方法：1) 通过偏置无关掩码识别需要遗忘的偏置诱导样本，平衡公平性提升与准确性保持；2) 高效估计并移除这些样本对模型参数的影响，实现去偏。

Result: 大量实验表明，FUDLR能有效且高效地提升公平性，同时保持推荐准确性，为构建社会责任的LLM推荐系统提供了实用路径。

Conclusion: FUDLR通过将去偏问题重新定义为机器遗忘任务，解决了LLM推荐系统中的公平性问题，提供了一种通用、高效的去偏方法，适应各种或共存的偏置类型。

Abstract: Large Language Models (LLMs) have revolutionized Recommender Systems (RS) through advanced generative user modeling. However, LLM-based RS (LLM-RS) often inadvertently perpetuates bias present in the training data, leading to severe fairness issues. Addressing these fairness problems in LLM-RS faces two significant challenges. 1) Existing debiasing methods, designed for specific bias types, lack the generality to handle diverse or emerging biases in real-world applications. 2) Debiasing methods relying on retraining are computationally infeasible given the massive parameter scale of LLMs. To overcome these challenges, we propose FUDLR (Fast Unified Debiasing for LLM-RS). The core idea is to reformulate the debiasing problem as an efficient machine unlearning task with two stages. First, FUDLR identifies bias-inducing samples to unlearn through a novel bias-agnostic mask, optimized to balance fairness improvement with accuracy preservation. Its bias-agnostic design allows adaptability to various or co-existing biases simply by incorporating different fairness metrics. Second, FUDLR performs efficient debiasing by estimating and removing the influence of identified samples on model parameters. Extensive experiments demonstrate that FUDLR effectively and efficiently improves fairness while preserving recommendation accuracy, offering a practical path toward socially responsible LLM-RS. The code and data are available at https://github.com/JinLi-i/FUDLR.

</details>


### [9] [To Case or Not to Case: An Empirical Study in Learned Sparse Retrieval](https://arxiv.org/abs/2601.17500)
*Emmanouil Georgios Lionis,Jia-Huei Ju,Angelos Nalmpantis,Casper Thuis,Sean MacAvaney,Andrew Yates*

Main category: cs.IR

TL;DR: 研究发现在LSR中使用cased模型时性能显著下降，但通过文本小写化预处理可以消除这一差距，使cased模型表现得像uncased模型


<details>
  <summary>Details</summary>
Motivation: 当前最先进的LSR方法几乎完全依赖uncased骨干模型，但最新的SOTA语言模型只有cased版本。骨干模型的大小写对LSR的影响尚未研究，这可能威胁到该方法的未来发展

Method: 系统评估配对的cased和uncased版本的相同骨干模型在多个数据集上的表现，通过文本小写化预处理，并进行token级别的分析

Result: 默认情况下，使用cased骨干模型的LSR模型性能显著低于uncased版本；但通过小写化预处理可以完全消除这一性能差距；token分析显示小写化后cased模型几乎完全抑制了大小写敏感的词汇项，表现得像uncased模型

Conclusion: 这一结果扩展了最新cased模型在LSR设置中的适用性，并促进了更强骨干架构与稀疏检索的集成，通过简单的小写化预处理即可实现

Abstract: Learned Sparse Retrieval (LSR) methods construct sparse lexical representations of queries and documents that can be efficiently searched using inverted indexes. Existing LSR approaches have relied almost exclusively on uncased backbone models, whose vocabularies exclude case-sensitive distinctions, thereby reducing vocabulary mismatch. However, the most recent state-of-the-art language models are only available in cased versions. Despite this shift, the impact of backbone model casing on LSR has not been studied, potentially posing a risk to the viability of the method going forward. To fill this gap, we systematically evaluate paired cased and uncased versions of the same backbone models across multiple datasets to assess their suitability for LSR. Our findings show that LSR models with cased backbone models by default perform substantially worse than their uncased counterparts; however, this gap can be eliminated by pre-processing the text to lowercase. Moreover, our token-level analysis reveals that, under lowercasing, cased models almost entirely suppress cased vocabulary items and behave effectively as uncased models, explaining their restored performance. This result broadens the applicability of recent cased models to the LSR setting and facilitates the integration of stronger backbone architectures into sparse retrieval. The complete code and implementation for this project are available at: https://github.com/lionisakis/Uncased-vs-cased-models-in-LSR

</details>


### [10] [Pipeline Inspection, Visualization, and Interoperability in PyTerrier](https://arxiv.org/abs/2601.17502)
*Emmanouil Georgios Lionis,Craig Macdonald,Sean MacAvaney*

Main category: cs.IR

TL;DR: PyTerrier是一个声明式信息检索框架，新增了可编程检查、可视化和工具集成功能，旨在让研究人员、学生和AI代理更容易理解和使用各种IR管道。


<details>
  <summary>Details</summary>
Motivation: 当前信息检索管道通常难以理解和调试，研究人员、学生和AI代理在理解和使用各种IR管道时面临挑战。需要提高IR管道的可检查性、可视化能力和工具集成性。

Method: 通过PyTerrier声明式框架，引入新的管道操作，支持程序化检查、可视化，并通过模型上下文协议（MCP）与其他工具集成。

Result: 开发了改进的管道操作功能，使IR管道能够被程序化检查、可视化，并更好地与其他工具集成，增强了框架的可用性和可理解性。

Conclusion: PyTerrier的新功能显著提高了IR管道的可理解性和可用性，使研究人员、学生和AI代理能够更轻松地构建、实验和使用各种信息检索管道。

Abstract: PyTerrier provides a declarative framework for building and experimenting with Information Retrieval (IR) pipelines. In this demonstration, we highlight several recent pipeline operations that improve their ability to be programmatically inspected, visualized, and integrated with other tools (via the Model Context Protocol, MCP). These capabilities aim to make it easier for researchers, students, and AI agents to understand and use a wide array of IR pipelines.

</details>


### [11] [Real-Time Trend Prediction via Continually-Aligned LLM Query Generation](https://arxiv.org/abs/2601.17567)
*Zijing Hui,Wenhan Lyu,Shusen Wang,Li Chen,Chu Wang*

Main category: cs.IR

TL;DR: RTTP是一个实时趋势预测框架，通过LLM直接从新闻内容生成搜索查询，解决了低流量搜索环境中的冷启动问题，在Facebook和Meta AI产品中实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 低流量搜索环境中的趋势检测面临冷启动问题，现有基于关键词频率或查询峰值的方法在稀疏设置中反应缓慢且效果不佳，无法及时识别新兴或长尾趋势。

Method: RTTP采用持续学习LLM将新闻帖子转换为搜索式查询，并使用参与度强度和创作者权威性进行评分；提出Mix-Policy DPO方法，结合on-policy稳定性和off-policy新颖性，缓解模型升级时的灾难性遗忘。

Result: 在Facebook和Meta AI产品中部署，RTTP在尾部趋势检测精度@500上提升了91.4%，查询生成准确率比行业基线提高了19%，并在多周在线训练后保持稳定性能。

Conclusion: 研究表明，当LLM生成的合成搜索信号经过对齐和持续更新时，能够在低流量搜索环境中实现及时的趋势理解，解决了传统方法的延迟问题。

Abstract: Trending news detection in low-traffic search environments faces a fundamental cold-start problem, where a lack of query volume prevents systems from identifying emerging or long-tail trends. Existing methods relying on keyword frequency or query spikes are inherently slow and ineffective in these sparse settings, lagging behind real-world shifts in attention. We introduce RTTP, a novel Real-Time Trending Prediction framework that generates search queries directly from news content instead of waiting for users to issue them. RTTP leverages a continual learning LLM (CL-LLM) that converts posts into search-style queries and scores them using engagement strength + creator authority, enabling early trend surfacing before search volume forms. To ensure adaptation without degrading reasoning, we propose Mix-Policy DPO, a new preference-based continual learning approach that combines on-policy stability with off-policy novelty to mitigate catastrophic forgetting during model upgrades. Deployed at production scale on Facebook and Meta AI products, RTTP delivers +91.4% improvement in tail-trend detection precision@500 and +19% query generation accuracy over industry baselines, while sustaining stable performance after multi-week online training. This work demonstrates that LLM-generated synthetic search signals, when aligned and continually updated, unlock timely trend understanding in low-traffic search environments.

</details>


### [12] [Why They Link: An Intent Taxonomy for Including Hyperlinks in Social Posts](https://arxiv.org/abs/2601.17601)
*Fangping Lan,Abdullah Aljebreen,Eduard C. Dragut*

Main category: cs.IR

TL;DR: 该研究开发了一个社交媒体中URL意图分类系统，通过众包标注和LLM辅助构建了包含6个顶层类别和26个细粒度意图的完整分类体系，并应用于1000条推文分析。


<details>
  <summary>Details</summary>
Motivation: 社交媒体中URL链接普遍存在（约20%推文包含URL），但现有研究主要关注发布者意图，难以实际观察。为支持下游应用，本研究转向读者视角，探索用户如何理解帖子中链接的意图。

Method: 采用混合方法：首先通过大规模众包标注进行自下而上的数据驱动过程，然后利用大语言模型辅助生成描述性类别名称和精确定义，最终构建包含6个顶层类别和26个细粒度意图的分类体系。

Result: 构建了完整的URL意图分类体系，应用于1000条用户帖子标注分析，发现广告、争论和分享是最普遍的意图。广告意图占比最高，其次是争论和分享。

Conclusion: 该分类体系为意图感知的信息检索和NLP应用提供了基础，能够更准确地检索、推荐和理解社交媒体内容，支持下游应用开发。

Abstract: URLs serve as bridges between social media platforms and the broader web, linking user-generated content to external information resources. On Twitter (X), approximately one in five tweets contains at least one URL, underscoring their central role in information dissemination. While prior studies have examined the motivations of authors who share URLs, such author-centered intentions are difficult to observe in practice. To enable broader downstream use, this work investigates reader-centered interpretations, i.e., how users perceive the intentions behind hyperlinks included in posts. We develop an intent taxonomy for including hyperlinks in social posts through a hybrid approach that begins with a bottom-up, data-driven process using large-scale crowdsourced annotations, and is then refined using large language model assistance to generate descriptive category names and precise definitions. The final taxonomy comprises 6 top-level categories and 26 fine-grained intention classes, capturing diverse communicative purposes. Applying this taxonomy, we annotate and analyze 1000 user posts, revealing that advertising, arguing, and sharing are the most prevalent intentions. This resulting taxonomy provides a foundation for intent-aware information retrieval and NLP applications, enabling more accurate retrieval, recommendation, and understanding of social media content.

</details>


### [13] [Agentic Search in the Wild: Intents and Trajectory Dynamics from 14M+ Real Search Requests](https://arxiv.org/abs/2601.17617)
*Jingjie Ning,João Coelho,Yibo Kong,Yunfan Long,Bruno Martins,João Magalhães,Jamie Callan,Chenyan Xiong*

Main category: cs.IR

TL;DR: 大规模日志分析揭示LLM搜索代理的行为模式：90%多轮会话不超过10步，89%步骤间隔小于1分钟；不同意图行为差异大，事实查询重复率高，推理任务探索更广；54%新查询词可追溯到先前证据，支持跨步骤上下文追踪。


<details>
  <summary>Details</summary>
Motivation: 随着LLM驱动的搜索代理在多步信息检索任务中广泛应用，IR社区缺乏对代理搜索会话如何展开以及检索证据如何使用的实证理解。本文旨在通过大规模日志分析填补这一空白。

Method: 基于DeepResearchGym开源搜索API收集的1444万搜索请求（397万会话）进行大规模日志分析。使用会话化处理、基于LLM的会话级意图标注和步骤级查询重构标签，并提出上下文驱动的术语采纳率（CTAR）来量化新查询词是否可追溯到先前检索证据。

Result: 发现三个主要行为模式：1）90%多轮会话不超过10步，89%步骤间隔小于1分钟；2）不同意图行为差异显著，事实查询会话重复率高且随时间增加，推理任务会话维持更广泛探索；3）代理跨步骤重用证据，平均54%新查询词出现在累积证据上下文中，早期步骤贡献超出最近检索。

Conclusion: 代理搜索可能受益于重复感知的早期停止、意图自适应的检索预算和显式的跨步骤上下文追踪。研究计划发布匿名化日志以支持未来研究。

Abstract: LLM-powered search agents are increasingly being used for multi-step information seeking tasks, yet the IR community lacks empirical understanding of how agentic search sessions unfold and how retrieved evidence is used. This paper presents a large-scale log analysis of agentic search based on 14.44M search requests (3.97M sessions) collected from DeepResearchGym, i.e. an open-source search API accessed by external agentic clients. We sessionize the logs, assign session-level intents and step-wise query-reformulation labels using LLM-based annotation, and propose Context-driven Term Adoption Rate (CTAR) to quantify whether newly introduced query terms are traceable to previously retrieved evidence. Our analyses reveal distinctive behavioral patterns. First, over 90% of multi-turn sessions contain at most ten steps, and 89% of inter-step intervals fall under one minute. Second, behavior varies by intent. Fact-seeking sessions exhibit high repetition that increases over time, while sessions requiring reasoning sustain broader exploration. Third, agents reuse evidence across steps. On average, 54% of newly introduced query terms appear in the accumulated evidence context, with contributions from earlier steps beyond the most recent retrieval. The findings suggest that agentic search may benefit from repetition-aware early stopping, intent-adaptive retrieval budgets, and explicit cross-step context tracking. We plan to release the anonymized logs to support future research.

</details>


### [14] [LegalMALR:Multi-Agent Query Understanding and LLM-Based Reranking for Chinese Statute Retrieval](https://arxiv.org/abs/2601.17692)
*Yunhan Li,Mingjie Xie,Gaoli Kang,Zihan Gong,Gengshen Wu,Min Yang*

Main category: cs.IR

TL;DR: LegalMALR：结合多智能体查询理解系统与零样本大语言模型重排序的法律法规检索框架，显著提升对隐式、多问题、口语化法律查询的检索效果


<details>
  <summary>Details</summary>
Motivation: 现实世界中的法律查询往往是隐式、多问题、口语化或未充分指定的，这使得传统检索增强生成管道难以准确恢复所需的法律要素。密集检索器主要关注查询的字面形式，而轻量级重排序器缺乏评估法律适用性所需的法律推理能力。

Method: 提出LegalMALR框架：1）多智能体查询理解系统（MAS）生成多样化的法律基础重写并进行迭代密集检索以扩大候选覆盖；2）使用广义强化策略优化（GRPO）统一优化MAS策略以稳定LLM生成重写的随机行为；3）LLM重排序模块执行自然语言法律推理生成最终排序。构建了包含118个困难中文法律查询的CSAID数据集。

Result: 在CSAID数据集和公开STARD基准测试中，LegalMALR在分布内和分布外设置下均显著优于强大的检索增强生成基线，证明了结合多视角查询解释、基于强化的策略优化和大模型重排序的有效性。

Conclusion: LegalMALR通过整合多智能体查询理解、强化策略优化和LLM重排序，有效解决了法律检索中查询表达不明确的问题，为法律辅助和司法决策支持提供了更准确的法条检索方案。

Abstract: Statute retrieval is essential for legal assistance and judicial decision support, yet real-world legal queries are often implicit, multi-issue, and expressed in colloquial or underspecified forms. These characteristics make it difficult for conventional retrieval-augmented generation pipelines to recover the statutory elements required for accurate retrieval. Dense retrievers focus primarily on the literal surface form of the query, whereas lightweight rerankers lack the legal-reasoning capacity needed to assess statutory applicability. We present LegalMALR, a retrieval framework that integrates a Multi-Agent Query Understanding System (MAS) with a zero-shot large-language-model-based reranking module (LLM Reranker). MAS generates diverse, legally grounded reformulations and conducts iterative dense retrieval to broaden candidate coverage. To stabilise the stochastic behaviour of LLM-generated rewrites, we optimise a unified MAS policy using Generalized Reinforcement Policy Optimization(GRPO). The accumulated candidate set is subsequently evaluated by the LLM Reranker, which performs natural-language legal reasoning to produce the final ranking. We further construct CSAID, a dataset of 118 difficult Chinese legal queries annotated with multiple statutory labels, and evaluate LegalMALR on both CSAID and the public STARD benchmark. Experiments show that LegalMALR substantially outperforms strong Retrieval-augmented generation(RAG) baselines in both in-distribution and out-of-distribution settings, demonstrating the effectiveness of combining multi-perspective query interpretation, reinforcement-based policy optimisation, and large-model reranking for statute retrieval.

</details>


### [15] [Token-Weighted Multi-Target Learning for Generative Recommenders with Curriculum Learning](https://arxiv.org/abs/2601.17787)
*Wei-Ning Chiu,Chuan-Ju Wang,Pu-Jen Cheng*

Main category: cs.IR

TL;DR: 提出两种基于信息增益的token加权策略（Front-Greater Weighting和Frequency Weighting）和课程学习多目标框架，用于提升基于语义ID的生成式推荐系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐系统使用标准的下一个token似然优化，将所有token视为同等重要，这与基于语义ID的生成任务不匹配。需要针对语义ID推荐的特点设计专门的token加权策略。

Method: 1. Front-Greater Weighting：基于条件语义信息增益，优先加权早期token（能有效减少候选项目不确定性，编码粗粒度语义）。2. Frequency Weighting：基于边际信息增益，对稀有token进行加权以抵消流行度偏差。3. 结合课程学习的多目标学习框架，联合优化两个token加权目标和标准似然。

Result: 在基准数据集上的广泛实验表明，该方法一致优于强基线方法和现有token加权方法，具有更好的鲁棒性，在不同语义ID构建方式上表现出强泛化能力，在头部和尾部项目上都获得显著提升。

Conclusion: 提出的基于信息增益的token加权策略和多目标学习框架有效解决了语义ID生成式推荐中的token重要性不平衡问题，显著提升了推荐性能。

Abstract: Generative recommender systems have recently attracted attention by formulating next-item prediction as an autoregressive sequence generation task. However, most existing methods optimize standard next-token likelihood and implicitly treat all tokens as equally informative, which is misaligned with semantic-ID-based generation. Accordingly, we propose two complementary information-gain-based token-weighting strategies tailored to generative recommendation with semantic IDs. Front-Greater Weighting captures conditional semantic information gain by prioritizing early tokens that most effectively reduce candidate-item uncertainty given their prefixes and encode coarse semantics. Frequency Weighting models marginal information gain under long-tailed item and token distributions, upweighting rare tokens to counteract popularity bias. Beyond individual strategies, we introduce a multi-target learning framework with curriculum learning that jointly optimizes the two token-weighted objectives alongside standard likelihood, enabling stable optimization and adaptive emphasis across training stages. Extensive experiments on benchmark datasets show that our method consistently outperforms strong baselines and existing token-weighting approaches, with improved robustness, strong generalization across different semantic-ID constructions, and substantial gains on both head and tail items. Code is available at https://github.com/CHIUWEINING/Token-Weighted-Multi-Target-Learning-for-Generative-Recommenders-with-Curriculum-Learning.

</details>


### [16] [Unleashing the Potential of Sparse Attention on Long-term Behaviors for CTR Prediction](https://arxiv.org/abs/2601.17836)
*Weijiang Lai,Beihong Jin,Di Zhang,Siru Chen,Jiongyan Zhang,Yuhang Gou,Jian Dong,Xingxing Wang*

Main category: cs.IR

TL;DR: SparseCTR：针对用户长行为序列的高效推荐模型，通过个性化分块和三分支稀疏注意力机制，在提升效率的同时实现性能改进并展现缩放定律现象。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的缩放定律难以在工业推荐系统中部署，因为标准自注意力机制计算复杂度高，而现有的稀疏注意力机制不适合推荐场景。用户行为具有个性化和时序特性，不同用户行为模式差异大且随时间变化，数据分布与其他领域显著不同。

Method: 1. 个性化分块：将行为序列按个性化方式分块，避免分离连续行为并支持并行处理；2. 三分支稀疏自注意力机制：联合识别用户的全局兴趣、兴趣转移和短期兴趣；3. 复合相对时序编码：通过可学习的头特定偏置系数，更好地捕捉用户行为的序列和周期性关系。

Result: 实验结果显示SparseCTR不仅提高效率，而且优于最先进方法。更重要的是，它展现出明显的缩放定律现象，在三个数量级的FLOPs范围内保持性能改进。在线A/B测试中，CTR提升1.72%，CPM提升1.41%。

Conclusion: SparseCTR是针对用户长行为序列的高效有效模型，通过个性化分块和三分支稀疏注意力机制解决了推荐系统中的计算复杂度和个性化时序特性问题，在实际应用中表现出色并展现出缩放定律特性。

Abstract: In recent years, the success of large language models (LLMs) has driven the exploration of scaling laws in recommender systems. However, models that demonstrate scaling laws are actually challenging to deploy in industrial settings for modeling long sequences of user behaviors, due to the high computational complexity of the standard self-attention mechanism. Despite various sparse self-attention mechanisms proposed in other fields, they are not fully suited for recommendation scenarios. This is because user behaviors exhibit personalization and temporal characteristics: different users have distinct behavior patterns, and these patterns change over time, with data from these users differing significantly from data in other fields in terms of distribution. To address these challenges, we propose SparseCTR, an efficient and effective model specifically designed for long-term behaviors of users. To be precise, we first segment behavior sequences into chunks in a personalized manner to avoid separating continuous behaviors and enable parallel processing of sequences. Based on these chunks, we propose a three-branch sparse self-attention mechanism to jointly identify users' global interests, interest transitions, and short-term interests. Furthermore, we design a composite relative temporal encoding via learnable, head-specific bias coefficients, better capturing sequential and periodic relationships among user behaviors. Extensive experimental results show that SparseCTR not only improves efficiency but also outperforms state-of-the-art methods. More importantly, it exhibits an obvious scaling law phenomenon, maintaining performance improvements across three orders of magnitude in FLOPs. In online A/B testing, SparseCTR increased CTR by 1.72\% and CPM by 1.41\%. Our source code is available at https://github.com/laiweijiang/SparseCTR.

</details>


### [17] [Post-Training Denoising of User Profiles with LLMs in Collaborative Filtering Recommendation](https://arxiv.org/abs/2601.18009)
*Ervin Dervishaj,Maria Maistro,Tuukka Ruotsalo,Christina Lioma*

Main category: cs.IR

TL;DR: 提出一种基于LLM的后训练去噪方法，通过提示LLM从用户配置文件中移除噪声交互，提升协同过滤推荐效果，无需改变模型架构或额外数据。


<details>
  <summary>Details</summary>
Motivation: 隐式反馈数据存在噪声，传统去噪方法需要额外数据、改变模型架构或训练过程，成本高且数据需求大。需要一种无需改变现有推荐系统架构的后训练去噪方法。

Method: 提出后训练去噪方法：使用LLM处理用户配置文件、候选物品及其CF排名，通过提示LLM移除用户配置文件中的噪声物品来提升候选物品排名。无需改变CF模型架构或训练过程。

Result: 在3个数据集上使用4个开源和闭源LLM进行实验，与最先进的CF推荐器结合，去噪后的用户配置文件使推荐效果提升高达13%。

Conclusion: 基于LLM的后训练去噪方法能有效减少隐式反馈噪声，显著提升推荐效果，且无需额外数据或模型架构修改，具有实用性和可扩展性。

Abstract: Implicit feedback -- the main data source for training Recommender Systems (RSs) -- is inherently noisy and has been shown to negatively affect recommendation effectiveness. Denoising has been proposed as a method for removing noisy implicit feedback and improving recommendations. Prior work has focused on in-training denoising, however this requires additional data, changes to the model architecture and training procedure or fine-tuning, all of which can be costly and data hungry. In this work, we focus on post-training denoising. Different from in-training denoising, post-training denoising does not involve changing the architecture of the model nor its training procedure, and does not require additional data. Specifically, we present a method for post-training denoising user profiles using Large Language Models (LLMs) for Collaborative Filtering (CF) recommendations. Our approach prompts LLMs with (i) a user profile (user interactions), (ii) a candidate item, and (iii) its rank as given by the CF recommender, and asks the LLM to remove items from the user profile to improve the rank of the candidate item. Experiments with a state-of-the-art CF recommender and 4 open and closed source LLMs in 3 datasets show that our denoising yields improvements up to 13% in effectiveness over the original user profiles. Our code is available at https://github.com/edervishaj/denoising-user-profiles-LLM.

</details>


### [18] [Enhancing LLM-based Recommendation with Preference Hint Discovery from Knowledge Graph](https://arxiv.org/abs/2601.18096)
*Yuting Zhang,Ziliang Pei,Chao Wang,Ying Sun,Fuzhen Zhuang*

Main category: cs.IR

TL;DR: 提出基于交互知识图谱的偏好提示发现模型，通过选择性提取关键属性作为提示来增强基于LLM的推荐系统


<details>
  <summary>Details</summary>
Motivation: LLM在推荐系统中难以捕捉复杂偏好模式，传统推荐嵌入与LLM离散语义空间存在差距，需要找到有效方法将属性知识转化为LLM可理解的偏好提示

Method: 1) 协作偏好提示提取方案：利用相似用户显式交互的语义知识作为未见物品的提示；2) 实例级双重注意力机制：量化候选属性的偏好可信度；3) 扁平化提示组织方法缩短输入长度

Result: 在成对和列表推荐任务上的广泛实验验证了框架有效性，相对于基线平均相对改进超过3.02%

Conclusion: 通过偏好提示发现模型有效弥合了传统推荐嵌入与LLM语义空间的差距，显著提升了LLM在推荐任务中的性能

Abstract: LLMs have garnered substantial attention in recommendation systems. Yet they fall short of traditional recommenders when capturing complex preference patterns. Recent works have tried integrating traditional recommendation embeddings into LLMs to resolve this issue, yet a core gap persists between their continuous embedding and discrete semantic spaces. Intuitively, textual attributes derived from interactions can serve as critical preference rationales for LLMs' recommendation logic. However, directly inputting such attribute knowledge presents two core challenges: (1) Deficiency of sparse interactions in reflecting preference hints for unseen items; (2) Substantial noise introduction from treating all attributes as hints. To this end, we propose a preference hint discovery model based on the interaction-integrated knowledge graph, enhancing LLM-based recommendation. It utilizes traditional recommendation principles to selectively extract crucial attributes as hints. Specifically, we design a collaborative preference hint extraction schema, which utilizes semantic knowledge from similar users' explicit interactions as hints for unseen items. Furthermore, we develop an instance-wise dual-attention mechanism to quantify the preference credibility of candidate attributes, identifying hints specific to each unseen item. Using these item- and user-based hints, we adopt a flattened hint organization method to shorten input length and feed the textual hint information to the LLM for commonsense reasoning. Extensive experiments on both pair-wise and list-wise recommendation tasks verify the effectiveness of our proposed framework, indicating an average relative improvement of over 3.02% against baselines.

</details>


### [19] [Think When Needed: Model-Aware Reasoning Routing for LLM-based Ranking](https://arxiv.org/abs/2601.18146)
*Huizhong Guo,Tianjun Wei,Dongxia Wang,Yingpeng Du,Ziyan Wang,Jie Zhang,Zhu Sun*

Main category: cs.IR

TL;DR: 提出推理路由框架，通过轻量级路由器决定何时使用推理模式，在提升排序效果的同时显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: LLM在排序任务中应用广泛，推理提示能提升效果但计算成本高且效果不稳定，需要智能决定何时使用推理

Method: 使用轻量级可插拔路由器头，基于预生成信号（排序感知特征和模型感知难度信号）决定使用直接推理还是推理模式，路由器可沿验证Pareto前沿自适应选择策略

Result: 在三个公开排序数据集上，不同规模的LLM均显示排序效果提升且token消耗减少（如MovieLens上NDCG@10提升6.3%，token减少49.5%）

Conclusion: 推理路由框架是解决排序任务中准确性与效率权衡的实用解决方案，能动态分配计算资源到最需要推理的实例

Abstract: Large language models (LLMs) are increasingly applied to ranking tasks in retrieval and recommendation. Although reasoning prompting can enhance ranking utility, our preliminary exploration reveals that its benefits are inconsistent and come at a substantial computational cost, suggesting that when to reason is as crucial as how to reason. To address this issue, we propose a reasoning routing framework that employs a lightweight, plug-and-play router head to decide whether to use direct inference (Non-Think) or reasoning (Think) for each instance before generation. The router head relies solely on pre-generation signals: i) compact ranking-aware features (e.g., candidate dispersion) and ii) model-aware difficulty signals derived from a diagnostic checklist reflecting the model's estimated need for reasoning. By leveraging these features before generation, the router outputs a controllable token that determines whether to apply the Think mode. Furthermore, the router can adaptively select its operating policy along the validation Pareto frontier during deployment, enabling dynamic allocation of computational resources toward instances most likely to benefit from Think under varying system constraints. Experiments on three public ranking datasets with different scales of open-source LLMs show consistent improvements in ranking utility with reduced token consumption (e.g., +6.3\% NDCG@10 with -49.5\% tokens on MovieLens with Qwen3-4B), demonstrating reasoning routing as a practical solution to the accuracy-efficiency trade-off.

</details>


### [20] [DMAP: Human-Aligned Structural Document Map for Multimodal Document Understanding](https://arxiv.org/abs/2601.18203)
*ShunLiang Fu,Yanxin Zhang,Yixin Xiang,Xiaoyu Du,Jinhui Tang*

Main category: cs.IR

TL;DR: DMAP提出了一种文档级结构化表示方法，通过层次化组织和关系编码来提升多模态文档问答性能，相比传统扁平化检索方法有显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有多模态文档问答系统主要依赖扁平化语义检索，将文档表示为不连接的文本块，忽略了文档内在的层次结构和关系结构（如章节组织、图文对应、交叉引用等），这种扁平化破坏了人类自然理解文档时依赖的逻辑和空间依赖关系。

Method: 1. 引入文档级结构化文档地图（DMAP），显式编码多模态文档中的层次组织和元素间关系；2. 设计结构化语义理解代理来构建DMAP，将文本内容与图表等组织成符合人类认知的层次化模式，捕捉语义和布局依赖；3. 基于此表示，设计反思推理代理进行结构感知和证据驱动的推理，动态评估检索上下文的充分性，并通过与DMAP的定向交互迭代优化答案。

Result: 在MMDocQA基准测试上的广泛实验表明，DMAP能够生成与人类解释模式对齐的文档特定结构化表示，相比传统的基于RAG的方法，显著提高了检索精度、推理一致性和多模态理解能力。

Conclusion: DMAP通过显式编码文档的层次结构和关系，解决了现有扁平化检索方法的局限性，为多模态文档问答提供了更符合人类认知的结构化表示和推理框架，显著提升了系统性能。

Abstract: Existing multimodal document question-answering (QA) systems predominantly rely on flat semantic retrieval, representing documents as a set of disconnected text chunks and largely neglecting their intrinsic hierarchical and relational structures. Such flattening disrupts logical and spatial dependencies - such as section organization, figure-text correspondence, and cross-reference relations, that humans naturally exploit for comprehension. To address this limitation, we introduce a document-level structural Document MAP (DMAP), which explicitly encodes both hierarchical organization and inter-element relationships within multimodal documents. Specifically, we design a Structured-Semantic Understanding Agent to construct DMAP by organizing textual content together with figures, tables, charts, etc. into a human-aligned hierarchical schema that captures both semantic and layout dependencies. Building upon this representation, a Reflective Reasoning Agent performs structure-aware and evidence-driven reasoning, dynamically assessing the sufficiency of retrieved context and iteratively refining answers through targeted interactions with DMAP. Extensive experiments on MMDocQA benchmarks demonstrate that DMAP yields document-specific structural representations aligned with human interpretive patterns, substantially enhancing retrieval precision, reasoning consistency, and multimodal comprehension over conventional RAG-based approaches. Code is available at https://github.com/Forlorin/DMAP

</details>


### [21] [Generative Chain of Behavior for User Trajectory Prediction](https://arxiv.org/abs/2601.18213)
*Chengkai Huang,Xiaodi Chen,Hongtao Huang,Quan Z. Sheng,Lina Yao*

Main category: cs.IR

TL;DR: GCB是一个生成式行为链框架，用于建模用户多步未来行为轨迹，通过语义ID编码和自回归生成实现长期意图预测。


<details>
  <summary>Details</summary>
Motivation: 大多数序列推荐系统只关注下一个物品预测，忽略了多个未来动作之间的依赖关系，无法理解用户偏好演变和实现主动推荐。

Method: 使用RQ-VAE和k-means聚类将物品编码为语义ID，构建离散潜在空间保持语义邻近性；在此基础上，基于Transformer的自回归生成器预测多步未来行为，捕捉长期意图转换。

Result: 在基准数据集上的实验表明，GCB在多步准确性和轨迹一致性方面持续优于最先进的序列推荐方法。

Conclusion: GCB不仅取得了性能提升，还提供了一个统一的生成式框架来捕捉用户偏好演变，为理解用户行为轨迹提供了新方法。

Abstract: Modeling long-term user behavior trajectories is essential for understanding evolving preferences and enabling proactive recommendations. However, most sequential recommenders focus on next-item prediction, overlooking dependencies across multiple future actions. We propose Generative Chain of Behavior (GCB), a generative framework that models user interactions as an autoregressive chain of semantic behaviors over multiple future steps. GCB first encodes items into semantic IDs via RQ-VAE with k-means refinement, forming a discrete latent space that preserves semantic proximity. On top of this space, a transformer-based autoregressive generator predicts multi-step future behaviors conditioned on user history, capturing long-horizon intent transitions and generating coherent trajectories. Experiments on benchmark datasets show that GCB consistently outperforms state-of-the-art sequential recommenders in multi-step accuracy and trajectory consistency. Beyond these gains, GCB offers a unified generative formulation for capturing user preference evolution.

</details>


### [22] [GenCI: Generative Modeling of User Interest Shift via Cohort-based Intent Learning for CTR Prediction](https://arxiv.org/abs/2601.18251)
*Kesha Ou,Zhen Tian,Wayne Xin Zhao,Hongyu Lu,Ji-Rong Wen*

Main category: cs.IR

TL;DR: GenCI：基于生成式用户意图框架的CTR预测方法，通过语义兴趣群组建模动态用户偏好，解决历史特征过拟合和点式排序信息丢失问题。


<details>
  <summary>Details</summary>
Motivation: 当前CTR预测存在两大挑战：1）判别式范式过度拟合历史主导特征，难以适应快速兴趣变化；2）点式排序范式丢弃召回集的丰富上下文信号，导致长期偏好压制即时意图。

Method: 提出GenCI生成式用户意图框架：1）使用生成模型（NTP目标）主动生成候选兴趣群组作为用户即时意图的显式表示；2）分层候选感知网络通过交叉注意力注入上下文信号，对齐用户历史和目标商品；3）端到端训练。

Result: 在三个广泛使用的数据集上进行实验，证明了方法的有效性。

Conclusion: GenCI通过生成式兴趣群组建模动态用户意图，解决了传统CTR预测的过拟合和上下文信息丢失问题，实现了更对齐和有效的CTR预测流程。

Abstract: Click-through rate (CTR) prediction plays a pivotal role in online advertising and recommender systems. Despite notable progress in modeling user preferences from historical behaviors, two key challenges persist. First, exsiting discriminative paradigms focus on matching candidates to user history, often overfitting to historically dominant features and failing to adapt to rapid interest shifts. Second, a critical information chasm emerges from the point-wise ranking paradigm. By scoring each candidate in isolation, CTR models discard the rich contextual signal implied by the recalled set as a whole, leading to a misalignment where long-term preferences often override the user's immediate, evolving intent. To address these issues, we propose GenCI, a generative user intent framework that leverages semantic interest cohorts to model dynamic user preferences for CTR prediction. The framework first employs a generative model, trained with a next-item prediction (NTP) objective, to proactively produce candidate interest cohorts. These cohorts serve as explicit, candidate-agnostic representations of a user's immediate intent. A hierarchical candidate-aware network then injects this rich contextual signal into the ranking stage, refining them with cross-attention to align with both user history and the target item. The entire model is trained end-to-end, creating a more aligned and effective CTR prediction pipeline. Extensive experiments on three widely used datasets demonstrate the effectiveness of our approach.

</details>


### [23] [Orchestrating Specialized Agents for Trustworthy Enterprise RAG](https://arxiv.org/abs/2601.18267)
*Xincheng You,Qi Sun,Neha Bora,Huayi Li,Shubham Goel,Kang Li,Sean Culatana*

Main category: cs.IR

TL;DR: ADORE是一个用于企业知识工作的代理框架，通过结构化记忆库和迭代检索机制解决传统RAG在深度合成、可追溯性和完整性验证方面的不足。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成（RAG）在企业高价值决策场景中表现不佳，存在浅层总结、不一致的接地和弱完整性验证机制等问题，需要更强大的框架来支持深度合成、严格可追溯性和对不明确提示的恢复能力。

Method: ADORE采用代理框架，核心包括：1）结构化记忆库（Claim-Evidence Graph）实现记忆锁定合成；2）证据覆盖引导的执行机制，通过检索-反思循环审计证据完整性；3）章节打包长上下文接地技术，在上下文限制下实现长格式合成。

Result: 在DeepResearch Bench上获得52.65分排名第一，在DeepConsult上与商业系统相比获得77.2%的最高头对头偏好胜率。

Conclusion: ADORE通过结构化记忆库和迭代代理协调，显著提升了企业知识工作中RAG系统的深度合成能力、可追溯性和完整性验证，为高价值决策场景提供了有效解决方案。

Abstract: Retrieval-Augmented Generation (RAG) shows promise for enterprise knowledge work, yet it often underperforms in high-stakes decision settings that require deep synthesis, strict traceability, and recovery from underspecified prompts. One-pass retrieval-and-write pipelines frequently yield shallow summaries, inconsistent grounding, and weak mechanisms for completeness verification. We introduce ADORE (Adaptive Deep Orchestration for Research in Enterprise), an agentic framework that replaces linear retrieval with iterative, user-steered investigation coordinated by a central orchestrator and a set of specialized agents. ADORE's key insight is that a structured Memory Bank (a curated evidence store with explicit claim-evidence linkage and section-level admissible evidence) enables traceable report generation and systematic checks for evidence completeness. Our contributions are threefold: (1) Memory-locked synthesis - report generation is constrained to a structured Memory Bank (Claim-Evidence Graph) with section-level admissible evidence, enabling traceable claims and grounded citations; (2) Evidence-coverage-guided execution - a retrieval-reflection loop audits section-level evidence coverage to trigger targeted follow-up retrieval and terminates via an evidence-driven stopping criterion; (3) Section-packed long-context grounding - section-level packing, pruning, and citation-preserving compression make long-form synthesis feasible under context limits. Across our evaluation suite, ADORE ranks first on DeepResearch Bench (52.65) and achieves the highest head-to-head preference win rate on DeepConsult (77.2%) against commercial systems.

</details>


### [24] [TopKGAT: A Top-K Objective-Driven Architecture for Recommendation](https://arxiv.org/abs/2601.18432)
*Sirui Chen,Jiawei Chen,Canghong Jin,Sheng Zhou,Jingbang Chen,Wujie Sun,Can Wang*

Main category: cs.IR

TL;DR: TopKGAT是一种直接从top-K指标的可微分近似推导出的推荐架构，通过将前向计算与Precision@K的梯度上升动态对齐，显著提升了top-K推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统架构（如矩阵分解、深度神经网络、图神经网络）的设计通常没有明确与top-K目标对齐，这限制了它们在top-K推荐任务上的有效性。

Method: 提出TopKGAT架构，从top-K指标的可微分近似直接推导而来，其单层前向计算与Precision@K指标的梯度上升动态内在对齐，结构类似于图注意力网络且实现高效。

Result: 在四个基准数据集上的广泛实验表明，TopKGAT始终优于最先进的基线方法。

Conclusion: TopKGAT通过将架构设计与top-K目标直接对齐，有效提升了推荐系统的top-K准确性，为推荐架构设计提供了新思路。

Abstract: Recommendation systems (RS) aim to retrieve the top-K items most relevant to users, with metrics such as Precision@K and Recall@K commonly used to assess effectiveness. The architecture of an RS model acts as an inductive bias, shaping the patterns the model is inclined to learn. In recent years, numerous recommendation architectures have emerged, spanning traditional matrix factorization, deep neural networks, and graph neural networks. However, their designs are often not explicitly aligned with the top-K objective, thereby limiting their effectiveness.
  To address this limitation, we propose TopKGAT, a novel recommendation architecture directly derived from a differentiable approximation of top-K metrics. The forward computation of a single TopKGAT layer is intrinsically aligned with the gradient ascent dynamics of the Precision@K metric, enabling the model to naturally improve top-K recommendation accuracy. Structurally, TopKGAT resembles a graph attention network and can be implemented efficiently. Extensive experiments on four benchmark datasets demonstrate that TopKGAT consistently outperforms state-of-the-art baselines. The code is available at https://github.com/StupidThree/TopKGAT.

</details>


### [25] [Token-level Collaborative Alignment for LLM-based Generative Recommendation](https://arxiv.org/abs/2601.18457)
*Fake Lin,Binbin Hu,Zhi Zheng,Xi Zhu,Ziqi Liu,Zhiqiang Zhang,Jun Zhou,Tong Xu*

Main category: cs.IR

TL;DR: TCA4Rec提出了一种模型无关的即插即用框架，通过令牌级协同对齐解决LLM推荐系统中难以有效整合协同过滤信号的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的推荐系统难以有效整合协同过滤信号，因为CF的项级偏好建模与LLM的令牌级下一个令牌预测优化存在根本性不匹配。先前方法通常将CF作为上下文提示或表示偏差，需要多阶段训练来减少行为语义空间差异，导致CF无法明确调控LLM生成。

Method: TCA4Rec包含两个核心组件：(1) 协同分词器：将原始项级CF对数投影到与LLM令牌空间对齐的令牌级分布；(2) 软标签对齐：将这些CF信息分布与独热监督结合，优化软NTP目标。该设计保留了LLM训练的生成特性，同时实现与CF模型核心用户偏好的协同对齐。

Result: TCA4Rec与任意传统CF模型兼容，可泛化到广泛的基于解码器的LLM推荐架构。它提供了明确的机制来平衡行为对齐和语义流畅性，生成既准确又可控制的推荐。大量实验表明，TCA4Rec在各种CF模型和基于LLM的推荐系统中持续提升推荐性能。

Conclusion: TCA4Rec通过建立CF监督与LLM生成之间的显式优化级接口，解决了LLM推荐系统中协同过滤信号整合的难题，实现了生成式推荐在准确性和可控性方面的平衡。

Abstract: Large Language Models (LLMs) have demonstrated strong potential for generative recommendation by leveraging rich semantic knowledge. However, existing LLM-based recommender systems struggle to effectively incorporate collaborative filtering (CF) signals, due to a fundamental mismatch between item-level preference modeling in CF and token-level next-token prediction (NTP) optimization in LLMs. Prior approaches typically treat CF as contextual hints or representation bias, and resort to multi-stage training to reduce behavioral semantic space discrepancies, leaving CF unable to explicitly regulate LLM generation. In this work, we propose Token-level Collaborative Alignment for Recommendation (TCA4Rec), a model-agnostic and plug-and-play framework that establishes an explicit optimization-level interface between CF supervision and LLM generation. TCA4Rec consists of (i) Collaborative Tokenizer, which projects raw item-level CF logits into token-level distributions aligned with the LLM token space, and (ii) Soft Label Alignment, which integrates these CF-informed distributions with one-hot supervision to optimize a soft NTP objective. This design preserves the generative nature of LLM training while enabling collaborative alignment with essential user preference of CF models. We highlight TCA4Rec is compatible with arbitrary traditional CF models and generalizes across a wide range of decoder-based LLM recommender architectures. Moreover, it provides an explicit mechanism to balance behavioral alignment and semantic fluency, yielding generative recommendations that are both accurate and controllable. Extensive experiments demonstrate that TCA4Rec consistently improves recommendation performance across a broad spectrum of CF models and LLM-based recommender systems.

</details>


### [26] [Feature-Indexed Federated Recommendation with Residual-Quantized Codebooks](https://arxiv.org/abs/2601.18570)
*Mingzhe Han,Jiahao Liu,Dongsheng Li,Hansu Gu,Peng Zhang,Ning Gu,Tun Lu*

Main category: cs.IR

TL;DR: RQFedRec提出了一种基于特征索引通信范式的联邦推荐系统，通过残差量化K-means将物品表示为离散代码ID，传输代码嵌入而非原始物品嵌入，显著降低通信开销并提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦推荐方法采用ID索引通信范式，传输完整的物品嵌入，存在三个主要问题：1) 通信资源消耗不可控；2) 上传的物品信息无法泛化到相关未交互物品；3) 对客户端噪声反馈敏感。需要从根本上改变现有通信范式。

Method: 提出特征索引通信范式，使用残差量化K-means为每个物品分配离散代码ID列表。客户端基于服务器提供的代码ID生成和训练代码嵌入作为码本，服务器聚合码本而非物品嵌入。采用协作-语义双通道聚合策略，早期强调语义代码，训练过程中逐步增加协作代码贡献。

Result: 在真实世界数据集上的实验表明，RQFedRec在显著降低通信开销的同时，持续优于最先进的联邦推荐基线方法。

Conclusion: RQFedRec通过特征索引通信范式解决了现有联邦推荐系统的关键限制，实现了可控通信、更好的泛化能力和对噪声的鲁棒性，为联邦推荐提供了更高效的解决方案。

Abstract: Federated recommendation provides a privacy-preserving solution for training recommender systems without centralizing user interactions. However, existing methods follow an ID-indexed communication paradigm that transmit whole item embeddings between clients and the server, which has three major limitations: 1) consumes uncontrollable communication resources, 2) the uploaded item information cannot generalize to related non-interacted items, and 3) is sensitive to client noisy feedback. To solve these problems, it is necessary to fundamentally change the existing ID-indexed communication paradigm. Therefore, we propose a feature-indexed communication paradigm that transmits feature code embeddings as codebooks rather than raw item embeddings. Building on this paradigm, we present RQFedRec, which assigns each item a list of discrete code IDs via Residual Quantization (RQ)-Kmeans. Each client generates and trains code embeddings as codebooks based on discrete code IDs provided by the server, and the server collects and aggregates these codebooks rather than item embeddings. This design makes communication controllable since the codebooks could cover all items, enabling updates to propagate across related items in same code ID. In addition, since code embedding represents many items, which is more robust to a single noisy item. To jointly capture semantic and collaborative information, RQFedRec further adopts a collaborative-semantic dual-channel aggregation with a curriculum strategy that emphasizes semantic codes early and gradually increases the contribution of collaborative codes over training. Extensive experiments on real-world datasets demonstrate that RQFedRec consistently outperforms state-of-the-art federated recommendation baselines while significantly reducing communication overhead.

</details>


### [27] [FastInsight: Fast and Insightful Retrieval via Fusion Operators for Graph RAG](https://arxiv.org/abs/2601.18579)
*Seonho An,Chaejeong Hyun,Min-Soo Kim*

Main category: cs.IR

TL;DR: FastInsight：一种高效的图RAG方法，通过融合图模型搜索和向量图搜索，在保持高效的同时显著提升检索准确性和生成质量


<details>
  <summary>Details</summary>
Motivation: 现有图RAG方法依赖耗时的大型语言模型推理过程，无法实现时间高效的深度检索。通过图检索分类学分析，发现当前方法存在拓扑盲点（模型搜索）和语义盲点（图搜索）两个关键限制

Method: 提出FastInsight框架，包含两个新颖的融合算子：1) Graph-based Reranker (GRanker)：作为图模型搜索；2) Semantic-Topological eXpansion (STeX)：作为向量图搜索。通过交替使用这两个算子克服现有方法的局限性

Result: 在广泛的检索和生成数据集上进行实验，FastInsight相比最先进的基线方法，在检索准确性和生成质量方面都有显著提升，在效果和效率的权衡中实现了显著的帕累托改进

Conclusion: FastInsight通过创新的融合算子解决了现有图RAG方法的拓扑盲点和语义盲点问题，在保持时间效率的同时实现了深度洞察检索，为图检索领域提供了有效的解决方案

Abstract: Existing Graph RAG methods aiming for insightful retrieval on corpus graphs typically rely on time-intensive processes that interleave Large Language Model (LLM) reasoning. To enable time-efficient insightful retrieval, we propose FastInsight. We first introduce a graph retrieval taxonomy that categorizes existing methods into three fundamental operations: vector search, graph search, and model-based search. Through this taxonomy, we identify two critical limitations in current approaches: the topology-blindness of model-based search and the semantics-blindness of graph search. FastInsight overcomes these limitations by interleaving two novel fusion operators: the Graph-based Reranker (GRanker), which functions as a graph model-based search, and Semantic-Topological eXpansion (STeX), which operates as a vector-graph search. Extensive experiments on broad retrieval and generation datasets demonstrate that FastInsight significantly improves both retrieval accuracy and generation quality compared to state-of-the-art baselines, achieving a substantial Pareto improvement in the trade-off between effectiveness and efficiency.

</details>


### [28] [S$^2$GR: Stepwise Semantic-Guided Reasoning in Latent Space for Generative Recommendation](https://arxiv.org/abs/2601.18664)
*Zihao Guo,Jian Wang,Ruxin Zhou,Youhua Liu,Jiawei Guo,Jun Zhao,Xiaoxiao Xu,Yongqi Liu,Kaiqiao Zhan*

Main category: cs.IR

TL;DR: 提出S²GR框架，通过潜在空间中的逐步语义引导推理增强生成式推荐，解决现有方法推理与生成分离、计算不平衡和语义不可解释的问题。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐方法主要关注从交互序列直接生成语义ID，未能激活类似大语言模型的深度推理能力，限制了性能潜力。现有推理增强方法存在两个关键局限：1) 推理与生成步骤严格分离导致层次化语义ID代码计算不平衡；2) 生成的推理向量缺乏可解释语义，推理路径缺乏可验证监督。

Method: 提出S²GR框架：1) 通过代码本优化建立稳健语义基础，整合物品共现关系捕获行为模式，使用负载平衡和均匀性目标最大化代码本利用率并强化从粗到细的语义层次；2) 核心创新引入逐步推理机制，在每个语义ID生成步骤前插入思考标记，每个标记明确表示粗粒度语义，通过对比学习监督确保物理基础推理路径和所有语义ID代码的平衡计算关注。

Result: 大量实验证明S²GR的优越性，在线A/B测试在大型工业短视频平台上确认了其有效性。

Conclusion: S²GR通过潜在空间中的逐步语义引导推理，解决了生成式推荐中推理与生成分离的问题，实现了更平衡的计算关注和可解释的推理路径，显著提升了推荐性能。

Abstract: Generative Recommendation (GR) has emerged as a transformative paradigm with its end-to-end generation advantages. However, existing GR methods primarily focus on direct Semantic ID (SID) generation from interaction sequences, failing to activate deeper reasoning capabilities analogous to those in large language models and thus limiting performance potential. We identify two critical limitations in current reasoning-enhanced GR approaches: (1) Strict sequential separation between reasoning and generation steps creates imbalanced computational focus across hierarchical SID codes, degrading quality for SID codes; (2) Generated reasoning vectors lack interpretable semantics, while reasoning paths suffer from unverifiable supervision. In this paper, we propose stepwise semantic-guided reasoning in latent space (S$^2$GR), a novel reasoning enhanced GR framework. First, we establish a robust semantic foundation via codebook optimization, integrating item co-occurrence relationship to capture behavioral patterns, and load balancing and uniformity objectives that maximize codebook utilization while reinforcing coarse-to-fine semantic hierarchies. Our core innovation introduces the stepwise reasoning mechanism inserting thinking tokens before each SID generation step, where each token explicitly represents coarse-grained semantics supervised via contrastive learning against ground-truth codebook cluster distributions ensuring physically grounded reasoning paths and balanced computational focus across all SID codes. Extensive experiments demonstrate the superiority of S$^2$GR, and online A/B test confirms efficacy on large-scale industrial short video platform.

</details>


### [29] [Capturing P: On the Expressive Power and Efficient Evaluation of Boolean Retrieval](https://arxiv.org/abs/2601.18747)
*Amir Aavani*

Main category: cs.IR

TL;DR: 提出一种基于有向无环图（DAG）的检索语言，能高效处理复杂神经符号推理工作流中的逻辑和算术约束，解决现有检索架构的效率困境。


<details>
  <summary>Details</summary>
Motivation: 现代信息检索正从简单文档过滤转向复杂的神经符号推理工作流，但现有检索架构在处理逻辑和算术约束时面临效率困境：基于迭代器的引擎不支持复杂嵌套逻辑图，而递归方法则存在内存消耗过高的问题。

Method: 提出基于有向无环图（DAG）的形式化检索语言（ℒᵣ），证明其精确捕获复杂度类P。引入ComputePN算法，结合原生DAG遍历和内存高效的"正负"响应机制，确保ℒᵣ中任何查询的高效评估。

Result: 建立了将搜索索引转变为通用计算引擎的理论基础，提出的方法能够高效评估任何多项式时间属性，解决了复杂查询执行时的运行时性能和内存消耗问题。

Conclusion: 通过形式化检索语言和高效评估算法，为检索引擎处理复杂神经符号推理工作流提供了理论基础，使搜索索引能够作为通用计算引擎使用。

Abstract: Modern information retrieval is transitioning from simple document filtering to complex, neuro-symbolic reasoning workflows. However, current retrieval architectures face a fundamental efficiency dilemma when handling the rigorous logical and arithmetic constraints required by this new paradigm. Standard iterator-based engines (Document-at-a-Time) do not natively support complex, nested logic graphs; forcing them to execute such queries typically results in intractable runtime performance. Conversely, naive recursive approaches (Term-at-a-Time), while capable of supporting these structures, suffer from prohibitive memory consumption when enforcing broad logical exclusions.
  In this paper, we propose that a retrieval engine must be capable of ``Capturing $\mathbf{P}$'' -- evaluating any polynomial-time property directly over its index in a computationally efficient manner. We define a formal Retrieval Language ($\mathcal{L}_R$) based on Directed Acyclic Graphs (DAGs) and prove it precisely captures the complexity class $\mathbf{P}$. We introduce \texttt{ComputePN}, a novel evaluation algorithm that makes $\mathcal{L}_R$ tractable. By combining native DAG traversal with a memory-efficient ``Positive-Negative'' response mechanism, \texttt{ComputePN} ensures the efficient evaluation of any query in $\mathcal{L}_R$. This work establishes the theoretical foundation for turning the search index into a general-purpose computational engine.

</details>
