<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 6]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [LLM-Powered Nuanced Video Attribute Annotation for Enhanced Recommendations](https://arxiv.org/abs/2510.06657)
*Boyuan Long,Yueqi Wang,Hiloni Mehta,Mick Zomnir,Omkar Pathak,Changping Meng,Ruolin Jia,Yajun Peng,Dapeng Hong,Xia Wu,Mingyan Gao,Onkar Dalal,Ningren Han*

Main category: cs.IR

TL;DR: 使用LLM作为标注工具来增强短视频推荐系统的内容理解能力，显著缩短开发周期并实现细粒度属性标注，通过端到端工作流提升推荐效果。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习分类器开发周期长且缺乏深度、细粒度的内容理解能力，需要一种能够快速部署并理解内容细微特征的方法。

Method: 构建端到端工作流：1) 迭代定义和评估目标属性；2) 使用多模态特征的LLM进行批量标注，并通过知识蒸馏优化；3) 将丰富标注集成到在线推荐系统中。

Result: LLM在细粒度属性标注质量上超越人工标注者，在线A/B测试显示用户参与度和满意度显著提升。

Conclusion: LLM生成的细粒度理解能够有效增强内容发现、用户满意度和现代推荐系统的整体效能，展示了生产级LLM管道的设计和扩展潜力。

Abstract: This paper presents a case study on deploying Large Language Models (LLMs) as
an advanced "annotation" mechanism to achieve nuanced content understanding
(e.g., discerning content "vibe") at scale within a large-scale industrial
short-form video recommendation system. Traditional machine learning
classifiers for content understanding face protracted development cycles and a
lack of deep, nuanced comprehension. The "LLM-as-annotators" approach addresses
these by significantly shortening development times and enabling the annotation
of subtle attributes. This work details an end-to-end workflow encompassing:
(1) iterative definition and robust evaluation of target attributes, refined by
offline metrics and online A/B testing; (2) scalable offline bulk annotation of
video corpora using LLMs with multimodal features, optimized inference, and
knowledge distillation for broad application; and (3) integration of these rich
annotations into the online recommendation serving system, for example, through
personalized restrict retrieval. Experimental results demonstrate the efficacy
of this approach, with LLMs outperforming human raters in offline annotation
quality for nuanced attributes and yielding significant improvements of user
participation and satisfied consumption in online A/B tests. The study provides
insights into designing and scaling production-level LLM pipelines for rich
content evaluation, highlighting the adaptability and benefits of LLM-generated
nuanced understanding for enhancing content discovery, user satisfaction, and
the overall effectiveness of modern recommendation systems.

</details>


### [2] [Can We Hide Machines in the Crowd? Quantifying Equivalence in LLM-in-the-loop Annotation Tasks](https://arxiv.org/abs/2510.06658)
*Jiaman He,Zikang Leng,Dana McKay,Damiano Spina,Johanne R. Trippas*

Main category: cs.IR

TL;DR: 该研究开发了一种统计评估方法，用于测试LLM是否能融入人类标注者群体而不被区分，发现LLM在MovieLens数据集上与人无异，但在PolitiFact数据集上可区分。


<details>
  <summary>Details</summary>
Motivation: 超越传统仅关注LLM标注正确性的评估，探索LLM是否能模拟人类主观判断，作为替代标注机制。

Method: 基于Krippendorff's α、配对自助法和TOST等价性检验的统计评估方法，测试LLM是否与人类标注者无法区分。

Result: 在MovieLens 100K数据集上，LLM与人类标注者统计上无法区分(p=0.004)；在PolitiFact数据集上可区分(p=0.155)。

Conclusion: LLM作为标注工具的效果具有任务依赖性，该方法可在小样本上早期评估LLM是否适合特定应用的大规模标注。

Abstract: Many evaluations of large language models (LLMs) in text annotation focus
primarily on the correctness of the output, typically comparing model-generated
labels to human-annotated ``ground truth'' using standard performance metrics.
In contrast, our study moves beyond effectiveness alone. We aim to explore how
labeling decisions -- by both humans and LLMs -- can be statistically evaluated
across individuals. Rather than treating LLMs purely as annotation systems, we
approach LLMs as an alternative annotation mechanism that may be capable of
mimicking the subjective judgments made by humans. To assess this, we develop a
statistical evaluation method based on Krippendorff's $\alpha$, paired
bootstrapping, and the Two One-Sided t-Tests (TOST) equivalence test procedure.
This evaluation method tests whether an LLM can blend into a group of human
annotators without being distinguishable.
  We apply this approach to two datasets -- MovieLens 100K and PolitiFact --
and find that the LLM is statistically indistinguishable from a human annotator
in the former ($p = 0.004$), but not in the latter ($p = 0.155$), highlighting
task-dependent differences. It also enables early evaluation on a small sample
of human data to inform whether LLMs are suitable for large-scale annotation in
a given application.

</details>


### [3] [Reproducing and Extending Causal Insights Into Term Frequency Computation in Neural Rankers](https://arxiv.org/abs/2510.06728)
*Cile van Marken,Roxana Petcu*

Main category: cs.IR

TL;DR: 该论文通过激活修补方法研究神经检索模型的因果可解释性，验证并扩展了Chen等人的发现，识别出编码词频递减效应的注意力头。


<details>
  <summary>Details</summary>
Motivation: 解决神经排序模型内部决策过程不透明的问题，特别是随着模型规模增大，大多数可解释性方法只提供相关性见解而非因果关系。

Method: 采用激活修补这一因果可解释性方法，在信息检索领域应用该框架，并扩展包含额外的词频公理。

Result: 成功验证了Chen等人的主要发现，识别出编码词频递减效应的特定注意力头组，揭示了神经排序模型的内部决策机制。

Conclusion: 神经排序模型不仅捕获词频信息，而且这些表示可以定位到模型的特定组件，为理解模型内部计算过程提供了新的视角。

Abstract: Neural ranking models have shown outstanding performance across a variety of
tasks, such as document retrieval, re-ranking, question answering and
conversational retrieval. However, the inner decision process of these models
remains largely unclear, especially as models increase in size. Most
interpretability approaches, such as probing, focus on correlational insights
rather than establishing causal relationships. The paper 'Axiomatic Causal
Interventions for Reverse Engineering Relevance Computation in Neural Retrieval
Models' by Chen et al. addresses this gap by introducing a framework for
activation patching - a causal interpretability method - in the information
retrieval domain, offering insights into how neural retrieval models compute
document relevance. The study demonstrates that neural ranking models not only
capture term-frequency information, but also that these representations can be
localized to specific components of the model, such as individual attention
heads or layers. This paper aims to reproduce the findings by Chen et al. and
to further explore the presence of pre-defined retrieval axioms in neural IR
models. We validate the main claims made by Chen et al., and extend the
framework to include an additional term-frequency axiom, which states that the
impact of increasing query term frequency on document ranking diminishes as the
frequency becomes higher. We successfully identify a group of attention heads
that encode this axiom and analyze their behavior to give insight into the
inner decision-making process of neural ranking models.

</details>


### [4] [Crossing Domains without Labels: Distant Supervision for Term Extraction](https://arxiv.org/abs/2510.06838)
*Elena Senger,Yuri Campbell,Rob van der Goot,Barbara Plank*

Main category: cs.IR

TL;DR: 提出了一个基于LLM的自动术语提取模型，在7个不同领域的基准测试中超越了现有方法，平均提升10个百分点，并与GPT-4o教师模型表现相当。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的自动术语提取方法需要昂贵的人工标注，且在领域迁移方面表现不佳，限制了实际部署，需要更鲁棒、可扩展的解决方案和更现实的评估设置。

Method: 首先使用黑盒LLM在通用和科学领域生成伪标签以确保泛化性，然后微调首个用于ATE的LLM，最后引入轻量级后处理启发式方法来增强文档级一致性。

Result: 在7个领域中的5个领域超越了先前方法，平均提升10个百分点，与GPT-4o教师模型表现相当。

Conclusion: 提出的LLM-based方法在自动术语提取任务中表现出色，发布了数据集和微调模型以支持未来研究。

Abstract: Automatic Term Extraction (ATE) is a critical component in downstream NLP
tasks such as document tagging, ontology construction and patent analysis.
Current state-of-the-art methods require expensive human annotation and
struggle with domain transfer, limiting their practical deployment. This
highlights the need for more robust, scalable solutions and realistic
evaluation settings. To address this, we introduce a comprehensive benchmark
spanning seven diverse domains, enabling performance evaluation at both the
document- and corpus-levels. Furthermore, we propose a robust LLM-based model
that outperforms both supervised cross-domain encoder models and few-shot
learning baselines and performs competitively with its GPT-4o teacher on this
benchmark. The first step of our approach is generating psuedo-labels with this
black-box LLM on general and scientific domains to ensure generalizability.
Building on this data, we fine-tune the first LLMs for ATE. To further enhance
document-level consistency, oftentimes needed for downstream tasks, we
introduce lightweight post-hoc heuristics. Our approach exceeds previous
approaches on 5/7 domains with an average improvement of 10 percentage points.
We release our dataset and fine-tuned models to support future research in this
area.

</details>


### [5] [M3Retrieve: Benchmarking Multimodal Retrieval for Medicine](https://arxiv.org/abs/2510.06888)
*Arkadeep Acharya,Akash Ghosh,Pradeepika Verma,Kitsuchart Pasupa,Sriparna Saha,Priti Singh*

Main category: cs.IR

TL;DR: 提出了M3Retrieve——一个多模态医学检索基准，涵盖5个领域、16个医学专业和4个任务，包含120万文本文档和16.4万多模态查询，旨在解决医疗领域缺乏标准化多模态检索评估基准的问题。


<details>
  <summary>Details</summary>
Motivation: 随着检索增强生成(RAG)的广泛应用，强大的检索模型变得尤为重要。在医疗领域，结合文本和图像信息的多模态检索模型对问答、跨模态检索和多模态摘要等任务具有重要价值，但目前缺乏标准化的评估基准。

Method: 构建了M3Retrieve基准数据集，涵盖5个医学领域、16个专业领域和4个不同任务，收集了超过120万文本文档和16.4万多模态查询，所有数据均在批准许可下收集。

Result: 在M3Retrieve基准上评估了领先的多模态检索模型，探索了不同医学专业特有的挑战及其对检索性能的影响。

Conclusion: 通过发布M3Retrieve，旨在实现系统化评估，促进模型创新，并加速构建更强大可靠的医疗多模态检索系统的研究。

Abstract: With the increasing use of RetrievalAugmented Generation (RAG), strong
retrieval models have become more important than ever. In healthcare,
multimodal retrieval models that combine information from both text and images
offer major advantages for many downstream tasks such as question answering,
cross-modal retrieval, and multimodal summarization, since medical data often
includes both formats. However, there is currently no standard benchmark to
evaluate how well these models perform in medical settings. To address this
gap, we introduce M3Retrieve, a Multimodal Medical Retrieval Benchmark.
M3Retrieve, spans 5 domains,16 medical fields, and 4 distinct tasks, with over
1.2 Million text documents and 164K multimodal queries, all collected under
approved licenses. We evaluate leading multimodal retrieval models on this
benchmark to explore the challenges specific to different medical specialities
and to understand their impact on retrieval performance. By releasing
M3Retrieve, we aim to enable systematic evaluation, foster model innovation,
and accelerate research toward building more capable and reliable multimodal
retrieval systems for medical applications. The dataset and the baselines code
are available in this github page https://github.com/AkashGhosh/M3Retrieve.

</details>


### [6] [Ethical AI prompt recommendations in large language models using collaborative filtering](https://arxiv.org/abs/2510.06924)
*Jordan Nelson,Almas Baimagambetov,Konstantinos Avgerinakis,Nikolaos Polatidis*

Main category: cs.IR

TL;DR: 使用协同过滤技术来提升伦理提示选择，通过用户交互促进伦理指南并减少偏见，包括创建合成数据集和应用协同过滤方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，确保伦理提示推荐至关重要。传统监督方法难以扩展，需要动态解决方案来应对偏见、公平性和问责制问题。

Method: 提出使用推荐系统中的协同过滤技术，通过用户交互来增强伦理提示选择，并创建了用于提示推荐的合成数据集。

Result: 该方法能够促进伦理指南的实施，同时减少偏见，解决了伦理AI中的偏见缓解、透明度和防止不道德提示工程等挑战。

Conclusion: 协同过滤技术为伦理提示推荐提供了有效的动态解决方案，有助于构建更负责任的大语言模型应用。

Abstract: As large language models (LLMs) shape AI development, ensuring ethical prompt
recommendations is crucial. LLMs offer innovation but risk bias, fairness
issues, and accountability concerns. Traditional oversight methods struggle
with scalability, necessitating dynamic solutions. This paper proposes using
collaborative filtering, a technique from recommendation systems, to enhance
ethical prompt selection. By leveraging user interactions, it promotes ethical
guidelines while reducing bias. Contributions include a synthetic dataset for
prompt recommendations and the application of collaborative filtering. The work
also tackles challenges in ethical AI, such as bias mitigation, transparency,
and preventing unethical prompt engineering.

</details>
