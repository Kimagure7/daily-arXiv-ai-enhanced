<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 19]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [A Knowledge Graph and Deep Learning-Based Semantic Recommendation Database System for Advertisement Retrieval and Personalization](https://arxiv.org/abs/2601.00833)
*Tangtang Wang,Kaijie Zhang,Kuangcong Liu*

Main category: cs.IR

TL;DR: 提出基于知识图谱和深度学习的语义推荐数据库系统(KGSR-ADS)，用于广告检索和个性化推荐


<details>
  <summary>Details</summary>
Motivation: 现代数字营销中广告数据日益复杂，需要能够理解产品、受众和广告内容之间语义关系的智能系统

Method: 集成异构广告知识图谱(Ad-KG)、基于LLM的语义嵌入层、GNN+注意力模型以及基于向量索引的数据库优化检索层

Result: 实现了准确的语义匹配和可扩展的检索，支持大规模异构工作负载下的个性化广告推荐

Conclusion: 提出的KGSR-ADS系统能够有效解决广告数据语义理解挑战，实现智能化的广告检索和个性化推荐

Abstract: In modern digital marketing, the growing complexity of advertisement data demands intelligent systems capable of understanding semantic relationships among products, audiences, and advertising content. To address this challenge, this paper proposes a Knowledge Graph and Deep Learning-Based Semantic Recommendation Database System (KGSR-ADS) for advertisement retrieval and personalization. The proposed framework integrates a heterogeneous Ad-Knowledge Graph (Ad-KG) that captures multi-relational semantics, a Semantic Embedding Layer that leverages large language models (LLMs) such as GPT and LLaMA to generate context-aware vector representations, a GNN + Attention Model that infers cross-entity dependencies, and a Database Optimization & Retrieval Layer based on vector indexing (FAISS/Milvus) for efficient semantic search. This layered architecture enables both accurate semantic matching and scalable retrieval, allowing personalized ad recommendations under large-scale heterogeneous workloads.

</details>


### [2] [Enhancing Retrieval-Augmented Generation with Topic-Enriched Embeddings: A Hybrid Approach Integrating Traditional NLP Techniques](https://arxiv.org/abs/2601.00891)
*Rodrigo Kataishi*

Main category: cs.IR

TL;DR: 该论文提出了一种主题增强嵌入方法，结合TF-IDF、主题建模和降维技术，将术语信号和主题结构整合到上下文句子嵌入中，以提高RAG系统的检索质量。


<details>
  <summary>Details</summary>
Motivation: 在主题重叠和主题变化高的语料库中，RAG系统的检索质量会下降。现有的上下文嵌入方法在处理这类复杂语义结构时存在局限性，需要更好的方法来捕捉术语级和主题级语义。

Method: 结合TF-IDF与主题建模（LSA和LDA）和降维技术，将潜在主题组织编码到嵌入表示中，然后与紧凑的上下文编码器（all-MiniLM）融合，创建主题增强嵌入。

Result: 在法律文本语料库上的实验显示，主题增强嵌入在聚类一致性和检索指标上均取得一致提升，提高了语义聚类质量、检索精度，同时相对于纯上下文基线减少了计算负担。

Conclusion: 主题增强嵌入能够有效捕捉术语级和主题级语义，可以作为知识密集型RAG管道中更可靠的实用组件，提高检索系统的可靠性。

Abstract: Retrieval-augmented generation (RAG) systems rely on accurate document retrieval to ground large language models (LLMs) in external knowledge, yet retrieval quality often degrades in corpora where topics overlap and thematic variation is high. This work proposes topic-enriched embeddings that integrate term-based signals and topic structure with contextual sentence embeddings. The approach combines TF-IDF with topic modeling and dimensionality reduction, using Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA) to encode latent topical organization, and fuses these representations with a compact contextual encoder (all-MiniLM). By jointly capturing term-level and topic-level semantics, topic-enriched embeddings improve semantic clustering, increase retrieval precision, and reduce computational burden relative to purely contextual baselines. Experiments on a legal-text corpus show consistent gains in clustering coherence and retrieval metrics, suggesting that topic-enriched embeddings can serve as a practical component for more reliable knowledge-intensive RAG pipelines.

</details>


### [3] [The Discovery Gap: How Product Hunt Startups Vanish in LLM Organic Discovery Queries](https://arxiv.org/abs/2601.00912)
*Amit Prakash Sharma*

Main category: cs.IR

TL;DR: 研究测试112个Product Hunt热门产品在ChatGPT和Perplexity中的可见性，发现AI能准确识别具体产品名称（99.4%/94.3%），但在发现式查询中成功率暴跌至3.32%/8.29%。生成式引擎优化(GEO)无效，传统SEO和社区影响力才是关键。


<details>
  <summary>Details</summary>
Motivation: 探究AI助手（如ChatGPT）在推荐产品时的表现，特别是初创公司的新产品能否被AI发现。关注生成式引擎优化(GEO)是否真的能提升产品在AI搜索结果中的可见性。

Method: 从2025年Product Hunt排行榜前500名中随机选择112个初创产品，对每个产品进行2,240次查询测试，使用ChatGPT(gpt-4o-mini)和Perplexity(sonar with web search)两个大语言模型。分析GEO评分、SEO信号（外链域名）、Product Hunt排名、Reddit社区存在等指标与AI可见性的相关性。

Result: 1. 具体名称查询：ChatGPT识别率99.4%，Perplexity 94.3%；2. 发现式查询成功率暴跌：ChatGPT 3.32%，Perplexity 8.29%（相差30倍）；3. GEO优化与AI发现率无相关性；4. Perplexity中传统SEO信号（外链域名r=+0.319）和Product Hunt排名(r=-0.286)预测可见性；5. Reddit社区存在显著相关(r=+0.395)。

Conclusion: 不要直接为AI发现进行优化，生成式引擎优化(GEO)无效。应优先建立传统SEO基础（外链建设、社区参与、平台排名），大语言模型的可见性自然会随之提升。对初创公司的实际建议：专注于SEO和社区建设，而非专门的AI优化策略。

Abstract: When someone asks ChatGPT to recommend a project management tool, which products show up in the response? And more importantly for startup founders: will their newly launched product ever appear? This research set out to answer these questions.
  I randomly selected 112 startups from the top 500 products featured on the 2025 Product Hunt leaderboard and tested each one across 2,240 queries to two different large language models: ChatGPT (gpt-4o-mini) and Perplexity (sonar with web search).
  The results were striking. When users asked about products by name, both LLMs recognized them almost perfectly: 99.4% for ChatGPT and 94.3% for Perplexity. But when users asked discovery-style questions like "What are the best AI tools launched this year?" the success rates collapsed to 3.32% and 8.29% respectively. That's a gap of 30-to-1 for ChatGPT.
  Perhaps the most surprising finding was that Generative Engine Optimization (GEO), the practice of optimizing website content for AI visibility, showed no correlation with actual discovery rates. Products with high GEO scores were no more likely to appear in organic queries than products with low scores.
  What did matter? For Perplexity, traditional SEO signals like referring domains (r = +0.319, p < 0.001) and Product Hunt ranking (r = -0.286, p = 0.002) predicted visibility. After cleaning the Reddit data for false positives, community presence also emerged as significant (r = +0.395, p = 0.002).
  The practical takeaway is counterintuitive: don't optimize for AI discovery directly. Instead, build the SEO foundation first and LLM visibility will follow.

</details>


### [4] [MACA: A Framework for Distilling Trustworthy LLMs into Efficient Retrievers](https://arxiv.org/abs/2601.00926)
*Satya Swaroop Gudipudi,Sahil Girhepuje,Ponnurangam Kumaraguru,Kristine Ma*

Main category: cs.IR

TL;DR: MACA是一种元数据感知的跨模型对齐方法，将校准的LLM重排器蒸馏到紧凑的学生检索器中，避免在线LLM调用，提升企业检索系统性能。


<details>
  <summary>Details</summary>
Motivation: 企业检索系统需要处理简短、不明确的查询，语义细微差别和元数据很重要，但每个查询使用LLM重排和手动标注成本高昂。

Method: 提出元数据感知跨模型对齐(MACA)：1) 使用元数据感知提示验证教师LLM的可靠性；2) 提供列表评分、困难负样本和校准相关性边界；3) 学生通过MetaFusion目标训练，结合元数据条件排序损失和跨模型边界损失。

Result: 在专有消费者银行FAQ语料库和BankFAQs上，MACA教师比MAFA基线在Accuracy@1上分别提升5点和3点。MACA学生显著优于预训练编码器，如MiniLM的Accuracy@1从0.23提升到0.48，同时避免LLM调用并支持检索增强生成。

Conclusion: MACA通过将校准的元数据感知LLM重排器蒸馏到紧凑检索器中，有效解决了企业检索中查询简短、元数据重要但LLM调用成本高的问题，实现了性能显著提升。

Abstract: Modern enterprise retrieval systems must handle short, underspecified queries such as ``foreign transaction fee refund'' and ``recent check status''. In these cases, semantic nuance and metadata matter but per-query large language model (LLM) re-ranking and manual labeling are costly. We present Metadata-Aware Cross-Model Alignment (MACA), which distills a calibrated metadata aware LLM re-ranker into a compact student retriever, avoiding online LLM calls. A metadata-aware prompt verifies the teacher's trustworthiness by checking consistency under permutations and robustness to paraphrases, then supplies listwise scores, hard negatives, and calibrated relevance margins. The student trains with MACA's MetaFusion objective, which combines a metadata conditioned ranking loss with a cross model margin loss so it learns to push the correct answer above semantically similar candidates with mismatched topic, sub-topic, or entity. On a proprietary consumer banking FAQ corpus and BankFAQs, the MACA teacher surpasses a MAFA baseline at Accuracy@1 by five points on the proprietary set and three points on BankFAQs. MACA students substantially outperform pretrained encoders; e.g., on the proprietary corpus MiniLM Accuracy@1 improves from 0.23 to 0.48, while keeping inference free of LLM calls and supporting retrieval-augmented generation.

</details>


### [5] [AlignUSER: Human-Aligned LLM Agents via World Models for Recommender System Evaluation](https://arxiv.org/abs/2601.00930)
*Nicolas Bougie,Gian Maria Marconi,Tony Yip,Narimasa Watanabe*

Main category: cs.IR

TL;DR: AlignUSER：一个从人类交互中学习世界模型驱动的智能体框架，用于更准确地模拟用户行为评估推荐系统


<details>
  <summary>Details</summary>
Motivation: 推荐系统评估面临挑战：离线指标与真实用户行为存在差距，交互数据稀缺。现有LLM智能体方法依赖少样本提示，对环境理解浅层，难以忠实复现用户行为。

Method: 1) 将世界建模形式化为下一状态预测任务，帮助智能体内化环境理解；2) 围绕演示生成反事实轨迹，提示LLM比较其决策与人类选择，识别次优行动并提取经验；3) 学习策略驱动智能体与推荐系统交互。

Result: 在多个数据集上评估显示，AlignUSER比先前工作更接近真实人类行为，在微观和宏观层面都表现出更好的一致性。

Conclusion: AlignUSER框架通过学习世界模型驱动的智能体，能够更准确地模拟用户行为，为推荐系统评估提供更可靠的合成用户方法。

Abstract: Evaluating recommender systems remains challenging due to the gap between offline metrics and real user behavior, as well as the scarcity of interaction data. Recent work explores large language model (LLM) agents as synthetic users, yet they typically rely on few-shot prompting, which yields a shallow understanding of the environment and limits their ability to faithfully reproduce user actions. We introduce AlignUSER, a framework that learns world-model-driven agents from human interactions. Given rollout sequences of actions and states, we formalize world modeling as a next state prediction task that helps the agent internalize the environment. To align actions with human personas, we generate counterfactual trajectories around demonstrations and prompt the LLM to compare its decisions with human choices, identify suboptimal actions, and extract lessons. The learned policy is then used to drive agent interactions with the recommender system. We evaluate AlignUSER across multiple datasets and demonstrate closer alignment with genuine humans than prior work, both at the micro and macro levels.

</details>


### [6] [ScienceDB AI: An LLM-Driven Agentic Recommender System for Large-Scale Scientific Data Sharing Services](https://arxiv.org/abs/2601.01118)
*Qingqing Long,Haotian Chen,Chenyang Zhao,Xiaolei Du,Xuezhi Wang,Pengyao Wang,Chengzan Li,Yuanchun Zhou,Hengshu Zhu*

Main category: cs.IR

TL;DR: ScienceDB AI是首个基于大语言模型的对话式推荐系统，专门为大规模科学数据集共享服务设计，通过深度语义理解和个性化推荐解决科学数据集共享难题。


<details>
  <summary>Details</summary>
Motivation: 科学数据集包含复杂的领域知识和上下文，传统协同过滤推荐方法难以有效处理。随着AI4S的快速发展，需要更智能的系统来促进科学数据集的共享和利用。

Method: 系统采用LLM驱动的智能推荐架构，包含：科学意图感知器提取结构化实验元素，结构化记忆压缩器管理多轮对话，可信检索增强生成框架采用两阶段检索机制并提供可引用的数据集参考。

Result: 通过对超过1000万个真实世界数据集进行线下和线上实验，ScienceDB AI表现出显著的有效性，能够准确推荐符合研究人员科学意图和需求的数据集。

Conclusion: ScienceDB AI是首个专门为大规模科学数据集共享服务设计的LLM驱动对话式推荐系统，通过自然语言对话和深度推理显著提升了科学数据集推荐的准确性和可信度。

Abstract: The rapid growth of AI for Science (AI4S) has underscored the significance of scientific datasets, leading to the establishment of numerous national scientific data centers and sharing platforms. Despite this progress, efficiently promoting dataset sharing and utilization for scientific research remains challenging. Scientific datasets contain intricate domain-specific knowledge and contexts, rendering traditional collaborative filtering-based recommenders inadequate. Recent advances in Large Language Models (LLMs) offer unprecedented opportunities to build conversational agents capable of deep semantic understanding and personalized recommendations. In response, we present ScienceDB AI, a novel LLM-driven agentic recommender system developed on Science Data Bank (ScienceDB), one of the largest global scientific data-sharing platforms. ScienceDB AI leverages natural language conversations and deep reasoning to accurately recommend datasets aligned with researchers' scientific intents and evolving requirements. The system introduces several innovations: a Scientific Intention Perceptor to extract structured experimental elements from complicated queries, a Structured Memory Compressor to manage multi-turn dialogues effectively, and a Trustworthy Retrieval-Augmented Generation (Trustworthy RAG) framework. The Trustworthy RAG employs a two-stage retrieval mechanism and provides citable dataset references via Citable Scientific Task Record (CSTR) identifiers, enhancing recommendation trustworthiness and reproducibility. Through extensive offline and online experiments using over 10 million real-world datasets, ScienceDB AI has demonstrated significant effectiveness. To our knowledge, ScienceDB AI is the first LLM-driven conversational recommender tailored explicitly for large-scale scientific dataset sharing services. The platform is publicly accessible at: https://ai.scidb.cn/en.

</details>


### [7] [Adaptive Diffusion-based Augmentation for Recommendation](https://arxiv.org/abs/2601.01448)
*Na Li,Fanghui Sun,Yan Zou,Yangfu Zhu,Xiatian Zhu,Ying Ma*

Main category: cs.IR

TL;DR: ADAR提出了一种基于扩散模型的负样本生成方法，通过控制扩散过程的采样时间步来生成具有挑战性的负样本，提升推荐模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统基于隐式反馈，只能观察到正样本，负采样方法存在两个问题：1) 可能将潜在的正样本误标为负样本；2) 缺乏对负样本选择的精确控制。需要生成可控的负样本而非从现有物品池中采样。

Method: ADAR利用扩散模型的渐进式破坏过程，模拟从正样本到负样本的连续过渡，实现细粒度的样本难度控制。通过理论识别正样本转变为负样本的过渡点，并推导出基于分数的自适应函数来确定最优采样时间步，从而生成具有挑战性的负样本。

Result: 实验证明ADAR具有广泛的兼容性，能够显著提升现有推荐模型的性能，包括协同过滤和序列推荐模型，且无需修改模型架构。

Conclusion: ADAR通过扩散模型生成可控的负样本，解决了传统负采样方法的局限性，为推荐系统提供了一种有效的负样本增强方法。

Abstract: Recommendation systems often rely on implicit feedback, where only positive user-item interactions can be observed. Negative sampling is therefore crucial to provide proper negative training signals. However, existing methods tend to mislabel potentially positive but unobserved items as negatives and lack precise control over negative sample selection. We aim to address these by generating controllable negative samples, rather than sampling from the existing item pool. In this context, we propose Adaptive Diffusion-based Augmentation for Recommendation (ADAR), a novel and model-agnostic module that leverages diffusion to synthesize informative negatives. Inspired by the progressive corruption process in diffusion, ADAR simulates a continuous transition from positive to negative, allowing for fine-grained control over sample hardness. To mine suitable negative samples, we theoretically identify the transition point at which a positive sample turns negative and derive a score-aware function to adaptively determine the optimal sampling timestep. By identifying this transition point, ADAR generates challenging negative samples that effectively refine the model's decision boundary. Experiments confirm that ADAR is broadly compatible and boosts the performance of existing recommendation models substantially, including collaborative filtering and sequential recommendation, without architectural modifications.

</details>


### [8] [Breadcrumbs in the Digital Forest: Tracing Criminals through Torrent Metadata with OSINT](https://arxiv.org/abs/2601.01492)
*Annelies de Jong,Giuseppe Cascavilla,Jessica De Pascale*

Main category: cs.IR

TL;DR: 利用BitTorrent元数据进行开源情报分析，通过用户画像和行为分析检测高风险行为，包括儿童剥削材料等非法内容。


<details>
  <summary>Details</summary>
Motivation: 虽然P2P网络在隐私和性能方面已有深入研究，但其元数据很少用于调查目的。本研究旨在探索如何利用公开的种子元数据进行开源情报分析，以支持执法和网络安全应用。

Method: 采用五步OSINT流程：源识别、数据收集、数据丰富、行为分析和结果呈现。从The Pirate Bay和UDP跟踪器收集了206个热门种子的60,000多个唯一IP地址数据，并通过地理位置、匿名化状态和儿童剥削材料参与标志进行数据丰富。

Result: 网络分析揭示了节点聚类、共同下载模式以及可疑用户使用隐私工具的行为。敏感电子书的案例研究表明，此类数据有助于检测对非法内容的可能兴趣。研究证明公开的种子元数据可以支持可扩展的自动化OSINT画像。

Conclusion: 该研究为数字取证提出了一种从噪声数据中提取有用信号的新方法，在执法、网络安全和威胁分析方面具有应用价值，扩展了开源情报分析的数据源。

Abstract: This work investigates the potential of torrent metadata as a source for open-source intelligence (OSINT), with a focus on user profiling and behavioral analysis. While peer-to-peer (P2P) networks such as BitTorrent are well studied with respect to privacy and performance, their metadata is rarely used for investigative purposes. This work presents a proof of concept demonstrating how tracker responses, torrent index data, and enriched IP metadata can reveal patterns associated with high-risk behavior.
  The research follows a five-step OSINT process: source identification, data collection, enrichment, behavioral analysis, and presentation of the results. Data were collected from The Pirate Bay and UDP trackers, yielding a dataset of more than 60,000 unique IP addresses across 206 popular torrents. The data were enriched with geolocation, anonymization status, and flags of involvement in child exploitation material (CEM). A case study on sensitive e-books shows how such data can help detect possible interest in illicit content.
  Network analysis highlights peer clustering, co-download patterns, and the use of privacy tools by suspicious users. The study shows that publicly available torrent metadata can support scalable and automated OSINT profiling.
  This work adds to digital forensics by proposing a new method to extract useful signals from noisy data, with applications in law enforcement, cybersecurity, and threat analysis.

</details>


### [9] [OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly Novelty Assessment](https://arxiv.org/abs/2601.01576)
*Ming Zhang,Kexin Tan,Yueyuan Huang,Yujiong Shen,Chunchun Ma,Li Ju,Xinran Zhang,Yuhui Wang,Wenqing Jing,Jingyi Deng,Huayu Sha,Binze Hu,Jingqi Tong,Changhao Jiang,Yage Geng,Yuankai Ying,Yue Zhang,Zhangyue Yin,Zhiheng Xi,Shihan Dou,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.IR

TL;DR: OpenNovelty是一个基于LLM的代理系统，通过四阶段流程（提取贡献、检索相关文献、构建分类体系、生成结构化报告）为学术论文提供透明、基于证据的新颖性分析，已在500+ ICLR 2026投稿上部署。


<details>
  <summary>Details</summary>
Motivation: 同行评审中评估新颖性具有挑战性，因为评审者需要在庞大且快速发展的文献中评估投稿。现有方法缺乏透明度和证据基础，需要一种能够提供可验证判断的系统化工具。

Method: 系统采用四阶段流程：1) 提取核心任务和贡献声明生成检索查询；2) 通过语义搜索引擎检索相关先前工作；3) 构建核心任务相关工作的层次分类体系，并进行贡献级别的全文比较；4) 将所有分析综合成包含明确引用和证据片段的结构化新颖性报告。

Result: 系统已在500+ ICLR 2026投稿上部署，所有报告公开可用。初步分析表明系统能够识别相关先前工作，包括作者可能忽略的密切相关的论文，提供可验证的判断基础。

Conclusion: OpenNovelty旨在为研究社区提供一个可扩展的工具，促进公平、一致和基于证据的同行评审，通过将评估基于真实检索的论文来确保透明度和可验证性。

Abstract: Evaluating novelty is critical yet challenging in peer review, as reviewers must assess submissions against a vast, rapidly evolving literature. This report presents OpenNovelty, an LLM-powered agentic system for transparent, evidence-based novelty analysis. The system operates through four phases: (1) extracting the core task and contribution claims to generate retrieval queries; (2) retrieving relevant prior work based on extracted queries via semantic search engine; (3) constructing a hierarchical taxonomy of core-task-related work and performing contribution-level full-text comparisons against each contribution; and (4) synthesizing all analyses into a structured novelty report with explicit citations and evidence snippets. Unlike naive LLM-based approaches, \textsc{OpenNovelty} grounds all assessments in retrieved real papers, ensuring verifiable judgments. We deploy our system on 500+ ICLR 2026 submissions with all reports publicly available on our website, and preliminary analysis suggests it can identify relevant prior work, including closely related papers that authors may overlook. OpenNovelty aims to empower the research community with a scalable tool that promotes fair, consistent, and evidence-backed peer review.

</details>


### [10] [LACONIC: Dense-Level Effectiveness for Scalable Sparse Retrieval via a Two-Phase Training Curriculum](https://arxiv.org/abs/2601.01684)
*Zhichao Xu,Shengyao Zhuang,Crystina Zhang,Xueguang Ma,Yijun Tian,Maitrey Mehta,Jimmy Lin,Vivek Srikumar*

Main category: cs.IR

TL;DR: LACONIC是一个基于Llama-3架构的稀疏检索模型家族，通过两阶段训练实现高效检索，在性能接近密集模型的同时大幅减少内存使用和计算需求。


<details>
  <summary>Details</summary>
Motivation: 密集检索模型虽然性能优越，但部署时面临高内存需求和GPU依赖的限制。稀疏检索虽然能通过倒排索引实现高效搜索，但历史上关注度较低。需要一种既能保持高性能又能在CPU硬件上高效运行的检索解决方案。

Method: 基于Llama-3架构（1B、3B、8B）构建稀疏检索模型家族。采用两阶段训练策略：1）弱监督预微调，使因果LLM适应双向上下文处理；2）使用精心筛选的困难负例进行高质量微调。

Result: LACONIC-8B在MTEB检索基准测试中达到60.2 nDCG，在2026年1月1日的排行榜上排名第15位，同时比等效密集模型减少71%的索引内存使用。能够在商用CPU硬件上高效运行，计算预算远低于竞争模型。

Conclusion: LACONIC通过稀疏检索方法有效弥合了与密集模型的性能差距，提供了可扩展且高效的检索解决方案，特别适合实际搜索应用场景，在性能和效率之间取得了良好平衡。

Abstract: While dense retrieval models have become the standard for state-of-the-art information retrieval, their deployment is often constrained by high memory requirements and reliance on GPU accelerators for vector similarity search. Learned sparse retrieval offers a compelling alternative by enabling efficient search via inverted indices, yet it has historically received less attention than dense approaches. In this report, we introduce LACONIC, a family of learned sparse retrievers based on the Llama-3 architecture (1B, 3B, and 8B). We propose a streamlined two-phase training curriculum consisting of (1) weakly supervised pre-finetuning to adapt causal LLMs for bidirectional contextualization and (2) high-signal finetuning using curated hard negatives. Our results demonstrate that LACONIC effectively bridges the performance gap with dense models: the 8B variant achieves a state-of-the-art 60.2 nDCG on the MTEB Retrieval benchmark, ranking 15th on the leaderboard as of January 1, 2026, while utilizing 71\% less index memory than an equivalent dense model. By delivering high retrieval effectiveness on commodity CPU hardware with a fraction of the compute budget required by competing models, LACONIC provides a scalable and efficient solution for real-world search applications.

</details>


### [11] [When Attention Becomes Exposure in Generative Search](https://arxiv.org/abs/2601.01750)
*Shayan Alipour,Mehdi Kargar,Morteza Zihayat*

Main category: cs.IR

TL;DR: 生成式搜索引擎的引用存在曝光偏差，倾向于已有知名度的声音，这可能会固化现有优势并缩小观点多样性。


<details>
  <summary>Details</summary>
Motivation: 研究生成式搜索引擎在Web3企业查询中的引用曝光是否受到外部注意力市场的影响，以及这种影响如何塑造信息获取。

Method: 对44家Web3企业进行审计：1) 分析企业创作者社区的持续性；2) 通过企业特定查询检查引用曝光模式；3) 分析粉丝基础和创作者核心集中度与曝光排名的关系。

Result: 1) 企业创作者社区具有时间持续性；2) 更受欢迎的声音系统性获得更多引用曝光；3) 更大粉丝基础和更集中的创作者核心与更高排名曝光相关。

Conclusion: 生成式搜索引擎引用存在向已有知名声音倾斜的曝光偏差，这可能固化现有优势并减少观点多样性。

Abstract: Generative search engines are reshaping information access by replacing traditional ranked lists with synthesized answers and references. In parallel, with the growth of Web3 platforms, incentive-driven creator ecosystems have become an essential part of how enterprises build visibility and community by rewarding creators for contributing to shared narratives. However, the extent to which exposure in generative search engine citations is shaped by external attention markets remains uncertain. In this study, we audit the exposure for 44 Web3 enterprises. First, we show that the creator community around each enterprise is persistent over time. Second, enterprise-specific queries reveal that more popular voices systematically receive greater citation exposure than others. Third, we find that larger follower bases and enterprises with more concentrated creator cores are associated with higher-ranked exposure. Together, these results show that generative search engine citations exhibit exposure bias toward already prominent voices, which risks entrenching incumbents and narrowing viewpoint diversity.

</details>


### [12] [Query-Document Dense Vectors for LLM Relevance Judgment Bias Analysis](https://arxiv.org/abs/2601.01751)
*Samaneh Mohtadi,Gianluca Demartini*

Main category: cs.IR

TL;DR: 该研究提出了一种基于聚类的框架来分析LLM作为相关性评估者时的系统性错误模式，发现LLM与人类评估者的分歧集中在特定语义集群而非随机分布。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM已被用作信息检索评估中的相关性评估者，但现有研究主要关注LLM与人类评估者的平均表现差异，而缺乏对LLM系统性错误模式的理解。本研究旨在识别LLM在判断相关性时是否存在系统性错误，而不仅仅是评估其平均表现。

Method: 提出了一种新颖的表示方法，将查询-文档对嵌入到联合语义空间中，将相关性视为关系属性。引入基于聚类的框架来分析相关性标签分布，比较LLM和人类标签以识别分歧模式并定位系统性分歧区域。

Result: 在TREC Deep Learning 2019和2020数据集上的实验表明，人类与LLM之间的系统性分歧集中在特定语义集群中，而非随机分布。查询级分析揭示了在定义寻求、政策相关或模糊上下文中的重复性失败。具有跨集群大变异性的查询成为分歧热点，LLM倾向于低估相关内容或过度包含无关材料。

Conclusion: 该框架通过将全局诊断与局部聚类相结合，揭示了LLM判断中的隐藏弱点，为实现偏差感知和更可靠的信息检索评估提供了方法。

Abstract: Large Language Models (LLMs) have been used as relevance assessors for Information Retrieval (IR) evaluation collection creation due to reduced cost and increased scalability as compared to human assessors. While previous research has looked at the reliability of LLMs as compared to human assessors, in this work, we aim to understand if LLMs make systematic mistakes when judging relevance, rather than just understanding how good they are on average. To this aim, we propose a novel representational method for queries and documents that allows us to analyze relevance label distributions and compare LLM and human labels to identify patterns of disagreement and localize systematic areas of disagreement. We introduce a clustering-based framework that embeds query-document (Q-D) pairs into a joint semantic space, treating relevance as a relational property. Experiments on TREC Deep Learning 2019 and 2020 show that systematic disagreement between humans and LLMs is concentrated in specific semantic clusters rather than distributed randomly. Query-level analyses reveal recurring failures, most often in definition-seeking, policy-related, or ambiguous contexts. Queries with large variation in agreement across their clusters emerge as disagreement hotspots, where LLMs tend to under-recall relevant content or over-include irrelevant material. This framework links global diagnostics with localized clustering to uncover hidden weaknesses in LLM judgments, enabling bias-aware and more reliable IR evaluation.

</details>


### [13] [MergeRec: Model Merging for Data-Isolated Cross-Domain Sequential Recommendation](https://arxiv.org/abs/2601.01753)
*Hyunsoo Kim,Jaewan Moon,Seongmin Park,Jongwuk Lee*

Main category: cs.IR

TL;DR: MergeRec：基于模型融合的数据隔离跨域序列推荐新框架，无需共享原始用户数据，通过伪用户数据构建和协同融合优化提升跨域泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有跨域推荐方法依赖域间重叠用户/物品或忽略隐私约束，无法处理数据隔离的实际场景，需要一种无需共享原始数据的跨域推荐解决方案

Method: 包含三个核心组件：1) 融合初始化使用无训练融合技术；2) 伪用户数据构建，将每个物品视为虚拟序列；3) 协同融合优化，结合推荐损失和蒸馏损失联合优化域特定融合权重

Result: MergeRec在保持原始模型优势的同时显著提升对未见域的泛化能力，相比传统模型融合方法平均Recall@10提升达17.21%

Conclusion: 模型融合是构建通用推荐系统的可扩展有效方法，MergeRec为数据隔离跨域序列推荐提供了实用解决方案，具有实际应用价值

Abstract: Modern recommender systems trained on domain-specific data often struggle to generalize across multiple domains. Cross-domain sequential recommendation has emerged as a promising research direction to address this challenge; however, existing approaches face fundamental limitations, such as reliance on overlapping users or items across domains, or unrealistic assumptions that ignore privacy constraints. In this work, we propose a new framework, MergeRec, based on model merging under a new and realistic problem setting termed data-isolated cross-domain sequential recommendation, where raw user interaction data cannot be shared across domains. MergeRec consists of three key components: (1) merging initialization, (2) pseudo-user data construction, and (3) collaborative merging optimization. First, we initialize a merged model using training-free merging techniques. Next, we construct pseudo-user data by treating each item as a virtual sequence in each domain, enabling the synthesis of meaningful training samples without relying on real user interactions. Finally, we optimize domain-specific merging weights through a joint objective that combines a recommendation loss, which encourages the merged model to identify relevant items, and a distillation loss, which transfers collaborative filtering signals from the fine-tuned source models. Extensive experiments demonstrate that MergeRec not only preserves the strengths of the original models but also significantly enhances generalizability to unseen domains. Compared to conventional model merging methods, MergeRec consistently achieves superior performance, with average improvements of up to 17.21% in Recall@10, highlighting the potential of model merging as a scalable and effective approach for building universal recommender systems. The source code is available at https://github.com/DIALLab-SKKU/MergeRec.

</details>


### [14] [SRAS: A Lightweight Reinforcement Learning-based Document Selector for Edge-Native RAG Pipelines](https://arxiv.org/abs/2601.01785)
*Rajiv Chaitanya Muttur*

Main category: cs.IR

TL;DR: SRAS是一种基于强化学习的轻量级文档选择器，专为边缘设备RAG系统设计，在严格的计算和延迟约束下实现高效文档选择。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统使用固定的top-k文档选择机制，忽略了生成质量且计算开销大，无法满足边缘设备部署的严格延迟和计算限制需求。

Method: 使用强化学习训练轻量级策略，采用PPO算法和混合奖励信号（Relaxed F1 + BERTScore），在严格token和计算约束下学习紧凑的文档选择策略。

Result: SRAS仅占用约0.76MB内存，CPU上延迟<1秒，在合成QA基准上优于监督和随机选择器，在SQuAD v2上达到0.8546 BERTScore F1，无需领域特定调优。

Conclusion: 首次证明基于强化学习的文档选择可以实现超轻量、延迟感知且有效的边缘设备RAG部署，为资源受限环境提供了可行的解决方案。

Abstract: Retrieval-Augmented Generation (RAG) systems often rely on fixed top-k document selection mechanisms that ignore downstream generation quality and impose computational overheads. We propose SRAS (Sparse Reward-Aware Selector), a lightweight document selector trained via reinforcement learning (RL) for edge-native RAG deployment. Unlike prior RL-based retrievers that assume large memory and latency budgets, SRAS learns a compact (~0.76MB) policy using Proximal Policy Optimization (PPO), guided by a hybrid reward signal combining Relaxed F1 and BERTScore. Our method operates under tight token and compute constraints, maintaining <1s latency on CPU. SRAS outperforms supervised and random selectors on a synthetic QA benchmark, and generalizes to real-world data, achieving BERTScore F1 of 0.8546 on SQuAD v2 without domain-specific tuning. This work is the first to demonstrate that RL-based document selection can be made ultra-lightweight, latency-aware, and effective for on-device RAG pipelines.

</details>


### [15] [A Hybrid Architecture for Multi-Stage Claim Document Understanding: Combining Vision-Language Models and Machine Learning for Real-Time Processing](https://arxiv.org/abs/2601.01897)
*Lilu Cheng,Jingjun Lu,Yi Xuan Chan,Quoc Khai Nguyen,John Bi,Sean Ho*

Main category: cs.IR

TL;DR: 提出一个结合传统机器学习与视觉语言模型的多阶段管道，用于从异构医疗理赔文档中高效提取结构化信息，实现95%文档分类准确率和87%字段提取准确率，处理速度比人工快300倍。


<details>
  <summary>Details</summary>
Motivation: 医疗理赔文档通常以扫描PDF或照片形式存在，具有内容异构性（打字发票到手写医疗报告）、语言多样性、图像质量不一致和布局多样等问题，给自动化解析和信息提取带来重大挑战。Fullerton Health每年处理数千万份跨九国市场的理赔，急需高效自动化解决方案。

Method: 采用多阶段管道：1) 使用多语言OCR引擎PaddleOCR进行文本识别；2) 传统逻辑回归分类器进行文档类型分类；3) 紧凑型视觉语言模型Qwen 2.5-VL-7B进行字段提取。结合传统机器学习与现代VLM技术。

Result: 文档类型分类准确率超过95%，字段级提取准确率约87%，平均处理延迟低于2秒/文档。相比人工处理每份理赔约10分钟，效率提升300倍。系统已在移动应用中部署，每周处理越南和新加坡数万份理赔。

Conclusion: 传统机器学习模型与现代视觉语言模型结合，能够实现生产级准确率和速度，满足现实世界自动化需求。该解决方案已成功部署并处理大规模理赔数据，证明了其在实际应用中的有效性。

Abstract: Claims documents are fundamental to healthcare and insurance operations, serving as the basis for reimbursement, auditing, and compliance. However, these documents are typically not born digital; they often exist as scanned PDFs or photographs captured under uncontrolled conditions. Consequently, they exhibit significant content heterogeneity, ranging from typed invoices to handwritten medical reports, as well as linguistic diversity. This challenge is exemplified by operations at Fullerton Health, which handles tens of millions of claims annually across nine markets, including Singapore, the Philippines, Indonesia, Malaysia, Mainland China, Hong Kong, Vietnam, Papua New Guinea, and Cambodia. Such variability, coupled with inconsistent image quality and diverse layouts, poses a significant obstacle to automated parsing and structured information extraction.
  This paper presents a robust multi-stage pipeline that integrates the multilingual optical character recognition (OCR) engine PaddleOCR, a traditional Logistic Regression classifier, and a compact Vision-Language Model (VLM), Qwen 2.5-VL-7B, to achieve efficient and accurate field extraction from large-scale claims data. The proposed system achieves a document-type classification accuracy of over 95 percent and a field-level extraction accuracy of approximately 87 percent, while maintaining an average processing latency of under 2 seconds per document. Compared to manual processing, which typically requires around 10 minutes per claim, our system delivers a 300x improvement in efficiency. These results demonstrate that combining traditional machine learning models with modern VLMs enables production-grade accuracy and speed for real-world automation. The solution has been successfully deployed in our mobile application and is currently processing tens of thousands of claims weekly from Vietnam and Singapore.

</details>


### [16] [MCGI: Manifold-Consistent Graph Indexing for Billion-Scale Disk-Resident Vector Search](https://arxiv.org/abs/2601.01930)
*Dongfang Zhao*

Main category: cs.IR

TL;DR: MCGI是一种基于流形一致性的图索引方法，通过局部本征维度动态调整搜索策略，解决高维空间中欧几里得-测地线不匹配问题，显著提升高维ANN搜索性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于图的近似最近邻搜索在高维空间中存在"欧几里得-测地线不匹配"问题，贪婪路由会偏离底层数据流形，导致性能下降。需要一种能够适应数据内在几何结构的索引方法。

Method: 提出流形一致图索引(MCGI)，这是一种几何感知的磁盘驻留索引方法。利用局部本征维度(LID)进行原位几何分析，动态调整波束搜索预算，消除对静态超参数的依赖，保持流形一致的拓扑连接性。

Result: 在高维GIST1M数据集上，MCGI在95%召回率下实现了5.8倍的吞吐量提升；在十亿级SIFT1B数据集上，高召回查询延迟降低了3倍，同时在标准低维数据集上保持性能相当。

Conclusion: MCGI通过几何感知的索引设计有效解决了高维ANN搜索中的流形一致性问题，在保持理论近似保证的同时，显著提升了大规模高维数据集上的搜索性能。

Abstract: Graph-based Approximate Nearest Neighbor (ANN) search often suffers from performance degradation in high-dimensional spaces due to the ``Euclidean-Geodesic mismatch,'' where greedy routing diverges from the underlying data manifold. To address this, we propose Manifold-Consistent Graph Indexing (MCGI), a geometry-aware and disk-resident indexing method that leverages Local Intrinsic Dimensionality (LID) to dynamically adapt search strategies to the data's intrinsic geometry. Unlike standard algorithms that treat dimensions uniformly, MCGI modulates its beam search budget based on in situ geometric analysis, eliminating dependency on static hyperparameters. Theoretical analysis confirms that MCGI enables improved approximation guarantees by preserving manifold-consistent topological connectivity. Empirically, MCGI achieves 5.8$\times$ higher throughput at 95\% recall on high-dimensional GIST1M compared to state-of-the-art DiskANN. On the billion-scale SIFT1B dataset, MCGI further validates its scalability by reducing high-recall query latency by 3$\times$, while maintaining performance parity on standard lower-dimensional datasets.

</details>


### [17] [Exploring Diversity, Novelty, and Popularity Bias in ChatGPT's Recommendations](https://arxiv.org/abs/2601.01997)
*Dario Di Palma,Giovanni Maria Biancofiore,Vito Walter Anelli,Fedelucio Narducci,Tommaso Di Noia*

Main category: cs.IR

TL;DR: 该研究评估了ChatGPT-3.5和ChatGPT-4在推荐系统中的多样性、新颖性和流行度偏差表现，发现在Top-N推荐和冷启动场景中，ChatGPT-4能匹配或超越传统推荐系统，特别在冷启动场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管ChatGPT在推荐系统中的应用受到关注，但现有研究主要关注准确性，缺乏对其多样性、新颖性和潜在偏差（如流行度偏差）的全面分析。随着这些模型的广泛应用，理解这些方面对提升用户满意度和实现长期个性化至关重要。

Method: 研究评估了ChatGPT-3.5和ChatGPT-4在三个不同数据集上的表现，评估维度包括多样性、新颖性和流行度偏差，并测试了Top-N推荐和冷启动两种场景。

Result: ChatGPT-4能匹配或超越传统推荐系统，在推荐的新颖性和多样性之间取得良好平衡。在冷启动场景中，ChatGPT模型在准确性和新颖性方面都表现出色，特别适合新用户。

Conclusion: 该研究揭示了ChatGPT推荐的优势和局限，为理解这些模型在准确性之外提供推荐的能力提供了新视角，表明ChatGPT在推荐系统中具有重要应用潜力，特别是在冷启动场景中。

Abstract: ChatGPT has emerged as a versatile tool, demonstrating capabilities across diverse domains. Given these successes, the Recommender Systems (RSs) community has begun investigating its applications within recommendation scenarios primarily focusing on accuracy. While the integration of ChatGPT into RSs has garnered significant attention, a comprehensive analysis of its performance across various dimensions remains largely unexplored. Specifically, the capabilities of providing diverse and novel recommendations or exploring potential biases such as popularity bias have not been thoroughly examined. As the use of these models continues to expand, understanding these aspects is crucial for enhancing user satisfaction and achieving long-term personalization.
  This study investigates the recommendations provided by ChatGPT-3.5 and ChatGPT-4 by assessing ChatGPT's capabilities in terms of diversity, novelty, and popularity bias. We evaluate these models on three distinct datasets and assess their performance in Top-N recommendation and cold-start scenarios. The findings reveal that ChatGPT-4 matches or surpasses traditional recommenders, demonstrating the ability to balance novelty and diversity in recommendations. Furthermore, in the cold-start scenario, ChatGPT models exhibit superior performance in both accuracy and novelty, suggesting they can be particularly beneficial for new users. This research highlights the strengths and limitations of ChatGPT's recommendations, offering new perspectives on the capacity of these models to provide recommendations beyond accuracy-focused metrics.

</details>


### [18] [Exploring Approaches for Detecting Memorization of Recommender System Data in Large Language Models](https://arxiv.org/abs/2601.02002)
*Antonio Colacicco,Vito Guida,Dario Di Palma,Fedelucio Narducci,Tommaso Di Noia*

Main category: cs.IR

TL;DR: 论文评估了三种从LLM中提取记忆化推荐数据的方法：越狱提示工程、无监督潜在知识发现和自动提示工程，发现自动提示工程是最有前景的策略。


<details>
  <summary>Details</summary>
Motivation: LLM在推荐场景中应用日益广泛，但其训练数据不公开引发数据泄露担忧。已有研究表明MovieLens-1M数据集被LLaMA和OpenAI模型记忆，但现有提取方法仅依赖手动提示工程，需要探索更有效的检测和提取方法。

Method: 评估三种方法：1) 越狱提示工程；2) 无监督潜在知识发现，通过对比一致搜索(CCS)和聚类范数探测内部激活；3) 自动提示工程(APE)，将提示发现作为元学习过程迭代优化候选指令。

Result: 在MovieLens-1M数据集上实验发现：越狱提示无法改善记忆项检索且结果不一致；CCS能可靠区分真实与伪造电影标题但无法处理数值用户和评分数据；APE能中等程度成功检索项目级信息但难以恢复数值交互。

Conclusion: 自动优化提示是从LLM中提取记忆化样本的最有前景策略，但当前方法在数值数据提取方面仍有局限，需要进一步研究。

Abstract: Large Language Models (LLMs) are increasingly applied in recommendation scenarios due to their strong natural language understanding and generation capabilities. However, they are trained on vast corpora whose contents are not publicly disclosed, raising concerns about data leakage. Recent work has shown that the MovieLens-1M dataset is memorized by both the LLaMA and OpenAI model families, but the extraction of such memorized data has so far relied exclusively on manual prompt engineering. In this paper, we pose three main questions: Is it possible to enhance manual prompting? Can LLM memorization be detected through methods beyond manual prompting? And can the detection of data leakage be automated? To address these questions, we evaluate three approaches: (i) jailbreak prompt engineering; (ii) unsupervised latent knowledge discovery, probing internal activations via Contrast-Consistent Search (CCS) and Cluster-Norm; and (iii) Automatic Prompt Engineering (APE), which frames prompt discovery as a meta-learning process that iteratively refines candidate instructions. Experiments on MovieLens-1M using LLaMA models show that jailbreak prompting does not improve the retrieval of memorized items and remains inconsistent; CCS reliably distinguishes genuine from fabricated movie titles but fails on numerical user and rating data; and APE retrieves item-level information with moderate success yet struggles to recover numerical interactions. These findings suggest that automatically optimizing prompts is the most promising strategy for extracting memorized samples.

</details>


### [19] [Cold-Starting Podcast Ads and Promotions with Multi-Task Learning on Spotify](https://arxiv.org/abs/2601.02306)
*Shivam Verma,Hannes Karlbom,Yu Zhao,Nick Topping,Vivian Chen,Kieran Stanley,Bharath Rengarajan*

Main category: cs.IR

TL;DR: Spotify提出统一多目标模型，同时优化广告和推广活动的个性化推荐，通过迁移学习和多任务学习解决冷启动问题，显著降低成本和提升流媒体率。


<details>
  <summary>Details</summary>
Motivation: 解决播客生态系统中广告和推广活动的个性化推荐挑战，特别是针对新广告目标和冷启动问题。传统系统存在孤立的推荐管道，维护困难且冷启动性能不佳。

Method: 采用多任务学习框架，利用大规模广告和内容交互数据进行迁移学习。模型共享用户、内容、上下文和创意特征的表示，联合优化广告和推广的多个目标（如流媒体、点击、关注）。

Result: 在线A/B测试显示：有效每流成本降低高达22%（特别是较少播放的播客），播客流媒体率提升18-24%。离线实验验证了辅助目标和特征组对冷启动性能的贡献。

Conclusion: 统一建模策略提高了可维护性、冷启动性能和覆盖率，打破了历史上孤立的推荐管道。讨论了实际广告系统中联合模型的权衡取舍。

Abstract: We present a unified multi-objective model for targeting both advertisements and promotions within the Spotify podcast ecosystem. Our approach addresses key challenges in personalization and cold-start initialization, particularly for new advertising objectives. By leveraging transfer learning from large-scale ad and content interactions within a multi-task learning (MTL) framework, a single joint model can be fine-tuned or directly applied to new or low-data targeting tasks, including in-app promotions. This multi-objective design jointly optimizes podcast outcomes such as streams, clicks, and follows for both ads and promotions using a shared representation over user, content, context, and creative features, effectively supporting diverse business goals while improving user experience. Online A/B tests show up to a 22% reduction in effective Cost-Per-Stream (eCPS), particularly for less-streamed podcasts, and an 18-24% increase in podcast stream rates. Offline experiments and ablations highlight the contribution of ancillary objectives and feature groups to cold-start performance. Our experience shows that a unified modeling strategy improves maintainability, cold-start performance, and coverage, while breaking down historically siloed targeting pipelines. We discuss practical trade-offs of such joint models in a real-world advertising system.

</details>
