{"id": "2511.07573", "categories": ["cs.IR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.07573", "abs": "https://arxiv.org/abs/2511.07573", "authors": ["Kamand Kalashi", "Babak Teimourpour"], "title": "A Hybrid Multimodal Deep Learning Framework for Intelligent Fashion Recommendation", "comment": "8 pages, 1 figure", "summary": "The rapid expansion of online fashion platforms has created an increasing demand for intelligent recommender systems capable of understanding both visual and textual cues. This paper proposes a hybrid multimodal deep learning framework for fashion recommendation that jointly addresses two key tasks: outfit compatibility prediction and complementary item retrieval. The model leverages the visual and textual encoders of the CLIP architecture to obtain joint latent representations of fashion items, which are then integrated into a unified feature vector and processed by a transformer encoder. For compatibility prediction, an \"outfit token\" is introduced to model the holistic relationships among items, achieving an AUC of 0.95 on the Polyvore dataset. For complementary item retrieval, a \"target item token\" representing the desired item description is used to retrieve compatible items, reaching an accuracy of 69.24% under the Fill-in-the-Blank (FITB) metric. The proposed approach demonstrates strong performance across both tasks, highlighting the effectiveness of multimodal learning for fashion recommendation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCLIP\u67b6\u6784\u7684\u6df7\u5408\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u65f6\u5c1a\u63a8\u8350\uff0c\u540c\u65f6\u5904\u7406\u670d\u88c5\u642d\u914d\u517c\u5bb9\u6027\u9884\u6d4b\u548c\u4e92\u8865\u7269\u54c1\u68c0\u7d22\u4e24\u4e2a\u4efb\u52a1\u3002", "motivation": "\u5728\u7ebf\u65f6\u5c1a\u5e73\u53f0\u7684\u5feb\u901f\u53d1\u5c55\u5bf9\u80fd\u591f\u7406\u89e3\u89c6\u89c9\u548c\u6587\u672c\u7ebf\u7d22\u7684\u667a\u80fd\u63a8\u8350\u7cfb\u7edf\u63d0\u51fa\u4e86\u65e5\u76ca\u589e\u957f\u7684\u9700\u6c42\u3002", "method": "\u5229\u7528CLIP\u67b6\u6784\u7684\u89c6\u89c9\u548c\u6587\u672c\u7f16\u7801\u5668\u83b7\u53d6\u65f6\u5c1a\u7269\u54c1\u7684\u8054\u5408\u6f5c\u5728\u8868\u793a\uff0c\u901a\u8fc7transformer\u7f16\u7801\u5668\u5904\u7406\u7edf\u4e00\u7279\u5f81\u5411\u91cf\u3002\u5bf9\u4e8e\u517c\u5bb9\u6027\u9884\u6d4b\u5f15\u5165'\u642d\u914d\u6807\u8bb0'\u5efa\u6a21\u7269\u54c1\u95f4\u6574\u4f53\u5173\u7cfb\uff1b\u5bf9\u4e8e\u4e92\u8865\u7269\u54c1\u68c0\u7d22\u4f7f\u7528'\u76ee\u6807\u7269\u54c1\u6807\u8bb0'\u8868\u793a\u6240\u9700\u7269\u54c1\u63cf\u8ff0\u3002", "result": "\u5728Polyvore\u6570\u636e\u96c6\u4e0a\uff0c\u517c\u5bb9\u6027\u9884\u6d4bAUC\u8fbe\u52300.95\uff1b\u4e92\u8865\u7269\u54c1\u68c0\u7d22\u5728FITB\u6307\u6807\u4e0b\u51c6\u786e\u7387\u8fbe\u523069.24%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e24\u4e2a\u4efb\u52a1\u4e0a\u90fd\u8868\u73b0\u51fa\u5f3a\u52b2\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u5728\u65f6\u5c1a\u63a8\u8350\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.07595", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.07595", "abs": "https://arxiv.org/abs/2511.07595", "authors": ["\u00d6zay Ezerceli", "Gizem G\u00fcm\u00fc\u015f\u00e7eki\u00e7ci", "Tu\u011fba Erko\u00e7", "Berke \u00d6zen\u00e7"], "title": "TurkEmbed4Retrieval: Turkish Embedding Model for Retrieval Task", "comment": "4 pages, in Turkish language, 1 figure, conference", "summary": "In this work, we introduce TurkEmbed4Retrieval, a retrieval specialized variant of the TurkEmbed model originally designed for Natural Language Inference (NLI) and Semantic Textual Similarity (STS) tasks. By fine-tuning the base model on the MS MARCO TR dataset using advanced training techniques, including Matryoshka representation learning and a tailored multiple negatives ranking loss, we achieve SOTA performance for Turkish retrieval tasks. Extensive experiments demonstrate that our model outperforms Turkish colBERT by 19,26% on key retrieval metrics for the Scifact TR dataset, thereby establishing a new benchmark for Turkish information retrieval.", "AI": {"tldr": "\u63d0\u51fa\u4e86TurkEmbed4Retrieval\u6a21\u578b\uff0c\u8fd9\u662f\u4e13\u4e3a\u571f\u8033\u5176\u8bed\u68c0\u7d22\u4efb\u52a1\u4f18\u5316\u7684TurkEmbed\u53d8\u4f53\uff0c\u5728MS MARCO TR\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u540e\uff0c\u5728Scifact TR\u6570\u636e\u96c6\u4e0a\u6bd4\u571f\u8033\u5176colBERT\u6027\u80fd\u63d0\u534719.26%\u3002", "motivation": "\u539f\u59cb\u7684TurkEmbed\u6a21\u578b\u4e3b\u8981\u9488\u5bf9NLI\u548cSTS\u4efb\u52a1\u8bbe\u8ba1\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u571f\u8033\u5176\u8bed\u68c0\u7d22\u4efb\u52a1\u8fdb\u884c\u4f18\u5316\u3002", "method": "\u5728MS MARCO TR\u6570\u636e\u96c6\u4e0a\u5bf9\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u91c7\u7528Matryoshka\u8868\u793a\u5b66\u4e60\u548c\u5b9a\u5236\u7684\u591a\u8d1f\u4f8b\u6392\u5e8f\u635f\u5931\u7b49\u5148\u8fdb\u8bad\u7ec3\u6280\u672f\u3002", "result": "\u5728Scifact TR\u6570\u636e\u96c6\u7684\u5173\u952e\u68c0\u7d22\u6307\u6807\u4e0a\uff0c\u6bd4\u571f\u8033\u5176colBERT\u6027\u80fd\u63d0\u534719.26%\uff0c\u8fbe\u5230SOTA\u6c34\u5e73\u3002", "conclusion": "\u4e3a\u571f\u8033\u5176\u8bed\u4fe1\u606f\u68c0\u7d22\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\uff0c\u8bc1\u660e\u4e86\u4e13\u95e8\u9488\u5bf9\u68c0\u7d22\u4efb\u52a1\u4f18\u5316\u7684\u6a21\u578b\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.08006", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.08006", "abs": "https://arxiv.org/abs/2511.08006", "authors": ["Peiyu Hu", "Wayne Lu", "Jia Wang"], "title": "From IDs to Semantics: A Generative Framework for Cross-Domain Recommendation with Adaptive Semantic Tokenization", "comment": null, "summary": "Cross-domain recommendation (CDR) is crucial for improving recommendation accuracy and generalization, yet traditional methods are often hindered by the reliance on shared user/item IDs, which are unavailable in most real-world scenarios. Consequently, many efforts have focused on learning disentangled representations through multi-domain joint training to bridge the domain gaps. Recent Large Language Model (LLM)-based approaches show promise, they still face critical challenges, including: (1) the \\textbf{item ID tokenization dilemma}, which leads to vocabulary explosion and fails to capture high-order collaborative knowledge; and (2) \\textbf{insufficient domain-specific modeling} for the complex evolution of user interests and item semantics. To address these limitations, we propose \\textbf{GenCDR}, a novel \\textbf{Gen}erative \\textbf{C}ross-\\textbf{D}omain \\textbf{R}ecommendation framework. GenCDR first employs a \\textbf{Domain-adaptive Tokenization} module, which generates disentangled semantic IDs for items by dynamically routing between a universal encoder and domain-specific adapters. Symmetrically, a \\textbf{Cross-domain Autoregressive Recommendation} module models user preferences by fusing universal and domain-specific interests. Finally, a \\textbf{Domain-aware Prefix-tree} enables efficient and accurate generation. Extensive experiments on multiple real-world datasets demonstrate that GenCDR significantly outperforms state-of-the-art baselines. Our code is available in the supplementary materials.", "AI": {"tldr": "GenCDR\u662f\u4e00\u4e2a\u751f\u6210\u5f0f\u8de8\u57df\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7\u9886\u57df\u81ea\u9002\u5e94\u5206\u8bcd\u548c\u8de8\u57df\u81ea\u56de\u5f52\u63a8\u8350\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5171\u4eabID\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u63a8\u8350\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8de8\u57df\u63a8\u8350\u65b9\u6cd5\u4f9d\u8d56\u5171\u4eab\u7528\u6237/\u7269\u54c1ID\uff0c\u4f46\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u8fd9\u4e9bID\u901a\u5e38\u4e0d\u53ef\u7528\u3002\u73b0\u6709LLM\u65b9\u6cd5\u9762\u4e34\u8bcd\u6c47\u7206\u70b8\u548c\u9886\u57df\u7279\u5b9a\u5efa\u6a21\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faGenCDR\u6846\u67b6\uff0c\u5305\u542b\u9886\u57df\u81ea\u9002\u5e94\u5206\u8bcd\u6a21\u5757\uff08\u52a8\u6001\u8def\u7531\u901a\u7528\u7f16\u7801\u5668\u548c\u9886\u57df\u7279\u5b9a\u9002\u914d\u5668\uff09\u3001\u8de8\u57df\u81ea\u56de\u5f52\u63a8\u8350\u6a21\u5757\uff08\u878d\u5408\u901a\u7528\u548c\u9886\u57df\u7279\u5b9a\u5174\u8da3\uff09\u548c\u9886\u57df\u611f\u77e5\u524d\u7f00\u6811\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGenCDR\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "GenCDR\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u57df\u63a8\u8350\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u65e0\u5171\u4eabID\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.08029", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.08029", "abs": "https://arxiv.org/abs/2511.08029", "authors": ["Aarush Sinha", "Pavan Kumar S", "Roshan Balaji", "Nirav Pravinbhai Bhatt"], "title": "BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives", "comment": "Accepted for oral presentation at AAAI 2026", "summary": "Hard negatives are essential for training effective retrieval models. Hard-negative mining typically relies on ranking documents using cross-encoders or static embedding models based on similarity metrics such as cosine distance. Hard negative mining becomes challenging for biomedical and scientific domains due to the difficulty in distinguishing between source and hard negative documents. However, referenced documents naturally share contextual relevance with the source document but are not duplicates, making them well-suited as hard negatives. In this work, we propose BiCA: Biomedical Dense Retrieval with Citation-Aware Hard Negatives, an approach for hard-negative mining by utilizing citation links in 20,000 PubMed articles for improving a domain-specific small dense retriever. We fine-tune the GTE_small and GTE_Base models using these citation-informed negatives and observe consistent improvements in zero-shot dense retrieval using nDCG@10 for both in-domain and out-of-domain tasks on BEIR and outperform baselines on long-tailed topics in LoTTE using Success@5. Our findings highlight the potential of leveraging document link structure to generate highly informative negatives, enabling state-of-the-art performance with minimal fine-tuning and demonstrating a path towards highly data-efficient domain adaptation.", "AI": {"tldr": "\u63d0\u51faBiCA\u65b9\u6cd5\uff0c\u5229\u7528PubMed\u6587\u7ae0\u4e2d\u7684\u5f15\u7528\u94fe\u63a5\u751f\u6210\u9ad8\u8d28\u91cf\u8d1f\u6837\u672c\uff0c\u6539\u8fdb\u751f\u7269\u533b\u5b66\u9886\u57df\u7684\u5bc6\u96c6\u68c0\u7d22\u6a21\u578b\u6027\u80fd", "motivation": "\u751f\u7269\u533b\u5b66\u548c\u79d1\u5b66\u9886\u57df\u4e2d\uff0c\u533a\u5206\u6e90\u6587\u6863\u548c\u56f0\u96be\u8d1f\u6837\u672c\u5177\u6709\u6311\u6218\u6027\u3002\u5f15\u7528\u6587\u6863\u4e0e\u6e90\u6587\u6863\u5177\u6709\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u4f46\u4e0d\u662f\u91cd\u590d\u5185\u5bb9\uff0c\u9002\u5408\u4f5c\u4e3a\u56f0\u96be\u8d1f\u6837\u672c", "method": "\u4f7f\u752820,000\u7bc7PubMed\u6587\u7ae0\u7684\u5f15\u7528\u94fe\u63a5\u8fdb\u884c\u56f0\u96be\u8d1f\u6837\u672c\u6316\u6398\uff0c\u5fae\u8c03GTE_small\u548cGTE_Base\u6a21\u578b\uff0c\u5229\u7528\u5f15\u7528\u611f\u77e5\u7684\u8d1f\u6837\u672c\u8bad\u7ec3", "result": "\u5728BEIR\u6570\u636e\u96c6\u4e0a\uff0c\u96f6\u6837\u672c\u5bc6\u96c6\u68c0\u7d22\u7684nDCG@10\u6307\u6807\u5728\u9886\u57df\u5185\u548c\u9886\u57df\u5916\u4efb\u52a1\u4e2d\u5747\u83b7\u5f97\u6301\u7eed\u6539\u8fdb\uff1b\u5728LoTTE\u7684\u957f\u5c3e\u4e3b\u9898\u4e0a\u4f7f\u7528Success@5\u6307\u6807\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u5229\u7528\u6587\u6863\u94fe\u63a5\u7ed3\u6784\u751f\u6210\u4fe1\u606f\u4e30\u5bcc\u7684\u8d1f\u6837\u672c\uff0c\u80fd\u591f\u4ee5\u6700\u5c0f\u7684\u5fae\u8c03\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u9ad8\u6548\u6570\u636e\u57df\u9002\u5e94\u7684\u8def\u5f84"}}
{"id": "2511.08150", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.08150", "abs": "https://arxiv.org/abs/2511.08150", "authors": ["Xinpeng Zhao", "Yukun Zhao", "Zhenyang Li", "Mengqi Zhang", "Jun Feng", "Ran Chen", "Ying Zhou", "Zhumin Chen", "Shuaiqiang Wang", "Zhaochun Ren", "Dawei Yin", "Xin Xin"], "title": "DiffuGR: Generative Document Retrieval with Diffusion Language Models", "comment": "This paper is under review", "summary": "Generative retrieval (GR) re-frames document retrieval as a sequence-based document identifier (DocID) generation task, memorizing documents with model parameters and enabling end-to-end retrieval without explicit indexing. Existing GR methods are based on auto-regressive generative models, i.e., the token generation is performed from left to right. However, such auto-regressive methods suffer from: (1) mismatch between DocID generation and natural language generation, e.g., an incorrect DocID token generated in early left steps would lead to totally erroneous retrieval; and (2) failure to balance the trade-off between retrieval efficiency and accuracy dynamically, which is crucial for practical applications. To address these limitations, we propose generative document retrieval with diffusion language models, dubbed DiffuGR. It models DocID generation as a discrete diffusion process: during training, DocIDs are corrupted through a stochastic masking process, and a diffusion language model is learned to recover them under a retrieval-aware objective. For inference, DiffuGR attempts to generate DocID tokens in parallel and refines them through a controllable number of denoising steps. In contrast to conventional left-to-right auto-regressive decoding, DiffuGR provides a novel mechanism to first generate more confident DocID tokens and refine the generation through diffusion-based denoising. Moreover, DiffuGR also offers explicit runtime control over the qualitylatency tradeoff. Extensive experiments on benchmark retrieval datasets show that DiffuGR is competitive with strong auto-regressive generative retrievers, while offering flexible speed and accuracy tradeoffs through variable denoising budgets. Overall, our results indicate that non-autoregressive diffusion models are a practical and effective alternative for generative document retrieval.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDiffuGR\uff0c\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u5f0f\u6587\u6863\u68c0\u7d22\u65b9\u6cd5\uff0c\u5c06\u6587\u6863ID\u751f\u6210\u5efa\u6a21\u4e3a\u79bb\u6563\u6269\u6563\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u81ea\u56de\u5f52\u6a21\u578b\u7684\u751f\u6210\u5f0f\u68c0\u7d22\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a(1)\u6587\u6863ID\u751f\u6210\u4e0e\u81ea\u7136\u8bed\u8a00\u751f\u6210\u4e0d\u5339\u914d\uff0c\u65e9\u671f\u9519\u8bef\u4f1a\u5bfc\u81f4\u5b8c\u5168\u9519\u8bef\u7684\u68c0\u7d22\uff1b(2)\u65e0\u6cd5\u52a8\u6001\u5e73\u8861\u68c0\u7d22\u6548\u7387\u4e0e\u51c6\u786e\u6027\u7684\u6743\u8861\u3002", "method": "\u5c06\u6587\u6863ID\u751f\u6210\u5efa\u6a21\u4e3a\u79bb\u6563\u6269\u6563\u8fc7\u7a0b\uff1a\u8bad\u7ec3\u65f6\u901a\u8fc7\u968f\u673a\u63a9\u7801\u8fc7\u7a0b\u7834\u574f\u6587\u6863ID\uff0c\u5b66\u4e60\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5728\u68c0\u7d22\u611f\u77e5\u76ee\u6807\u4e0b\u6062\u590d\u5b83\u4eec\uff1b\u63a8\u7406\u65f6\u5e76\u884c\u751f\u6210\u6587\u6863ID\u6807\u8bb0\uff0c\u5e76\u901a\u8fc7\u53ef\u63a7\u7684\u53bb\u566a\u6b65\u9aa4\u8fdb\u884c\u7cbe\u70bc\u3002", "result": "\u5728\u57fa\u51c6\u68c0\u7d22\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDiffuGR\u4e0e\u5f3a\u81ea\u56de\u5f52\u751f\u6210\u68c0\u7d22\u5668\u5177\u6709\u7ade\u4e89\u529b\uff0c\u540c\u65f6\u901a\u8fc7\u53ef\u53d8\u53bb\u566a\u9884\u7b97\u63d0\u4f9b\u7075\u6d3b\u7684\u901f\u5ea6\u548c\u51c6\u786e\u6027\u6743\u8861\u3002", "conclusion": "\u975e\u81ea\u56de\u5f52\u6269\u6563\u6a21\u578b\u662f\u751f\u6210\u5f0f\u6587\u6863\u68c0\u7d22\u7684\u5b9e\u7528\u6709\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u63d0\u4f9b\u4e86\u663e\u5f0f\u7684\u8fd0\u884c\u65f6\u8d28\u91cf-\u5ef6\u8fdf\u6743\u8861\u63a7\u5236\u3002"}}
{"id": "2511.08181", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08181", "abs": "https://arxiv.org/abs/2511.08181", "authors": ["Seung Hwan Cho", "Yujin Yang", "Danik Baeck", "Minjoo Kim", "Young-Min Kim", "Heejung Lee", "Sangjin Park"], "title": "MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System", "comment": "13 pages, 2 figures, Accepted at RDGENAI at CIKM 2025 workshop", "summary": "Recommender systems (RS) are currently being studied to mitigate limitations during cold-start conditions by leveraging modality information or introducing Agent concepts based on the exceptional reasoning capabilities of Large Language Models (LLMs). Meanwhile, food and beverage recommender systems have traditionally used knowledge graph and ontology concepts due to the domain's unique data attributes and relationship characteristics. On this background, we propose MARC, a multimodal and multi-task cocktail recommender system based on Agentic Retrieval-Augmented Generation (RAG) utilizing graph database under cold-start conditions. The proposed system generates high-quality, contextually appropriate answers through two core processes: a task recognition router and a reflection process. The graph database was constructed by processing cocktail data from Kaggle, and its effectiveness was evaluated using 200 manually crafted questions. The evaluation used both LLM-as-a-judge and human evaluation to demonstrate that answers generated via the graph database outperformed those from a simple vector database in terms of quality. The code is available at https://github.com/diddbwls/cocktail_rec_agentrag", "AI": {"tldr": "MARC\u662f\u4e00\u4e2a\u57fa\u4e8eAgentic RAG\u7684\u591a\u6a21\u6001\u591a\u4efb\u52a1\u9e21\u5c3e\u9152\u63a8\u8350\u7cfb\u7edf\uff0c\u5728\u51b7\u542f\u52a8\u6761\u4ef6\u4e0b\u5229\u7528\u56fe\u6570\u636e\u5e93\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4e0a\u4e0b\u6587\u76f8\u5173\u63a8\u8350\u3002", "motivation": "\u89e3\u51b3\u63a8\u8350\u7cfb\u7edf\u5728\u51b7\u542f\u52a8\u6761\u4ef6\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u5229\u7528\u6a21\u6001\u4fe1\u606f\u548cLLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u7ed3\u5408\u9e21\u5c3e\u9152\u9886\u57df\u72ec\u7279\u7684\u6570\u636e\u5c5e\u6027\u548c\u5173\u7cfb\u7279\u5f81\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eAgentic RAG\u7684\u65b9\u6cd5\uff0c\u5305\u542b\u4efb\u52a1\u8bc6\u522b\u8def\u7531\u5668\u548c\u53cd\u601d\u8fc7\u7a0b\uff0c\u6784\u5efa\u56fe\u6570\u636e\u5e93\u5904\u7406\u9e21\u5c3e\u9152\u6570\u636e\u3002", "result": "\u901a\u8fc7200\u4e2a\u624b\u5de5\u5236\u4f5c\u7684\u95ee\u9898\u8fdb\u884c\u8bc4\u4f30\uff0c\u4f7f\u7528LLM\u548c\u4eba\u5de5\u8bc4\u4f30\uff0c\u8bc1\u660e\u57fa\u4e8e\u56fe\u6570\u636e\u5e93\u7684\u7b54\u6848\u8d28\u91cf\u4f18\u4e8e\u7b80\u5355\u5411\u91cf\u6570\u636e\u5e93\u3002", "conclusion": "MARC\u7cfb\u7edf\u5728\u51b7\u542f\u52a8\u6761\u4ef6\u4e0b\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u9e21\u5c3e\u9152\u63a8\u8350\uff0c\u56fe\u6570\u636e\u5e93\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2511.08378", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08378", "abs": "https://arxiv.org/abs/2511.08378", "authors": ["Xiao Wang", "Ke Qin", "Dongyang Zhang", "Xiurui Xie", "Shuang Liang"], "title": "Bid Farewell to Seesaw: Towards Accurate Long-tail Session-based Recommendation via Dual Constraints of Hybrid Intents", "comment": null, "summary": "Session-based recommendation (SBR) aims to predict anonymous users' next interaction based on their interaction sessions. In the practical recommendation scenario, low-exposure items constitute the majority of interactions, creating a long-tail distribution that severely compromises recommendation diversity. Existing approaches attempt to address this issue by promoting tail items but incur accuracy degradation, exhibiting a \"see-saw\" effect between long-tail and accuracy performance. We attribute such conflict to session-irrelevant noise within the tail items, which existing long-tail approaches fail to identify and constrain effectively. To resolve this fundamental conflict, we propose \\textbf{HID} (\\textbf{H}ybrid \\textbf{I}ntent-based \\textbf{D}ual Constraint Framework), a plug-and-play framework that transforms the conventional \"see-saw\" into \"win-win\" through introducing the hybrid intent-based dual constraints for both long-tail and accuracy. Two key innovations are incorporated in this framework: (i) \\textit{Hybrid Intent Learning}, where we reformulate the intent extraction strategies by employing attribute-aware spectral clustering to reconstruct the item-to-intent mapping. Furthermore, discrimination of session-irrelevant noise is achieved through the assignment of the target and noise intents to each session. (ii) \\textit{Intent Constraint Loss}, which incorporates two novel constraint paradigms regarding the \\textit{diversity} and \\textit{accuracy} to regulate the representation learning process of both items and sessions. These two objectives are unified into a single training loss through rigorous theoretical derivation. Extensive experiments across multiple SBR models and datasets demonstrate that HID can enhance both long-tail performance and recommendation accuracy, establishing new state-of-the-art performance in long-tail recommender systems.", "AI": {"tldr": "HID\u6846\u67b6\u901a\u8fc7\u6df7\u5408\u610f\u56fe\u5b66\u4e60\u548c\u610f\u56fe\u7ea6\u675f\u635f\u5931\uff0c\u89e3\u51b3\u4e86\u57fa\u4e8e\u4f1a\u8bdd\u63a8\u8350\u4e2d\u957f\u5c3e\u9879\u76ee\u4e0e\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u5b9e\u73b0\u4e86\u957f\u5c3e\u6027\u80fd\u548c\u63a8\u8350\u7cbe\u5ea6\u7684\u53cc\u8d62\u3002", "motivation": "\u5728\u57fa\u4e8e\u4f1a\u8bdd\u7684\u63a8\u8350\u4e2d\uff0c\u4f4e\u66dd\u5149\u9879\u76ee\u6784\u6210\u5927\u591a\u6570\u4ea4\u4e92\uff0c\u5f62\u6210\u957f\u5c3e\u5206\u5e03\uff0c\u4e25\u91cd\u635f\u5bb3\u63a8\u8350\u591a\u6837\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u8bd5\u56fe\u901a\u8fc7\u63a8\u5e7f\u5c3e\u90e8\u9879\u76ee\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u4f1a\u5bfc\u81f4\u51c6\u786e\u6027\u4e0b\u964d\uff0c\u8868\u73b0\u51fa\u957f\u5c3e\u4e0e\u51c6\u786e\u6027\u6027\u80fd\u4e4b\u95f4\u7684'\u8df7\u8df7\u677f'\u6548\u5e94\u3002", "method": "\u63d0\u51faHID\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a(1)\u6df7\u5408\u610f\u56fe\u5b66\u4e60\uff1a\u901a\u8fc7\u5c5e\u6027\u611f\u77e5\u8c31\u805a\u7c7b\u91cd\u6784\u9879\u76ee\u5230\u610f\u56fe\u7684\u6620\u5c04\uff0c\u901a\u8fc7\u4e3a\u76ee\u6807\u548c\u566a\u58f0\u610f\u56fe\u5206\u914d\u6bcf\u4e2a\u4f1a\u8bdd\u6765\u533a\u5206\u4f1a\u8bdd\u65e0\u5173\u566a\u58f0\uff1b(2)\u610f\u56fe\u7ea6\u675f\u635f\u5931\uff1a\u5f15\u5165\u5173\u4e8e\u591a\u6837\u6027\u548c\u51c6\u786e\u6027\u7684\u4e24\u79cd\u65b0\u7ea6\u675f\u8303\u5f0f\uff0c\u901a\u8fc7\u4e25\u683c\u7684\u7406\u8bba\u63a8\u5bfc\u5c06\u8fd9\u4e24\u4e2a\u76ee\u6807\u7edf\u4e00\u5230\u5355\u4e2a\u8bad\u7ec3\u635f\u5931\u4e2d\u3002", "result": "\u5728\u591a\u4e2aSBR\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cHID\u53ef\u4ee5\u540c\u65f6\u589e\u5f3a\u957f\u5c3e\u6027\u80fd\u548c\u63a8\u8350\u51c6\u786e\u6027\uff0c\u5728\u957f\u5c3e\u63a8\u8350\u7cfb\u7edf\u4e2d\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "HID\u6846\u67b6\u6210\u529f\u5730\u5c06\u4f20\u7edf\u7684'\u8df7\u8df7\u677f'\u8f6c\u5316\u4e3a'\u53cc\u8d62'\uff0c\u901a\u8fc7\u5f15\u5165\u6df7\u5408\u610f\u56fe\u7684\u53cc\u91cd\u7ea6\u675f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u5c3e\u63a8\u8350\u4e2d\u591a\u6837\u6027\u4e0e\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u6839\u672c\u51b2\u7a81\u3002"}}
{"id": "2511.08476", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.08476", "abs": "https://arxiv.org/abs/2511.08476", "authors": ["Hadi Ghaemi", "Lauren Snyder", "Markus Stocker"], "title": "Advancing Scientific Knowledge Retrieval and Reuse with a Novel Digital Library for Machine-Readable Knowledge", "comment": null, "summary": "Digital libraries for research, such as the ACM Digital Library or Semantic Scholar, do not enable the machine-supported, efficient reuse of scientific knowledge (e.g., in synthesis research). This is because these libraries are based on document-centric models with narrative text knowledge expressions that require manual or semi-automated knowledge extraction, structuring, and organization. We present ORKG reborn, an emerging digital library that supports finding, accessing, and reusing accurate, fine-grained, and reproducible machine-readable expressions of scientific knowledge that relate scientific statements and their supporting evidence in terms of data and code. The rich expressions of scientific knowledge are published as reborn (born-reusable) articles and provide novel possibilities for scientific knowledge retrieval, for instance by statistical methods, software packages, variables, or data matching specific constraints. We describe the proposed system and demonstrate its practical viability and potential for information retrieval in contrast to state-of-the-art digital libraries and document-centric scholarly communication using several published articles in research fields ranging from computer science to soil science. Our work underscores the enormous potential of scientific knowledge databases and a viable approach to their construction.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
