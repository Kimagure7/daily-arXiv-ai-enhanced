<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 39]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.LG](#cs.LG) [Total: 54]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Gender-Neutral Rewriting in Italian: Models, Approaches, and Trade-offs](https://arxiv.org/abs/2509.13480)
*Andrea Piergentili,Beatrice Savoldi,Matteo Negri,Luisa Bentivogli*

Main category: cs.CL

TL;DR: 本文首次系统评估了大型语言模型在意大利语性别中性改写任务中的表现，提出了衡量中立性和语义保真度的二维框架，发现开源模型优于现有专用模型，微调后的小模型性能可媲美大型模型。


<details>
  <summary>Details</summary>
Motivation: 意大利语等语法性别语言中的性别中性改写具有挑战性，需要消除不必要的性别指定同时保持原意，但目前缺乏系统评估。

Method: 采用少样本提示比较多个LLM，对选定模型进行微调，并应用针对性清洗提升任务相关性，使用二维框架评估中立性和语义保真度。

Result: 开源权重LLM优于意大利语GNR专用模型，微调后的小型模型以更小规模达到或超过最佳开源LLM性能。

Conclusion: 研究揭示了在优化训练数据时中立性和意义保持之间的权衡关系，为语法性别语言的性别中性改写提供了有效方法。

Abstract: Gender-neutral rewriting (GNR) aims to reformulate text to eliminate
unnecessary gender specifications while preserving meaning, a particularly
challenging task in grammatical-gender languages like Italian. In this work, we
conduct the first systematic evaluation of state-of-the-art large language
models (LLMs) for Italian GNR, introducing a two-dimensional framework that
measures both neutrality and semantic fidelity to the input. We compare
few-shot prompting across multiple LLMs, fine-tune selected models, and apply
targeted cleaning to boost task relevance. Our findings show that open-weight
LLMs outperform the only existing model dedicated to GNR in Italian, whereas
our fine-tuned models match or exceed the best open-weight LLM's performance at
a fraction of its size. Finally, we discuss the trade-off between optimizing
the training data for neutrality and meaning preservation.

</details>


### [2] [Op-Fed: Opinion, Stance, and Monetary Policy Annotations on FOMC Transcripts Using Active Learning](https://arxiv.org/abs/2509.13539)
*Alisa Kanganis,Katherine A. Keith*

Main category: cs.CL

TL;DR: Op-Fed数据集包含1044条人工标注的FOMC会议记录句子，用于货币政策立场分析，通过分层标注方案和主动学习解决类别不平衡和上下文依赖问题。


<details>
  <summary>Details</summary>
Motivation: 美联储公开市场委员会(FOMC)的货币政策决策影响数百万人，但现有数据集中非中性立场句子不足8%，且65%需要句子级以上的上下文理解，需要专门的数据集来支持货币政策立场分析。

Method: 开发五层分层标注方案分离观点、货币政策和立场要素；使用主动学习选择标注实例，使正例数量翻倍；评估大语言模型在零样本设置下的分类性能。

Result: 最佳闭源LLM在观点分类上达到0.80准确率，但在货币政策立场分类上仅0.61，低于人类基准0.89。数据集可用于模型训练、置信度校准和后续标注工作。

Conclusion: Op-Fed数据集有效解决了货币政策文本分析中的技术挑战，揭示了LLM在复杂金融文本理解方面的局限性，为未来研究提供了重要基础资源。

Abstract: The U.S. Federal Open Market Committee (FOMC) regularly discusses and sets
monetary policy, affecting the borrowing and spending decisions of millions of
people. In this work, we release Op-Fed, a dataset of 1044 human-annotated
sentences and their contexts from FOMC transcripts. We faced two major
technical challenges in dataset creation: imbalanced classes -- we estimate
fewer than 8% of sentences express a non-neutral stance towards monetary policy
-- and inter-sentence dependence -- 65% of instances require context beyond the
sentence-level. To address these challenges, we developed a five-stage
hierarchical schema to isolate aspects of opinion, monetary policy, and stance
towards monetary policy as well as the level of context needed. Second, we
selected instances to annotate using active learning, roughly doubling the
number of positive instances across all schema aspects. Using Op-Fed, we found
a top-performing, closed-weight LLM achieves 0.80 zero-shot accuracy in opinion
classification but only 0.61 zero-shot accuracy classifying stance towards
monetary policy -- below our human baseline of 0.89. We expect Op-Fed to be
useful for future model training, confidence calibration, and as a seed dataset
for future annotation efforts.

</details>


### [3] [Overview of Dialog System Evaluation Track: Dimensionality, Language, Culture and Safety at DSTC 12](https://arxiv.org/abs/2509.13569)
*John Mendonça,Lining Zhang,Rahul Mallidi,Alon Lavie,Isabel Trancoso,Luis Fernando D'Haro,João Sedoc*

Main category: cs.CL

TL;DR: DSTC12 Track 1 针对对话系统评估的两个关键挑战：多维度自动评估指标和多语言文化安全检测，结果显示当前方法在多维度评估上仍有很大改进空间，在文化安全检测方面存在明显不足


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的快速发展凸显了对话系统评估的重要性，但传统指标不足且安全考虑往往定义狭窄或存在文化偏见，需要更全面的评估框架

Method: 通过两个子任务进行评估：(1) 对话级多维度自动评估指标（10个维度），(2) 多语言和多文化安全检测，使用Llama-3-8B和Llama-Guard-3-1B作为基线模型

Result: 任务1中基线模型平均Spearman相关系数仅为0.1681，表明多维度评估仍有很大改进空间；任务2中参赛团队在多语言安全子集上表现优异（最高ROC-AUC 0.9648），但在文化子集上基线模型表现更好（0.5126 ROC-AUC），凸显文化感知安全的不足

Conclusion: 对话系统评估在多维度自动评估和文化感知安全检测方面仍面临重大挑战，需要开发更先进的评估方法和文化敏感的安全检测技术

Abstract: The rapid advancement of Large Language Models (LLMs) has intensified the
need for robust dialogue system evaluation, yet comprehensive assessment
remains challenging. Traditional metrics often prove insufficient, and safety
considerations are frequently narrowly defined or culturally biased. The DSTC12
Track 1, "Dialog System Evaluation: Dimensionality, Language, Culture and
Safety," is part of the ongoing effort to address these critical gaps. The
track comprised two subtasks: (1) Dialogue-level, Multi-dimensional Automatic
Evaluation Metrics, and (2) Multilingual and Multicultural Safety Detection.
For Task 1, focused on 10 dialogue dimensions, a Llama-3-8B baseline achieved
the highest average Spearman's correlation (0.1681), indicating substantial
room for improvement. In Task 2, while participating teams significantly
outperformed a Llama-Guard-3-1B baseline on the multilingual safety subset (top
ROC-AUC 0.9648), the baseline proved superior on the cultural subset (0.5126
ROC-AUC), highlighting critical needs in culturally-aware safety. This paper
describes the datasets and baselines provided to participants, as well as
submission evaluation results for each of the two proposed subtasks.

</details>


### [4] [Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning](https://arxiv.org/abs/2509.13624)
*Shambhavi Krishna,Atharva Naik,Chaitali Agarwal,Sudharshan Govindan,Taesung Lee,Haw-Shiuan Chang*

Main category: cs.CL

TL;DR: 提出了一个分析框架来研究LLM跨任务迁移学习中的潜在能力和副作用，发现性能提升主要受隐藏统计因素和语言特征影响，而非表面数据集相似性


<details>
  <summary>Details</summary>
Motivation: 由于无法为所有任务获取高质量训练数据，需要依赖不同特性的数据集进行迁移学习，并应对分布外请求，因此需要分析跨任务交互的复杂动态

Method: 构建迁移学习矩阵和降维分析框架，训练10个模型来识别潜在能力（推理、情感分类、自然语言理解、算术等）并发现迁移学习的副作用

Result: 性能改进往往无法用表面数据集相似性或源数据质量来解释，而是受源数据集的隐藏统计因素（如类别分布、生成长度倾向）和特定语言特征的影响更大

Conclusion: 这项工作为理解迁移学习的复杂动态提供了见解，为更可预测和有效的LLM适应铺平了道路

Abstract: Large language models are increasingly deployed across diverse applications.
This often includes tasks LLMs have not encountered during training. This
implies that enumerating and obtaining the high-quality training data for all
tasks is infeasible. Thus, we often need to rely on transfer learning using
datasets with different characteristics, and anticipate out-of-distribution
requests. Motivated by this practical need, we propose an analysis framework,
building a transfer learning matrix and dimensionality reduction, to dissect
these cross-task interactions. We train and analyze 10 models to identify
latent abilities (e.g., Reasoning, Sentiment Classification, NLU, Arithmetic)
and discover the side effects of the transfer learning. Our findings reveal
that performance improvements often defy explanations based on surface-level
dataset similarity or source data quality. Instead, hidden statistical factors
of the source dataset, such as class distribution and generation length
proclivities, alongside specific linguistic features, are actually more
influential. This work offers insights into the complex dynamics of transfer
learning, paving the way for more predictable and effective LLM adaptation.

</details>


### [5] [Sparse Neurons Carry Strong Signals of Question Ambiguity in LLMs](https://arxiv.org/abs/2509.13664)
*Zhuoxuan Zhang,Jinhao Duan,Edward Kim,Kaidi Xu*

Main category: cs.CL

TL;DR: 研究发现LLMs在浅层神经元中线性编码问题歧义信息，通过操纵这些歧义编码神经元可以控制模型从直接回答转为弃权


<details>
  <summary>Details</summary>
Motivation: 现实问题普遍存在歧义性，但大语言模型往往给出自信回答而非寻求澄清，需要研究模型内部如何表示和处理歧义

Method: 在模型预填充阶段识别歧义编码神经元(AENs)，训练探测器进行歧义检测，并通过神经元操纵控制模型行为

Result: 发现少量神经元(最少1个)编码歧义信息，基于AENs的探测器在歧义检测上表现优异且具有跨数据集泛化能力，通过操纵AENs可控制模型行为

Conclusion: LLMs形成了紧凑的内部歧义表示，支持可解释和可控的行为，为模型安全性和可靠性提供了新途径

Abstract: Ambiguity is pervasive in real-world questions, yet large language models
(LLMs) often respond with confident answers rather than seeking clarification.
In this work, we show that question ambiguity is linearly encoded in the
internal representations of LLMs and can be both detected and controlled at the
neuron level. During the model's pre-filling stage, we identify that a small
number of neurons, as few as one, encode question ambiguity information. Probes
trained on these Ambiguity-Encoding Neurons (AENs) achieve strong performance
on ambiguity detection and generalize across datasets, outperforming
prompting-based and representation-based baselines. Layerwise analysis reveals
that AENs emerge from shallow layers, suggesting early encoding of ambiguity
signals in the model's processing pipeline. Finally, we show that through
manipulating AENs, we can control LLM's behavior from direct answering to
abstention. Our findings reveal that LLMs form compact internal representations
of question ambiguity, enabling interpretable and controllable behavior.

</details>


### [6] [CL$^2$GEC: A Multi-Discipline Benchmark for Continual Learning in Chinese Literature Grammatical Error Correction](https://arxiv.org/abs/2509.13672)
*Shang Qin,Jingheng Ye,Yinghui Li,Hai-Tao Zheng,Qi Li,Jinxiao Shan,Zhixing Li,Hong-Gee Kim*

Main category: cs.CL

TL;DR: 提出了第一个中文文献语法纠错持续学习基准CL²GEC，包含10个学科的1万条标注句子，评估大语言模型在跨学科语法纠错中的持续学习能力。


<details>
  <summary>Details</summary>
Motivation: 现有中文语法纠错研究缺乏多学科学术写作的专门基准，忽视了持续学习作为处理领域特定语言变异和防止灾难性遗忘的解决方案。

Method: 构建包含10个学科1万条人工标注句子的基准，评估大语言模型在顺序调优、参数高效适应和四种代表性持续学习算法下的表现，使用标准GEC指标和适应任务级变异的持续学习指标。

Result: 实验结果表明，基于正则化的方法比基于回放或朴素顺序方法更能有效缓解遗忘问题。

Conclusion: 该基准为未来跨学科学术领域的自适应语法纠错研究提供了严格的基础。

Abstract: The growing demand for automated writing assistance in diverse academic
domains highlights the need for robust Chinese Grammatical Error Correction
(CGEC) systems that can adapt across disciplines. However, existing CGEC
research largely lacks dedicated benchmarks for multi-disciplinary academic
writing, overlooking continual learning (CL) as a promising solution to handle
domain-specific linguistic variation and prevent catastrophic forgetting. To
fill this crucial gap, we introduce CL$^2$GEC, the first Continual Learning
benchmark for Chinese Literature Grammatical Error Correction, designed to
evaluate adaptive CGEC across multiple academic fields. Our benchmark includes
10,000 human-annotated sentences spanning 10 disciplines, each exhibiting
distinct linguistic styles and error patterns. CL$^2$GEC focuses on evaluating
grammatical error correction in a continual learning setting, simulating
sequential exposure to diverse academic disciplines to reflect real-world
editorial dynamics. We evaluate large language models under sequential tuning,
parameter-efficient adaptation, and four representative CL algorithms, using
both standard GEC metrics and continual learning metrics adapted to task-level
variation. Experimental results reveal that regularization-based methods
mitigate forgetting more effectively than replay-based or naive sequential
approaches. Our benchmark provides a rigorous foundation for future research in
adaptive grammatical error correction across diverse academic domains.

</details>


### [7] [AgentCTG: Harnessing Multi-Agent Collaboration for Fine-Grained Precise Control in Text Generation](https://arxiv.org/abs/2509.13677)
*Xinxu Zhou,Jiaqi Bai,Zhenqi Sun,Fanxiang Zeng,Yue Liu*

Main category: cs.CL

TL;DR: AgentCTG是一个新颖的可扩展框架，通过模拟多智能体工作流中的控制和调节机制，实现对文本生成的精确复杂控制，在多个公开数据集上达到最先进效果。


<details>
  <summary>Details</summary>
Motivation: 自然语言处理中的受控文本生成面临精细条件控制的挑战，特别是在实际应用中需要考虑成本、可扩展性、领域知识学习和更精确控制等多重需求。

Method: 提出AgentCTG框架，通过多智能体协作机制模拟控制调节过程，引入自动提示模块增强生成效果，并应用于角色驱动重写任务。

Result: 在多个公开数据集上取得最先进结果，在角色扮演在线导航应用中显著提升驾驶体验，通过优化上下文相关文本生成增强在线社区的沉浸式交互。

Conclusion: AgentCTG框架通过多智能体协作机制有效解决了受控文本生成的精细控制问题，在实际应用中展现出良好的效果和可扩展性。

Abstract: Although significant progress has been made in many tasks within the field of
Natural Language Processing (NLP), Controlled Text Generation (CTG) continues
to face numerous challenges, particularly in achieving fine-grained conditional
control over generation. Additionally, in real scenario and online
applications, cost considerations, scalability, domain knowledge learning and
more precise control are required, presenting more challenge for CTG. This
paper introduces a novel and scalable framework, AgentCTG, which aims to
enhance precise and complex control over the text generation by simulating the
control and regulation mechanisms in multi-agent workflows. We explore various
collaboration methods among different agents and introduce an auto-prompt
module to further enhance the generation effectiveness. AgentCTG achieves
state-of-the-art results on multiple public datasets. To validate its
effectiveness in practical applications, we propose a new challenging
Character-Driven Rewriting task, which aims to convert the original text into
new text that conform to specific character profiles and simultaneously
preserve the domain knowledge. When applied to online navigation with
role-playing, our approach significantly enhances the driving experience
through improved content delivery. By optimizing the generation of contextually
relevant text, we enable a more immersive interaction within online
communities, fostering greater personalization and user engagement.

</details>


### [8] [Improving Context Fidelity via Native Retrieval-Augmented Reasoning](https://arxiv.org/abs/2509.13683)
*Suyuchen Wang,Jinlin Wang,Xinyu Wang,Shiqi Li,Xiangru Tang,Sirui Hong,Xiao-Wen Chang,Chenglin Wu,Bang Liu*

Main category: cs.CL

TL;DR: 提出了CARE框架，通过让LLMs在推理过程中显式整合上下文证据，显著提升了检索准确性和答案生成性能，无需昂贵的监督微调。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理基于给定信息的问答时经常出现上下文保真度问题，产生不一致的答案。现有方法要么依赖昂贵的监督微调，要么训练模型进行网络搜索但未能有效利用给定上下文。

Method: CARE是一个新颖的原生检索增强推理框架，教导LLMs在推理过程中显式整合上下文证据，利用模型自身的检索能力，仅需要有限的标记证据数据。

Result: 在多个真实世界和反事实QA基准测试中，该方法显著优于监督微调、传统检索增强生成方法和外部检索解决方案。

Conclusion: 这项工作代表了在使LLMs更准确、可靠和高效处理知识密集型任务方面的根本性进展。

Abstract: Large language models (LLMs) often struggle with context fidelity, producing
inconsistent answers when responding to questions based on provided
information. Existing approaches either rely on expensive supervised
fine-tuning to generate evidence post-answer or train models to perform web
searches without necessarily improving utilization of the given context. We
propose CARE, a novel native retrieval-augmented reasoning framework that
teaches LLMs to explicitly integrate in-context evidence within their reasoning
process with the model's own retrieval capabilities. Our method requires
limited labeled evidence data while significantly enhancing both retrieval
accuracy and answer generation performance through strategically retrieved
in-context tokens in the reasoning chain. Extensive experiments on multiple
real-world and counterfactual QA benchmarks demonstrate that our approach
substantially outperforms supervised fine-tuning, traditional
retrieval-augmented generation methods, and external retrieval solutions. This
work represents a fundamental advancement in making LLMs more accurate,
reliable, and efficient for knowledge-intensive tasks.

</details>


### [9] [Can Large Language Models Robustly Perform Natural Language Inference for Japanese Comparatives?](https://arxiv.org/abs/2509.13695)
*Yosuke Mikami,Daiki Matsuoka,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 本文构建了一个专注于比较级的日语NLI数据集，评估了LLM在零样本和少样本设置下的表现，发现模型对提示格式敏感，且在处理日语特有语言现象时存在困难，但包含逻辑语义表示的提示能帮助模型解决困难推理问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言推理方面表现优异，但涉及数值和逻辑表达式的推理仍然具有挑战性。比较级是与此类推理相关的关键语言现象，但LLM在处理这些现象时的鲁棒性，特别是在训练数据中非主导语言（如日语）方面，尚未得到充分探索。

Method: 构建了一个专注于比较级的日语NLI数据集，并在零样本和少样本设置下评估了各种LLM。分析了不同提示格式的影响以及少样本示例中黄金标签的影响。

Result: 模型在零样本设置中对提示格式敏感，在少样本设置中受黄金标签影响。LLM在处理日语特有语言现象时存在困难。包含逻辑语义表示的提示能帮助模型预测那些即使在少样本示例下也难以解决的推理问题的正确标签。

Conclusion: LLM在处理日语比较级推理时存在局限性，但通过适当的提示工程（特别是包含逻辑语义表示）可以显著提升模型性能，这为改进多语言NLI任务提供了重要见解。

Abstract: Large Language Models (LLMs) perform remarkably well in Natural Language
Inference (NLI). However, NLI involving numerical and logical expressions
remains challenging. Comparatives are a key linguistic phenomenon related to
such inference, but the robustness of LLMs in handling them, especially in
languages that are not dominant in the models' training data, such as Japanese,
has not been sufficiently explored. To address this gap, we construct a
Japanese NLI dataset that focuses on comparatives and evaluate various LLMs in
zero-shot and few-shot settings. Our results show that the performance of the
models is sensitive to the prompt formats in the zero-shot setting and
influenced by the gold labels in the few-shot examples. The LLMs also struggle
to handle linguistic phenomena unique to Japanese. Furthermore, we observe that
prompts containing logical semantic representations help the models predict the
correct labels for inference problems that they struggle to solve even with
few-shot examples.

</details>


### [10] [Integrating Text and Time-Series into (Large) Language Models to Predict Medical Outcomes](https://arxiv.org/abs/2509.13696)
*Iyadh Ben Cheikh Larbi,Ajay Madhavan Ravichandran,Aljoscha Burchardt,Roland Roller*

Main category: cs.CL

TL;DR: 本文通过DSPy提示优化方法，使指令调优的大语言模型能够联合处理临床文本和结构化EHR数据，在临床分类任务上达到与专用多模态系统相当的性能


<details>
  <summary>Details</summary>
Motivation: 大语言模型在文本生成方面表现出色，但在处理涉及结构化数据（如时间序列）的临床分类任务方面的能力尚未得到充分探索

Method: 使用基于DSPy的提示优化技术来适配指令调优的大语言模型，使其能够联合处理临床笔记和结构化电子健康记录（EHR）输入

Result: 该方法在性能上与专用多模态系统相当，同时需要更少的复杂性，并在不同任务间具有更好的适应性

Conclusion: 基于提示优化的LLM适配方法为临床分类任务提供了一种简单而有效的解决方案，展示了在处理多模态医疗数据方面的潜力

Abstract: Large language models (LLMs) excel at text generation, but their ability to
handle clinical classification tasks involving structured data, such as time
series, remains underexplored. In this work, we adapt instruction-tuned LLMs
using DSPy-based prompt optimization to process clinical notes and structured
EHR inputs jointly. Our results show that this approach achieves performance on
par with specialized multimodal systems while requiring less complexity and
offering greater adaptability across tasks.

</details>


### [11] [Combining Evidence and Reasoning for Biomedical Fact-Checking](https://arxiv.org/abs/2509.13879)
*Mariano Barone,Antonio Romano,Giuseppe Riccio,Marco Postiglione,Vincenzo Moscato*

Main category: cs.CL

TL;DR: CER框架结合科学证据检索、大语言模型推理和监督真实性预测，用于生物医学事实核查，在多个数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 医疗错误信息（如疫苗犹豫和未经证实的治疗方法）对公共卫生和医疗系统信任构成风险。生物医学声明验证具有独特挑战性，需要处理复杂术语、领域专业知识，并基于科学证据进行验证。

Method: 提出CER（Combining Evidence and Reasoning）框架，整合科学证据检索、大语言模型推理和监督真实性预测。通过将大语言模型的文本生成能力与高质量生物医学科学证据的检索技术相结合，有效减少幻觉风险。

Result: 在专家标注数据集（HealthFC、BioASQ-7b、SciFact）上评估显示达到最先进性能，并展现出良好的跨数据集泛化能力。

Conclusion: CER框架为生物医学事实核查提供了有效解决方案，通过证据检索和推理的结合确保了输出的可验证性和证据基础，代码和数据已开源以促进透明度和可重复性。

Abstract: Misinformation in healthcare, from vaccine hesitancy to unproven treatments,
poses risks to public health and trust in medical systems. While machine
learning and natural language processing have advanced automated fact-checking,
validating biomedical claims remains uniquely challenging due to complex
terminology, the need for domain expertise, and the critical importance of
grounding in scientific evidence. We introduce CER (Combining Evidence and
Reasoning), a novel framework for biomedical fact-checking that integrates
scientific evidence retrieval, reasoning via large language models, and
supervised veracity prediction. By integrating the text-generation capabilities
of large language models with advanced retrieval techniques for high-quality
biomedical scientific evidence, CER effectively mitigates the risk of
hallucinations, ensuring that generated outputs are grounded in verifiable,
evidence-based sources. Evaluations on expert-annotated datasets (HealthFC,
BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising
cross-dataset generalization. Code and data are released for transparency and
reproducibility: https: //github.com/PRAISELab-PicusLab/CER.

</details>


### [12] [DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models](https://arxiv.org/abs/2509.13702)
*Xiao Zheng*

Main category: cs.CL

TL;DR: DSCC-HS是一个主动干预自回归解码的框架，通过紧凑代理模型动态校准大语言模型的输出，在TruthfulQA上达到99.2%的事实一致性率。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型幻觉问题，当前方法如RAG多为被动响应，需要更主动的干预机制来提升模型事实性。

Method: 基于双过程认知理论，使用Factual Alignment Proxy和Hallucination Detection Proxy两个对抗训练的代理模型，在解码时注入实时转向向量来动态校准目标模型输出。

Result: 在TruthfulQA上达到99.2%的事实一致性率，在BioGEN长文本基准上获得最高FActScore 46.50，达到最先进性能。

Conclusion: DSCC-HS是一个原理清晰、高效的大语言模型事实性增强解决方案，无需修改目标模型即可实现即插即用。

Abstract: Large Language Model (LLM) hallucination is a significant barrier to their
reliable deployment. Current methods like Retrieval-Augmented Generation (RAG)
are often reactive. We introduce **Dynamic Self-reinforcing Calibration for
Hallucination Suppression (DSCC-HS)**, a novel, proactive framework that
intervenes during autoregressive decoding. Inspired by dual-process cognitive
theory, DSCC-HS uses a compact proxy model, trained in adversarial roles as a
Factual Alignment Proxy (FAP) and a Hallucination Detection Proxy (HDP). During
inference, these proxies dynamically steer a large target model by injecting a
real-time steering vector, which is the difference between FAP and HDP logits,
at each decoding step. This plug-and-play approach requires no modification to
the target model. Our experiments on TruthfulQA and BioGEN show DSCC-HS
achieves state-of-the-art performance. On TruthfulQA, it reached a 99.2%
Factual Consistency Rate (FCR). On the long-form BioGEN benchmark, it attained
the highest FActScore of 46.50. These results validate DSCC-HS as a principled
and efficient solution for enhancing LLM factuality.

</details>


### [13] [Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification](https://arxiv.org/abs/2509.13888)
*Mariano Barone,Antonio Romano,Giuseppe Riccio,Marco Postiglione,Vincenzo Moscato*

Main category: cs.CL

TL;DR: CER是一个用于生物医学事实核查的新框架，结合科学证据检索、大语言模型推理和监督真实性预测，在多个数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 医疗错误信息（如疫苗犹豫和未经证实的治疗方法）对公共卫生和医疗系统信任构成风险。生物医学声明验证具有独特挑战性，包括复杂术语、需要领域专业知识以及必须基于科学证据。

Method: CER框架整合科学证据检索、大语言模型推理和监督真实性预测。通过将大语言模型的文本生成能力与高质量生物医学科学证据的先进检索技术相结合，有效减少幻觉风险。

Result: 在专家标注的数据集（HealthFC、BioASQ-7b、SciFact）上评估显示达到最先进性能，并展现出有前景的跨数据集泛化能力。

Conclusion: CER框架通过结合证据检索和推理，为生物医学事实核查提供了有效的解决方案，代码和数据已开源以确保透明度和可复现性。

Abstract: Misinformation in healthcare, from vaccine hesitancy to unproven treatments,
poses risks to public health and trust in medical systems. While machine
learning and natural language processing have advanced automated fact-checking,
validating biomedical claims remains uniquely challenging due to complex
terminology, the need for domain expertise, and the critical importance of
grounding in scientific evidence. We introduce CER (Combining Evidence and
Reasoning), a novel framework for biomedical fact-checking that integrates
scientific evidence retrieval, reasoning via large language models, and
supervised veracity prediction. By integrating the text-generation capabilities
of large language models with advanced retrieval techniques for high-quality
biomedical scientific evidence, CER effectively mitigates the risk of
hallucinations, ensuring that generated outputs are grounded in verifiable,
evidence-based sources. Evaluations on expert-annotated datasets (HealthFC,
BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising
cross-dataset generalization. Code and data are released for transparency and
reproducibility: https://github.com/PRAISELab-PicusLab/CER

</details>


### [14] [Automated Triaging and Transfer Learning of Incident Learning Safety Reports Using Large Language Representational Models](https://arxiv.org/abs/2509.13706)
*Peter Beidler,Mark Nguyen,Kevin Lybarger,Ola Holmberg,Eric Ford,John Kang*

Main category: cs.CL

TL;DR: 开发了基于自然语言处理的筛查工具，用于自动检测放射肿瘤学中的高严重性事件报告，通过跨机构迁移学习提升了模型泛化能力


<details>
  <summary>Details</summary>
Motivation: 医疗事件报告的手动审查耗时且需要专业知识，需要自动化工具来提高安全性和质量改进效率

Method: 使用支持向量机(SVM)和预训练语言模型BlueBERT，在两个机构的放射肿瘤学事件报告数据集上进行训练和评估，采用跨机构迁移学习策略

Result: BlueBERT_TRANSFER模型在跨机构测试集上达到AUROC 0.78，在人工编辑的数据集上与人类表现相当(AUROC 0.74 vs 0.81)

Conclusion: 成功开发了跨机构的NLP模型，能够像人类专家一样检测高严重性事件报告，为医疗安全质量改进提供了有效的自动化工具

Abstract: PURPOSE: Incident reports are an important tool for safety and quality
improvement in healthcare, but manual review is time-consuming and requires
subject matter expertise. Here we present a natural language processing (NLP)
screening tool to detect high-severity incident reports in radiation oncology
across two institutions.
  METHODS AND MATERIALS: We used two text datasets to train and evaluate our
NLP models: 7,094 reports from our institution (Inst.), and 571 from IAEA
SAFRON (SF), all of which had severity scores labeled by clinical content
experts. We trained and evaluated two types of models: baseline support vector
machines (SVM) and BlueBERT which is a large language model pretrained on
PubMed abstracts and hospitalized patient data. We assessed for
generalizability of our model in two ways. First, we evaluated models trained
using Inst.-train on SF-test. Second, we trained a BlueBERT_TRANSFER model that
was first fine-tuned on Inst.-train then on SF-train before testing on SF-test
set. To further analyze model performance, we also examined a subset of 59
reports from our Inst. dataset, which were manually edited for clarity.
  RESULTS Classification performance on the Inst. test achieved AUROC 0.82
using SVM and 0.81 using BlueBERT. Without cross-institution transfer learning,
performance on the SF test was limited to an AUROC of 0.42 using SVM and 0.56
using BlueBERT. BlueBERT_TRANSFER, which was fine-tuned on both datasets,
improved the performance on SF test to AUROC 0.78. Performance of SVM, and
BlueBERT_TRANSFER models on the manually curated Inst. reports (AUROC 0.85 and
0.74) was similar to human performance (AUROC 0.81).
  CONCLUSION: In summary, we successfully developed cross-institution NLP
models on incident report text from radiation oncology centers. These models
were able to detect high-severity reports similarly to humans on a curated
dataset.

</details>


### [15] [DSPC: Dual-Stage Progressive Compression Framework for Efficient Long-Context Reasoning](https://arxiv.org/abs/2509.13723)
*Yaxin Gao,Yao Lu,Zongfei Zhang,Jiaqi Nie,Shanqing Yu,Qi Xuan*

Main category: cs.CL

TL;DR: 提出DSPC双阶段渐进压缩方法，无需训练即可压缩LLM提示词，在保持语义的同时显著减少计算成本


<details>
  <summary>Details</summary>
Motivation: 解决LLM提示词越来越长导致的计算成本增加问题，现有方法需要训练辅助模型带来额外计算开销

Method: 两阶段无训练方法：粗粒度阶段基于TF-IDF过滤低语义价值句子；细粒度阶段通过注意力贡献、跨模型损失差异和位置重要性评估token重要性，剪枝低效用token

Result: 在LLaMA-3.1-8B-Instruct和GPT-3.5-Turbo上验证，在FewShot任务中仅用1/3 token就达到49.17性能，比最佳基线LongLLMLingua提升7.76

Conclusion: DSPC方法有效解决了提示词膨胀问题，无需训练即可实现高效压缩，在受限token预算下表现优异

Abstract: Large language models (LLMs) have achieved remarkable success in many natural
language processing (NLP) tasks. To achieve more accurate output, the prompts
used to drive LLMs have become increasingly longer, which incurs higher
computational costs. To address this prompt inflation problem, prompt
compression has been proposed. However, most existing methods require training
a small auxiliary model for compression, incurring a significant amount of
additional computation. To avoid this, we propose a two-stage, training-free
approach, called Dual-Stage Progressive Compression (DSPC). In the
coarse-grained stage, semantic-related sentence filtering removes sentences
with low semantic value based on TF-IDF. In the fine-grained stage, token
importance is assessed using attention contribution, cross-model loss
difference, and positional importance, enabling the pruning of low-utility
tokens while preserving semantics. We validate DSPC on LLaMA-3.1-8B-Instruct
and GPT-3.5-Turbo under a constrained token budget and observe consistent
improvements. For instance, in the FewShot task of the Longbench dataset, DSPC
achieves a performance of 49.17 by using only 3x fewer tokens, outperforming
the best state-of-the-art baseline LongLLMLingua by 7.76.

</details>


### [16] [Implementing a Logical Inference System for Japanese Comparatives](https://arxiv.org/abs/2509.13734)
*Yosuke Mikami,Daiki Matsuoka,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 提出了基于组合语义的日语比较级逻辑推理系统ccg-jcomp，解决了日语比较级与英语的形态语义差异问题，并在日语NLI数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 日语和英语比较级存在形态和语义差异，现有基于英语的逻辑推理系统无法直接应用于日语比较级推理，需要专门针对日语特点开发新的推理系统。

Method: 基于组合语义学构建逻辑推理系统ccg-jcomp，专门处理日语比较级的数值和逻辑表达式推理。

Result: 在包含比较表达的日语NLI数据集上评估，系统准确性优于现有大型语言模型。

Conclusion: ccg-jcomp系统能有效处理日语比较级的逻辑推理任务，证明了基于组合语义的逻辑方法在日语NLI中的优势。

Abstract: Natural Language Inference (NLI) involving comparatives is challenging
because it requires understanding quantities and comparative relations
expressed by sentences. While some approaches leverage Large Language Models
(LLMs), we focus on logic-based approaches grounded in compositional semantics,
which are promising for robust handling of numerical and logical expressions.
Previous studies along these lines have proposed logical inference systems for
English comparatives. However, it has been pointed out that there are several
morphological and semantic differences between Japanese and English
comparatives. These differences make it difficult to apply such systems
directly to Japanese comparatives. To address this gap, this study proposes
ccg-jcomp, a logical inference system for Japanese comparatives based on
compositional semantics. We evaluate the proposed system on a Japanese NLI
dataset containing comparative expressions. We demonstrate the effectiveness of
our system by comparing its accuracy with that of existing LLMs.

</details>


### [17] [Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications](https://arxiv.org/abs/2509.13775)
*Vani Kanjirangat,Ljiljana Dolamic,Fabio Rinaldi*

Main category: cs.CL

TL;DR: 本文探索了阿拉伯语方言识别(ADI)的数据高效和参数高效方法，包括各种软提示策略、LoRA重参数化以及零样本和少样本推理，发现LLMs在方言识别方面表现不佳，而LoRA微调模型效果最佳


<details>
  <summary>Details</summary>
Motivation: 研究阿拉伯语方言识别的高效方法，旨在解决大语言模型在区分方言细微差别方面的困难，同时探索参数和数据高效的学习策略

Method: 使用软提示策略(prefix-tuning、prompt-tuning、P-tuning、P-tuning V2)、LoRA重参数化、零样本和少样本推理，在阿拉伯语专用编码器模型和多语言解码器模型上进行实验

Result: LLMs在零样本和少样本设置下难以区分方言细微差别；软提示编码器变体表现更好；基于LoRA的微调模型表现最佳，甚至超过完全微调

Conclusion: LoRA微调是阿拉伯语方言识别的最有效方法，而大语言模型在方言识别任务上仍有局限性，需要专门的参数高效微调策略

Abstract: This paper discusses our exploration of different data-efficient and
parameter-efficient approaches to Arabic Dialect Identification (ADI). In
particular, we investigate various soft-prompting strategies, including
prefix-tuning, prompt-tuning, P-tuning, and P-tuning V2, as well as LoRA
reparameterizations. For the data-efficient strategy, we analyze hard prompting
with zero-shot and few-shot inferences to analyze the dialect identification
capabilities of Large Language Models (LLMs). For the parameter-efficient PEFT
approaches, we conducted our experiments using Arabic-specific encoder models
on several major datasets. We also analyzed the n-shot inferences on
open-source decoder-only models, a general multilingual model (Phi-3.5), and an
Arabic-specific one(SILMA). We observed that the LLMs generally struggle to
differentiate the dialectal nuances in the few-shot or zero-shot setups. The
soft-prompted encoder variants perform better, while the LoRA-based fine-tuned
models perform best, even surpassing full fine-tuning.

</details>


### [18] [Teaching According to Talents! Instruction Tuning LLMs with Competence-Aware Curriculum Learning](https://arxiv.org/abs/2509.13790)
*Yangning Li,Tingwei Lu,Yinghui Li,Yankai Chen,Wei-Chieh Huang,Wenhao Jiang,Hui Wang,Hai-Tao Zheng,Philip S. Yu*

Main category: cs.CL

TL;DR: CAMPUS是一个动态多视角课程学习框架，通过能力感知的课程调整和动态子课程选择，解决了传统课程学习中静态难度度量导致的课程僵化问题，在指令调优中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统课程学习方法依赖静态启发式难度度量，无法适应模型在训练过程中不断演进的能力，导致固定的、可能次优的学习轨迹，需要动态调整的课程学习策略。

Method: 提出CAMPUS框架，包含三个核心优势：动态子课程选择、能力感知的课程进度调整、基于多难度的调度策略，实现自适应课程学习。

Result: 大量实验证明，CAMPUS在高效指令调优方面优于其他最先进的基线方法，展现出显著性能提升。

Conclusion: CAMPUS通过动态多视角课程学习有效解决了课程僵化问题，为大规模语言模型的指令调优提供了更有效的训练策略。

Abstract: Efficient instruction tuning aims to enhance the ultimate performance of
large language models (LLMs) trained on a given instruction dataset. Curriculum
learning as a typical data organization strategy has shown preliminary
effectiveness in instruction tuning. However, current curriculum tuning methods
suffer from the curriculum rigidity, since they rely solely on static heuristic
difficulty metrics. These methods fail to adapt to the evolving capabilities of
models during training, resulting in a fixed and potentially sub-optimal
learning trajectory. To address the issue, Competence-Aware Multi-Perspective
cUrriculum inStruction tuning framework termed CAMPUS is proposed. CAMPUS
offers several advantages: (1) Dynamic selection for sub-curriculum. (2)
Competency-aware adjustment to the curriculum schedule. (3) Multiple
difficulty-based scheduling. Extensive experiments prove the superior
performance of CAMPUS, compared to other state-of-the-art baselines for
efficient instruction tuning.

</details>


### [19] [Measuring Gender Bias in Job Title Matching for Grammatical Gender Languages](https://arxiv.org/abs/2509.13803)
*Laura García-Sardiña,Hermenegildo Fabregat,Daniel Deniz,Rabih Zbib*

Main category: cs.CL

TL;DR: 本文研究了语法性别在职位名称中的显式分配如何影响自动职位排名系统的结果，提出了基于RBO的性别偏见评估方法，并在四种语法性别语言中创建了测试集来评估多语言模型的性别偏见。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索语法性别在职位名称中的显式分配对自动职位排名系统的影响，特别是评估这些系统中存在的性别偏见问题。

Method: 提出了使用RBO（Rank-Biased Overlap）指标来比较控制性别的排名结果，生成了四种语法性别语言的测试集，包含男性和女性形式的职位名称，并标注了性别和匹配相关性。

Result: 使用新测试集和方法评估了多个现成的多语言模型，结果显示所有模型都表现出不同程度的性别偏见。

Conclusion: 研究为评估职位排名系统中的性别偏见奠定了基础，证明了现有多语言模型普遍存在性别偏见问题，需要进一步改进。

Abstract: This work sets the ground for studying how explicit grammatical gender
assignment in job titles can affect the results of automatic job ranking
systems. We propose the usage of metrics for ranking comparison controlling for
gender to evaluate gender bias in job title ranking systems, in particular RBO
(Rank-Biased Overlap). We generate and share test sets for a job title matching
task in four grammatical gender languages, including occupations in masculine
and feminine form and annotated by gender and matching relevance. We use the
new test sets and the proposed methodology to evaluate the gender bias of
several out-of-the-box multilingual models to set as baselines, showing that
all of them exhibit varying degrees of gender bias.

</details>


### [20] [Geometric Uncertainty for Detecting and Correcting Hallucinations in LLMs](https://arxiv.org/abs/2509.13813)
*Edward Phillips,Sean Wu,Soheila Molaei,Danielle Belgrave,Anshul Thakur,David Clifton*

Main category: cs.CL

TL;DR: 提出基于几何框架的黑盒不确定性量化方法，通过原型分析同时提供全局和局部不确定性估计，用于检测大语言模型的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒方法只能提供全局不确定性估计，而局部方法需要白盒访问模型内部状态。需要一种仅通过黑盒访问就能同时提供全局和局部不确定性量化的方法。

Method: 基于响应嵌入的原型分析几何框架。全局层面使用几何体积测量原型凸包体积，局部层面使用几何怀疑度对响应可靠性排序，通过优先选择实现幻觉减少。

Result: 在短形式问答数据集上表现相当或优于现有方法，在医疗数据集上取得优越结果（幻觉风险特别关键），并理论证明了凸包体积与熵的联系。

Conclusion: 提出的几何框架有效解决了黑盒设置下的全局和局部不确定性量化问题，为幻觉检测提供了实用工具，特别是在高风险领域具有重要价值。

Abstract: Large language models demonstrate impressive results across diverse tasks but
are still known to hallucinate, generating linguistically plausible but
incorrect answers to questions. Uncertainty quantification has been proposed as
a strategy for hallucination detection, but no existing black-box approach
provides estimates for both global and local uncertainty. The former attributes
uncertainty to a batch of responses, while the latter attributes uncertainty to
individual responses. Current local methods typically rely on white-box access
to internal model states, whilst black-box methods only provide global
uncertainty estimates. We introduce a geometric framework to address this,
based on archetypal analysis of batches of responses sampled with only
black-box model access. At the global level, we propose Geometric Volume, which
measures the convex hull volume of archetypes derived from response embeddings.
At the local level, we propose Geometric Suspicion, which ranks responses by
reliability and enables hallucination reduction through preferential response
selection. Unlike prior dispersion methods which yield only a single global
score, our approach provides semantic boundary points which have utility for
attributing reliability to individual responses. Experiments show that our
framework performs comparably to or better than prior methods on short form
question-answering datasets, and achieves superior results on medical datasets
where hallucinations carry particularly critical risks. We also provide
theoretical justification by proving a link between convex hull volume and
entropy.

</details>


### [21] [Findings of the Third Automatic Minuting (AutoMin) Challenge](https://arxiv.org/abs/2509.13814)
*Kartik Shinde,Laurent Besacier,Ondrej Bojar,Thibaut Thonet,Tirthankar Ghosal*

Main category: cs.CL

TL;DR: AutoMin 2025共享任务聚焦于会议纪要自动生成和问答两个任务，涵盖英语和捷克语，参与团队较少但包含多个基线系统评估。


<details>
  <summary>Details</summary>
Motivation: 推动自动会议纪要生成技术的发展，评估当前大语言模型在结构化会议纪要生成和基于会议转录的问答任务上的表现。

Method: 设置两个主要任务：minuting（结构化会议纪要生成）和question answering（问答）。minuting任务覆盖英语和捷克语两种语言，项目会议和欧洲议会两个领域；QA任务专注于项目会议，包含单语（英语）和跨语言（捷克语问答英语会议）两种设置。

Result: 2025年参与度较低，minuting任务只有1个团队参加，QA任务有2个团队参与。组织者提供了多个基线系统进行综合评估。

Conclusion: 尽管参与团队有限，但通过基线系统的设置，为评估当前大语言模型在自动会议纪要任务上的性能提供了有价值的基准。

Abstract: This paper presents the third edition of AutoMin, a shared task on automatic
meeting summarization into minutes. In 2025, AutoMin featured the main task of
minuting, the creation of structured meeting minutes, as well as a new task:
question answering (QA) based on meeting transcripts.
  The minuting task covered two languages, English and Czech, and two domains:
project meetings and European Parliament sessions. The QA task focused solely
on project meetings and was available in two settings: monolingual QA in
English, and cross-lingual QA, where questions were asked and answered in Czech
based on English meetings.
  Participation in 2025 was more limited compared to previous years, with only
one team joining the minuting task and two teams participating in QA. However,
as organizers, we included multiple baseline systems to enable a comprehensive
evaluation of current (2025) large language models (LLMs) on both tasks.

</details>


### [22] [Large Language Models Discriminate Against Speakers of German Dialects](https://arxiv.org/abs/2509.13835)
*Minh Duc Bui,Carolin Holtermann,Valentin Hofmann,Anne Lauscher,Katharina von der Wense*

Main category: cs.CL

TL;DR: 研究发现大型语言模型对德国方言使用者存在显著的命名偏见和使用偏见，在关联任务和决策任务中都表现出负面刻板印象，且明确提及语言人口统计信息会放大偏见。


<details>
  <summary>Details</summary>
Motivation: 尽管方言具有重要文化价值，但方言使用者常面临负面社会刻板印象。研究旨在探讨大型语言模型是否也反映了这种对方言使用者的偏见。

Method: 基于社会语言学文献分析方言使用者相关特质，通过关联任务和决策任务评估LLMs的方言命名偏见和使用偏见，构建包含7种德国地区方言及其标准德语对应句子的评估语料库。

Result: 所有评估的LLMs都表现出显著的方言命名和使用偏见，反映在负面形容词关联上；所有模型在决策中都重现这些偏见；明确标注语言人口统计信息比隐式线索更能放大偏见。

Conclusion: 大型语言模型确实反映了社会中对德国方言使用者的负面刻板印象，且显性人口统计标注会加剧偏见，这对LLMs的公平性评估和偏见缓解具有重要意义。

Abstract: Dialects represent a significant component of human culture and are found
across all regions of the world. In Germany, more than 40% of the population
speaks a regional dialect (Adler and Hansen, 2022). However, despite cultural
importance, individuals speaking dialects often face negative societal
stereotypes. We examine whether such stereotypes are mirrored by large language
models (LLMs). We draw on the sociolinguistic literature on dialect perception
to analyze traits commonly associated with dialect speakers. Based on these
traits, we assess the dialect naming bias and dialect usage bias expressed by
LLMs in two tasks: an association task and a decision task. To assess a model's
dialect usage bias, we construct a novel evaluation corpus that pairs sentences
from seven regional German dialects (e.g., Alemannic and Bavarian) with their
standard German counterparts. We find that: (1) in the association task, all
evaluated LLMs exhibit significant dialect naming and dialect usage bias
against German dialect speakers, reflected in negative adjective associations;
(2) all models reproduce these dialect naming and dialect usage biases in their
decision making; and (3) contrary to prior work showing minimal bias with
explicit demographic mentions, we find that explicitly labeling linguistic
demographics--German dialect speakers--amplifies bias more than implicit cues
like dialect usage.

</details>


### [23] [Do LLMs Align Human Values Regarding Social Biases? Judging and Explaining Social Biases with LLMs](https://arxiv.org/abs/2509.13869)
*Yang Liu,Chenhui Chu*

Main category: cs.CL

TL;DR: 本研究探讨大型语言模型(LLMs)在不同类型偏见场景中与人类价值观的对齐情况，发现模型规模与对齐程度不一定正相关，模型对特定场景类型有偏好，且同一模型家族的判断一致性更高。


<details>
  <summary>Details</summary>
Motivation: 现有研究揭示了LLMs与人类价值观的错位问题，但尚不清楚在不同类型偏见场景(如负面与非负面问题)中的对齐差异，需要系统研究LLMs在社会偏见价值观方面的对齐特性。

Method: 通过对4个模型家族的12个LLMs和4个数据集进行广泛分析，研究模型参数规模与对齐率的关系，评估模型对特定场景类型的偏好，分析解释生成能力，并通过微调较小语言模型来赋予解释能力。

Result: 大规模模型不一定有更低的对齐错误率和攻击成功率；模型对特定场景类型有对齐偏好；同一模型家族判断一致性更高；不同LLMs对偏见价值观理解无显著差异；模型偏好自身生成的解释；微调后的小模型生成更可读但模型认同度较低的解释。

Conclusion: LLMs在社会偏见价值观对齐方面存在复杂模式，模型规模不是决定性因素，场景类型和模型家族特性影响对齐表现，需要通过更精细的方法来改善模型与人类价值观的对齐。

Abstract: Large language models (LLMs) can lead to undesired consequences when
misaligned with human values, especially in scenarios involving complex and
sensitive social biases. Previous studies have revealed the misalignment of
LLMs with human values using expert-designed or agent-based emulated bias
scenarios. However, it remains unclear whether the alignment of LLMs with human
values differs across different types of scenarios (e.g., scenarios containing
negative vs. non-negative questions). In this study, we investigate the
alignment of LLMs with human values regarding social biases (HVSB) in different
types of bias scenarios. Through extensive analysis of 12 LLMs from four model
families and four datasets, we demonstrate that LLMs with large model parameter
scales do not necessarily have lower misalignment rate and attack success rate.
Moreover, LLMs show a certain degree of alignment preference for specific types
of scenarios and the LLMs from the same model family tend to have higher
judgment consistency. In addition, we study the understanding capacity of LLMs
with their explanations of HVSB. We find no significant differences in the
understanding of HVSB across LLMs. We also find LLMs prefer their own generated
explanations. Additionally, we endow smaller language models (LMs) with the
ability to explain HVSB. The generation results show that the explanations
generated by the fine-tuned smaller LMs are more readable, but have a
relatively lower model agreeability.

</details>


### [24] [Do Large Language Models Understand Word Senses?](https://arxiv.org/abs/2509.13905)
*Domenico Meconi,Simone Stirpe,Federico Martelli,Leonardo Lavalle,Roberto Navigli*

Main category: cs.CL

TL;DR: 论文评估了指令调优LLM的词义消歧能力，发现GPT-4o和DeepSeek-V3等领先模型在WSD任务上达到与专门系统相当的性能，并在生成任务中表现出高达98%的准确率。


<details>
  <summary>Details</summary>
Motivation: 尽管已有大量评估工作，但LLM是否真正理解词义仍未被充分探索。本文旨在填补这一空白，评估LLM的词义消歧能力和对词义的理解程度。

Method: 评估指令调优LLM的词义消歧能力，并与专门设计的SOTA系统进行比较；评估两个顶级开源和闭源LLM在三种生成设置下的词义理解能力：定义生成、自由形式解释和示例生成。

Result: 在WSD任务中，GPT-4o和DeepSeek-V3达到与专门WSD系统相当的性能，且在不同领域和难度级别上表现出更强的鲁棒性；在生成任务中，LLM能以高达98%的准确率解释上下文中的词义，自由形式解释任务表现最佳。

Conclusion: LLM在词义消歧任务上已达到专门系统的水平，并在生成任务中展现出强大的词义理解能力，特别是在自由形式解释方面表现最优。

Abstract: Understanding the meaning of words in context is a fundamental capability for
Large Language Models (LLMs). Despite extensive evaluation efforts, the extent
to which LLMs show evidence that they truly grasp word senses remains
underexplored. In this paper, we address this gap by evaluating both i) the
Word Sense Disambiguation (WSD) capabilities of instruction-tuned LLMs,
comparing their performance to state-of-the-art systems specifically designed
for the task, and ii) the ability of two top-performing open- and closed-source
LLMs to understand word senses in three generative settings: definition
generation, free-form explanation, and example generation. Notably, we find
that, in the WSD task, leading models such as GPT-4o and DeepSeek-V3 achieve
performance on par with specialized WSD systems, while also demonstrating
greater robustness across domains and levels of difficulty. In the generation
tasks, results reveal that LLMs can explain the meaning of words in context up
to 98\% accuracy, with the highest performance observed in the free-form
explanation task, which best aligns with their generative capabilities.

</details>


### [25] [Linguistic Nepotism: Trading-off Quality for Language Preference in Multilingual RAG](https://arxiv.org/abs/2509.13930)
*Dayeon Ki,Marine Carpuat,Paul McNamee,Daniel Khashabi,Eugene Yang,Dawn Lawrie,Kevin Duh*

Main category: cs.CL

TL;DR: 研究发现多语言检索增强生成系统存在语言偏见，模型倾向于引用英文来源，特别是对于低资源语言和中间位置的文档，有时甚至会牺牲文档相关性来选择语言偏好。


<details>
  <summary>Details</summary>
Motivation: 研究多语言检索增强生成系统中不同文档语言的混合是否会对生成和引用产生意外影响，特别是模型是否会在引用时表现出语言偏好。

Method: 采用控制方法，利用模型内部机制来衡量语言偏好，同时保持文档相关性等其他因素不变，在8种语言和6个开源模型上进行测试。

Result: 模型在英文查询时优先引用英文来源，这种偏见在低资源语言和中间位置的文档中更加明显，模型有时会为了语言偏好而牺牲文档相关性。

Conclusion: 引用选择并非总是由信息量驱动，研究揭示了语言模型如何利用多语言上下文并影响引用行为，这对多语言检索增强生成系统的设计和评估具有重要意义。

Abstract: Multilingual Retrieval-Augmented Generation (mRAG) systems enable language
models to answer knowledge-intensive queries with citation-supported responses
across languages. While such systems have been proposed, an open questions is
whether the mixture of different document languages impacts generation and
citation in unintended ways. To investigate, we introduce a controlled
methodology using model internals to measure language preference while holding
other factors such as document relevance constant. Across eight languages and
six open-weight models, we find that models preferentially cite English sources
when queries are in English, with this bias amplified for lower-resource
languages and for documents positioned mid-context. Crucially, we find that
models sometimes trade-off document relevance for language preference,
indicating that citation choices are not always driven by informativeness
alone. Our findings shed light on how language models leverage multilingual
context and influence citation behavior.

</details>


### [26] [Long-context Reference-based MT Quality Estimation](https://arxiv.org/abs/2509.13980)
*Sami Ul Haq,Chinonso Cynthia Osuji,Sheila Castilho,Brian Davis*

Main category: cs.CL

TL;DR: 基于COMET框架构建的翻译质量评估系统，通过长上下文数据增强训练来预测错误跨度标注分数，整合多种人工标注数据集，实验表明长上下文信息能提升与人工评估的相关性


<details>
  <summary>Details</summary>
Motivation: 解决传统翻译质量评估模型仅基于短片段训练的局限性，通过引入长上下文信息来更好地捕捉翻译质量的整体连贯性和上下文依赖性

Method: 使用COMET框架，通过拼接领域内人工标注句子构建长上下文训练数据，计算加权平均分数，整合MQM、SQM、DA等多种人工评估数据集并进行尺度归一化，训练多语言回归模型

Result: 实验结果显示，相比仅使用短片段训练的模型，融入长上下文信息的模型与人工评估结果具有更高的相关性

Conclusion: 长上下文信息的引入能有效提升翻译质量评估模型的性能，为自动化翻译质量评估提供了新的改进方向

Abstract: In this paper, we present our submission to the Tenth Conference on Machine
Translation (WMT25) Shared Task on Automated Translation Quality Evaluation.
  Our systems are built upon the COMET framework and trained to predict
segment-level Error Span Annotation (ESA) scores using augmented long-context
data.
  To construct long-context training data, we concatenate in-domain,
human-annotated sentences and compute a weighted average of their scores.
  We integrate multiple human judgment datasets (MQM, SQM, and DA) by
normalising their scales and train multilingual regression models to predict
quality scores from the source, hypothesis, and reference translations.
  Experimental results show that incorporating long-context information
improves correlations with human judgments compared to models trained only on
short segments.

</details>


### [27] [Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency](https://arxiv.org/abs/2509.13990)
*Colin Hong,Xu Guo,Anand Chaanan Singh,Esha Choukse,Dmitrii Ustiugov*

Main category: cs.CL

TL;DR: Slim-SC是一种新的测试时缩放技术，通过逐步剪枝策略减少Self-Consistency中的冗余推理链，在保持或提高准确性的同时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: Self-Consistency(SC)虽然能提升LLM推理性能，但其数量级的计算开销限制了广泛应用。现有加速方法主要依赖模型置信度或经验启发式，缺乏理论和实证支持。

Method: 提出Slim-SC方法，在思想层面利用链间相似性识别和移除冗余推理链，采用逐步剪枝策略来优化SC的效率。

Result: 在三个STEM推理数据集和两种LLM架构上的实验表明，Slim-SC将推理延迟和KVC使用分别降低高达45%和26%，同时保持或提高了准确性。

Conclusion: Slim-SC为SC提供了一种简单而高效的测试时缩放替代方案，通过理论分析和实证验证的有效剪枝策略解决了计算效率问题。

Abstract: Recently, Test-Time Scaling (TTS) has gained increasing attention for
improving LLM reasoning performance at test time without retraining the model.
A notable TTS technique is Self-Consistency (SC), which generates multiple
reasoning chains in parallel and selects the final answer via majority voting.
While effective, the order-of-magnitude computational overhead limits its broad
deployment. Prior attempts to accelerate SC mainly rely on model-based
confidence scores or heuristics with limited empirical support. For the first
time, we theoretically and empirically analyze the inefficiencies of SC and
reveal actionable opportunities for improvement. Building on these insights, we
propose Slim-SC, a step-wise pruning strategy that identifies and removes
redundant chains using inter-chain similarity at the thought level. Experiments
on three STEM reasoning datasets and two recent LLM architectures show that
Slim-SC reduces inference latency and KVC usage by up to 45% and 26%,
respectively, with R1-Distill, while maintaining or improving accuracy, thus
offering a simple yet efficient TTS alternative for SC.

</details>


### [28] [Early Stopping Chain-of-thoughts in Large Language Models](https://arxiv.org/abs/2509.14004)
*Minjia Mao,Bowen Yin,Yu Zhu,Xiao Fang*

Main category: cs.CL

TL;DR: ES-CoT是一种推理时方法，通过检测答案收敛性来提前停止思维链生成，在保持准确性的同时平均减少41%的推理token使用量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在解决复杂问题时需要生成长思维链，但这种方法推理成本高昂。研究旨在减少推理成本的同时保持性能。

Method: 在每个推理步骤结束时提示LLM输出当前最终答案（步骤答案），跟踪连续相同步骤答案的运行长度作为收敛度量，当运行长度出现急剧增加并超过阈值时终止生成。

Result: 在五个推理数据集和三个LLM上的实验表明，ES-CoT平均减少约41%的推理token，同时保持与标准CoT相当的准确性。该方法还能与自一致性提示无缝集成。

Conclusion: ES-CoT是一种实用有效的推理效率提升方法，具有理论和实证支持，对超参数选择具有鲁棒性。

Abstract: Reasoning large language models (LLMs) have demonstrated superior capacities
in solving complicated problems by generating long chain-of-thoughts (CoT), but
such a lengthy CoT incurs high inference costs. In this study, we introduce
ES-CoT, an inference-time method that shortens CoT generation by detecting
answer convergence and stopping early with minimal performance loss. At the end
of each reasoning step, we prompt the LLM to output its current final answer,
denoted as a step answer. We then track the run length of consecutive identical
step answers as a measure of answer convergence. Once the run length exhibits a
sharp increase and exceeds a minimum threshold, the generation is terminated.
We provide both empirical and theoretical support for this heuristic: step
answers steadily converge to the final answer, and large run-length jumps
reliably mark this convergence. Experiments on five reasoning datasets across
three LLMs show that ES-CoT reduces the number of inference tokens by about
41\% on average while maintaining accuracy comparable to standard CoT. Further,
ES-CoT integrates seamlessly with self-consistency prompting and remains robust
across hyperparameter choices, highlighting it as a practical and effective
approach for efficient reasoning.

</details>


### [29] [Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale](https://arxiv.org/abs/2509.14008)
*Hasan Abed Al Kader Hammoud,Mohammad Zbeeb,Bernard Ghanem*

Main category: cs.CL

TL;DR: Hala是一个阿拉伯语为中心的指令和翻译模型家族，通过翻译调优流程构建，在阿拉伯语基准测试中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯语NLP中高质量指令数据的稀缺问题，提升阿拉伯语语言模型的性能。

Method: 使用FP8压缩的教师模型生成高质量双语监督数据，训练轻量级语言模型进行翻译，创建百万级阿拉伯语指令数据集，并通过slerp合并技术平衡阿拉伯语专业化和基础模型优势。

Result: 在阿拉伯语基准测试中，Hala模型在"nano"(≤2B)和"small"(7-9B)类别中都取得了最先进的结果，超越了基础模型。

Conclusion: Hala模型系列为阿拉伯语NLP研究提供了有效的解决方案，并发布了模型、数据、评估和训练方法以加速该领域研究。

Abstract: We present Hala, a family of Arabic-centric instruction and translation
models built with our translate-and-tune pipeline. We first compress a strong
AR$\leftrightarrow$EN teacher to FP8 (yielding $\sim$2$\times$ higher
throughput with no quality loss) and use it to create high-fidelity bilingual
supervision. A lightweight language model LFM2-1.2B is then fine-tuned on this
data and used to translate high-quality English instruction sets into Arabic,
producing a million-scale corpus tailored to instruction following. We train
Hala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to
balance Arabic specialization with base-model strengths. On Arabic-centric
benchmarks, Hala achieves state-of-the-art results within both the "nano"
($\leq$2B) and "small" (7-9B) categories, outperforming their bases. We release
models, data, evaluation, and recipes to accelerate research in Arabic NLP.

</details>


### [30] [Audio-Based Crowd-Sourced Evaluation of Machine Translation Quality](https://arxiv.org/abs/2509.14023)
*Sami Ul Haq,Sheila Castilho,Yvette Graham*

Main category: cs.CL

TL;DR: 本研究比较了文本和音频两种方式对机器翻译系统进行评估的效果，发现基于音频的评估能更自然地反映真实应用场景，且在部分情况下能识别出文本评估无法发现的系统差异


<details>
  <summary>Details</summary>
Motivation: 当前机器翻译质量评估主要依赖文本方式，但许多实际应用（如语音翻译）涉及口语输出，需要更自然的语音评估方法来更好地反映真实使用场景

Method: 使用Amazon Mechanical Turk众包平台收集评估数据，对WMT General MT共享任务的10个机器翻译系统进行文本和音频两种方式的评估，并进行统计显著性检验和自重复实验验证可靠性

Result: 基于音频的评估结果与文本评估基本一致，但在某些情况下能识别出翻译系统之间的显著差异，这归因于语音提供了更丰富、更自然的模态信息

Conclusion: 建议将基于语音的评估纳入未来机器翻译评估框架，以更好地反映实际应用场景中的翻译质量

Abstract: Machine Translation (MT) has achieved remarkable performance, with growing
interest in speech translation and multimodal approaches. However, despite
these advancements, MT quality assessment remains largely text centric,
typically relying on human experts who read and compare texts. Since many
real-world MT applications (e.g Google Translate Voice Mode, iFLYTEK
Translator) involve translation being spoken rather printed or read, a more
natural way to assess translation quality would be through speech as opposed
text-only evaluations. This study compares text-only and audio-based
evaluations of 10 MT systems from the WMT General MT Shared Task, using
crowd-sourced judgments collected via Amazon Mechanical Turk. We additionally,
performed statistical significance testing and self-replication experiments to
test reliability and consistency of audio-based approach. Crowd-sourced
assessments based on audio yield rankings largely consistent with text only
evaluations but, in some cases, identify significant differences between
translation systems. We attribute this to speech richer, more natural modality
and propose incorporating speech-based assessments into future MT evaluation
frameworks.

</details>


### [31] [You Are What You Train: Effects of Data Composition on Training Context-aware Machine Translation Models](https://arxiv.org/abs/2509.14031)
*Paweł Mąka,Yusuf Can Semerci,Jan Scholtes,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: 论文验证了上下文训练数据稀疏性是机器翻译中上下文利用困难的关键瓶颈，并通过构建控制比例的训练数据集证实了这一点。提出了两种训练策略，在单语和多语设置中分别实现了最高6%和8%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 标准训练数据中上下文丰富示例的稀疏性被认为是机器翻译难以有效利用上下文的主要原因，本研究旨在系统验证这一假设。

Method: 构建了具有受控比例上下文相关示例的训练数据集，在单语和多语设置下进行实验，并提出并评估了两种专门设计的训练策略。

Result: 证实了训练数据稀疏性与模型性能之间的强关联，发现不同上下文现象的改进不具有泛化性，跨语言迁移有限。提出的训练策略在ctxPro评估中分别实现了6%和8%的准确率提升。

Conclusion: 上下文训练数据稀疏性是机器翻译上下文利用的关键瓶颈，需要针对性的训练策略来有效利用可用数据，不同上下文现象需要专门的处理方法。

Abstract: Achieving human-level translations requires leveraging context to ensure
coherence and handle complex phenomena like pronoun disambiguation. Sparsity of
contextually rich examples in the standard training data has been hypothesized
as the reason for the difficulty of context utilization. In this work, we
systematically validate this claim in both single- and multilingual settings by
constructing training datasets with a controlled proportions of contextually
relevant examples. We demonstrate a strong association between training data
sparsity and model performance confirming sparsity as a key bottleneck.
Importantly, we reveal that improvements in one contextual phenomenon do no
generalize to others. While we observe some cross-lingual transfer, it is not
significantly higher between languages within the same sub-family. Finally, we
propose and empirically evaluate two training strategies designed to leverage
the available data. These strategies improve context utilization, resulting in
accuracy gains of up to 6 and 8 percentage points on the ctxPro evaluation in
single- and multilingual settings respectively.

</details>


### [32] [Enhancing Multi-Agent Debate System Performance via Confidence Expression](https://arxiv.org/abs/2509.14034)
*Zijie Lin,Bryan Hooi*

Main category: cs.CL

TL;DR: 提出了ConfMAD框架，在多智能体辩论系统中引入置信度表达机制，让LLM能够明确表达置信水平，从而提升辩论效果和系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有MAD系统中，即使某些LLM在特定任务上具有更优的知识或推理能力，但由于缺乏置信度表达，难以在辩论中清晰传达这种优势。不恰当的置信度表达会导致智能体固执坚持错误信念或过早收敛于次优答案。

Method: 开发ConfMAD框架，在多智能体辩论过程中集成置信度表达机制，让LLM能够明确表达其置信水平。

Result: 实验结果表明该方法有效，并进一步分析了置信度如何影响辩论动态，为置信度感知的MAD系统设计提供了见解。

Conclusion: 引入置信度表达能够显著提升多智能体辩论系统的性能，通过让LLM明确表达置信水平来改善辩论过程和最终决策质量。

Abstract: Generative Large Language Models (LLMs) have demonstrated remarkable
performance across a wide range of tasks. Recent research has introduced
Multi-Agent Debate (MAD) systems, which leverage multiple LLMs to simulate
human debate and thereby improve task performance. However, while some LLMs may
possess superior knowledge or reasoning capabilities for specific tasks, they
often struggle to clearly communicate this advantage during debates, in part
due to a lack of confidence expression. Moreover, inappropriate confidence
expression can cause agents in MAD systems to either stubbornly maintain
incorrect beliefs or converge prematurely on suboptimal answers, ultimately
reducing debate effectiveness and overall system performance. To address these
challenges, we propose incorporating confidence expression into MAD systems to
allow LLMs to explicitly communicate their confidence levels. To validate this
approach, we develop ConfMAD, a MAD framework that integrates confidence
expression throughout the debate process. Experimental results demonstrate the
effectiveness of our method, and we further analyze how confidence influences
debate dynamics, offering insights into the design of confidence-aware MAD
systems.

</details>


### [33] [SSL-SSAW: Self-Supervised Learning with Sigmoid Self-Attention Weighting for Question-Based Sign Language Translation](https://arxiv.org/abs/2509.14036)
*Zekang Liu,Wei Feng,Fanhua Shang,Lianyu Hu,Jichao Feng,Liqing Gao*

Main category: cs.CL

TL;DR: 提出基于问题的手语翻译任务(QB-SLT)，通过对话上下文提升翻译效果，开发了SSL-SSAW跨模态自监督学习方法，在新建数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 手语翻译中对话提供重要上下文线索，但传统依赖gloss标注成本高且不自然。对话在交流中自然产生且更易标注，因此探索如何有效整合对话信息来改进手语翻译。

Method: 提出跨模态自监督学习结合Sigmoid自注意力加权(SSL-SSAW)融合方法：1)使用对比学习对齐多模态特征；2)SSAW模块自适应提取问题和手语序列特征；3)通过自监督学习利用问题文本增强表示能力。

Result: 在新建的CSL-Daily-QA和PHOENIX-2014T-QA数据集上达到state-of-the-art性能，易获取的问题辅助可以达到甚至超过gloss辅助的性能，可视化结果证明了对话整合对翻译质量提升的有效性。

Conclusion: QB-SLT任务证明了对话上下文在手语翻译中的重要性，提出的SSL-SSAW方法有效解决了多模态特征对齐和上下文利用的挑战，为手语翻译提供了更实用和高效的解决方案。

Abstract: Sign Language Translation (SLT) bridges the communication gap between deaf
people and hearing people, where dialogue provides crucial contextual cues to
aid in translation. Building on this foundational concept, this paper proposes
Question-based Sign Language Translation (QB-SLT), a novel task that explores
the efficient integration of dialogue. Unlike gloss (sign language
transcription) annotations, dialogue naturally occurs in communication and is
easier to annotate. The key challenge lies in aligning multimodality features
while leveraging the context of the question to improve translation. To address
this issue, we propose a cross-modality Self-supervised Learning with Sigmoid
Self-attention Weighting (SSL-SSAW) fusion method for sign language
translation. Specifically, we employ contrastive learning to align
multimodality features in QB-SLT, then introduce a Sigmoid Self-attention
Weighting (SSAW) module for adaptive feature extraction from question and sign
language sequences. Additionally, we leverage available question text through
self-supervised learning to enhance representation and translation
capabilities. We evaluated our approach on newly constructed CSL-Daily-QA and
PHOENIX-2014T-QA datasets, where SSL-SSAW achieved SOTA performance. Notably,
easily accessible question assistance can achieve or even surpass the
performance of gloss assistance. Furthermore, visualization results demonstrate
the effectiveness of incorporating dialogue in improving translation quality.

</details>


### [34] [Canary-1B-v2 & Parakeet-TDT-0.6B-v3: Efficient and High-Performance Models for Multilingual ASR and AST](https://arxiv.org/abs/2509.14128)
*Monica Sekoyan,Nithin Rao Koluguri,Nune Tadevosyan,Piotr Zelasko,Travis Bartley,Nick Karpov,Jagadeesh Balam,Boris Ginsburg*

Main category: cs.CL

TL;DR: Canary-1B-v2是一个快速、鲁棒的多语言语音识别和语音翻译模型，支持25种欧洲语言，比Whisper-large-v3快10倍且性能更优


<details>
  <summary>Details</summary>
Motivation: 开发一个高效的多语言自动语音识别和语音翻译模型，在保持高性能的同时显著提升推理速度，并减少幻觉问题

Method: 采用FastConformer编码器和Transformer解码器架构，使用17M小时数据进行两阶段预训练和微调，包含非语音音频数据以减少幻觉，使用NeMo强制对齐器提供时间戳

Result: 在英语ASR上超越Whisper-large-v3且速度快10倍，在多语言ASR和AST任务上与Seamless-M4T-v2-large等更大模型竞争，同时发布了更小的Parakeet-TDT-0.6B-v3模型

Conclusion: Canary-1B-v2展示了在语音处理任务中高效架构和大规模数据训练的重要性，为多语言语音识别和翻译提供了快速可靠的解决方案

Abstract: This report introduces Canary-1B-v2, a fast, robust multilingual model for
Automatic Speech Recognition (ASR) and Speech-to-Text Translation (AST). Built
with a FastConformer encoder and Transformer decoder, it supports 25 languages
primarily European. The model was trained on 1.7M hours of total data samples,
including Granary and NeMo ASR Set 3.0, with non-speech audio added to reduce
hallucinations for ASR and AST. We describe its two-stage pre-training and
fine-tuning process with dynamic data balancing, as well as experiments with an
nGPT encoder. Results show nGPT scales well with massive data, while
FastConformer excels after fine-tuning. For timestamps, Canary-1B-v2 uses the
NeMo Forced Aligner (NFA) with an auxiliary CTC model, providing reliable
segment-level timestamps for ASR and AST. Evaluations show Canary-1B-v2
outperforms Whisper-large-v3 on English ASR while being 10x faster, and
delivers competitive multilingual ASR and AST performance against larger models
like Seamless-M4T-v2-large and LLM-based systems. We also release
Parakeet-TDT-0.6B-v3, a successor to v2, offering multilingual ASR across the
same 25 languages with just 600M parameters.

</details>


### [35] [CS-FLEURS: A Massively Multilingual and Code-Switched Speech Dataset](https://arxiv.org/abs/2509.14161)
*Brian Yan,Injy Hamed,Shuichiro Shimizu,Vasista Lodagala,William Chen,Olga Iakovenko,Bashar Talafha,Amir Hussein,Alexander Polok,Kalvin Chang,Dominik Klement,Sara Althubaiti,Puyuan Peng,Matthew Wiesner,Thamar Solorio,Ahmed Ali,Sanjeev Khudanpur,Shinji Watanabe,Chih-Chen Chen,Zhen Wu,Karim Benharrak,Anuj Diwan,Samuele Cornell,Eunjung Yeo,Kwanghee Choi,Carlos Carvalho,Karen Rosero*

Main category: cs.CL

TL;DR: CS-FLEURS是一个新的代码切换语音识别和翻译数据集，包含4个测试集，覆盖113种语言对和52种语言，旨在扩展代码切换语音研究的范围。


<details>
  <summary>Details</summary>
Motivation: 当前代码切换语音研究主要局限于高资源语言，缺乏对低资源语言的支持。为了开发和评估更广泛的代码切换语音识别和翻译系统，需要构建一个涵盖多种语言对的综合数据集。

Method: 构建了4个测试集：1）14个X-英语语言对的真实语音朗读合成代码切换句子；2）16个X-英语语言对的生成式文本转语音；3）60个{阿拉伯语、普通话、印地语、西班牙语}-X语言对的生成式文本转语音；4）45个X-英语低资源语言对的拼接式文本转语音。同时提供128小时的训练数据。

Result: 创建了CS-FLEURS数据集，包含113个独特的代码切换语言对，覆盖52种语言，为代码切换语音研究提供了全面的评估基准。

Conclusion: CS-FLEURS数据集有助于拓宽未来代码切换语音研究的范围，为开发多语言代码切换语音系统提供了重要资源。

Abstract: We present CS-FLEURS, a new dataset for developing and evaluating
code-switched speech recognition and translation systems beyond high-resourced
languages. CS-FLEURS consists of 4 test sets which cover in total 113 unique
code-switched language pairs across 52 languages: 1) a 14 X-English language
pair set with real voices reading synthetically generated code-switched
sentences, 2) a 16 X-English language pair set with generative text-to-speech
3) a 60 {Arabic, Mandarin, Hindi, Spanish}-X language pair set with the
generative text-to-speech, and 4) a 45 X-English lower-resourced language pair
test set with concatenative text-to-speech. Besides the four test sets,
CS-FLEURS also provides a training set with 128 hours of generative
text-to-speech data across 16 X-English language pairs. Our hope is that
CS-FLEURS helps to broaden the scope of future code-switched speech research.
Dataset link: https://huggingface.co/datasets/byan/cs-fleurs.

</details>


### [36] [AssoCiAm: A Benchmark for Evaluating Association Thinking while Circumventing Ambiguity](https://arxiv.org/abs/2509.14171)
*Yifan Liu,Wenkuan Zhao,Shanshan Zhong,Jinghui Qin,Mingfu Liang,Zhongzhan Huang,Wushao Wen*

Main category: cs.CL

TL;DR: 提出了AssoCiAm基准，通过分解歧义为内部和外部歧义，使用混合计算方法评估多模态大语言模型的联想能力，发现认知与联想之间存在强正相关关系。


<details>
  <summary>Details</summary>
Motivation: 现有评估框架忽视了联想任务中固有的歧义性，这种歧义源于联想的发散性，会削弱评估的可靠性。

Method: 将歧义分解为内部歧义和外部歧义，引入AssoCiAm基准，采用混合计算方法来规避歧义问题。

Result: 实验显示认知与联想能力存在强正相关，歧义会导致模型行为更随机，验证了方法的有效性。

Conclusion: AssoCiAm基准能够提供更准确可靠的联想能力评估，解决了现有评估中的歧义问题。

Abstract: Recent advancements in multimodal large language models (MLLMs) have garnered
significant attention, offering a promising pathway toward artificial general
intelligence (AGI). Among the essential capabilities required for AGI,
creativity has emerged as a critical trait for MLLMs, with association serving
as its foundation. Association reflects a model' s ability to think creatively,
making it vital to evaluate and understand. While several frameworks have been
proposed to assess associative ability, they often overlook the inherent
ambiguity in association tasks, which arises from the divergent nature of
associations and undermines the reliability of evaluations. To address this
issue, we decompose ambiguity into two types-internal ambiguity and external
ambiguity-and introduce AssoCiAm, a benchmark designed to evaluate associative
ability while circumventing the ambiguity through a hybrid computational
method. We then conduct extensive experiments on MLLMs, revealing a strong
positive correlation between cognition and association. Additionally, we
observe that the presence of ambiguity in the evaluation process causes MLLMs'
behavior to become more random-like. Finally, we validate the effectiveness of
our method in ensuring more accurate and reliable evaluations. See Project Page
for the data and codes.

</details>


### [37] [Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation Framework for Personal Finance LLMs](https://arxiv.org/abs/2509.14180)
*Akhil Theerthala*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的端到端财务顾问框架，通过整合金融背景和行为金融学研究构建监督数据，在8B参数模型上实现了与更大模型(14-32B)相当的性能，同时成本降低80%。


<details>
  <summary>Details</summary>
Motivation: 个性化财务建议需要考虑用户目标、约束、风险承受能力和司法管辖区。现有LLM工作主要关注投资者支持系统，而代理管道维护成本高且财务回报低于预期(不到25%)。

Method: 开发了一个可复现框架，整合相关金融背景和行为金融学研究来构建监督数据，创建了19k样本的推理数据集，并对Qwen-3-8B模型进行了全面微调。

Result: 通过保留测试集和盲法LLM评审研究显示，8B模型在事实准确性、流畅性和个性化指标上与更大的基线模型(14-32B参数)性能相当，同时成本比大型对应模型低80%。

Conclusion: 通过精心数据策划和行为整合，较小的8B参数模型可以实现与显著更大模型相当的性能，同时大幅降低成本，为高效个性化财务建议系统提供了可行方案。

Abstract: Personalized financial advice requires consideration of user goals,
constraints, risk tolerance, and jurisdiction. Prior LLM work has focused on
support systems for investors and financial planners. Simultaneously, numerous
recent studies examine broader personal finance tasks, including budgeting,
debt management, retirement, and estate planning, through agentic pipelines
that incur high maintenance costs, yielding less than 25% of their expected
financial returns. In this study, we introduce a novel and reproducible
framework that integrates relevant financial context with behavioral finance
studies to construct supervision data for end-to-end advisors. Using this
framework, we create a 19k sample reasoning dataset and conduct a comprehensive
fine-tuning of the Qwen-3-8B model on the dataset. Through a held-out test
split and a blind LLM-jury study, we demonstrate that through careful data
curation and behavioral integration, our 8B model achieves performance
comparable to significantly larger baselines (14-32B parameters) across factual
accuracy, fluency, and personalization metrics while incurring 80% lower costs
than the larger counterparts.

</details>


### [38] [Framing Migration: A Computational Analysis of UK Parliamentary Discourse](https://arxiv.org/abs/2509.14197)
*Vahid Ghafouri,Robert McNeil,Teodor Yankov,Madeleine Sumption,Luc Rocher,Scott A. Hale,Adam Mahdi*

Main category: cs.CL

TL;DR: 使用大语言模型分析英美议会75年移民话语，发现美国日益极化而英国党派态度相对一致，但保守党与工党意识形态差距在2025年达到最负面水平，且话语转向安全化叙事


<details>
  <summary>Details</summary>
Motivation: 通过大规模计算分析比较英美议会移民话语，探索LLM在政治历史语境中进行细粒度话语分析的可行性

Method: 使用开源大语言模型标注移民立场态度，结合半自动化框架提取细粒度叙事框架，追踪跨时间和政党的移民话语趋势

Result: 美国话语日益极化，英国各党派态度相对一致但保守党与工党意识形态差距在2025年达到最负面；英国话语转向边境管控和非法移民等安全化叙事，社会融合等长期整合框架减少；移民讨论从国内法转向国际法和人权

Conclusion: 大语言模型能够支持政治历史语境中可扩展的细粒度话语分析，揭示了移民话语的演变趋势和国别差异

Abstract: We present a large-scale computational analysis of migration-related
discourse in UK parliamentary debates spanning over 75 years and compare it
with US congressional discourse. Using open-weight LLMs, we annotate each
statement with high-level stances toward migrants and track the net tone toward
migrants across time and political parties. For the UK, we extend this with a
semi-automated framework for extracting fine-grained narrative frames to
capture nuances of migration discourse. Our findings show that, while US
discourse has grown increasingly polarised, UK parliamentary attitudes remain
relatively aligned across parties, with a persistent ideological gap between
Labour and the Conservatives, reaching its most negative level in 2025. The
analysis of narrative frames in the UK parliamentary statements reveals a shift
toward securitised narratives such as border control and illegal immigration,
while longer-term integration-oriented frames such as social integration have
declined. Moreover, discussions of national law about immigration have been
replaced over time by international law and human rights, revealing nuances in
discourse trends. Taken together broadly, our findings demonstrate how LLMs can
support scalable, fine-grained discourse analysis in political and historical
contexts.

</details>


### [39] [Apertus: Democratizing Open and Compliant LLMs for Global Language Environments](https://arxiv.org/abs/2509.14233)
*Alejandro Hernández-Cano,Alexander Hägele,Allen Hao Huang,Angelika Romanou,Antoni-Joan Solergibert,Barna Pasztor,Bettina Messmer,Dhia Garbaya,Eduard Frank Ďurech,Ido Hakimi,Juan García Giraldo,Mete Ismayilzada,Negar Foroutan,Skander Moalla,Tiancheng Chen,Vinko Sabolčec,Yixuan Xu,Michael Aerni,Badr AlKhamissi,Ines Altemir Marinas,Mohammad Hossein Amani,Matin Ansaripour,Ilia Badanin,Harold Benoit,Emanuela Boros,Nicholas Browning,Fabian Bösch,Maximilian Böther,Niklas Canova,Camille Challier,Clement Charmillot,Jonathan Coles,Jan Deriu,Arnout Devos,Lukas Drescher,Daniil Dzenhaliou,Maud Ehrmann,Dongyang Fan,Simin Fan,Silin Gao,Miguel Gila,María Grandury,Diba Hashemi,Alexander Hoyle,Jiaming Jiang,Mark Klein,Andrei Kucharavy,Anastasiia Kucherenko,Frederike Lübeck,Roman Machacek,Theofilos Manitaras,Andreas Marfurt,Kyle Matoba,Simon Matrenok,Henrique Mendoncça,Fawzi Roberto Mohamed,Syrielle Montariol,Luca Mouchel,Sven Najem-Meyer,Jingwei Ni,Gennaro Oliva,Matteo Pagliardini,Elia Palme,Andrei Panferov,Léo Paoletti,Marco Passerini,Ivan Pavlov,Auguste Poiroux,Kaustubh Ponkshe,Nathan Ranchin,Javi Rando,Mathieu Sauser,Jakhongir Saydaliev,Muhammad Ali Sayfiddinov,Marian Schneider,Stefano Schuppli,Marco Scialanga,Andrei Semenov,Kumar Shridhar,Raghav Singhal,Anna Sotnikova,Alexander Sternfeld,Ayush Kumar Tarun,Paul Teiletche,Jannis Vamvas,Xiaozhe Yao,Hao Zhao Alexander Ilic,Ana Klimovic,Andreas Krause,Caglar Gulcehre,David Rosenthal,Elliott Ash,Florian Tramèr,Joost VandeVondele,Livio Veraldi,Martin Rajman,Thomas Schulthess,Torsten Hoefler,Antoine Bosselut,Martin Jaggi,Imanol Schlag*

Main category: cs.CL

TL;DR: Apertus是一个完全开源的LLM套件，专注于数据合规性和多语言覆盖，使用开放数据训练并采用Goldfish目标减少记忆风险，在8B和70B规模上达到先进的多语言性能。


<details>
  <summary>Details</summary>
Motivation: 解决当前开源模型生态系统中数据合规性不足和多语言代表性不够的问题，避免使用未经许可的数据并尊重内容所有者权利。

Method: 使用完全开放可用数据进行预训练，遵循robots.txt排除规则，过滤非许可、有毒和个人身份信息内容，采用Goldfish目标抑制数据记忆，覆盖1800多种语言的15T tokens，40%为非英语内容。

Result: Apertus模型在多语言基准测试中达到或超越同类开源权重模型的最先进结果，同时保持了良好的下游任务性能。

Conclusion: Apertus提供了一个完全透明、合规的开源LLM解决方案，不仅发布模型权重，还提供完整的数据处理脚本、检查点、评估套件和训练代码，支持审计和扩展。

Abstract: We present Apertus, a fully open suite of large language models (LLMs)
designed to address two systemic shortcomings in today's open model ecosystem:
data compliance and multilingual representation. Unlike many prior models that
release weights without reproducible data pipelines or regard for content-owner
rights, Apertus models are pretrained exclusively on openly available data,
retroactively respecting robots.txt exclusions and filtering for
non-permissive, toxic, and personally identifiable content. To mitigate risks
of memorization, we adopt the Goldfish objective during pretraining, strongly
suppressing verbatim recall of data while retaining downstream task
performance. The Apertus models also expand multilingual coverage, training on
15T tokens from over 1800 languages, with ~40% of pretraining data allocated to
non-English content. Released at 8B and 70B scales, Apertus approaches
state-of-the-art results among fully open models on multilingual benchmarks,
rivalling or surpassing open-weight counterparts. Beyond model weights, we
release all scientific artifacts from our development cycle with a permissive
license, including data preparation scripts, checkpoints, evaluation suites,
and training code, enabling transparent audit and extension.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [40] [MA-DPR: Manifold-aware Distance Metrics for Dense Passage Retrieval](https://arxiv.org/abs/2509.13562)
*Yifan Liu,Qianfeng Wen,Mark Zhao,Jiazhou Liang,Scott Sanner*

Main category: cs.IR

TL;DR: MA-DPR提出了一种基于流形感知的距离度量方法，通过构建最近邻图来建模段落嵌入的内在流形结构，在OOD设置下比欧几里得和余弦距离提升高达26%的检索性能


<details>
  <summary>Details</summary>
Motivation: 传统DPR方法依赖欧几里得或余弦距离在嵌入空间中衡量查询-段落相关性，但当嵌入位于低维非线性流形时（特别是在OOD设置下），这些距离度量无法有效捕捉语义相似性

Method: 使用最近邻图建模段落的内在流形结构，基于查询和段落在图中的最短路径距离来度量相关性，能够利用相邻段落的上下文信息

Result: 在OOD段落检索中比欧几里得和余弦距离提升高达26%，在分布内性能相当，查询推理时间仅轻微增加

Conclusion: 流形感知距离度量使DPR能够有效利用相关相邻段落的上下文，即使在缺乏直接语义重叠的情况下也能有效工作，可广泛应用于各种密集嵌入和检索任务

Abstract: Dense Passage Retrieval (DPR) typically relies on Euclidean or cosine
distance to measure query-passage relevance in embedding space, which is
effective when embeddings lie on a linear manifold. However, our experiments
across DPR benchmarks suggest that embeddings often lie on lower-dimensional,
non-linear manifolds, especially in out-of-distribution (OOD) settings, where
cosine and Euclidean distance fail to capture semantic similarity. To address
this limitation, we propose a manifold-aware distance metric for DPR (MA-DPR)
that models the intrinsic manifold structure of passages using a nearest
neighbor graph and measures query-passage distance based on their shortest path
in this graph. We show that MA-DPR outperforms Euclidean and cosine distances
by up to 26% on OOD passage retrieval with comparable in-distribution
performance across various embedding models while incurring a minimal increase
in query inference time. Empirical evidence suggests that manifold-aware
distance allows DPR to leverage context from related neighboring passages,
making it effective even in the absence of direct semantic overlap. MADPR can
be applied to a wide range of dense embedding and retrieval tasks, offering
potential benefits across a wide spectrum of domains.

</details>


### [41] [Modernizing Facebook Scoped Search: Keyword and Embedding Hybrid Retrieval with LLM Evaluation](https://arxiv.org/abs/2509.13603)
*Yongye Su,Zeya Zhang,Jane Kou,Cheng Ju,Shubhojeet Sarkar,Yamin Wang,Ji Liu,Shengbo Guo*

Main category: cs.IR

TL;DR: Facebook提出混合关键词检索和嵌入检索的社交网络搜索框架，通过LLM评估验证了系统能显著提升搜索相关性和用户参与度


<details>
  <summary>Details</summary>
Motivation: 社交网络搜索需要在其社交背景下检索信息和发现潜在连接，传统关键词搜索在语义理解和多样性方面存在局限

Method: 将语义检索集成到现有关键词搜索管道中，结合关键词检索和嵌入检索(EBR)，并引入基于LLM的离线相关性评估框架

Result: 混合检索系统显著提升了用户参与度和搜索质量，在线指标和LLM评估均验证了效果提升

Conclusion: 这项工作为在大型现实社交平台部署和评估高级检索系统提供了实用见解

Abstract: Beyond general web-scale search, social network search uniquely enables users
to retrieve information and discover potential connections within their social
context. We introduce a framework of modernized Facebook Group Scoped Search by
blending traditional keyword-based retrieval with embedding-based retrieval
(EBR) to improve the search relevance and diversity of search results. Our
system integrates semantic retrieval into the existing keyword search pipeline,
enabling users to discover more contextually relevant group posts. To
rigorously assess the impact of this blended approach, we introduce a novel
evaluation framework that leverages large language models (LLMs) to perform
offline relevance assessments, providing scalable and consistent quality
benchmarks. Our results demonstrate that the blended retrieval system
significantly enhances user engagement and search quality, as validated by both
online metrics and LLM-based evaluation. This work offers practical insights
for deploying and evaluating advanced retrieval systems in large-scale,
real-world social platforms.

</details>


### [42] [Mind the Gap: Aligning Knowledge Bases with User Needs to Enhance Mental Health Retrieval](https://arxiv.org/abs/2509.13626)
*Amanda Chan,James Jiayu Liu,He Kai,Onno P. Kampman*

Main category: cs.IR

TL;DR: 提出基于AI的gap-informed框架，通过分析用户自然语言数据识别知识库中的缺失主题，有针对性地进行语料库扩充，相比随机扩充能显著减少所需内容量并达到相近性能


<details>
  <summary>Details</summary>
Motivation: 解决心理健康信息检索系统中内容覆盖不足的问题，当用户使用非正式或情境化语言表达时，现有系统检索性能较差，需要更有效的语料库扩充方法

Method: 开发gap-informed语料库增强框架，通过叠加论坛帖子等自然用户数据识别代表性不足的主题，基于覆盖度和有用性优先进行定向扩充，并在四个RAG流水线中比较定向与非定向扩充的效果

Result: 定向扩充仅需适度扩展即可达到接近最优性能（查询转换42%，重排和分层74%，基线318%），而非定向扩充需要大幅更多扩展（232%-763%）才能达到可比性能

Conclusion: 战略性定向语料库增长能减少内容创建需求同时保持高质量检索，为构建可信健康信息库和支持高风险领域生成式AI应用提供可扩展方法

Abstract: Access to reliable mental health information is vital for early help-seeking,
yet expanding knowledge bases is resource-intensive and often misaligned with
user needs. This results in poor performance of retrieval systems when
presented concerns are not covered or expressed in informal or contextualized
language. We present an AI-based gap-informed framework for corpus augmentation
that authentically identifies underrepresented topics (gaps) by overlaying
naturalistic user data such as forum posts in order to prioritize expansions
based on coverage and usefulness. In a case study, we compare Directed
(gap-informed augmentations) with Non-Directed augmentation (random additions),
evaluating the relevance and usefulness of retrieved information across four
retrieval-augmented generation (RAG) pipelines. Directed augmentation achieved
near-optimal performance with modest expansions--requiring only a 42% increase
for Query Transformation, 74% for Reranking and Hierarchical, and 318% for
Baseline--to reach ~95% of the performance of an exhaustive reference corpus.
In contrast, Non-Directed augmentation required substantially larger and thus
practically infeasible expansions to achieve comparable performance (232%,
318%, 403%, and 763%, respectively). These results show that strategically
targeted corpus growth can reduce content creation demands while sustaining
high retrieval and provision quality, offering a scalable approach for building
trusted health information repositories and supporting generative AI
applications in high-stakes domains.

</details>


### [43] [Enhancing Time Awareness in Generative Recommendation](https://arxiv.org/abs/2509.13957)
*Sunkyung Lee,Seongmin Park,Jonghyo Kim,Mincheol Yoon,Jongwuk Lee*

Main category: cs.IR

TL;DR: GRUT是一个生成式推荐模型，通过时间感知提示和趋势感知推理来捕捉用户偏好随时间的变化，在多个基准数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐方法主要关注物品序列顺序，但忽略了物品间的时间动态性，而时间动态可以反映用户偏好的演变。

Method: 提出时间感知提示（用户级时间上下文和物品级转移上下文）来建模个性化时间模式和转移模式，并设计训练免费的趋势感知推理方法来增强排序。

Result: 在四个基准数据集上，GRUT在Recall@5和NDCG@5指标上分别取得了最高15.4%和14.3%的性能提升。

Conclusion: GRUT通过有效利用时间信号成功捕捉了隐藏的用户偏好，证明了时间动态性在生成式推荐中的重要性。

Abstract: Generative recommendation has emerged as a promising paradigm that formulates
the recommendations into a text-to-text generation task, harnessing the vast
knowledge of large language models. However, existing studies focus on
considering the sequential order of items and neglect to handle the temporal
dynamics across items, which can imply evolving user preferences. To address
this limitation, we propose a novel model, Generative Recommender Using Time
awareness (GRUT), effectively capturing hidden user preferences via various
temporal signals. We first introduce Time-aware Prompting, consisting of two
key contexts. The user-level temporal context models personalized temporal
patterns across timestamps and time intervals, while the item-level transition
context provides transition patterns across users. We also devise Trend-aware
Inference, a training-free method that enhances rankings by incorporating trend
information about items with generation likelihood. Extensive experiments
demonstrate that GRUT outperforms state-of-the-art models, with gains of up to
15.4% and 14.3% in Recall@5 and NDCG@5 across four benchmark datasets. The
source code is available at https://github.com/skleee/GRUT.

</details>


### [44] [GEM-Bench: A Benchmark for Ad-Injected Response Generation within Generative Engine Marketing](https://arxiv.org/abs/2509.14221)
*Silan Hu,Shiqi Zhang,Yimin Shi,Xiaokui Xiao*

Main category: cs.IR

TL;DR: GEM-Bench是首个针对生成引擎营销中广告注入响应生成的综合基准测试，包含三个数据集、多维度评估指标和基线方法，发现现有方法在用户满意度和参与度之间存在权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试不专门针对生成引擎营销中的广告注入响应生成，限制了该领域的研究发展，需要专门的评估框架。

Method: 提出GEM-Bench基准，包括三个精心策划的数据集（涵盖聊天机器人和搜索场景）、捕获用户满意度和参与度的多维度指标本体，以及在可扩展多代理框架中实现的基线解决方案。

Result: 初步结果显示，简单的基于提示的方法能获得合理的参与度（如点击率），但往往降低用户满意度；而基于预生成无广告响应插入广告的方法能缓解此问题但带来额外开销。

Conclusion: 需要未来研究设计更有效和高效的解决方案来生成GEM中的广告注入响应，以平衡用户满意度和商业目标。

Abstract: Generative Engine Marketing (GEM) is an emerging ecosystem for monetizing
generative engines, such as LLM-based chatbots, by seamlessly integrating
relevant advertisements into their responses. At the core of GEM lies the
generation and evaluation of ad-injected responses. However, existing
benchmarks are not specifically designed for this purpose, which limits future
research. To address this gap, we propose GEM-Bench, the first comprehensive
benchmark for ad-injected response generation in GEM. GEM-Bench includes three
curated datasets covering both chatbot and search scenarios, a metric ontology
that captures multiple dimensions of user satisfaction and engagement, and
several baseline solutions implemented within an extensible multi-agent
framework. Our preliminary results indicate that, while simple prompt-based
methods achieve reasonable engagement such as click-through rate, they often
reduce user satisfaction. In contrast, approaches that insert ads based on
pre-generated ad-free responses help mitigate this issue but introduce
additional overhead. These findings highlight the need for future research on
designing more effective and efficient solutions for generating ad-injected
responses in GEM.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [45] [Unified Spatiotemopral Physics-Informed Learning (USPIL): A Framework for Modeling Complex Predator-Prey Dynamics](https://arxiv.org/abs/2509.13425)
*Julian Evan Chrisnanto,Yulison Herry Chrisnanto,Ferry Faizal*

Main category: cs.LG

TL;DR: USPIL框架通过物理信息神经网络和守恒定律统一建模捕食者-猎物系统的时空动力学，在保持物理一致性的同时实现高效计算和机制解释。


<details>
  <summary>Details</summary>
Motivation: 生态系统的复杂多尺度动力学挑战传统建模方法，需要新方法来捕捉时空振荡和涌现模式，同时遵守守恒原理。

Method: 提出统一时空物理信息学习(USPIL)框架，结合物理信息神经网络(PINNs)和守恒定律，使用自动微分实施物理约束和自适应损失加权来平衡数据保真度与物理一致性。

Result: 在Lotka-Volterra系统中，1D时间动力学达到98.9%相关性(loss: 0.0219)，2D系统捕捉到复杂螺旋波(pattern correlation: 0.94)，守恒定律遵守度在0.5%以内，推理速度比数值求解器快10-50倍。

Conclusion: USPIL为多尺度生态建模开辟了新途径，是生态预测、保护规划和理解生态系统恢复力的变革性工具，确立了物理信息深度学习作为强大且科学严谨的范式。

Abstract: Ecological systems exhibit complex multi-scale dynamics that challenge
traditional modeling. New methods must capture temporal oscillations and
emergent spatiotemporal patterns while adhering to conservation principles. We
present the Unified Spatiotemporal Physics-Informed Learning (USPIL) framework,
a deep learning architecture integrating physics-informed neural networks
(PINNs) and conservation laws to model predator-prey dynamics across
dimensional scales. The framework provides a unified solution for both ordinary
(ODE) and partial (PDE) differential equation systems, describing temporal
cycles and reaction-diffusion patterns within a single neural network
architecture. Our methodology uses automatic differentiation to enforce physics
constraints and adaptive loss weighting to balance data fidelity with physical
consistency. Applied to the Lotka-Volterra system, USPIL achieves 98.9%
correlation for 1D temporal dynamics (loss: 0.0219, MAE: 0.0184) and captures
complex spiral waves in 2D systems (loss: 4.7656, pattern correlation: 0.94).
Validation confirms conservation law adherence within 0.5% and shows a 10-50x
computational speedup for inference compared to numerical solvers. USPIL also
enables mechanistic understanding through interpretable physics constraints,
facilitating parameter discovery and sensitivity analysis not possible with
purely data-driven methods. Its ability to transition between dimensional
formulations opens new avenues for multi-scale ecological modeling. These
capabilities make USPIL a transformative tool for ecological forecasting,
conservation planning, and understanding ecosystem resilience, establishing
physics-informed deep learning as a powerful and scientifically rigorous
paradigm.

</details>


### [46] [An Analysis of Optimizer Choice on Energy Efficiency and Performance in Neural Network Training](https://arxiv.org/abs/2509.13516)
*Tom Almog*

Main category: cs.LG

TL;DR: 本文通过360次实验研究了不同优化器对神经网络训练能耗的影响，发现AdamW和NAdam在能效方面表现最佳，而SGD在复杂数据集上性能更好但碳排放更高。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型变得越来越复杂和计算密集，理解训练决策对环境的影响对于可持续AI发展变得至关重要。

Method: 在三个基准数据集(MNIST、CIFAR-10、CIFAR-100)上使用8种流行优化器进行360次受控实验，使用CodeCarbon在Apple M1 Pro硬件上精确追踪能耗。

Result: 发现训练速度、准确性和环境影响之间存在显著权衡，这些权衡因数据集和模型复杂性而异。AdamW和NAdam表现出一致的效率，而SGD在复杂数据集上表现出色但排放更高。

Conclusion: 研究结果为从业者在机器学习工作流中平衡性能和可持续性提供了可行的见解。

Abstract: As machine learning models grow increasingly complex and computationally
demanding, understanding the environmental impact of training decisions becomes
critical for sustainable AI development. This paper presents a comprehensive
empirical study investigating the relationship between optimizer choice and
energy efficiency in neural network training. We conducted 360 controlled
experiments across three benchmark datasets (MNIST, CIFAR-10, CIFAR-100) using
eight popular optimizers (SGD, Adam, AdamW, RMSprop, Adagrad, Adadelta, Adamax,
NAdam) with 15 random seeds each. Using CodeCarbon for precise energy tracking
on Apple M1 Pro hardware, we measured training duration, peak memory usage,
carbon dioxide emissions, and final model performance. Our findings reveal
substantial trade-offs between training speed, accuracy, and environmental
impact that vary across datasets and model complexity. We identify AdamW and
NAdam as consistently efficient choices, while SGD demonstrates superior
performance on complex datasets despite higher emissions. These results provide
actionable insights for practitioners seeking to balance performance and
sustainability in machine learning workflows.

</details>


### [47] [Learning Nonlinear Responses in PET Bottle Buckling with a Hybrid DeepONet-Transolver Framework](https://arxiv.org/abs/2509.13520)
*Varun Kumar,Jing Bi,Cyril Ngo Ngoc,Victor Oancea,George Em Karniadakis*

Main category: cs.LG

TL;DR: 提出混合DeepONet-Transolver框架，用于解决PET瓶屈曲分析问题，能够同时预测节点位移场和时间相关的反作用力，在几何参数化设计上表现出良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络方法在处理非参数化几何域变化的PDE问题时泛化能力有限，而传统的有限元分析计算成本高昂，需要开发高效的计算替代模型。

Method: 采用混合DeepONet-Transolver框架，结合深度算子网络和变换求解器，对两个参数化瓶体几何家族（2参数和4参数）进行非线性有限元分析训练，共508个独特设计。

Result: 在4参数瓶体家族上，位移场的平均相对L2误差为2.5-13%，时间相关反作用力误差约2.4%，点位移绝对误差在10^-4-10^-3量级，成功捕捉了屈曲等关键物理现象。

Conclusion: 该框架展示了作为可扩展计算替代模型的潜力，特别适用于计算力学中的多任务预测和需要快速设计评估的应用场景。

Abstract: Neural surrogates and operator networks for solving partial differential
equation (PDE) problems have attracted significant research interest in recent
years. However, most existing approaches are limited in their ability to
generalize solutions across varying non-parametric geometric domains. In this
work, we address this challenge in the context of Polyethylene Terephthalate
(PET) bottle buckling analysis, a representative packaging design problem
conventionally solved using computationally expensive finite element analysis
(FEA). We introduce a hybrid DeepONet-Transolver framework that simultaneously
predicts nodal displacement fields and the time evolution of reaction forces
during top load compression. Our methodology is evaluated on two families of
bottle geometries parameterized by two and four design variables. Training data
is generated using nonlinear FEA simulations in Abaqus for 254 unique designs
per family. The proposed framework achieves mean relative $L^{2}$ errors of
2.5-13% for displacement fields and approximately 2.4% for time-dependent
reaction forces for the four-parameter bottle family. Point-wise error analyses
further show absolute displacement errors on the order of $10^{-4}$-$10^{-3}$,
with the largest discrepancies confined to localized geometric regions.
Importantly, the model accurately captures key physical phenomena, such as
buckling behavior, across diverse bottle geometries. These results highlight
the potential of our framework as a scalable and computationally efficient
surrogate, particularly for multi-task predictions in computational mechanics
and applications requiring rapid design evaluation.

</details>


### [48] [AERIS: Argonne Earth Systems Model for Reliable and Skillful Predictions](https://arxiv.org/abs/2509.13523)
*Väinö Hatanpää,Eugene Ku,Jason Stock,Murali Emani,Sam Foreman,Chunyong Jung,Sandeep Madireddy,Tung Nguyen,Varuni Sastry,Ray A. O. Sinurat,Sam Wheeler,Huihuo Zheng,Troy Arcomano,Venkatram Vishwanath,Rao Kotamarthi*

Main category: cs.LG

TL;DR: AERIS是一个10-800亿参数的像素级Swin扩散变换器，用于改进天气预测的集合校准和稳定性，在Aurora超级计算机上实现了10.21 ExaFLOPS的性能，超越了IFS ENS系统。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散方法在高分辨率下难以稳定扩展的问题，利用生成式机器学习更好地理解复杂地球系统动力学。

Method: 采用像素级Swin扩散变换器架构，结合SWiPe技术（窗口并行与序列/流水线并行组合），在不增加通信成本或全局批大小的情况下分片基于窗口的变换器。

Result: 在Aurora（10,080节点）上实现10.21 ExaFLOPS混合精度性能，峰值达11.21 ExaFLOPS，弱缩放效率95.5%，强缩放效率81.6%。在0.25° ERA5数据集上表现优于IFS ENS，季节尺度稳定性达90天。

Conclusion: 十亿参数扩散模型在天气和气候预测方面具有巨大潜力，AERIS展示了在高分辨率下稳定扩展的能力。

Abstract: Generative machine learning offers new opportunities to better understand
complex Earth system dynamics. Recent diffusion-based methods address spectral
biases and improve ensemble calibration in weather forecasting compared to
deterministic methods, yet have so far proven difficult to scale stably at high
resolutions. We introduce AERIS, a 1.3 to 80B parameter pixel-level Swin
diffusion transformer to address this gap, and SWiPe, a generalizable technique
that composes window parallelism with sequence and pipeline parallelism to
shard window-based transformers without added communication cost or increased
global batch size. On Aurora (10,080 nodes), AERIS sustains 10.21 ExaFLOPS
(mixed precision) and a peak performance of 11.21 ExaFLOPS with $1 \times 1$
patch size on the 0.25{\deg} ERA5 dataset, achieving 95.5% weak scaling
efficiency, and 81.6% strong scaling efficiency. AERIS outperforms the IFS ENS
and remains stable on seasonal scales to 90 days, highlighting the potential of
billion-parameter diffusion models for weather and climate prediction.

</details>


### [49] [Meta-Learning Linear Models for Molecular Property Prediction](https://arxiv.org/abs/2509.13527)
*Yulia Pimonova,Michael G. Taylor,Alice Allen,Ping Yang,Nicholas Lubbers*

Main category: cs.LG

TL;DR: LAMeL是一种线性元学习算法，在保持可解释性的同时提高多个化学性质预测的准确性，性能比标准岭回归提升1.1-25倍


<details>
  <summary>Details</summary>
Motivation: 化学研究中高质量数据集有限，机器学习方法对数据需求增加，需要平衡预测准确性和人类可理解性的可解释AI方法

Method: 采用元学习框架识别相关任务间的共享模型参数，学习共同的功能流形作为新任务的更明智起点，即使任务不共享数据

Result: 在不同数据集上性能提升1.1-25倍，始终优于或匹配传统线性方法，在准确性和可解释性都重要的化学性质预测中表现可靠

Conclusion: LAMeL是化学性质预测中可靠的工具，成功弥合了预测准确性和人类可理解性之间的差距

Abstract: Chemists in search of structure-property relationships face great challenges
due to limited high quality, concordant datasets. Machine learning (ML) has
significantly advanced predictive capabilities in chemical sciences, but these
modern data-driven approaches have increased the demand for data. In response
to the growing demand for explainable AI (XAI) and to bridge the gap between
predictive accuracy and human comprehensibility, we introduce LAMeL - a Linear
Algorithm for Meta-Learning that preserves interpretability while improving the
prediction accuracy across multiple properties. While most approaches treat
each chemical prediction task in isolation, LAMeL leverages a meta-learning
framework to identify shared model parameters across related tasks, even if
those tasks do not share data, allowing it to learn a common functional
manifold that serves as a more informed starting point for new unseen tasks.
Our method delivers performance improvements ranging from 1.1- to 25-fold over
standard ridge regression, depending on the domain of the dataset. While the
degree of performance enhancement varies across tasks, LAMeL consistently
outperforms or matches traditional linear methods, making it a reliable tool
for chemical property prediction where both accuracy and interpretability are
critical.

</details>


### [50] [Is GPT-4o mini Blinded by its Own Safety Filters? Exposing the Multimodal-to-Unimodal Bottleneck in Hate Speech Detection](https://arxiv.org/abs/2509.13608)
*Niruthiha Selvanayagam,Ted Kurti*

Main category: cs.LG

TL;DR: 研究发现GPT-4o mini存在"单模态瓶颈"缺陷，其先进的多模态推理能力被上下文无关的安全过滤器系统性地阻断，导致在仇恨言论检测任务中出现可预测的误报。


<details>
  <summary>Details</summary>
Motivation: 随着大型多模态模型(LMMs)成为日常生活的重要组成部分，理解其安全架构对于AI对齐至关重要。本文旨在系统分析OpenAI的GPT-4o mini在困难的多模态仇恨言论检测任务中的表现。

Method: 使用Hateful Memes Challenge数据集，对500个样本进行多阶段调查，探究模型的推理和失败模式。对144个内容政策拒绝案例进行定量验证。

Result: 实验识别出"单模态瓶颈"架构缺陷，安全过滤器在视觉(50%)和文本(50%)内容上均等触发覆盖。安全系统脆弱，不仅阻止高风险图像，还阻止良性的常见meme格式。

Conclusion: 这些发现揭示了最先进LMMs中能力与安全之间的根本张力，突显了需要更集成、上下文感知的对齐策略，以确保AI系统能够安全有效地部署。

Abstract: As Large Multimodal Models (LMMs) become integral to daily digital life,
understanding their safety architectures is a critical problem for AI
Alignment. This paper presents a systematic analysis of OpenAI's GPT-4o mini, a
globally deployed model, on the difficult task of multimodal hate speech
detection. Using the Hateful Memes Challenge dataset, we conduct a multi-phase
investigation on 500 samples to probe the model's reasoning and failure modes.
Our central finding is the experimental identification of a "Unimodal
Bottleneck," an architectural flaw where the model's advanced multimodal
reasoning is systematically preempted by context-blind safety filters. A
quantitative validation of 144 content policy refusals reveals that these
overrides are triggered in equal measure by unimodal visual 50% and textual 50%
content. We further demonstrate that this safety system is brittle, blocking
not only high-risk imagery but also benign, common meme formats, leading to
predictable false positives. These findings expose a fundamental tension
between capability and safety in state-of-the-art LMMs, highlighting the need
for more integrated, context-aware alignment strategies to ensure AI systems
can be deployed both safely and effectively.

</details>


### [51] [Sequential Data Augmentation for Generative Recommendation](https://arxiv.org/abs/2509.13648)
*Geon Lee,Bhuvesh Kumar,Clark Mingxuan Ju,Tong Zhao,Kijung Shin,Neil Shah,Liam Collins*

Main category: cs.LG

TL;DR: 本文提出了GenPAS框架，系统性地分析和优化生成式推荐中的数据增强策略，通过三个可控步骤统一现有方法并显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐系统中数据增强策略被简化处理且缺乏系统性研究，不同增强策略会导致显著的性能差异，需要建立原则性框架来指导训练数据构建。

Method: 提出GenPAS框架，将数据增强建模为包含三个偏差控制步骤的随机采样过程：序列采样、目标采样和输入采样，统一现有策略并实现训练分布的灵活控制。

Result: 在基准和工业数据集上的实验表明，GenPAS在准确性、数据效率和参数效率方面均优于现有策略。

Conclusion: GenPAS为生成式推荐提供了原则性的训练数据构建指导，通过系统化的数据增强框架显著提升了模型性能。

Abstract: Generative recommendation plays a crucial role in personalized systems,
predicting users' future interactions from their historical behavior sequences.
A critical yet underexplored factor in training these models is data
augmentation, the process of constructing training data from user interaction
histories. By shaping the training distribution, data augmentation directly and
often substantially affects model generalization and performance. Nevertheless,
in much of the existing work, this process is simplified, applied
inconsistently, or treated as a minor design choice, without a systematic and
principled understanding of its effects.
  Motivated by our empirical finding that different augmentation strategies can
yield large performance disparities, we conduct an in-depth analysis of how
they reshape training distributions and influence alignment with future targets
and generalization to unseen inputs. To systematize this design space, we
propose GenPAS, a generalized and principled framework that models augmentation
as a stochastic sampling process over input-target pairs with three
bias-controlled steps: sequence sampling, target sampling, and input sampling.
This formulation unifies widely used strategies as special cases and enables
flexible control of the resulting training distribution. Our extensive
experiments on benchmark and industrial datasets demonstrate that GenPAS yields
superior accuracy, data efficiency, and parameter efficiency compared to
existing strategies, providing practical guidance for principled training data
construction in generative recommendation.

</details>


### [52] [Unsupervised Anomaly Detection in ALS EPICS Event Logs](https://arxiv.org/abs/2509.13621)
*Antonin Sulc,Thorsten Hellert,Steven Hunt*

Main category: cs.LG

TL;DR: 基于自然语言处理技术的自动化故障分析框架，通过语义嵌入和序列感知神经网络实时检测EPICS控制系统事件日志中的异常


<details>
  <summary>Details</summary>
Motivation: 为先进光源(ALS)控制系统开发自动化故障分析方法，帮助操作人员快速识别复杂系统故障前的关键事件序列

Method: 将事件日志条目视为自然语言，使用语义嵌入技术转换为上下文向量表示，然后通过序列感知神经网络对正常操作数据进行训练，为每个事件分配实时异常分数

Result: 该方法能够标记与基线行为的偏差，有效识别系统故障前的异常事件序列

Conclusion: 提出的框架能够实现实时异常检测，显著提高故障诊断效率，为大型科学设施的控制系统提供有效的故障分析解决方案

Abstract: This paper introduces an automated fault analysis framework for the Advanced
Light Source (ALS) that processes real-time event logs from its EPICS control
system. By treating log entries as natural language, we transform them into
contextual vector representations using semantic embedding techniques. A
sequence-aware neural network, trained on normal operational data, assigns a
real-time anomaly score to each event. This method flags deviations from
baseline behavior, enabling operators to rapidly identify the critical event
sequences that precede complex system failures.

</details>


### [53] [Privacy-Aware In-Context Learning for Large Language Models](https://arxiv.org/abs/2509.13625)
*Bishnu Bhusal,Manoj Acharya,Ramneet Kaur,Colin Samplawski,Anirban Roy,Adam D. Cobb,Rohit Chadha,Susmit Jha*

Main category: cs.LG

TL;DR: 提出了一种基于差分隐私的文本生成框架，通过聚合token级输出分布来生成高质量合成文本，在保证隐私的同时保持高实用性


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在隐私泄露风险，攻击者可能从提示中提取敏感信息，需要开发既能保护隐私又能保持文本质量的生成方法

Method: 利用差分隐私框架，对私有记录进行推理并聚合每个token的输出分布，同时提出混合操作结合私有和公共推理来提升实用性

Result: 实证评估显示该方法在上下文学习任务上优于现有最先进方法，能够生成长且连贯的合成文本

Conclusion: 该方法为隐私保护文本生成提供了有前景的方向，在保持高实用性的同时提供强大的隐私保障

Abstract: Large language models (LLMs) have significantly transformed natural language
understanding and generation, but they raise privacy concerns due to potential
exposure of sensitive information. Studies have highlighted the risk of
information leakage, where adversaries can extract sensitive information
embedded in the prompts. In this work, we introduce a novel private prediction
framework for generating high-quality synthetic text with strong privacy
guarantees. Our approach leverages the Differential Privacy (DP) framework to
ensure worst-case theoretical bounds on information leakage without requiring
any fine-tuning of the underlying models.The proposed method performs inference
on private records and aggregates the resulting per-token output distributions.
This enables the generation of longer and coherent synthetic text while
maintaining privacy guarantees. Additionally, we propose a simple blending
operation that combines private and public inference to further enhance
utility. Empirical evaluations demonstrate that our approach outperforms
previous state-of-the-art methods on in-context-learning (ICL) tasks, making it
a promising direction for privacy-preserving text generation while maintaining
high utility.

</details>


### [54] [DeepLogit: A sequentially constrained explainable deep learning modeling approach for transport policy analysis](https://arxiv.org/abs/2509.13633)
*Jeremy Oon,Rakhi Manohar Mepparambath,Ling Feng*

Main category: cs.LG

TL;DR: 提出DeepLogit模型，通过序列约束方法结合深度学习与离散选择模型，在保持参数可解释性的同时提升预测精度


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在规划和政策分析领域的应用受限于其黑盒特性，需要开发既能保持可解释性又能提高准确性的方法

Method: 两阶段方法：首先估计仅含线性项的CNN模型（等价于线性参数多项logit模型），然后约束需要可解释的参数值，引入高阶项或Transformer等先进架构

Result: 在真实世界新加坡公交智能卡数据的公交路线选择案例中，方法在保持参数可解释性的同时显著提高了模型精度

Conclusion: 展示了理论驱动的离散选择模型与数据驱动的AI模型相结合的统一方法潜力，可在保持规划政策应用适用性的同时实现更准确建模

Abstract: Despite the significant progress of deep learning models in multitude of
applications, their adaption in planning and policy related areas remains
challenging due to the black-box nature of these models. In this work, we
develop a set of DeepLogit models that follow a novel sequentially constrained
approach in estimating deep learning models for transport policy analysis. In
the first step of the proposed approach, we estimate a convolutional neural
network (CNN) model with only linear terms, which is equivalent of a
linear-in-parameter multinomial logit model. We then estimate other deep
learning models by constraining the parameters that need interpretability at
the values obtained in the linear-in-parameter CNN model and including higher
order terms or by introducing advanced deep learning architectures like
Transformers. Our approach can retain the interpretability of the selected
parameters, yet provides significantly improved model accuracy than the
discrete choice model. We demonstrate our approach on a transit route choice
example using real-world transit smart card data from Singapore. This study
shows the potential for a unifying approach, where theory-based discrete choice
model (DCM) and data-driven AI models can leverage each other's strengths in
interpretability and predictive power. With the availability of larger datasets
and more complex constructions, such approach can lead to more accurate models
using discrete choice models while maintaining its applicability in planning
and policy-related areas. Our code is available on
https://github.com/jeremyoon/route-choice/ .

</details>


### [55] [Secure UAV-assisted Federated Learning: A Digital Twin-Driven Approach with Zero-Knowledge Proofs](https://arxiv.org/abs/2509.13634)
*Md Bokhtiar Al Zami,Md Raihan Uddin,Dinh C. Nguyen*

Main category: cs.LG

TL;DR: 本文提出了一种结合数字孪生和零知识联邦学习的新型框架，用于解决无人机辅助联邦学习系统中的能耗、通信效率和安全性问题，通过动态资源分配策略显著降低能耗29.6%。


<details>
  <summary>Details</summary>
Motivation: 解决无人机辅助联邦学习系统中存在的能耗过高、通信效率低下和安全漏洞等关键问题，确保系统的可靠运行。

Method: 集成数字孪生技术实现实时系统监控和预测性维护，采用零知识证明技术增强安全性，引入基于块坐标下降和凸优化技术的动态资源分配策略来优化无人机飞行路径、传输功率和处理速率。

Result: 相比传统联邦学习方法，系统能耗降低达29.6%，仿真结果显示学习性能、安全性和可扩展性均有显著提升。

Conclusion: 该框架为下一代无人机智能网络提供了一个有前景的解决方案，在能效、安全和性能方面表现出色。

Abstract: Federated learning (FL) has gained popularity as a privacy-preserving method
of training machine learning models on decentralized networks. However to
ensure reliable operation of UAV-assisted FL systems, issues like as excessive
energy consumption, communication inefficiencies, and security vulnerabilities
must be solved. This paper proposes an innovative framework that integrates
Digital Twin (DT) technology and Zero-Knowledge Federated Learning (zkFed) to
tackle these challenges. UAVs act as mobile base stations, allowing scattered
devices to train FL models locally and upload model updates for aggregation. By
incorporating DT technology, our approach enables real-time system monitoring
and predictive maintenance, improving UAV network efficiency. Additionally,
Zero-Knowledge Proofs (ZKPs) strengthen security by allowing model verification
without exposing sensitive data. To optimize energy efficiency and resource
management, we introduce a dynamic allocation strategy that adjusts UAV flight
paths, transmission power, and processing rates based on network conditions.
Using block coordinate descent and convex optimization techniques, our method
significantly reduces system energy consumption by up to 29.6% compared to
conventional FL approaches. Simulation results demonstrate improved learning
performance, security, and scalability, positioning this framework as a
promising solution for next-generation UAV-based intelligent networks.

</details>


### [56] [Multimodal signal fusion for stress detection using deep neural networks: a novel approach for converting 1D signals to unified 2D images](https://arxiv.org/abs/2509.13636)
*Yasin Hasanpoor,Bahram Tarvirdizadeh,Khalil Alipour,Mohammad Ghamari*

Main category: cs.LG

TL;DR: 将多模态生理信号（PPG、GSR、ACC）转换为2D图像矩阵，使用CNN进行压力检测的新方法，通过信号融合和图像化处理提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法单独处理多模态生理信号或依赖固定编码，无法有效捕捉信号间的时空依赖关系，需要一种能够更好融合多模态信息并提升模型泛化能力的方法。

Method: 将PPG、GSR和ACC信号转换为2D图像矩阵，通过系统性的信号重组和多格式组合，构建多阶段训练流程，利用CNN捕捉时空和跨信号依赖关系。

Result: 该方法显著提升了分类性能，不仅提高了压力检测的准确性，还增强了模型的泛化能力和鲁棒性，同时改善了结果的可解释性。

Conclusion: 提出的图像化转换方法为多模态生理信号处理提供了有效解决方案，具有广泛的适用性，为可穿戴技术实现更准确、个性化和实时的健康监测铺平了道路。

Abstract: This study introduces a novel method that transforms multimodal physiological
signalsphotoplethysmography (PPG), galvanic skin response (GSR), and
acceleration (ACC) into 2D image matrices to enhance stress detection using
convolutional neural networks (CNNs). Unlike traditional approaches that
process these signals separately or rely on fixed encodings, our technique
fuses them into structured image representations that enable CNNs to capture
temporal and cross signal dependencies more effectively. This image based
transformation not only improves interpretability but also serves as a robust
form of data augmentation. To further enhance generalization and model
robustness, we systematically reorganize the fused signals into multiple
formats, combining them in a multi stage training pipeline. This approach
significantly boosts classification performance. While demonstrated here in the
context of stress detection, the proposed method is broadly applicable to any
domain involving multimodal physiological signals, paving the way for more
accurate, personalized, and real time health monitoring through wearable
technologies.

</details>


### [57] [LLM-I: LLMs are Naturally Interleaved Multimodal Creators](https://arxiv.org/abs/2509.13642)
*Zirun Guo,Feng Zhang,Kai Jia,Tao Jin*

Main category: cs.LG

TL;DR: LLM-Interleaved是一个将交错图像-文本生成重新定义为工具使用问题的动态框架，通过强化学习训练LLM智能协调多种视觉工具，在多个基准测试中大幅超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前统一模型存在"单一工具"瓶颈，仅限于合成图像生成，难以处理需要事实基础或程序化精度的任务。需要开发能够灵活协调多种专业视觉工具的框架。

Method: 设计了一个强化学习框架，训练中央LLM/MLLM代理智能选择和协调专业视觉工具（在线图像搜索、扩散生成、代码执行、图像编辑），采用结合规则逻辑和LLM/MLLM评估的混合奖励系统。

Result: 在四个基准测试中实现了最先进的性能，大幅超越现有方法。通过新的测试时缩放策略获得了进一步的性能提升。

Conclusion: LLM-Interleaved框架成功解决了当前模型的局限性，通过工具使用范式实现了灵活且动态的交错图像-文本生成，为多模态任务提供了新的解决方案。

Abstract: We propose LLM-Interleaved (LLM-I), a flexible and dynamic framework that
reframes interleaved image-text generation as a tool-use problem. LLM-I is
designed to overcome the "one-tool" bottleneck of current unified models, which
are limited to synthetic imagery and struggle with tasks requiring factual
grounding or programmatic precision. Our framework empowers a central LLM or
MLLM agent to intelligently orchestrate a diverse toolkit of specialized visual
tools, including online image search, diffusion-based generation, code
execution, and image editing. The agent is trained to select and apply these
tools proficiently via a Reinforcement Learning (RL) framework that features a
hybrid reward system combining rule-based logic with judgments from LLM and
MLLM evaluators. Trained on a diverse new dataset using four different model
backbones, LLM-I demonstrates state-of-the-art performance, outperforming
existing methods by a large margin across four benchmarks. We also introduce a
novel test-time scaling strategy that provides further performance gains.
Project Page: https://github.com/ByteDance-BandAI/LLM-I.

</details>


### [58] [Controllable Pareto Trade-off between Fairness and Accuracy](https://arxiv.org/abs/2509.13651)
*Yongkang Du,Jieyu Zhao,Yijun Yang,Tianyi Zhou*

Main category: cs.LG

TL;DR: 提出CPT方法，通过多目标优化实现公平性与准确性的可控权衡，使用移动平均梯度稳定性和关键参数梯度剪枝技术，在仇恨言论检测和职业分类任务中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前NLP任务中的公平性-准确性权衡研究主要寻找单一"最优"解决方案，但帕累托前沿存在多种解决方案。本文旨在根据用户偏好提供可控的权衡选择。

Method: 提出Controllable Pareto Trade-off (CPT)方法：1) 使用随机梯度的移动平均来稳定公平性更新方向；2) 通过仅保留关键参数的梯度来进行梯度剪枝。

Result: 在仇恨言论检测和职业分类任务上的实验表明，CPT能够在帕累托前沿获得比基线方法更高质量的解决方案集，并展现出更好的可控性，能够精确遵循人工定义的参考向量。

Conclusion: CPT方法有效解决了公平性-准确性权衡的可控性问题，通过多目标优化技术实现了根据用户偏好精确调整模型性能的能力。

Abstract: The fairness-accuracy trade-off is a key challenge in NLP tasks. Current work
focuses on finding a single "optimal" solution to balance the two objectives,
which is limited considering the diverse solutions on the Pareto front. This
work intends to provide controllable trade-offs according to the user's
preference of the two objectives, which is defined as a reference vector. To
achieve this goal, we apply multi-objective optimization (MOO), which can find
solutions from various regions of the Pareto front. However, it is challenging
to precisely control the trade-off due to the stochasticity of the training
process and the high dimentional gradient vectors. Thus, we propose
Controllable Pareto Trade-off (CPT) that can effectively train models to
perform different trade-offs according to users' preferences. CPT 1) stabilizes
the fairness update with a moving average of stochastic gradients to determine
the update direction, and 2) prunes the gradients by only keeping the gradients
of the critical parameters. We evaluate CPT on hate speech detection and
occupation classification tasks. Experiments show that CPT can achieve a
higher-quality set of solutions on the Pareto front than the baseline methods.
It also exhibits better controllability and can precisely follow the
human-defined reference vectors.

</details>


### [59] [RF-LSCM: Pushing Radiance Fields to Multi-Domain Localized Statistical Channel Modeling for Cellular Network Optimization](https://arxiv.org/abs/2509.13686)
*Bingsheng Peng,Shutao Zhang,Xi Zheng,Ye Xue,Xinyu Qin,Tsung-Hui Chang*

Main category: cs.LG

TL;DR: RF-LSCM是一个基于辐射场的多域无线信道建模框架，通过物理感知的频率相关衰减模型和点云辅助环境增强方法，解决了传统单细胞单频段信道建模的局限性，在计算效率和精度上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统局部统计信道建模方法局限于单细胞、单网格和单载波频率分析，无法捕捉复杂的跨域交互，需要新的框架来支持多细胞多频段的信道建模需求。

Method: 提出RF-LSCM框架，采用辐射场联合建模大尺度信号衰减和多径分量；引入物理感知的频率相关衰减模型实现跨频段泛化；使用点云辅助环境增强方法支持多细胞多网格建模；采用低秩张量表示和分层张量角度建模算法提高计算效率。

Result: 在真实多细胞数据集上的实验表明，RF-LSCM显著优于现有方法，覆盖预测的平均绝对误差降低30%，通过有效融合多频数据使MAE提升22%。

Conclusion: RF-LSCM通过创新的辐射场建模和高效计算架构，成功解决了传统信道建模的局限性，为蜂窝网络优化提供了更准确和高效的信道建模解决方案。

Abstract: Accurate localized wireless channel modeling is a cornerstone of cellular
network optimization, enabling reliable prediction of network performance
during parameter tuning. Localized statistical channel modeling (LSCM) is the
state-of-the-art channel modeling framework tailored for cellular network
optimization. However, traditional LSCM methods, which infer the channel's
Angular Power Spectrum (APS) from Reference Signal Received Power (RSRP)
measurements, suffer from critical limitations: they are typically confined to
single-cell, single-grid and single-carrier frequency analysis and fail to
capture complex cross-domain interactions. To overcome these challenges, we
propose RF-LSCM, a novel framework that models the channel APS by jointly
representing large-scale signal attenuation and multipath components within a
radiance field. RF-LSCM introduces a multi-domain LSCM formulation with a
physics-informed frequency-dependent Attenuation Model (FDAM) to facilitate the
cross frequency generalization as well as a point-cloud-aided environment
enhanced method to enable multi-cell and multi-grid channel modeling.
Furthermore, to address the computational inefficiency of typical neural
radiance fields, RF-LSCM leverages a low-rank tensor representation,
complemented by a novel Hierarchical Tensor Angular Modeling (HiTAM) algorithm.
This efficient design significantly reduces GPU memory requirements and
training time while preserving fine-grained accuracy. Extensive experiments on
real-world multi-cell datasets demonstrate that RF-LSCM significantly
outperforms state-of-the-art methods, achieving up to a 30% reduction in mean
absolute error (MAE) for coverage prediction and a 22% MAE improvement by
effectively fusing multi-frequency data.

</details>


### [60] [A Conformal Prediction Framework for Uncertainty Quantification in Physics-Informed Neural Networks](https://arxiv.org/abs/2509.13717)
*Yifan Yu,Cheuk Hin Ho,Yangshuai Wang*

Main category: cs.LG

TL;DR: 本文提出了一个基于共形预测的分布无关不确定性量化框架，为物理信息神经网络(PINNs)提供具有严格统计保证的预测区间校准。


<details>
  <summary>Details</summary>
Motivation: 现有PINNs的不确定性量化方法缺乏严格的统计保证，需要建立具有理论保障的分布无关UQ框架来提升PINNs的可靠性和校准能力。

Method: 采用分布无关的共形预测框架，通过在校准集上构建非共形性分数来校准预测区间，并引入局部共形分位数估计来处理空间异方差性。

Result: 在典型PDE系统（阻尼谐振子、泊松、Allen-Cahn和亥姆霍兹方程）上的系统评估表明，该框架实现了可靠的校准和局部自适应不确定性区间，一致优于启发式UQ方法。

Conclusion: 该工作通过将PINNs与分布无关UQ相结合，不仅增强了校准和可靠性，还为复杂PDE系统的不确定性感知建模开辟了新途径。

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework
for solving PDEs, yet existing uncertainty quantification (UQ) approaches for
PINNs generally lack rigorous statistical guarantees. In this work, we bridge
this gap by introducing a distribution-free conformal prediction (CP) framework
for UQ in PINNs. This framework calibrates prediction intervals by constructing
nonconformity scores on a calibration set, thereby yielding distribution-free
uncertainty estimates with rigorous finite-sample coverage guarantees for
PINNs. To handle spatial heteroskedasticity, we further introduce local
conformal quantile estimation, enabling spatially adaptive uncertainty bands
while preserving theoretical guarantee. Through systematic evaluations on
typical PDEs (damped harmonic oscillator, Poisson, Allen-Cahn, and Helmholtz
equations) and comprehensive testing across multiple uncertainty metrics, our
results demonstrate that the proposed framework achieves reliable calibration
and locally adaptive uncertainty intervals, consistently outperforming
heuristic UQ approaches. By bridging PINNs with distribution-free UQ, this work
introduces a general framework that not only enhances calibration and
reliability, but also opens new avenues for uncertainty-aware modeling of
complex PDE systems.

</details>


### [61] [WatchAnxiety: A Transfer Learning Approach for State Anxiety Prediction from Smartwatch Data](https://arxiv.org/abs/2509.13725)
*Md Sabbir Ahmed,Noah French,Mark Rucker,Zhiyuan Wang,Taylor Myers-Brower,Kaitlyn Petz,Mehdi Boukhechba,Bethany A. Teachman,Laura E. Barnes*

Main category: cs.LG

TL;DR: 本研究开发了一个基于智能手表的系统，通过心率和特质测量来预测社交焦虑患者的瞬时焦虑波动，在内部和外部数据集上分别达到60.4%和59.1%的平衡准确率。


<details>
  <summary>Details</summary>
Motivation: 社交焦虑是一种常见心理健康问题，但之前研究很少测量和预测日常焦虑波动。捕捉这些日内动态对于设计实时个性化干预措施至关重要。

Method: 使用定制智能手表系统收集91名社交焦虑大学生的数据，每天7次生态瞬时评估。基于外部心率数据预训练基础模型，结合特质测量构建元学习器进行概率预测。

Result: 在内部数据集上达到60.4%的平衡准确率，在外部TILES-18数据集上达到59.1%的平衡准确率，比先前工作至少提高7%。

Conclusion: 该方法能够有效预测社交焦虑患者的瞬时焦虑波动，为实时个性化干预提供了可行技术方案，具有良好的泛化能力。

Abstract: Social anxiety is a common mental health condition linked to significant
challenges in academic, social, and occupational functioning. A core feature is
elevated momentary (state) anxiety in social situations, yet little prior work
has measured or predicted fluctuations in this anxiety throughout the day.
Capturing these intra-day dynamics is critical for designing real-time,
personalized interventions such as Just-In-Time Adaptive Interventions
(JITAIs). To address this gap, we conducted a study with socially anxious
college students (N=91; 72 after exclusions) using our custom smartwatch-based
system over an average of 9.03 days (SD = 2.95). Participants received seven
ecological momentary assessments (EMAs) per day to report state anxiety. We
developed a base model on over 10,000 days of external heart rate data,
transferred its representations to our dataset, and fine-tuned it to generate
probabilistic predictions. These were combined with trait-level measures in a
meta-learner. Our pipeline achieved 60.4% balanced accuracy in state anxiety
detection in our dataset. To evaluate generalizability, we applied the training
approach to a separate hold-out set from the TILES-18 dataset-the same dataset
used for pretraining. On 10,095 once-daily EMAs, our method achieved 59.1%
balanced accuracy, outperforming prior work by at least 7%.

</details>


### [62] [State Space Models over Directed Graphs](https://arxiv.org/abs/2509.13735)
*Junzhi She,Xunkai Li,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 提出了DirGraphSSM，首个将状态空间模型系统扩展到有向图学习的创新方法，通过k-hop ego图序列化和消息传递机制，在保持高效训练的同时实现了SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有GNN和图Transformer在处理有向图时面临两大挑战：有效捕捉长距离因果依赖关系，以及在处理大规模图数据时平衡准确性和训练效率。现有的图状态空间模型仅适用于无向图，限制了其性能

Method: 提出DirEgo2Token方法通过k-hop ego图将有向图序列化，并在此基础上开发DirGraphSSM架构，通过消息传递机制在有向图上实现状态空间模型

Result: 在三个代表性有向图学习任务上达到SOTA性能，在另外两个任务上获得竞争性性能，训练速度比现有SOTA模型提升1.5-2倍

Conclusion: DirGraphSSM成功将有向图序列化与状态空间模型相结合，为有向图学习提供了高效且准确的解决方案，填补了该领域的技术空白

Abstract: Directed graphs are ubiquitous across numerous domains, where the
directionality of edges encodes critical causal dependencies. However, existing
GNNs and graph Transformers tailored for directed graphs face two major
challenges: (1) effectively capturing long-range causal dependencies derived
from directed edges; (2) balancing accuracy and training efficiency when
processing large-scale graph datasets. In recent years, state space models
(SSMs) have achieved substantial progress in causal sequence tasks, and their
variants designed for graphs have demonstrated state-of-the-art accuracy while
maintaining high efficiency across various graph learning benchmarks. However,
existing graph state space models are exclusively designed for undirected
graphs, which limits their performance in directed graph learning. To this end,
we propose an innovative approach DirEgo2Token which sequentializes directed
graphs via k-hop ego graphs. This marks the first systematic extension of state
space models to the field of directed graph learning. Building upon this, we
develop DirGraphSSM, a novel directed graph neural network architecture that
implements state space models on directed graphs via the message-passing
mechanism. Experimental results demonstrate that DirGraphSSM achieves
state-of-the-art performance on three representative directed graph learning
tasks while attaining competitive performance on two additional tasks with
1.5$\times $ to 2$\times $ training speed improvements compared to existing
state-of-the-art models.

</details>


### [63] [ParaAegis: Parallel Protection for Flexible Privacy-preserved Federated Learning](https://arxiv.org/abs/2509.13739)
*Zihou Wu,Yuecheng Li,Tianchi Liao,Jian Lou,Chuan Chen*

Main category: cs.LG

TL;DR: ParaAegis是一个并行保护框架，通过模型分区策略在联邦学习中实现隐私-效用-效率的灵活平衡控制，将轻量级差分隐私应用于低重要性部分，高重要性部分使用同态加密保护。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习保护机制（如差分隐私和同态加密）存在刚性权衡，必须在模型效用和计算效率之间做出选择，缺乏灵活性，阻碍了实际应用。

Method: 提出战略模型分区方案：对模型低范数部分应用轻量级差分隐私，对剩余部分使用同态加密保护，并通过分布式投票机制确保分区共识。

Result: 理论分析证实了在相同隐私保护下效率与效用的可调节性，实验结果表明通过调整超参数可以灵活优先考虑模型精度或训练时间。

Conclusion: ParaAegis框架为联邦学习提供了灵活的隐私-效用-效率平衡控制，解决了现有保护机制的刚性权衡问题，具有实际应用价值。

Abstract: Federated learning (FL) faces a critical dilemma: existing protection
mechanisms like differential privacy (DP) and homomorphic encryption (HE)
enforce a rigid trade-off, forcing a choice between model utility and
computational efficiency. This lack of flexibility hinders the practical
implementation. To address this, we introduce ParaAegis, a parallel protection
framework designed to give practitioners flexible control over the
privacy-utility-efficiency balance. Our core innovation is a strategic model
partitioning scheme. By applying lightweight DP to the less critical, low norm
portion of the model while protecting the remainder with HE, we create a
tunable system. A distributed voting mechanism ensures consensus on this
partitioning. Theoretical analysis confirms the adjustments between efficiency
and utility with the same privacy. Crucially, the experimental results
demonstrate that by adjusting the hyperparameters, our method enables flexible
prioritization between model accuracy and training time.

</details>


### [64] [ST-LINK: Spatially-Aware Large Language Models for Spatio-Temporal Forecasting](https://arxiv.org/abs/2509.13753)
*Hyotaek Jeon,Hyunwook Lee,Juwon Kim,Sungahn Ko*

Main category: cs.LG

TL;DR: ST-LINK是一个增强大语言模型捕捉时空依赖性的新框架，通过空间增强注意力和记忆检索前馈网络解决LLM在交通预测中的空间建模局限性


<details>
  <summary>Details</summary>
Motivation: 大语言模型在交通预测中表现出潜力，但其主要为序列标记处理设计，难以有效捕捉空间依赖关系，特别是在处理图结构空间数据方面存在架构不兼容问题

Method: 提出ST-LINK框架，包含两个关键组件：1) 空间增强注意力(SE-Attention)，将旋转位置嵌入扩展到空间相关性整合；2) 记忆检索前馈网络(MRFFN)，动态检索历史模式捕捉复杂时间依赖

Result: 在基准数据集上的综合实验表明，ST-LINK超越了传统深度学习和LLM方法，能有效捕捉常规交通模式和突变

Conclusion: ST-LINK成功解决了LLM在交通预测中的空间建模限制，为增强大语言模型处理时空依赖关系提供了有效解决方案

Abstract: Traffic forecasting represents a crucial problem within intelligent
transportation systems. In recent research, Large Language Models (LLMs) have
emerged as a promising method, but their intrinsic design, tailored primarily
for sequential token processing, introduces notable challenges in effectively
capturing spatial dependencies. Specifically, the inherent limitations of LLMs
in modeling spatial relationships and their architectural incompatibility with
graph-structured spatial data remain largely unaddressed. To overcome these
limitations, we introduce ST-LINK, a novel framework that enhances the
capability of Large Language Models to capture spatio-temporal dependencies.
Its key components are Spatially-Enhanced Attention (SE-Attention) and the
Memory Retrieval Feed-Forward Network (MRFFN). SE-Attention extends rotary
position embeddings to integrate spatial correlations as direct rotational
transformations within the attention mechanism. This approach maximizes spatial
learning while preserving the LLM's inherent sequential processing structure.
Meanwhile, MRFFN dynamically retrieves and utilizes key historical patterns to
capture complex temporal dependencies and improve the stability of long-term
forecasting. Comprehensive experiments on benchmark datasets demonstrate that
ST-LINK surpasses conventional deep learning and LLM approaches, and
effectively captures both regular traffic patterns and abrupt changes.

</details>


### [65] [Beyond Correlation: Causal Multi-View Unsupervised Feature Selection Learning](https://arxiv.org/abs/2509.13763)
*Zongxin Shen,Yanyong Huang,Bin Wang,Jinyuan Chang,Shiyu Liu,Tianrui Li*

Main category: cs.LG

TL;DR: 本文从因果视角分析多视图无监督特征选择，提出CAUSA方法，通过因果正则化模块分离混杂因子并平衡分布，有效缓解虚假相关性，在多个实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多视图无监督特征选择方法通过捕捉特征与聚类标签间的相关性来选择特征，但忽略了混杂因子导致的虚假相关性，可能选择不相关特征。

Method: 提出CAUSA方法：1）使用广义无监督谱回归模型识别信息特征；2）引入因果正则化模块自适应分离混杂因子并学习视图共享样本权重来平衡混杂因子分布；3）将两者整合到统一学习框架中。

Result: 综合实验表明CAUSA在多个基准数据集上优于现有最先进方法，能够选择因果信息特征。

Conclusion: 这是首个在无监督设置下深入研究因果多视图特征选择的工作，为多视图无监督特征选择提供了新的因果视角和方法框架。

Abstract: Multi-view unsupervised feature selection (MUFS) has recently received
increasing attention for its promising ability in dimensionality reduction on
multi-view unlabeled data. Existing MUFS methods typically select
discriminative features by capturing correlations between features and
clustering labels. However, an important yet underexplored question remains:
\textit{Are such correlations sufficiently reliable to guide feature
selection?} In this paper, we analyze MUFS from a causal perspective by
introducing a novel structural causal model, which reveals that existing
methods may select irrelevant features because they overlook spurious
correlations caused by confounders. Building on this causal perspective, we
propose a novel MUFS method called CAusal multi-view Unsupervised feature
Selection leArning (CAUSA). Specifically, we first employ a generalized
unsupervised spectral regression model that identifies informative features by
capturing dependencies between features and consensus clustering labels. We
then introduce a causal regularization module that can adaptively separate
confounders from multi-view data and simultaneously learn view-shared sample
weights to balance confounder distributions, thereby mitigating spurious
correlations. Thereafter, integrating both into a unified learning framework
enables CAUSA to select causally informative features. Comprehensive
experiments demonstrate that CAUSA outperforms several state-of-the-art
methods. To our knowledge, this is the first in-depth study of causal
multi-view feature selection in the unsupervised setting.

</details>


### [66] [Floating-Body Hydrodynamic Neural Networks](https://arxiv.org/abs/2509.13783)
*Tianshuo Zhang,Wenzhe Zhai,Rui Yann,Jia Gao,He Cao,Xianglei Xing*

Main category: cs.LG

TL;DR: 提出了FHNN框架，通过物理结构化的神经网络预测可解释的水动力参数，结合解析运动方程，在耗散流体动力学建模中优于传统黑盒方法


<details>
  <summary>Details</summary>
Motivation: 传统黑盒神经网络模型在流体-结构相互作用建模中存在可解释性差和长期预测不稳定的问题，需要一种既能保持物理一致性又能提供可解释参数的方法

Method: 使用物理结构化的神经网络框架，预测方向附加质量、阻力系数和基于流函数的流动场等水动力参数，并与解析运动方程耦合

Result: 在合成涡流数据集上，FHNN比神经ODE的误差低一个数量级，能恢复物理一致的流场，相比哈密顿和拉格朗日神经网络能更有效处理耗散动力学

Conclusion: FHNN填补了黑盒学习和透明系统识别之间的空白，通过物理约束增强了可解释性和积分稳定性，为流体-结构相互作用建模提供了新范式

Abstract: Fluid-structure interaction is common in engineering and natural systems,
where floating-body motion is governed by added mass, drag, and background
flows. Modeling these dissipative dynamics is difficult: black-box neural
models regress state derivatives with limited interpretability and unstable
long-horizon predictions. We propose Floating-Body Hydrodynamic Neural Networks
(FHNN), a physics-structured framework that predicts interpretable hydrodynamic
parameters such as directional added masses, drag coefficients, and a
streamfunction-based flow, and couples them with analytic equations of motion.
This design constrains the hypothesis space, enhances interpretability, and
stabilizes integration. On synthetic vortex datasets, FHNN achieves up to an
order-of-magnitude lower error than Neural ODEs, recovers physically consistent
flow fields. Compared with Hamiltonian and Lagrangian neural networks, FHNN
more effectively handles dissipative dynamics while preserving
interpretability, which bridges the gap between black-box learning and
transparent system identification.

</details>


### [67] [Towards a Physics Foundation Model](https://arxiv.org/abs/2509.13805)
*Florian Wiesner,Matthias Wessling,Stephen Baek*

Main category: cs.LG

TL;DR: 提出了通用物理变换器(GPhyT)，这是一个基于1.8TB多样化模拟数据训练的物理基础模型，能够在多个物理领域实现零样本泛化和稳定长期预测。


<details>
  <summary>Details</summary>
Motivation: 当前基于物理的机器学习方法局限于单一狭窄领域，需要为每个新系统重新训练。物理基础模型(PFM)可以民主化高保真模拟的访问，加速科学发现，消除专门求解器开发的需求。

Method: 使用Transformer架构，通过在多样化物理模拟数据上进行训练，让模型从上下文中学习推断控制动力学，无需告知底层方程。

Result: GPhyT在多个物理领域表现优异，比专门架构性能提升高达29倍；通过上下文学习实现零样本泛化到全新物理系统；能够进行50个时间步的稳定长期预测。

Conclusion: 这项工作证明了单个模型可以从数据中学习可泛化的物理原理，为通向可能改变计算科学和工程的通用物理基础模型开辟了道路。

Abstract: Foundation models have revolutionized natural language processing through a
``train once, deploy anywhere'' paradigm, where a single pre-trained model
adapts to countless downstream tasks without retraining. Access to a Physics
Foundation Model (PFM) would be transformative -- democratizing access to
high-fidelity simulations, accelerating scientific discovery, and eliminating
the need for specialized solver development. Yet current physics-aware machine
learning approaches remain fundamentally limited to single, narrow domains and
require retraining for each new system. We present the General Physics
Transformer (GPhyT), trained on 1.8 TB of diverse simulation data, that
demonstrates foundation model capabilities are achievable for physics. Our key
insight is that transformers can learn to infer governing dynamics from
context, enabling a single model to simulate fluid-solid interactions, shock
waves, thermal convection, and multi-phase dynamics without being told the
underlying equations. GPhyT achieves three critical breakthroughs: (1) superior
performance across multiple physics domains, outperforming specialized
architectures by up to 29x, (2) zero-shot generalization to entirely unseen
physical systems through in-context learning, and (3) stable long-term
predictions through 50-timestep rollouts. By establishing that a single model
can learn generalizable physical principles from data alone, this work opens
the path toward a universal PFM that could transform computational science and
engineering.

</details>


### [68] [Hybrid Quantum-Classical Neural Networks for Few-Shot Credit Risk Assessment](https://arxiv.org/abs/2509.13818)
*Zheng-an Wang,Yanbo J. Wang,Jiachi Zhang,Qi Xu,Yilun Zhao,Jintao Li,Yipeng Zhang,Bo Yang,Xinkai Gao,Xiaofeng Cao,Kai Xu,Pengpeng Hao,Xuan Yang,Heng Fan*

Main category: cs.LG

TL;DR: 提出了一种混合量子-经典工作流，用于少样本信用风险评估，在量子硬件上实现了0.88的AUC，超越了经典基准模型


<details>
  <summary>Details</summary>
Motivation: 解决普惠金融中数据稀缺和类别不平衡导致的信用风险评估难题，传统方法在此类少样本场景下效果有限

Method: 结合经典机器学习模型（逻辑回归、随机森林、XGBoost）进行特征工程和降维，然后使用参数平移规则训练的量子神经网络作为核心分类器

Result: 在279个真实信用数据样本上，量子神经网络在仿真中获得0.852±0.027的平均AUC，在Quafu量子云平台的超导处理器上获得0.88的AUC，召回率表现尤其突出

Conclusion: 为NISQ时代数据受限的金融场景提供了实用的量子计算应用蓝图，证明了量子机器学习在高风险普惠金融应用中的潜力

Abstract: Quantum Machine Learning (QML) offers a new paradigm for addressing complex
financial problems intractable for classical methods. This work specifically
tackles the challenge of few-shot credit risk assessment, a critical issue in
inclusive finance where data scarcity and imbalance limit the effectiveness of
conventional models. To address this, we design and implement a novel hybrid
quantum-classical workflow. The methodology first employs an ensemble of
classical machine learning models (Logistic Regression, Random Forest, XGBoost)
for intelligent feature engineering and dimensionality reduction. Subsequently,
a Quantum Neural Network (QNN), trained via the parameter-shift rule, serves as
the core classifier. This framework was evaluated through numerical simulations
and deployed on the Quafu Quantum Cloud Platform's ScQ-P21 superconducting
processor. On a real-world credit dataset of 279 samples, our QNN achieved a
robust average AUC of 0.852 +/- 0.027 in simulations and yielded an impressive
AUC of 0.88 in the hardware experiment. This performance surpasses a suite of
classical benchmarks, with a particularly strong result on the recall metric.
This study provides a pragmatic blueprint for applying quantum computing to
data-constrained financial scenarios in the NISQ era and offers valuable
empirical evidence supporting its potential in high-stakes applications like
inclusive finance.

</details>


### [69] [An End-to-End Differentiable, Graph Neural Network-Embedded Pore Network Model for Permeability Prediction](https://arxiv.org/abs/2509.13841)
*Qingqi Zhao,Heng Xiao*

Main category: cs.LG

TL;DR: 提出了一种端到端可微分混合框架，将图神经网络嵌入孔隙网络模型中，用于多孔介质渗透率预测，避免了传统方法的理想化几何假设，同时保持了物理基础。


<details>
  <summary>Details</summary>
Motivation: 传统纯数据驱动模型缺乏跨尺度泛化能力且不包含明确物理约束，而孔隙网络模型虽然基于物理但依赖理想化几何假设来估算孔喉水力传导度，限制了在复杂结构中的准确性。

Method: 开发端到端可微分混合框架，用基于图神经网络的预测替代传统解析公式进行传导度计算，通过自动微分和离散伴随方法实现全耦合训练，仅需单一渗透率标量作为训练目标。

Result: 该模型实现了高精度和良好的跨尺度泛化能力，优于纯数据驱动和传统孔隙网络模型方法，梯度敏感性分析显示物理一致的特征影响。

Conclusion: 该方法为复杂多孔介质渗透率预测提供了可扩展且物理信息丰富的框架，减少了模型不确定性并提高了准确性。

Abstract: Accurate prediction of permeability in porous media is essential for modeling
subsurface flow. While pure data-driven models offer computational efficiency,
they often lack generalization across scales and do not incorporate explicit
physical constraints. Pore network models (PNMs), on the other hand, are
physics-based and efficient but rely on idealized geometric assumptions to
estimate pore-scale hydraulic conductance, limiting their accuracy in complex
structures. To overcome these limitations, we present an end-to-end
differentiable hybrid framework that embeds a graph neural network (GNN) into a
PNM. In this framework, the analytical formulas used for conductance
calculations are replaced by GNN-based predictions derived from pore and throat
features. The predicted conductances are then passed to the PNM solver for
permeability computation. In this way, the model avoids the idealized geometric
assumptions of PNM while preserving the physics-based flow calculations. The
GNN is trained without requiring labeled conductance data, which can number in
the thousands per pore network; instead, it learns conductance values by using
a single scalar permeability as the training target. This is made possible by
backpropagating gradients through both the GNN (via automatic differentiation)
and the PNM solver (via a discrete adjoint method), enabling fully coupled,
end-to-end training. The resulting model achieves high accuracy and generalizes
well across different scales, outperforming both pure data-driven and
traditional PNM approaches. Gradient-based sensitivity analysis further reveals
physically consistent feature influences, enhancing model interpretability.
This approach offers a scalable and physically informed framework for
permeability prediction in complex porous media, reducing model uncertainty and
improving accuracy.

</details>


### [70] [Graph-Regularized Learning of Gaussian Mixture Models](https://arxiv.org/abs/2509.13855)
*Shamsiiat Abdurakhmanova,Alex Jung*

Main category: cs.LG

TL;DR: 提出一种图正则化的分布式高斯混合模型学习方法，利用相似性图指导节点间参数共享，在异构小样本场景下优于集中式和本地训练方法


<details>
  <summary>Details</summary>
Motivation: 解决分布式环境下数据异构且样本有限时，传统集中式或本地训练GMM效果不佳的问题，同时避免原始数据传输

Method: 基于图正则化的GMM学习框架，利用节点相似性图指导参数共享，实现邻居参数的灵活聚合

Result: 在异构低样本场景下，该方法性能优于集中式训练和本地训练的GMM模型

Conclusion: 图正则化方法为分布式异构数据下的GMM学习提供了有效解决方案，实现了隐私保护和性能提升的双重目标

Abstract: We present a graph-regularized learning of Gaussian Mixture Models (GMMs) in
distributed settings with heterogeneous and limited local data. The method
exploits a provided similarity graph to guide parameter sharing among nodes,
avoiding the transfer of raw data. The resulting model allows for flexible
aggregation of neighbors' parameters and outperforms both centralized and
locally trained GMMs in heterogeneous, low-sample regimes.

</details>


### [71] [Masked Diffusion Models as Energy Minimization](https://arxiv.org/abs/2509.13866)
*Sitong Chen,Shen Nie,Jiacheng Sun,Zijin Feng,Zhenguo Li,Ji-Rong Wen,Chongxuan Li*

Main category: cs.LG

TL;DR: 本文提出了一个理论框架，将掩码扩散模型解释为离散最优传输中的能量最小化问题，证明了三种能量形式的数学等价性，并通过Beta分布参数化调度实现了高效的后训练调优。


<details>
  <summary>Details</summary>
Motivation: 统一掩码扩散模型的理论基础，澄清其数学本质，并为实际采样改进提供理论指导。

Method: 建立数学框架证明三种能量形式（动能、条件动能、测地能量）在掩码扩散模型中的等价性；使用Beta分布参数化插值调度，将调度设计空间简化为2D搜索。

Result: 理论证明了三种能量最小化问题的等价性；实验表明能量启发的调度在合成和真实基准测试中优于手工设计的基线，特别是在低步采样设置中表现突出。

Conclusion: 该工作为掩码扩散模型提供了坚实的理论基础，提出的能量最小化视角不仅统一了现有理论，还启发了实用的调度优化方法，显著提升了采样效率。

Abstract: We present a systematic theoretical framework that interprets masked
diffusion models (MDMs) as solutions to energy minimization problems in
discrete optimal transport. Specifically, we prove that three distinct energy
formulations--kinetic, conditional kinetic, and geodesic energy--are
mathematically equivalent under the structure of MDMs, and that MDMs minimize
all three when the mask schedule satisfies a closed-form optimality condition.
This unification not only clarifies the theoretical foundations of MDMs, but
also motivates practical improvements in sampling. By parameterizing
interpolation schedules via Beta distributions, we reduce the schedule design
space to a tractable 2D search, enabling efficient post-training tuning without
model modification. Experiments on synthetic and real-world benchmarks
demonstrate that our energy-inspired schedules outperform hand-crafted
baselines, particularly in low-step sampling settings.

</details>


### [72] [FedSSG: Expectation-Gated and History-Aware Drift Alignment for Federated Learning](https://arxiv.org/abs/2509.13895)
*Zhanting Zhou,Jinshan Lai,Fengchun Zhang,Zeqin Wu,Fengli Zhang*

Main category: cs.LG

TL;DR: FedSSG是一种联邦学习方法，通过历史感知的漂移对齐机制，利用随机采样指导来解决非IID数据和部分参与导致的客户端漂移问题，显著提升收敛速度和准确率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中非IID数据和部分客户端参与会导致客户端漂移和不一致的局部最优解，造成收敛不稳定和准确率下降，需要一种有效的漂移对齐方法。

Method: FedSSG维护每个客户端的漂移记忆，积累局部模型差异作为历史梯度的轻量级草图；通过基于参与率的平滑门控函数控制记忆更新和局部对齐项，早期保持弱平滑，后期加强对齐。

Result: 在CIFAR-10/100数据集上，FedSSG相比基线方法提升测试准确率约0.9-2.7个百分点，收敛速度提升约4.5倍，仅增加O(d)客户端内存和常数时间开销。

Conclusion: FedSSG证明采样统计可以转化为原则性的历史感知相位控制，有效稳定和加速联邦训练，在非IID和非均匀采样情况下表现优异。

Abstract: Non-IID data and partial participation induce client drift and inconsistent
local optima in federated learning, causing unstable convergence and accuracy
loss. We present FedSSG, a stochastic sampling-guided, history-aware drift
alignment method. FedSSG maintains a per-client drift memory that accumulates
local model differences as a lightweight sketch of historical gradients;
crucially, it gates both the memory update and the local alignment term by a
smooth function of the observed/expected participation ratio (a
phase-by-expectation signal derived from the server sampler). This
statistically grounded gate stays weak and smooth when sampling noise dominates
early, then strengthens once participation statistics stabilize, contracting
the local-global gap without extra communication. Across CIFAR-10/100 with
100/500 clients and 2-15 percent participation, FedSSG consistently outperforms
strong drift-aware baselines and accelerates convergence; on our benchmarks it
improves test accuracy by up to a few points (e.g., about +0.9 on CIFAR-10 and
about +2.7 on CIFAR-100 on average over the top-2 baseline) and yields about
4.5x faster target-accuracy convergence on average. The method adds only O(d)
client memory and a constant-time gate, and degrades gracefully to a mild
regularizer under near-IID or uniform sampling. FedSSG shows that sampling
statistics can be turned into a principled, history-aware phase control to
stabilize and speed up federated training.

</details>


### [73] [TFMAdapter: Lightweight Instance-Level Adaptation of Foundation Models for Forecasting with Covariates](https://arxiv.org/abs/2509.13906)
*Afrin Dange,Sunita Sarawagi*

Main category: cs.LG

TL;DR: TFMAdapter是一个轻量级适配器，无需微调即可为时间序列基础模型添加协变量信息，通过两阶段方法实现，在真实数据集上比基础模型提升24-27%


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型无法利用协变量信息，而协变量在许多应用中对准确预测至关重要，需要一种轻量级方法来增强基础模型的协变量处理能力

Method: 采用两阶段方法：1) 使用简单回归模型生成伪预测 2) 训练高斯过程回归器，结合伪预测、TSFM预测和协变量来优化预测结果

Result: 在真实数据集上，TFMAdapter持续优于基础模型和监督基线，相比基础模型提升24-27%，且数据和计算开销最小

Conclusion: 轻量级适配器有潜力弥合通用基础模型与领域特定预测需求之间的差距，为时间序列预测提供了有效的协变量整合方案

Abstract: Time Series Foundation Models (TSFMs) have recently achieved state-of-the-art
performance in univariate forecasting on new time series simply by conditioned
on a brief history of past values. Their success demonstrates that large-scale
pretraining across diverse domains can acquire the inductive bias to generalize
from temporal patterns in a brief history. However, most TSFMs are unable to
leverage covariates -- future-available exogenous variables critical for
accurate forecasting in many applications -- due to their domain-specific
nature and the lack of associated inductive bias. We propose TFMAdapter, a
lightweight, instance-level adapter that augments TSFMs with covariate
information without fine-tuning. Instead of retraining, TFMAdapter operates on
the limited history provided during a single model call, learning a
non-parametric cascade that combines covariates with univariate TSFM forecasts.
However, such learning would require univariate forecasts at all steps in the
history, requiring too many calls to the TSFM. To enable training on the full
historical context while limiting TSFM invocations, TFMAdapter uses a two-stage
method: (1) generating pseudo-forecasts with a simple regression model, and (2)
training a Gaussian Process regressor to refine predictions using both pseudo-
and TSFM forecasts alongside covariates. Extensive experiments on real-world
datasets demonstrate that TFMAdapter consistently outperforms both foundation
models and supervised baselines, achieving a 24-27\% improvement over base
foundation models with minimal data and computational overhead. Our results
highlight the potential of lightweight adapters to bridge the gap between
generic foundation models and domain-specific forecasting needs.

</details>


### [74] [APFEx: Adaptive Pareto Front Explorer for Intersectional Fairness](https://arxiv.org/abs/2509.13908)
*Priyobrata Mondal,Faizanuddin Ansari,Swagatam Das*

Main category: cs.LG

TL;DR: APFEx是首个显式建模交叉公平性的框架，通过多目标优化解决多个敏感属性组合的公平性问题，在保持准确性的同时显著减少公平性违规


<details>
  <summary>Details</summary>
Motivation: 现有公平性方法只处理单一敏感属性，无法捕捉交叉子群体面临的复杂、复合性偏见，需要专门针对交叉公平性的解决方案

Method: APFEx框架包含三个创新：自适应多目标优化器（动态切换帕累托锥投影、梯度加权和探索策略）、可微交叉公平性度量、以及理论收敛保证

Result: 在四个真实数据集上的实验表明，APFEx在保持竞争力的准确性的同时，显著减少了公平性违规，优于现有方法

Conclusion: APFEx填补了公平机器学习中的重要空白，为交叉公平性提供了可扩展、模型无关的解决方案

Abstract: Ensuring fairness in machine learning models is critical, especially when
biases compound across intersecting protected attributes like race, gender, and
age. While existing methods address fairness for single attributes, they fail
to capture the nuanced, multiplicative biases faced by intersectional
subgroups. We introduce Adaptive Pareto Front Explorer (APFEx), the first
framework to explicitly model intersectional fairness as a joint optimization
problem over the Cartesian product of sensitive attributes. APFEx combines
three key innovations- (1) an adaptive multi-objective optimizer that
dynamically switches between Pareto cone projection, gradient weighting, and
exploration strategies to navigate fairness-accuracy trade-offs, (2)
differentiable intersectional fairness metrics enabling gradient-based
optimization of non-smooth subgroup disparities, and (3) theoretical guarantees
of convergence to Pareto-optimal solutions. Experiments on four real-world
datasets demonstrate APFEx's superiority, reducing fairness violations while
maintaining competitive accuracy. Our work bridges a critical gap in fair ML,
providing a scalable, model-agnostic solution for intersectional fairness.

</details>


### [75] [Ensemble of Pre-Trained Models for Long-Tailed Trajectory Prediction](https://arxiv.org/abs/2509.13914)
*Divya Thuremella,Yi Yang,Simon Wanna,Lars Kunze,Daniele De Martini*

Main category: cs.LG

TL;DR: 本文提出了一种简单的置信度加权平均方法，无需重新训练即可组合多个最先进的深度学习轨迹预测模型，在NuScenes和Argoverse数据集上实现了10%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶领域大型预测模型的不断涌现，如何在不进行昂贵重新训练的情况下结合这些大模型的优势成为一个重要挑战。

Method: 使用置信度加权平均方法直接组合现成的深度学习轨迹预测模型，无需重新训练或微调。

Result: 该方法在NuScenes和Argoverse数据集上比最佳单一模型性能提升10%，特别是在长尾指标上表现优异，且改进在整个数据分布中都有效。

Conclusion: 简单的模型集成方法能够有效提升轨迹预测性能，证明了无需复杂重新训练即可结合多个先进模型的可行性。

Abstract: This work explores the application of ensemble modeling to the
multidimensional regression problem of trajectory prediction for vehicles in
urban environments. As newer and bigger state-of-the-art prediction models for
autonomous driving continue to emerge, an important open challenge is the
problem of how to combine the strengths of these big models without the need
for costly re-training. We show how, perhaps surprisingly, combining
state-of-the-art deep learning models out-of-the-box (without retraining or
fine-tuning) with a simple confidence-weighted average method can enhance the
overall prediction. Indeed, while combining trajectory prediction models is not
straightforward, this simple approach enhances performance by 10% over the best
prediction model, especially in the long-tailed metrics. We show that this
performance improvement holds on both the NuScenes and Argoverse datasets, and
that these improvements are made across the dataset distribution. The code for
our work is open source.

</details>


### [76] [Adaptive Client Selection via Q-Learning-based Whittle Index in Wireless Federated Learning](https://arxiv.org/abs/2509.13933)
*Qiyue Li,Yingxin Liu,Hang Qi,Jieping Luo,Zhizhang Liu,Jingjin Wu*

Main category: cs.LG

TL;DR: 提出WILF-Q方法，使用Q-learning自适应学习客户端Whittle指数，解决无线联邦学习中的客户端选择问题，显著提升学习效率


<details>
  <summary>Details</summary>
Motivation: 无线联邦学习中客户端动态状态变化影响计算和通信效率，服务器无法直接观测这些状态，需要高效选择客户端来减少达到特定学习精度所需的总时间

Method: 将客户端选择问题建模为restless多臂老虎机问题，提出WILF-Q方法，通过Q-learning自适应学习和更新每个客户端的近似Whittle指数，然后选择指数最高的客户端

Result: 实验结果表明WILF-Q在learning efficiency方面显著优于现有基线策略

Conclusion: WILF-Q不需要客户端状态转移或数据分布的显式知识，适合在实际联邦学习设置中部署，为无线联邦学习中的客户端选择提供了鲁棒高效的方法

Abstract: We consider the client selection problem in wireless Federated Learning (FL),
with the objective of reducing the total required time to achieve a certain
level of learning accuracy. Since the server cannot observe the clients'
dynamic states that can change their computation and communication efficiency,
we formulate client selection as a restless multi-armed bandit problem. We
propose a scalable and efficient approach called the Whittle Index Learning in
Federated Q-learning (WILF-Q), which uses Q-learning to adaptively learn and
update an approximated Whittle index associated with each client, and then
selects the clients with the highest indices. Compared to existing approaches,
WILF-Q does not require explicit knowledge of client state transitions or data
distributions, making it well-suited for deployment in practical FL settings.
Experiment results demonstrate that WILF-Q significantly outperforms existing
baseline policies in terms of learning efficiency, providing a robust and
efficient approach to client selection in wireless FL.

</details>


### [77] [eXtended Physics Informed Neural Network Method for Fracture Mechanics Problems](https://arxiv.org/abs/2509.13952)
*Amin Lotfalian,Mohammad Reza Banan,Pooyan Broumand*

Main category: cs.LG

TL;DR: X-PINN是一种基于物理信息的神经网络扩展框架，用于解决多裂纹断裂力学问题，通过能量损失函数、定制积分方案和域分解方法，结合XFEM思想在神经网络解空间中引入特殊函数来捕捉裂纹不连续性和奇异性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理多裂纹断裂力学问题时面临挑战，需要开发一种能够有效捕捉裂纹不连续性和奇异性的稳健计算框架。

Method: 提出扩展物理信息神经网络(X-PINN)，采用基于能量的损失函数、定制积分方案和域分解程序，借鉴XFEM方法在神经网络解空间中引入特殊函数来显式捕捉裂纹体不连续性和裂纹尖端奇异性。

Result: 数值实验验证了该方法在1D和2D域中处理复杂多裂纹问题的有效性和稳健性，并具有良好的扩展到3D问题的能力。

Conclusion: X-PINN框架为多裂纹断裂力学问题提供了一种灵活有效的解决方案，通过神经网络与物理信息的结合成功处理了裂纹相关的不连续性和奇异性问题。

Abstract: This paper presents eXtended Physics-Informed Neural Network (X-PINN), a
novel and robust framework for addressing fracture mechanics problems involving
multiple cracks in fractured media. To address this, an energy-based loss
function, customized integration schemes, and domain decomposition procedures
are proposed. Inspired by the Extended Finite Element Method (XFEM), the neural
network solution space is enriched with specialized functions that allow crack
body discontinuities and singularities at crack tips to be explicitly captured.
Furthermore, a structured framework is introduced in which standard and
enriched solution components are modeled using distinct neural networks,
enabling flexible and effective simulations of complex multiple-crack problems
in 1D and 2D domains, with convenient extensibility to 3D problems. Numerical
experiments are conducted to validate the effectiveness and robustness of the
proposed method.

</details>


### [78] [Personalization on a Budget: Minimally-Labeled Continual Learning for Resource-Efficient Seizure Detection](https://arxiv.org/abs/2509.13974)
*Amirhossein Shahbazinia,Jonathan Dan,Jose A. Miranda,Giovanni Ansaloni,David Atienza*

Main category: cs.LG

TL;DR: EpiSMART是一个用于癫痫发作检测的持续学习框架，通过选择性保留高熵和发作预测样本，在最小内存和计算需求下实现个性化适应，在CHB-MIT数据集上F1分数提升21%。


<details>
  <summary>Details</summary>
Motivation: 癫痫诊断依赖专家分析脑电图，过程耗时且需要专业知识。现有静态深度学习模型存在灾难性遗忘问题，无法适应患者脑电图信号的动态变化。

Method: 提出EpiSMART持续学习框架，使用大小受限的重放缓冲区和智能样本选择策略，选择性地保留高熵和发作预测样本，逐步适应患者特定的脑电图信号。

Result: 在CHB-MIT数据集验证中，EpiSMART相比无更新的基线模型F1分数提升21%，平均每天仅需6.46分钟标记数据和6.28次更新，适合可穿戴系统实时部署。

Conclusion: EpiSMART能够在资源受限的现实条件下，有效整合新数据而不破坏过去知识，实现稳健的个性化癫痫发作检测，推动可穿戴医疗系统的实际应用。

Abstract: Objective: Epilepsy, a prevalent neurological disease, demands careful
diagnosis and continuous care. Seizure detection remains challenging, as
current clinical practice relies on expert analysis of electroencephalography,
which is a time-consuming process and requires specialized knowledge.
Addressing this challenge, this paper explores automated epileptic seizure
detection using deep learning, focusing on personalized continual learning
models that adapt to each patient's unique electroencephalography signal
features, which evolve over time. Methods: In this context, our approach
addresses the challenge of integrating new data into existing models without
catastrophic forgetting, a common issue in static deep learning models. We
propose EpiSMART, a continual learning framework for seizure detection that
uses a size-constrained replay buffer and an informed sample selection strategy
to incrementally adapt to patient-specific electroencephalography signals. By
selectively retaining high-entropy and seizure-predicted samples, our method
preserves critical past information while maintaining high performance with
minimal memory and computational requirements. Results: Validation on the
CHB-MIT dataset, shows that EpiSMART achieves a 21% improvement in the F1 score
over a trained baseline without updates in all other patients. On average,
EpiSMART requires only 6.46 minutes of labeled data and 6.28 updates per day,
making it suitable for real-time deployment in wearable systems.
Conclusion:EpiSMART enables robust and personalized seizure detection under
realistic and resource-constrained conditions by effectively integrating new
data into existing models without degrading past knowledge. Significance: This
framework advances automated seizure detection by providing a continual
learning approach that supports patient-specific adaptation and practical
deployment in wearable healthcare systems.

</details>


### [79] [Deep Temporal Graph Networks for Real-Time Correction of GNSS Jamming-Induced Deviations](https://arxiv.org/abs/2509.14000)
*Ivana Kesić,Aljaž Blatnik,Carolina Fortuna,Blaž Bertalanič*

Main category: cs.LG

TL;DR: 提出基于动态图回归的GNSS干扰抑制方法，使用异构图卷积LSTM网络实时预测并校正接收机水平偏差，在多种干扰场景下显著优于传统时间序列基线模型


<details>
  <summary>Details</summary>
Motivation: GNSS系统日益受到故意干扰的影响，在需要保持定位和定时功能时降低可用性，需要开发有效的干扰抑制技术

Method: 将卫星接收机环境表示为异构星形图，使用单层异构图卷积LSTM（HeteroGCLSTM）聚合空间上下文和时间动态，输出2D偏差向量进行实时校正

Result: 在-45dBm干扰下达到3.64-7.74cm的MAE，在-60至-70dBm时改善至1.65-2.08cm，混合模式下MAE为3.78-4.25cm，数据效率优异（仅10%训练数据仍优于基线）

Conclusion: 该方法能有效抑制GNSS干扰，在各种干扰类型和功率水平下均表现优异，具有很好的实用价值和数据效率

Abstract: Global Navigation Satellite Systems (GNSS) are increasingly disrupted by
intentional jamming, degrading availability precisely when positioning and
timing must remain operational. We address this by reframing jamming mitigation
as dynamic graph regression and introducing a receiver-centric deep temporal
graph network that predicts, and thus corrects, the receivers horizontal
deviation in real time. At each 1 Hz epoch, the satellite receiver environment
is represented as a heterogeneous star graph (receiver center, tracked
satellites as leaves) with time varying attributes (e.g., SNR, azimuth,
elevation, latitude/longitude). A single layer Heterogeneous Graph ConvLSTM
(HeteroGCLSTM) aggregates one hop spatial context and temporal dynamics over a
short history to output the 2D deviation vector applied for on the fly
correction.
  We evaluate on datasets from two distinct receivers under three jammer
profiles, continuous wave (cw), triple tone (cw3), and wideband FM, each
exercised at six power levels between -45 and -70 dBm, with 50 repetitions per
scenario (prejam/jam/recovery). Against strong multivariate time series
baselines (MLP, uniform CNN, and Seq2Point CNN), our model consistently attains
the lowest mean absolute error (MAE). At -45 dBm, it achieves 3.64 cm
(GP01/cw), 7.74 cm (GP01/cw3), 4.41 cm (ublox/cw), 4.84 cm (ublox/cw3), and
4.82 cm (ublox/FM), improving to 1.65-2.08 cm by -60 to -70 dBm. On mixed mode
datasets pooling all powers, MAE is 3.78 cm (GP01) and 4.25 cm (ublox10),
outperforming Seq2Point, MLP, and CNN. A split study shows superior data
efficiency: with only 10\% training data our approach remains well ahead of
baselines (20 cm vs. 36-42 cm).

</details>


### [80] [Differentially private federated learning for localized control of infectious disease dynamics](https://arxiv.org/abs/2509.14024)
*Raouf Kerkouche,Henrik Zunker,Mario Fritz,Martin J. Kühn*

Main category: cs.LG

TL;DR: 提出基于联邦学习和差分隐私的隐私保护流行病预测方法，在德国县级层面实现本地化预测，平衡数据隐私和预测效用


<details>
  <summary>Details</summary>
Motivation: 在流行病爆发时需要快速反应，但本地化机器学习模型训练面临数据不足问题，而集中化数据又存在隐私敏感性问题，需要找到既能保护隐私又能提供详细情境数据的解决方案

Method: 采用联邦学习框架，以县/社区为客户端，使用多层感知机在滑动窗口上进行病例数预测，客户端只交换经过范数裁剪的更新，服务器使用差分隐私噪声聚合更新

Result: 在适度隐私保护水平下，差分隐私模型接近非隐私模型性能：2020年11月R²=0.94（vs 0.95），MAPE=26%；2022年3月R²=0.88（vs 0.93），MAPE=21%

Conclusion: 客户端级差分隐私联邦学习能够提供有用的县级预测，具有强大的隐私保证，可行的隐私预算取决于流行病阶段，允许卫生当局进行隐私合规的本地预测协作

Abstract: In times of epidemics, swift reaction is necessary to mitigate epidemic
spreading. For this reaction, localized approaches have several advantages,
limiting necessary resources and reducing the impact of interventions on a
larger scale. However, training a separate machine learning (ML) model on a
local scale is often not feasible due to limited available data. Centralizing
the data is also challenging because of its high sensitivity and privacy
constraints. In this study, we consider a localized strategy based on the
German counties and communities managed by the related local health authorities
(LHA). For the preservation of privacy to not oppose the availability of
detailed situational data, we propose a privacy-preserving forecasting method
that can assist public health experts and decision makers. ML methods with
federated learning (FL) train a shared model without centralizing raw data.
Considering the counties, communities or LHAs as clients and finding a balance
between utility and privacy, we study a FL framework with client-level
differential privacy (DP). We train a shared multilayer perceptron on sliding
windows of recent case counts to forecast the number of cases, while clients
exchange only norm-clipped updates and the server aggregated updates with DP
noise. We evaluate the approach on COVID-19 data on county-level during two
phases. As expected, very strict privacy yields unstable, unusable forecasts.
At a moderately strong level, the DP model closely approaches the non-DP model:
$R^2= 0.94$ (vs. 0.95) and mean absolute percentage error (MAPE) of 26 % in
November 2020; $R^2= 0.88$ (vs. 0.93) and MAPE of 21 % in March 2022. Overall,
client-level DP-FL can deliver useful county-level predictions with strong
privacy guarantees, and viable privacy budgets depend on epidemic phase,
allowing privacy-compliant collaboration among health authorities for local
forecasting.

</details>


### [81] [Deep Learning-Driven Peptide Classification in Biological Nanopores](https://arxiv.org/abs/2509.14029)
*Samuel Tovey,Julian Hoßbach,Sandro Kuppel,Tobias Ensslen,Jan C. Behrends,Christian Holm*

Main category: cs.LG

TL;DR: 使用小波变换将纳米孔电流信号转换为尺度图图像，结合机器学习实现蛋白质实时分类，准确率达到81%，创下该领域新纪录


<details>
  <summary>Details</summary>
Motivation: 开发能够在临床环境中实时分类蛋白质的设备，实现廉价快速的疾病诊断。纳米孔技术虽然有望实现这一目标，但复杂的电流信号限制了其准确性

Method: 通过小波变换将纳米孔电流信号转换为尺度图图像，捕获振幅、频率和时间信息，使用机器学习算法进行分类

Result: 在42种肽测试中达到约81%的分类准确率，创下该领域最新记录，并展示了模型迁移技术

Conclusion: 该方法为实时疾病诊断开辟了新途径，推动了纳米孔技术在临床点护理诊断中的实际应用

Abstract: A device capable of performing real time classification of proteins in a
clinical setting would allow for inexpensive and rapid disease diagnosis. One
such candidate for this technology are nanopore devices. These devices work by
measuring a current signal that arises when a protein or peptide enters a
nanometer-length-scale pore. Should this current be uniquely related to the
structure of the peptide and its interactions with the pore, the signals can be
used to perform identification. While such a method would allow for real time
identification of peptides and proteins in a clinical setting, to date, the
complexities of these signals limit their accuracy. In this work, we tackle the
issue of classification by converting the current signals into scaleogram
images via wavelet transforms, capturing amplitude, frequency, and time
information in a modality well-suited to machine learning algorithms. When
tested on 42 peptides, our method achieved a classification accuracy of
~$81\,\%$, setting a new state-of-the-art in the field and taking a step toward
practical peptide/protein diagnostics at the point of care. In addition, we
demonstrate model transfer techniques that will be critical when deploying
these models into real hardware, paving the way to a new method for real-time
disease diagnosis.

</details>


### [82] [Queen Detection in Beehives via Environmental Sensor Fusion for Low-Power Edge Computing](https://arxiv.org/abs/2509.14061)
*Chiara De Luca,Elisa Donati*

Main category: cs.LG

TL;DR: 提出基于环境传感器融合的轻量级蜂王检测系统，使用温湿度压力差分特征，在STM32微控制器上实现实时低功耗边缘计算，准确率超过99%


<details>
  <summary>Details</summary>
Motivation: 传统蜂王检测依赖人工检查，劳动强度大且干扰蜂群；现有音频方法功耗高、预处理复杂且易受环境噪声影响，需要更可持续的解决方案

Method: 采用环境传感器融合技术（温度、湿度、压力内外差分），在商用STM32微控制器上部署量化决策树推理算法，实现实时低功耗边缘计算

Result: 仅使用环境输入即可达到超过99%的蜂王检测准确率，音频特征未带来显著性能提升，系统具有高能效和实用性

Conclusion: 该工作提供了一种可扩展、可持续的非侵入式蜂巢监测解决方案，为使用现成节能硬件实现自主精准养蜂铺平了道路

Abstract: Queen bee presence is essential for the health and stability of honeybee
colonies, yet current monitoring methods rely on manual inspections that are
labor-intensive, disruptive, and impractical for large-scale beekeeping. While
recent audio-based approaches have shown promise, they often require high power
consumption, complex preprocessing, and are susceptible to ambient noise. To
overcome these limitations, we propose a lightweight, multimodal system for
queen detection based on environmental sensor fusion-specifically, temperature,
humidity, and pressure differentials between the inside and outside of the
hive. Our approach employs quantized decision tree inference on a commercial
STM32 microcontroller, enabling real-time, low-power edge computing without
compromising accuracy. We show that our system achieves over 99% queen
detection accuracy using only environmental inputs, with audio features
offering no significant performance gain. This work presents a scalable and
sustainable solution for non-invasive hive monitoring, paving the way for
autonomous, precision beekeeping using off-the-shelf, energy-efficient
hardware.

</details>


### [83] [Online Bayesian Risk-Averse Reinforcement Learning](https://arxiv.org/abs/2509.14077)
*Yuhao Wang,Enlu Zhou*

Main category: cs.LG

TL;DR: 本文研究强化学习中的贝叶斯风险规避方法，通过BRMDP处理模型参数不确定性，证明了贝叶斯风险价值函数与真实价值函数之间的渐近正态性差异，并提出了在线RL和CMAB的后验采样算法，获得了次线性遗憾界。


<details>
  <summary>Details</summary>
Motivation: 处理强化学习中由于数据不足导致的认知不确定性，通过贝叶斯风险规避方法来考虑未知基础模型的参数不确定性。

Method: 采用贝叶斯风险马尔可夫决策过程(BRMDP)，推导贝叶斯风险价值函数与真实价值函数的渐近正态性关系，提出基于后验采样的在线RL和CMAB算法。

Result: 贝叶斯风险规避方法会悲观地低估原始价值函数，这种差异随风险规避强度增加而增大，随数据量增加而减小。算法在在线RL和CMAB设置下都获得了次线性遗憾界。

Conclusion: 贝叶斯风险规避方法能有效处理认知不确定性，理论分析和数值实验验证了方法的有效性和理论性质。

Abstract: In this paper, we study the Bayesian risk-averse formulation in reinforcement
learning (RL). To address the epistemic uncertainty due to a lack of data, we
adopt the Bayesian Risk Markov Decision Process (BRMDP) to account for the
parameter uncertainty of the unknown underlying model. We derive the asymptotic
normality that characterizes the difference between the Bayesian risk value
function and the original value function under the true unknown distribution.
The results indicate that the Bayesian risk-averse approach tends to
pessimistically underestimate the original value function. This discrepancy
increases with stronger risk aversion and decreases as more data become
available. We then utilize this adaptive property in the setting of online RL
as well as online contextual multi-arm bandits (CMAB), a special case of online
RL. We provide two procedures using posterior sampling for both the general RL
problem and the CMAB problem. We establish a sub-linear regret bound, with the
regret defined as the conventional regret for both the RL and CMAB settings.
Additionally, we establish a sub-linear regret bound for the CMAB setting with
the regret defined as the Bayesian risk regret. Finally, we conduct numerical
experiments to demonstrate the effectiveness of the proposed algorithm in
addressing epistemic uncertainty and verifying the theoretical properties.

</details>


### [84] [Language models' activations linearly encode training-order recency](https://arxiv.org/abs/2509.14223)
*Dmitrii Krasheninnikov,Richard E. Turner,David Krueger*

Main category: cs.LG

TL;DR: 语言模型的激活值线性编码了信息在训练过程中被学习的时间顺序，通过分析模型激活可以准确区分早期和晚期学习的信息


<details>
  <summary>Details</summary>
Motivation: 研究语言模型是否以及如何在其激活中编码信息的学习时间顺序，这对于理解模型如何处理冲突数据和知识修改具有重要意义

Method: 通过顺序微调Llama-3.2-1B模型在六个不相交但相似的数据集上，分析测试样本的平均激活值在2D子空间中的排列顺序

Result: 发现激活质心按训练顺序精确排列在一条直线上，线性探针能以约90%准确率区分早期和晚期实体，模型微调后能以约80%准确率报告未见实体的训练阶段

Conclusion: 模型能够按获取时间区分信息，这一发现对理解模型如何处理冲突数据和知识修改具有重要启示意义

Abstract: We show that language models' activations linearly encode when information
was learned during training. Our setup involves creating a model with a known
training order by sequentially fine-tuning Llama-3.2-1B on six disjoint but
otherwise similar datasets about named entities. We find that the average
activations of test samples for the six training datasets encode the training
order: when projected into a 2D subspace, these centroids are arranged exactly
in the order of training and lie on a straight line. Further, we show that
linear probes can accurately (~90%) distinguish "early" vs. "late" entities,
generalizing to entities unseen during the probes' own training. The model can
also be fine-tuned to explicitly report an unseen entity's training stage (~80%
accuracy). Interestingly, this temporal signal does not seem attributable to
simple differences in activation magnitudes, losses, or model confidence. Our
paper demonstrates that models are capable of differentiating information by
its acquisition time, and carries significant implications for how they might
manage conflicting data and respond to knowledge modifications.

</details>


### [85] [Exploring the Relationship between Brain Hemisphere States and Frequency Bands through Deep Learning Optimization Techniques](https://arxiv.org/abs/2509.14078)
*Robiul Islam,Dmitry I. Ignatov,Karl Kaberg,Roman Nabatchikov*

Main category: cs.LG

TL;DR: 该研究比较了不同优化器和神经网络架构在EEG频段分类任务中的性能，发现Adagrad和RMSprop优化器表现最佳，CNN在空间特征提取方面表现优异，并通过SHAP分析揭示了EEG频段对分类准确性的贡献。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索不同优化器和神经网络架构在EEG频段分类中的性能差异，以及如何有效预测左右脑半球的分类任务，为神经影像分类任务提供优化策略。

Method: 使用TensorFlow和PyTorch框架实现三种神经网络架构（深度密集网络、浅层三层网络和CNN），比较多种优化器（Adagrad、RMSprop、Adadelta、SGD、FTRL）在不同EEG频段的性能，并采用SHAP进行特征重要性分析。

Result: Adagrad在beta频段表现最佳，RMSprop在gamma频段表现最优；CNN获得第二高准确率且擅长空间特征提取；深度密集网络在学习复杂模式方面有竞争力；浅层网络计算效率高但准确率较低。

Conclusion: 优化器选择、模型架构和EEG频段分析对提升分类器性能至关重要，研究为神经影像分类任务的特征重要性和模型优化提供了重要见解。

Abstract: This study investigates classifier performance across EEG frequency bands
using various optimizers and evaluates efficient class prediction for the left
and right hemispheres. Three neural network architectures - a deep dense
network, a shallow three-layer network, and a convolutional neural network
(CNN) - are implemented and compared using the TensorFlow and PyTorch
frameworks. Results indicate that the Adagrad and RMSprop optimizers
consistently perform well across different frequency bands, with Adadelta
exhibiting robust performance in cross-model evaluations. Specifically, Adagrad
excels in the beta band, while RMSprop achieves superior performance in the
gamma band. Conversely, SGD and FTRL exhibit inconsistent performance. Among
the models, the CNN demonstrates the second highest accuracy, particularly in
capturing spatial features of EEG data. The deep dense network shows
competitive performance in learning complex patterns, whereas the shallow
three-layer network, sometimes being less accurate, provides computational
efficiency. SHAP (Shapley Additive Explanations) plots are employed to identify
efficient class prediction, revealing nuanced contributions of EEG frequency
bands to model accuracy. Overall, the study highlights the importance of
optimizer selection, model architecture, and EEG frequency band analysis in
enhancing classifier performance and understanding feature importance in
neuroimaging-based classification tasks.

</details>


### [86] [From Distributional to Quantile Neural Basis Models: the case of Electricity Price Forecasting](https://arxiv.org/abs/2509.14113)
*Alessandro Brusaferri,Danial Ramin,Andrea Ballarino*

Main category: cs.LG

TL;DR: 提出了Quantile Neural Basis Model，将分位数广义可加模型的可解释性原理融入神经网络框架，在保持预测性能的同时提供模型行为洞察


<details>
  <summary>Details</summary>
Motivation: 虽然神经网络在多水平概率预测中取得了高精度，但理解特征条件输出的底层机制仍然是一个重大挑战，需要提高模型的可解释性

Method: 利用共享基分解和权重分解，避免参数分布假设，将Quantile Generalized Additive Models的可解释性原则整合到端到端神经网络训练框架中

Result: 在日前电价预测任务中验证，预测性能与分布回归和分位数回归神经网络相当，同时通过学习到的从输入特征到输出预测的非线性映射提供有价值的模型行为洞察

Conclusion: Quantile Neural Basis Model成功地将可解释性融入神经网络预测框架，在保持竞争力的预测性能的同时提供了对模型决策过程的理解

Abstract: While neural networks are achieving high predictive accuracy in multi-horizon
probabilistic forecasting, understanding the underlying mechanisms that lead to
feature-conditioned outputs remains a significant challenge for forecasters. In
this work, we take a further step toward addressing this critical issue by
introducing the Quantile Neural Basis Model, which incorporates the
interpretability principles of Quantile Generalized Additive Models into an
end-to-end neural network training framework. To this end, we leverage shared
basis decomposition and weight factorization, complementing Neural Models for
Location, Scale, and Shape by avoiding any parametric distributional
assumptions. We validate our approach on day-ahead electricity price
forecasting, achieving predictive performance comparable to distributional and
quantile regression neural networks, while offering valuable insights into
model behavior through the learned nonlinear mappings from input features to
output predictions across the horizon.

</details>


### [87] [Breaking the Cycle of Incarceration With Targeted Mental Health Outreach: A Case Study in Machine Learning for Public Policy](https://arxiv.org/abs/2509.14129)
*Kit T. Rodolfa,Erika Salomon,Jin Yao,Steve Yoder,Robert Sullivan,Kevin McGuire,Allie Dickinson,Rob MacDougall,Brian Seidler,Christina Sung,Claire Herdeman,Rayid Ghani*

Main category: cs.LG

TL;DR: 该研究通过机器学习预测高风险再监禁人群，并进行针对性心理健康干预，发现对最高风险群体干预效果最显著。


<details>
  <summary>Details</summary>
Motivation: 监狱系统难以有效处理囚犯的心理健康、药物依赖等复杂需求，导致再犯罪和监禁循环，特别是对有色人种社区影响更大，需要创新方法来打破这一循环。

Method: 使用预测建模方法识别高风险再监禁个体，设计并实施现场试验，进行针对性心理健康外展干预，评估模型预测能力和干预效果。

Result: 模型高度预测新监狱收容情况，最高风险群体中超过一半在一年内重返监狱。干预对最高风险个体最有效，显著影响心理健康服务使用、急救调度和刑事司法参与。

Conclusion: 针对性心理健康外展干预对最高再监禁风险人群效果最佳，为打破监禁循环提供了有效策略，特别是在改善心理健康服务和减少刑事司法参与方面。

Abstract: Many incarcerated individuals face significant and complex challenges,
including mental illness, substance dependence, and homelessness, yet jails and
prisons are often poorly equipped to address these needs. With little support
from the existing criminal justice system, these needs can remain untreated and
worsen, often leading to further offenses and a cycle of incarceration with
adverse outcomes both for the individual and for public safety, with
particularly large impacts on communities of color that continue to widen the
already extensive racial disparities in criminal justice outcomes. Responding
to these failures, a growing number of criminal justice stakeholders are
seeking to break this cycle through innovative approaches such as
community-driven and alternative approaches to policing, mentoring, community
building, restorative justice, pretrial diversion, holistic defense, and social
service connections. Here we report on a collaboration between Johnson County,
Kansas, and Carnegie Mellon University to perform targeted, proactive mental
health outreach in an effort to reduce reincarceration rates.
  This paper describes the data used, our predictive modeling approach and
results, as well as the design and analysis of a field trial conducted to
confirm our model's predictive power, evaluate the impact of this targeted
outreach, and understand at what level of reincarceration risk outreach might
be most effective. Through this trial, we find that our model is highly
predictive of new jail bookings, with more than half of individuals in the
trial's highest-risk group returning to jail in the following year. Outreach
was most effective among these highest-risk individuals, with impacts on mental
health utilization, EMS dispatches, and criminal justice involvement.

</details>


### [88] [A Compositional Kernel Model for Feature Learning](https://arxiv.org/abs/2509.14158)
*Feng Ruan,Keli Liu,Michael Jordan*

Main category: cs.LG

TL;DR: 该论文研究了一种组合核岭回归方法，通过坐标重加权进行特征学习，证明了在某些核函数下能够有效识别相关变量并消除噪声变量。


<details>
  <summary>Details</summary>
Motivation: 研究组合架构中的特征学习问题，特别是在核岭回归框架下如何通过坐标重加权来识别相关变量和消除噪声变量。

Method: 采用变分问题建模的组合核岭回归方法，分析全局最小值和驻点在不同核函数（如拉普拉斯核和高斯核）下的特征选择性能。

Result: 研究发现：1）全局最小值和驻点都能有效消除高斯分布噪声变量；2）拉普拉斯核等ℓ1型核能在驻点恢复非线性效应的特征；3）高斯核只能恢复线性特征。

Conclusion: 组合核岭回归为特征学习提供了有效的测试平台，不同核函数在特征选择方面表现出显著差异，ℓ1型核在捕捉非线性特征方面优于高斯核。

Abstract: We study a compositional variant of kernel ridge regression in which the
predictor is applied to a coordinate-wise reweighting of the inputs. Formulated
as a variational problem, this model provides a simple testbed for feature
learning in compositional architectures. From the perspective of variable
selection, we show how relevant variables are recovered while noise variables
are eliminated. We establish guarantees showing that both global minimizers and
stationary points discard noise coordinates when the noise variables are
Gaussian distributed. A central finding is that $\ell_1$-type kernels, such as
the Laplace kernel, succeed in recovering features contributing to nonlinear
effects at stationary points, whereas Gaussian kernels recover only linear
ones.

</details>


### [89] [Deconstructing Intraocular Pressure: A Non-invasive Multi-Stage Probabilistic Inverse Framework](https://arxiv.org/abs/2509.14167)
*Md Rezwan Jaher,Abul Mukid Mohammad Mukaddes,A. B. M. Abdul Malek*

Main category: cs.LG

TL;DR: 开发了一个端到端框架，通过AI架构和新型数据生成策略PCDS，从稀疏的常规数据中非侵入性地估计青光眼等疾病中无法测量的关键参数（如小梁网渗透性），解决了缺乏真实数据和计算成本高的逆问题挑战。


<details>
  <summary>Details</summary>
Motivation: 许多关键医疗决策因无法测量基础参数而受限，如青光眼中的眼内压主要决定因素小梁网渗透性无法在体内测量，临床依赖间接替代指标，同时缺乏真实数据和计算成本高阻碍了预测模型开发。

Method: 结合多阶段AI架构功能分离问题；提出PCDS数据生成策略，避免数十万次昂贵模拟，将有效计算时间从数年缩短至数小时；使用贝叶斯引擎量化预测不确定性。

Result: 非侵入性估计的流出设施与最先进的眼压测量技术高度一致，精度接近直接物理仪器；新推导的渗透性生物标志物在按疾病风险分层临床队列方面表现出高准确性。

Conclusion: 该框架为解决其他数据稀缺、计算密集型领域的类似逆问题建立了可推广的蓝图，具有重要的诊断潜力。

Abstract: Many critical healthcare decisions are challenged by the inability to measure
key underlying parameters. Glaucoma, a leading cause of irreversible blindness
driven by elevated intraocular pressure (IOP), provides a stark example. The
primary determinant of IOP, a tissue property called trabecular meshwork
permeability, cannot be measured in vivo, forcing clinicians to depend on
indirect surrogates. This clinical challenge is compounded by a broader
computational one: developing predictive models for such ill-posed inverse
problems is hindered by a lack of ground-truth data and prohibitive cost of
large-scale, high-fidelity simulations. We address both challenges with an
end-to-end framework to noninvasively estimate unmeasurable variables from
sparse, routine data. Our approach combines a multi-stage artificial
intelligence architecture to functionally separate the problem; a novel data
generation strategy we term PCDS that obviates the need for hundreds of
thousands of costly simulations, reducing the effective computational time from
years to hours; and a Bayesian engine to quantify predictive uncertainty. Our
framework deconstructs a single IOP measurement into its fundamental components
from routine inputs only, yielding estimates for the unmeasurable tissue
permeability and a patient's outflow facility. Our noninvasively estimated
outflow facility achieved excellent agreement with state-of-the-art tonography
with precision comparable to direct physical instruments. Furthermore, the
newly derived permeability biomarker demonstrates high accuracy in stratifying
clinical cohorts by disease risk, highlighting its diagnostic potential. More
broadly, our framework establishes a generalizable blueprint for solving
similar inverse problems in other data-scarce, computationally-intensive
domains.

</details>


### [90] [TopoSizing: An LLM-aided Framework of Topology-based Understanding and Sizing for AMS Circuits](https://arxiv.org/abs/2509.14169)
*Ziming Wei,Zichen Kong,Yuan Wang,David Z. Pan,Xiyuan Tang*

Main category: cs.LG

TL;DR: TopoSizing是一个端到端框架，通过图算法和LLM代理实现电路层次化理解，并将这些知识整合到贝叶斯优化中，提高模拟电路设计的效率和可行性


<details>
  <summary>Details</summary>
Motivation: 模拟和混合信号电路设计面临高质量数据短缺和领域知识难以嵌入自动化流程的挑战，传统黑盒优化缺乏电路理解，而学习型方法成本高且需要重新训练

Method: 首先使用图算法将电路组织成层次化的设备-模块-阶段表示，然后LLM代理执行假设-验证-精炼循环进行显式标注，最后将验证的洞察整合到贝叶斯优化中，通过LLM引导的初始采样和停滞触发的信任区域更新

Result: 该方法能够直接从原始网表进行稳健的电路理解，并将知识转化为优化增益

Conclusion: TopoSizing框架通过结合图算法和LLM代理，实现了对电路的深度理解，并有效提升了优化效率，同时保持了设计的可行性

Abstract: Analog and mixed-signal circuit design remains challenging due to the
shortage of high-quality data and the difficulty of embedding domain knowledge
into automated flows. Traditional black-box optimization achieves sampling
efficiency but lacks circuit understanding, which often causes evaluations to
be wasted in low-value regions of the design space. In contrast, learning-based
methods embed structural knowledge but are case-specific and costly to retrain.
Recent attempts with large language models show potential, yet they often rely
on manual intervention, limiting generality and transparency. We propose
TopoSizing, an end-to-end framework that performs robust circuit understanding
directly from raw netlists and translates this knowledge into optimization
gains. Our approach first applies graph algorithms to organize circuits into a
hierarchical device-module-stage representation. LLM agents then execute an
iterative hypothesis-verification-refinement loop with built-in consistency
checks, producing explicit annotations. Verified insights are integrated into
Bayesian optimization through LLM-guided initial sampling and
stagnation-triggered trust-region updates, improving efficiency while
preserving feasibility.

</details>


### [91] [TGPO: Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning](https://arxiv.org/abs/2509.14172)
*Ziyuan Chen,Zhenghui Zhao,Zhangye Han,Miancan Liu,Xianhang Ye,Yiqing Li,Hongbo Min,Jinkui Ren,Xiantao Zhang,Guitao Cao*

Main category: cs.LG

TL;DR: TGPO是一个离线强化学习框架，通过树形轨迹表示和过程奖励模型解决Web Agent训练中的信用分配、标注成本和奖励稀疏性问题，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和视觉语言模型的发展，使用大模型作为Web Agent进行自动化网页交互变得重要，但强化学习训练面临信用分配不当、标注成本高和奖励稀疏等关键挑战。

Method: 提出Tree-Guided Preference Optimization (TGPO)框架，采用树形轨迹表示合并语义相同的状态消除标签冲突，包含过程奖励模型自动生成细粒度奖励（通过子目标进度、冗余检测和动作验证），以及动态权重机制优先处理高影响力决策点。

Result: 在Online-Mind2Web和自建C-WebShop数据集上的实验表明，TGPO显著优于现有方法，以更少的冗余步骤实现了更高的成功率。

Conclusion: TGPO框架有效解决了Web Agent训练中的关键问题，通过创新的树形结构和过程奖励机制提升了训练效率和性能。

Abstract: With the rapid advancement of large language models and vision-language
models, employing large models as Web Agents has become essential for automated
web interaction. However, training Web Agents with reinforcement learning faces
critical challenges including credit assignment misallocation, prohibitively
high annotation costs, and reward sparsity. To address these issues, we propose
Tree-Guided Preference Optimization (TGPO), an offline reinforcement learning
framework that proposes a tree-structured trajectory representation merging
semantically identical states across trajectories to eliminate label conflicts.
Our framework incorporates a Process Reward Model that automatically generates
fine-grained rewards through subgoal progress, redundancy detection, and action
verification. Additionally, a dynamic weighting mechanism prioritizes
high-impact decision points during training. Experiments on Online-Mind2Web and
our self-constructed C-WebShop datasets demonstrate that TGPO significantly
outperforms existing methods, achieving higher success rates with fewer
redundant steps.

</details>


### [92] [Bridging Past and Future: Distribution-Aware Alignment for Time Series Forecasting](https://arxiv.org/abs/2509.14181)
*Yifan Hu,Jie Yang,Tian Zhou,Peiyuan Liu,Yujin Tang,Rong Jin,Liang Sun*

Main category: cs.LG

TL;DR: TimeAlign是一个轻量级的即插即用框架，通过简单的重构任务学习辅助特征来弥补时间序列输入历史与未来目标之间的分布差距，显著提升预测性能


<details>
  <summary>Details</summary>
Motivation: 现有对比学习等表示学习方法在时间序列预测中表现不佳，作者认为显式的表示对齐可以提供关键信息来弥合输入历史与未来目标之间的分布差距

Method: 提出TimeAlign框架，通过重构任务学习辅助特征，然后将这些特征反馈给任何基础预测器，该方法是架构无关的且计算开销很小

Result: 在8个基准测试上验证了其优越性能，研究表明性能提升主要来自于纠正历史输入与未来输出之间的频率不匹配

Conclusion: TimeAlign可以作为现代深度学习时间序列预测系统的通用对齐模块，理论分析表明其能有效增加学习表示与预测目标之间的互信息

Abstract: Representation learning techniques like contrastive learning have long been
explored in time series forecasting, mirroring their success in computer vision
and natural language processing. Yet recent state-of-the-art (SOTA) forecasters
seldom adopt these representation approaches because they have shown little
performance advantage. We challenge this view and demonstrate that explicit
representation alignment can supply critical information that bridges the
distributional gap between input histories and future targets. To this end, we
introduce TimeAlign, a lightweight, plug-and-play framework that learns
auxiliary features via a simple reconstruction task and feeds them back to any
base forecaster. Extensive experiments across eight benchmarks verify its
superior performance. Further studies indicate that the gains arises primarily
from correcting frequency mismatches between historical inputs and future
outputs. We also provide a theoretical justification for the effectiveness of
TimeAlign in increasing the mutual information between learned representations
and predicted targets. As it is architecture-agnostic and incurs negligible
overhead, TimeAlign can serve as a general alignment module for modern deep
learning time-series forecasting systems. The code is available at
https://github.com/TROUBADOUR000/TimeAlign.

</details>


### [93] [A Variational Framework for Residual-Based Adaptivity in Neural PDE Solvers and Operator Learning](https://arxiv.org/abs/2509.14198)
*Juan Diego Toscano,Daniel T. Chen,Vivek Oommen,George Em Karniadakis*

Main category: cs.LG

TL;DR: 提出了一个统一的变分框架来形式化基于残差的自适应策略，通过凸变换将离散化选择与误差度量直接关联，为科学机器学习提供了理论依据和系统化设计方法。


<details>
  <summary>Details</summary>
Motivation: 基于残差的自适应策略在科学机器学习中广泛使用但缺乏理论依据，需要建立一个统一框架来形式化这些方法并建立离散化与误差度量之间的理论联系。

Method: 引入变分框架，通过凸变换整合残差，不同变换对应不同的目标函数（如指数权重对应最小化均匀误差，线性权重对应最小化二次误差），将自适应加权等价于选择优化原始目标的采样分布。

Result: 该框架实现了三个好处：系统化设计跨范数的自适应方案、通过减少损失估计器方差来降低离散化误差、通过改善梯度信噪比来增强学习动态。在算子学习中展示了显著的性能提升。

Conclusion: 为基于残差的自适应性提供了理论依据，建立了离散化和训练策略的原则性基础，能够显著提升科学机器学习方法的性能和理论可靠性。

Abstract: Residual-based adaptive strategies are widely used in scientific machine
learning but remain largely heuristic. We introduce a unifying variational
framework that formalizes these methods by integrating convex transformations
of the residual. Different transformations correspond to distinct objective
functionals: exponential weights target the minimization of uniform error,
while linear weights recover the minimization of quadratic error. Within this
perspective, adaptive weighting is equivalent to selecting sampling
distributions that optimize the primal objective, thereby linking
discretization choices directly to error metrics. This principled approach
yields three benefits: (1) it enables systematic design of adaptive schemes
across norms, (2) reduces discretization error through variance reduction of
the loss estimator, and (3) enhances learning dynamics by improving the
gradient signal-to-noise ratio. Extending the framework to operator learning,
we demonstrate substantial performance gains across optimizers and
architectures. Our results provide a theoretical justification of
residual-based adaptivity and establish a foundation for principled
discretization and training strategies.

</details>


### [94] [A Universal Banach--Bregman Framework for Stochastic Iterations: Unifying Stochastic Mirror Descent, Learning and LLM Training](https://arxiv.org/abs/2509.14216)
*Johnny R. Zhang,Xiaomei Mi,Gaoyuan Du,Qianyi Sun,Shiqi Wang,Jiaxuan Li,Wenhua Zhou*

Main category: cs.LG

TL;DR: 该论文提出了一个开创性的Banach-Bregman随机优化框架，突破了传统Hilbert空间的限制，为下一代优化理论提供了统一的基础。


<details>
  <summary>Details</summary>
Motivation: 现有随机优化理论主要局限于Hilbert空间，无法有效处理非欧几里得设置，如单纯形上的镜像下降、稀疏学习的Bregman近端方法、信息几何中的自然梯度下降等。需要建立一个更通用的Banach空间优化框架。

Method: 提出了基于Bregman几何的统一模板，包括Bregman投影和Bregman-Fejer单调性，涵盖了随机逼近、镜像下降、自然梯度、自适应方法和镜像-近端方法。建立了非Hilbert设置中的超松弛方法（λ > 2），并提供了从几乎必然有界性到几何速率的收敛定理。

Result: 在机器学习（UCI基准测试）、深度学习（Transformer训练）、强化学习（actor-critic）和大语言模型（WikiText-2与distilGPT-2）上的实证研究表明，相比经典基线方法，收敛速度提升高达20%，方差减小，准确性提高。

Conclusion: Banach-Bregman几何成为统一优化理论和实践的核心基石，为跨AI核心范式的下一代优化提供了理论基础和实际应用价值。

Abstract: Stochastic optimization powers the scalability of modern artificial
intelligence, spanning machine learning, deep learning, reinforcement learning,
and large language model training. Yet, existing theory remains largely
confined to Hilbert spaces, relying on inner-product frameworks and
orthogonality. This paradigm fails to capture non-Euclidean settings, such as
mirror descent on simplices, Bregman proximal methods for sparse learning,
natural gradient descent in information geometry, or
Kullback--Leibler-regularized language model training. Unlike Euclidean-based
Hilbert-space methods, this approach embraces general Banach spaces. This work
introduces a pioneering Banach--Bregman framework for stochastic iterations,
establishing Bregman geometry as a foundation for next-generation optimization.
It (i) provides a unified template via Bregman projections and Bregman--Fejer
monotonicity, encompassing stochastic approximation, mirror descent, natural
gradient, adaptive methods, and mirror-prox; (ii) establishes super-relaxations
($\lambda > 2$) in non-Hilbert settings, enabling flexible geometries and
elucidating their acceleration effect; and (iii) delivers convergence theorems
spanning almost-sure boundedness to geometric rates, validated on synthetic and
real-world tasks. Empirical studies across machine learning (UCI benchmarks),
deep learning (e.g., Transformer training), reinforcement learning
(actor--critic), and large language models (WikiText-2 with distilGPT-2) show
up to 20% faster convergence, reduced variance, and enhanced accuracy over
classical baselines. These results position Banach--Bregman geometry as a
cornerstone unifying optimization theory and practice across core AI paradigms.

</details>


### [95] [Data Denoising and Derivative Estimation for Data-Driven Modeling of Nonlinear Dynamical Systems](https://arxiv.org/abs/2509.14219)
*Jiaqi Yao,Lewis Mitchell,John Maclean,Hemanth Saratchandran*

Main category: cs.LG

TL;DR: RKTV-INR：一种基于龙格-库塔积分和全变分的隐式神经表示去噪框架，用于非线性动力系统的噪声抑制和方程识别


<details>
  <summary>Details</summary>
Motivation: 非线性动力系统的数据驱动建模常受测量噪声影响，需要有效的去噪方法来准确恢复系统动力学方程

Method: 使用隐式神经表示（INR）直接拟合噪声观测数据，通过龙格-库塔积分和全变分约束确保重建状态是动力系统轨迹，并保持与原始数据的接近性

Result: 实验证明该方法能有效抑制噪声，提供精确的导数估计，并实现可靠的系统识别

Conclusion: RKTV-INR框架成功结合了神经表示和物理约束，为噪声环境下的非线性动力系统建模提供了有效的解决方案

Abstract: Data-driven modeling of nonlinear dynamical systems is often hampered by
measurement noise. We propose a denoising framework, called Runge-Kutta and
Total Variation Based Implicit Neural Representation (RKTV-INR), that
represents the state trajectory with an implicit neural representation (INR)
fitted directly to noisy observations. Runge-Kutta integration and total
variation are imposed as constraints to ensure that the reconstructed state is
a trajectory of a dynamical system that remains close to the original data. The
trained INR yields a clean, continuous trajectory and provides accurate
first-order derivatives via automatic differentiation. These denoised states
and derivatives are then supplied to Sparse Identification of Nonlinear
Dynamics (SINDy) to recover the governing equations. Experiments demonstrate
effective noise suppression, precise derivative estimation, and reliable system
identification.

</details>


### [96] [Defending Diffusion Models Against Membership Inference Attacks via Higher-Order Langevin Dynamics](https://arxiv.org/abs/2509.14225)
*Benjamin Sterling,Yousef El-Laham,Mónica F. Bugallo*

Main category: cs.LG

TL;DR: 本文提出使用临界阻尼高阶朗之万动力学来防御扩散模型对抗成员推理攻击的方法，通过在扩散过程中引入辅助变量来混合外部随机性，从而保护训练数据隐私。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI应用的快速发展，数据安全面临新的挑战。扩散模型虽然相比其他生成模型对成员推理攻击具有更强的内在抵抗性，但仍然存在被攻击的风险，需要有效的防御机制来保护训练数据的隐私。

Method: 采用临界阻尼高阶朗之万动力学，引入多个辅助变量和联合扩散过程。通过在扩散过程早期利用辅助变量混合外部随机性来破坏敏感输入数据，从而增强模型对成员推理攻击的防御能力。

Result: 在玩具数据集和语音数据集上进行了理论分析和实验验证，使用AUROC曲线和FID指标评估防御效果，证明了该方法的有效性。

Conclusion: 提出的基于高阶朗之万动力学的防御机制能够有效增强扩散模型对成员推理攻击的抵抗能力，为生成式AI的数据安全提供了新的保护方案。

Abstract: Recent advances in generative artificial intelligence applications have
raised new data security concerns. This paper focuses on defending diffusion
models against membership inference attacks. This type of attack occurs when
the attacker can determine if a certain data point was used to train the model.
Although diffusion models are intrinsically more resistant to membership
inference attacks than other generative models, they are still susceptible. The
defense proposed here utilizes critically-damped higher-order Langevin
dynamics, which introduces several auxiliary variables and a joint diffusion
process along these variables. The idea is that the presence of auxiliary
variables mixes external randomness that helps to corrupt sensitive input data
earlier on in the diffusion process. This concept is theoretically investigated
and validated on a toy dataset and a speech dataset using the Area Under the
Receiver Operating Characteristic (AUROC) curves and the FID metric.

</details>


### [97] [NIRVANA: Structured pruning reimagined for large language models compression](https://arxiv.org/abs/2509.14230)
*Mengting Ai,Tianxin Wei,Sirui Chen,Jingrui He*

Main category: cs.LG

TL;DR: NIRVANA是一种新颖的结构化剪枝方法，通过神经正切核理论和自适应稀疏分配机制，在保持零样本准确性的同时实现高效LLM压缩。


<details>
  <summary>Details</summary>
Motivation: 解决现有结构化剪枝方法在零样本设置下性能显著下降的问题，避免昂贵的恢复技术如监督微调或适配器插入。

Method: 基于Adam优化动态的神经正切核一阶显著性准则，结合跨层和模块的自适应稀疏分配机制，以及基于KL散度的校准数据选择策略。

Result: 在Llama3、Qwen和T5模型上的实验表明，NIRVANA在同等稀疏度约束下优于现有结构化剪枝方法。

Conclusion: NIRVANA提供了一个理论上有依据且实用的LLM压缩方法，平衡了零样本准确性保持和微调能力。

Abstract: Structured pruning of large language models (LLMs) offers substantial
efficiency improvements by removing entire hidden units, yet current approaches
often suffer from significant performance degradation, particularly in
zero-shot settings, and necessitate costly recovery techniques such as
supervised fine-tuning (SFT) or adapter insertion. To address these critical
shortcomings, we introduce NIRVANA, a novel pruning method explicitly designed
to balance immediate zero-shot accuracy preservation with robust fine-tuning
capability. Leveraging a first-order saliency criterion derived from the Neural
Tangent Kernel under Adam optimization dynamics, NIRVANA provides a
theoretically grounded pruning strategy that respects essential model training
behaviors. To further address the unique challenges posed by structured
pruning, NIRVANA incorporates an adaptive sparsity allocation mechanism across
layers and modules (attention vs. MLP), which adjusts pruning intensity between
modules in a globally balanced manner. Additionally, to mitigate the high
sensitivity of pruning decisions to calibration data quality, we propose a
simple yet effective KL divergence-based calibration data selection strategy,
ensuring more reliable and task-agnostic pruning outcomes. Comprehensive
experiments conducted on Llama3, Qwen, and T5 models demonstrate that NIRVANA
outperforms existing structured pruning methods under equivalent sparsity
constraints, providing a theoretically sound and practical approach to LLM
compression. The code is available at
https://github.com/iDEA-iSAIL-Lab-UIUC/NIRVANA.

</details>


### [98] [Compute as Teacher: Turning Inference Compute Into Reference-Free Supervision](https://arxiv.org/abs/2509.14234)
*Dulhan Jayalath,Shashwat Goel,Thomas Foster,Parag Jain,Suchin Gururangan,Cheng Zhang,Anirudh Goyal,Alan Schelten*

Main category: cs.LG

TL;DR: Compute as Teacher (CaT) 通过将推理时的探索转化为无参考监督，利用模型自身生成的多个rollout合成参考信号，在无真实标签的后训练中提供学习信号，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决后训练阶段缺乏真实标签时学习信号来源的问题，探索如何将推理时的额外计算转化为有效的监督信号。

Method: 通过并行rollout生成多个输出，使用冻结的初始策略作为锚点来协调冲突和遗漏，合成单一参考信号。在可验证任务中使用程序等价性，在不可验证任务中使用自提议的评分标准并由独立LLM评判。

Result: 在Gemma 3 4B、Qwen 3 4B和Llama 3.1 8B上显著提升性能（MATH-500上最高+27%，HealthBench上+12%）。结合强化学习（CaT-RL）后获得进一步增益（最高+33%和+30%）。

Conclusion: CaT成功将推理时计算转化为有效的教师信号，性能随rollout数量扩展，合成方法优于选择方法，训练后的策略甚至能超越初始教师信号。

Abstract: Where do learning signals come from when there is no ground truth in
post-training? We propose turning exploration into supervision through Compute
as Teacher (CaT), which converts the model's own exploration at inference-time
into reference-free supervision by synthesizing a single reference from a group
of parallel rollouts and then optimizing toward it. Concretely, the current
policy produces a group of rollouts; a frozen anchor (the initial policy)
reconciles omissions and contradictions to estimate a reference, turning extra
inference-time compute into a teacher signal. We turn this into rewards in two
regimes: (i) verifiable tasks use programmatic equivalence on final answers;
(ii) non-verifiable tasks use self-proposed rubrics-binary, auditable criteria
scored by an independent LLM judge, with reward given by the fraction
satisfied. Unlike selection methods (best-of-N, majority, perplexity, or judge
scores), synthesis may disagree with the majority and be correct even when all
rollouts are wrong; performance scales with the number of rollouts. As a
test-time procedure, CaT improves Gemma 3 4B, Qwen 3 4B, and Llama 3.1 8B (up
to +27% on MATH-500; +12% on HealthBench). With reinforcement learning
(CaT-RL), we obtain further gains (up to +33% and +30%), with the trained
policy surpassing the initial teacher signal.

</details>
