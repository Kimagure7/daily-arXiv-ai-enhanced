{"id": "2509.25494", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.25494", "abs": "https://arxiv.org/abs/2509.25494", "authors": ["Nick Hagar", "Nicholas Diakopoulos", "Jeremy Gilbert"], "title": "On-Premise AI for the Newsroom: Evaluating Small Language Models for Investigative Document Search", "comment": "Accepted to Computation + Journalism Symposium 2025", "summary": "Investigative journalists routinely confront large document collections.\nLarge language models (LLMs) with retrieval-augmented generation (RAG)\ncapabilities promise to accelerate the process of document discovery, but\nnewsroom adoption remains limited due to hallucination risks, verification\nburden, and data privacy concerns. We present a journalist-centered approach to\nLLM-powered document search that prioritizes transparency and editorial control\nthrough a five-stage pipeline -- corpus summarization, search planning,\nparallel thread execution, quality evaluation, and synthesis -- using small,\nlocally-deployable language models that preserve data security and maintain\ncomplete auditability through explicit citation chains. Evaluating three\nquantized models (Gemma 3 12B, Qwen 3 14B, and GPT-OSS 20B) on two corpora, we\nfind substantial variation in reliability. All models achieved high citation\nvalidity and ran effectively on standard desktop hardware (e.g., 24 GB of\nmemory), demonstrating feasibility for resource-constrained newsrooms. However,\nsystematic challenges emerged, including error propagation through multi-stage\nsynthesis and dramatic performance variation based on training data overlap\nwith corpus content. These findings suggest that effective newsroom AI\ndeployment requires careful model selection and system design, alongside human\noversight for maintaining standards of accuracy and accountability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u65b0\u95fb\u5de5\u4f5c\u8005\u7684LLM\u6587\u6863\u641c\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e94\u9636\u6bb5\u6d41\u7a0b\uff08\u8bed\u6599\u5e93\u6458\u8981\u3001\u641c\u7d22\u89c4\u5212\u3001\u5e76\u884c\u7ebf\u7a0b\u6267\u884c\u3001\u8d28\u91cf\u8bc4\u4f30\u548c\u5408\u6210\uff09\u4f7f\u7528\u5c0f\u578b\u672c\u5730\u90e8\u7f72\u6a21\u578b\uff0c\u786e\u4fdd\u6570\u636e\u5b89\u5168\u548c\u53ef\u5ba1\u8ba1\u6027\u3002", "motivation": "\u89e3\u51b3\u65b0\u95fb\u7f16\u8f91\u5ba4\u91c7\u7528LLM\u8fdb\u884c\u6587\u6863\u53d1\u73b0\u65f6\u9762\u4e34\u7684\u5e7b\u89c9\u98ce\u9669\u3001\u9a8c\u8bc1\u8d1f\u62c5\u548c\u6570\u636e\u9690\u79c1\u95ee\u9898\uff0c\u4e3a\u8bb0\u8005\u63d0\u4f9b\u900f\u660e\u53ef\u63a7\u7684AI\u5de5\u5177\u3002", "method": "\u4f7f\u7528\u5c0f\u578b\u672c\u5730\u90e8\u7f72\u8bed\u8a00\u6a21\u578b\uff08Gemma 3 12B\u3001Qwen 3 14B\u548cGPT-OSS 20B\uff09\uff0c\u6784\u5efa\u4e94\u9636\u6bb5\u7ba1\u9053\uff1a\u8bed\u6599\u5e93\u6458\u8981\u3001\u641c\u7d22\u89c4\u5212\u3001\u5e76\u884c\u7ebf\u7a0b\u6267\u884c\u3001\u8d28\u91cf\u8bc4\u4f30\u548c\u5408\u6210\uff0c\u901a\u8fc7\u663e\u5f0f\u5f15\u7528\u94fe\u786e\u4fdd\u5b8c\u5168\u53ef\u5ba1\u8ba1\u6027\u3002", "result": "\u6240\u6709\u6a21\u578b\u90fd\u5b9e\u73b0\u4e86\u9ad8\u5f15\u7528\u6709\u6548\u6027\uff0c\u53ef\u5728\u6807\u51c6\u684c\u9762\u786c\u4ef6\uff08\u598224GB\u5185\u5b58\uff09\u4e0a\u6709\u6548\u8fd0\u884c\uff0c\u4f46\u5b58\u5728\u9519\u8bef\u4f20\u64ad\u548c\u591a\u9636\u6bb5\u5408\u6210\u4e2d\u7684\u6027\u80fd\u53d8\u5316\u95ee\u9898\uff0c\u6027\u80fd\u53d7\u8bad\u7ec3\u6570\u636e\u4e0e\u8bed\u6599\u5185\u5bb9\u91cd\u53e0\u5ea6\u5f71\u54cd\u663e\u8457\u3002", "conclusion": "\u6709\u6548\u7684\u65b0\u95fb\u7f16\u8f91\u5ba4AI\u90e8\u7f72\u9700\u8981\u4ed4\u7ec6\u7684\u6a21\u578b\u9009\u62e9\u548c\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u4ee5\u53ca\u4eba\u5de5\u76d1\u7763\u6765\u4fdd\u6301\u51c6\u786e\u6027\u548c\u95ee\u8d23\u6807\u51c6\u3002"}}
{"id": "2509.25602", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.25602", "abs": "https://arxiv.org/abs/2509.25602", "authors": ["Mouly Dewan", "Jiqun Liu", "Chirag Shah"], "title": "TRUE: A Reproducible Framework for LLM-Driven Relevance Judgment in Information Retrieval", "comment": null, "summary": "LLM-based relevance judgment generation has become a crucial approach in\nadvancing evaluation methodologies in Information Retrieval (IR). It has\nprogressed significantly, often showing high correlation with human judgments\nas reflected in LLMJudge leaderboards \\cite{rahmani2025judging}. However,\nexisting methods for relevance judgments, rely heavily on sensitive prompting\nstrategies, lacking standardized workflows for generating reliable labels. To\nfill this gap, we reintroduce our method, \\textit{Task-aware Rubric-based\nEvaluation} (TRUE), for relevance judgment generation. Originally developed for\nusefulness evaluation in search sessions, we extend TRUE to mitigate the gap in\nrelevance judgment due to its demonstrated effectiveness and reproducible\nworkflow. This framework leverages iterative data sampling and reasoning to\nevaluate relevance judgments across multiple factors including intent,\ncoverage, specificity, accuracy and usefulness. In this paper, we evaluate TRUE\non the TREC DL 2019, 2020 and LLMJudge datasets and our results show that TRUE\nachieves strong performance on the system-ranking LLM leaderboards. The primary\nfocus of this work is to provide a reproducible framework for LLM-based\nrelevance judgments, and we further analyze the effectiveness of TRUE across\nmultiple dimensions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TRUE\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u57fa\u4e8eLLM\u7684\u76f8\u5173\u6027\u5224\u65ad\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u6807\u51c6\u5316\u5de5\u4f5c\u6d41\u7a0b\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u76f8\u5173\u6027\u5224\u65ad\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u654f\u611f\u63d0\u793a\u7b56\u7565\uff0c\u7f3a\u4e4f\u751f\u6210\u53ef\u9760\u6807\u7b7e\u7684\u6807\u51c6\u5316\u5de5\u4f5c\u6d41\u7a0b\u3002", "method": "\u91c7\u7528\u4efb\u52a1\u611f\u77e5\u7684\u57fa\u4e8e\u6807\u51c6\u7684\u8bc4\u4f30(TRUE)\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u6570\u636e\u91c7\u6837\u548c\u63a8\u7406\uff0c\u4ece\u610f\u56fe\u3001\u8986\u76d6\u8303\u56f4\u3001\u7279\u5f02\u6027\u3001\u51c6\u786e\u6027\u548c\u6709\u7528\u6027\u7b49\u591a\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u76f8\u5173\u6027\u5224\u65ad\u3002", "result": "\u5728TREC DL 2019\u30012020\u548cLLMJudge\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cTRUE\u5728\u7cfb\u7edf\u6392\u540dLLM\u6392\u884c\u699c\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "TRUE\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u590d\u73b0\u7684LLM\u76f8\u5173\u6027\u5224\u65ad\u6846\u67b6\uff0c\u5e76\u5728\u591a\u4e2a\u7ef4\u5ea6\u4e0a\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2509.25755", "categories": ["cs.IR", "cs.SI"], "pdf": "https://arxiv.org/pdf/2509.25755", "abs": "https://arxiv.org/abs/2509.25755", "authors": ["Ruiqi Luo", "Ran Jin", "Zhenglong Li", "Kaixi Hu", "Xiaohui Tao", "Lin Li"], "title": "HiFIRec: Towards High-Frequency yet Low-Intention Behaviors for Multi-Behavior Recommendation", "comment": null, "summary": "Multi-behavior recommendation leverages multiple types of user-item\ninteractions to address data sparsity and cold-start issues, providing\npersonalized services in domains such as healthcare and e-commerce. Most\nexisting methods utilize graph neural networks to model user intention in a\nunified manner, which inadequately considers the heterogeneity across different\nbehaviors. Especially, high-frequency yet low-intention behaviors may\nimplicitly contain noisy signals, and frequent patterns that are plausible\nwhile misleading, thereby hindering the learning of user intentions. To this\nend, this paper proposes a novel multi-behavior recommendation method, HiFIRec,\nthat corrects the effect of high-frequency yet low-intention behaviors by\ndifferential behavior modeling. To revise the noisy signals, we hierarchically\nsuppress it across layers by extracting neighborhood information through\nlayer-wise neighborhood aggregation and further capturing user intentions\nthrough adaptive cross-layer feature fusion. To correct plausible frequent\npatterns, we propose an intensity-aware non-sampling strategy that dynamically\nadjusts the weights of negative samples. Extensive experiments on two\nbenchmarks show that HiFIRec relatively improves HR@10 by 4.21%-6.81% over\nseveral state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51faHiFIRec\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u884c\u4e3a\u5efa\u6a21\u89e3\u51b3\u591a\u884c\u4e3a\u63a8\u8350\u4e2d\u9ad8\u9891\u4f4e\u610f\u56fe\u884c\u4e3a\u7684\u566a\u58f0\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u63a8\u8350\u6027\u80fd", "motivation": "\u73b0\u6709\u591a\u884c\u4e3a\u63a8\u8350\u65b9\u6cd5\u7edf\u4e00\u5efa\u6a21\u7528\u6237\u610f\u56fe\uff0c\u672a\u80fd\u5145\u5206\u8003\u8651\u4e0d\u540c\u884c\u4e3a\u95f4\u7684\u5f02\u8d28\u6027\uff0c\u7279\u522b\u662f\u9ad8\u9891\u4f4e\u610f\u56fe\u884c\u4e3a\u53ef\u80fd\u5305\u542b\u566a\u58f0\u4fe1\u53f7\u548c\u8bef\u5bfc\u6027\u9891\u7e41\u6a21\u5f0f\uff0c\u963b\u788d\u7528\u6237\u610f\u56fe\u5b66\u4e60", "method": "\u91c7\u7528\u5206\u5c42\u884c\u4e3a\u5efa\u6a21\uff1a\u901a\u8fc7\u5c42\u95f4\u90bb\u57df\u805a\u5408\u63d0\u53d6\u90bb\u57df\u4fe1\u606f\uff0c\u81ea\u9002\u5e94\u8de8\u5c42\u7279\u5f81\u878d\u5408\u6355\u6349\u7528\u6237\u610f\u56fe\uff1b\u63d0\u51fa\u5f3a\u5ea6\u611f\u77e5\u975e\u91c7\u6837\u7b56\u7565\u52a8\u6001\u8c03\u6574\u8d1f\u6837\u672c\u6743\u91cd", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cHiFIRec\u76f8\u6bd4\u591a\u4e2a\u6700\u5148\u8fdb\u65b9\u6cd5\uff0cHR@10\u6307\u6807\u76f8\u5bf9\u63d0\u53474.21%-6.81%", "conclusion": "HiFIRec\u901a\u8fc7\u5206\u5c42\u6291\u5236\u9ad8\u9891\u4f4e\u610f\u56fe\u884c\u4e3a\u7684\u566a\u58f0\u5f71\u54cd\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u884c\u4e3a\u63a8\u8350\u7684\u6027\u80fd"}}
{"id": "2509.25803", "categories": ["cs.IR", "cs.AI", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25803", "abs": "https://arxiv.org/abs/2509.25803", "authors": ["Wanying Ding", "Savinay Narendra", "Xiran Shi", "Adwait Ratnaparkhi", "Chengrui Yang", "Nikoo Sabzevar", "Ziyan Yin"], "title": "Better with Less: Small Proprietary Models Surpass Large Language Models in Financial Transaction Understanding", "comment": "9 pages, 5 figures", "summary": "Analyzing financial transactions is crucial for ensuring regulatory\ncompliance, detecting fraud, and supporting decisions. The complexity of\nfinancial transaction data necessitates advanced techniques to extract\nmeaningful insights and ensure accurate analysis. Since Transformer-based\nmodels have shown outstanding performance across multiple domains, this paper\nseeks to explore their potential in understanding financial transactions. This\npaper conducts extensive experiments to evaluate three types of Transformer\nmodels: Encoder-Only, Decoder-Only, and Encoder-Decoder models. For each type,\nwe explore three options: pretrained LLMs, fine-tuned LLMs, and small\nproprietary models developed from scratch. Our analysis reveals that while\nLLMs, such as LLaMA3-8b, Flan-T5, and SBERT, demonstrate impressive\ncapabilities in various natural language processing tasks, they do not\nsignificantly outperform small proprietary models in the specific context of\nfinancial transaction understanding. This phenomenon is particularly evident in\nterms of speed and cost efficiency. Proprietary models, tailored to the unique\nrequirements of transaction data, exhibit faster processing times and lower\noperational costs, making them more suitable for real-time applications in the\nfinancial sector. Our findings highlight the importance of model selection\nbased on domain-specific needs and underscore the potential advantages of\ncustomized proprietary models over general-purpose LLMs in specialized\napplications. Ultimately, we chose to implement a proprietary decoder-only\nmodel to handle the complex transactions that we previously couldn't manage.\nThis model can help us to improve 14% transaction coverage, and save more than\n\\$13 million annual cost.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u4e09\u79cdTransformer\u6a21\u578b\u5728\u91d1\u878d\u4ea4\u6613\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u9488\u5bf9\u7279\u5b9a\u9886\u57df\u5f00\u53d1\u7684\u4e13\u6709\u5c0f\u6a21\u578b\u5728\u901f\u5ea6\u548c\u6210\u672c\u6548\u7387\u4e0a\u4f18\u4e8e\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6700\u7ec8\u91c7\u7528\u4e13\u6709\u89e3\u7801\u5668\u6a21\u578b\u5b9e\u73b0\u4e8614%\u7684\u4ea4\u6613\u8986\u76d6\u63d0\u5347\u548c1300\u4e07\u7f8e\u5143\u7684\u5e74\u6210\u672c\u8282\u7ea6\u3002", "motivation": "\u91d1\u878d\u4ea4\u6613\u5206\u6790\u5bf9\u76d1\u7ba1\u5408\u89c4\u3001\u6b3a\u8bc8\u68c0\u6d4b\u548c\u51b3\u7b56\u652f\u6301\u81f3\u5173\u91cd\u8981\u3002\u9274\u4e8eTransformer\u6a21\u578b\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5176\u5728\u91d1\u878d\u4ea4\u6613\u7406\u89e3\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u5bf9\u4e09\u79cdTransformer\u6a21\u578b\u7c7b\u578b\uff08\u4ec5\u7f16\u7801\u5668\u3001\u4ec5\u89e3\u7801\u5668\u3001\u7f16\u7801\u5668-\u89e3\u7801\u5668\uff09\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u6bcf\u79cd\u7c7b\u578b\u8bc4\u4f30\u9884\u8bad\u7ec3LLM\u3001\u5fae\u8c03LLM\u548c\u4ece\u5934\u5f00\u53d1\u7684\u4e13\u6709\u5c0f\u6a21\u578b\u4e09\u79cd\u9009\u9879\u3002", "result": "LLaMA3-8b\u3001Flan-T5\u548cSBERT\u7b49LLM\u5728\u91d1\u878d\u4ea4\u6613\u7406\u89e3\u4efb\u52a1\u4e2d\u5e76\u672a\u663e\u8457\u4f18\u4e8e\u4e13\u6709\u5c0f\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u901f\u5ea6\u548c\u6210\u672c\u6548\u7387\u65b9\u9762\u3002\u4e13\u6709\u6a21\u578b\u5904\u7406\u901f\u5ea6\u66f4\u5feb\u3001\u8fd0\u8425\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "\u57fa\u4e8e\u9886\u57df\u7279\u5b9a\u9700\u6c42\u8fdb\u884c\u6a21\u578b\u9009\u62e9\u81f3\u5173\u91cd\u8981\uff0c\u5728\u4e13\u4e1a\u5e94\u7528\u4e2d\u5b9a\u5236\u5316\u7684\u4e13\u6709\u6a21\u578b\u76f8\u6bd4\u901a\u7528LLM\u5177\u6709\u6f5c\u5728\u4f18\u52bf\u3002\u6700\u7ec8\u91c7\u7528\u4e13\u6709\u4ec5\u89e3\u7801\u5668\u6a21\u578b\u5904\u7406\u590d\u6742\u4ea4\u6613\uff0c\u5b9e\u73b014%\u4ea4\u6613\u8986\u76d6\u63d0\u5347\u548c1300\u4e07\u7f8e\u5143\u5e74\u6210\u672c\u8282\u7ea6\u3002"}}
{"id": "2509.25839", "categories": ["cs.IR", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.25839", "abs": "https://arxiv.org/abs/2509.25839", "authors": ["Han Zhang", "Dongfang Zhao"], "title": "RAE: A Neural Network Dimensionality Reduction Method for Nearest Neighbors Preservation in Vector Search", "comment": "submitted to ICLR 2026", "summary": "While high-dimensional embedding vectors are being increasingly employed in\nvarious tasks like Retrieval-Augmented Generation and Recommendation Systems,\npopular dimensionality reduction (DR) methods such as PCA and UMAP have rarely\nbeen adopted for accelerating the retrieval process due to their inability of\npreserving the nearest neighbor (NN) relationship among vectors. Empowered by\nneural networks' optimization capability and the bounding effect of Rayleigh\nquotient, we propose a Regularized Auto-Encoder (RAE) for k-NN preserving\ndimensionality reduction. RAE constrains the network parameter variation\nthrough regularization terms, adjusting singular values to control embedding\nmagnitude changes during reduction, thus preserving k-NN relationships. We\nprovide a rigorous mathematical analysis demonstrating that regularization\nestablishes an upper bound on the norm distortion rate of transformed vectors,\nthereby offering provable guarantees for k-NN preservation. With modest\ntraining overhead, RAE achieves superior k-NN recall compared to existing DR\napproaches while maintaining fast retrieval efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6b63\u5219\u5316\u81ea\u7f16\u7801\u5668(RAE)\u7528\u4e8ek-NN\u4fdd\u6301\u7684\u964d\u7ef4\u65b9\u6cd5\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u80fd\u529b\u548c\u745e\u5229\u5546\u754c\u7684\u7ea6\u675f\uff0c\u5728\u964d\u7ef4\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u6700\u8fd1\u90bb\u5173\u7cfb\uff0c\u76f8\u6bd4\u4f20\u7edf\u964d\u7ef4\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684k-NN\u53ec\u56de\u7387\u548c\u68c0\u7d22\u6548\u7387\u3002", "motivation": "\u9ad8\u7ef4\u5d4c\u5165\u5411\u91cf\u5728RAG\u548c\u63a8\u8350\u7cfb\u7edf\u7b49\u4efb\u52a1\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u4f20\u7edf\u964d\u7ef4\u65b9\u6cd5\u5982PCA\u548cUMAP\u7531\u4e8e\u65e0\u6cd5\u4fdd\u6301\u5411\u91cf\u95f4\u7684\u6700\u8fd1\u90bb\u5173\u7cfb\uff0c\u5f88\u5c11\u88ab\u7528\u4e8e\u52a0\u901f\u68c0\u7d22\u8fc7\u7a0b\u3002", "method": "\u4f7f\u7528\u6b63\u5219\u5316\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u6b63\u5219\u5316\u9879\u7ea6\u675f\u7f51\u7edc\u53c2\u6570\u53d8\u5316\uff0c\u8c03\u6574\u5947\u5f02\u503c\u6765\u63a7\u5236\u5d4c\u5165\u5411\u91cf\u5728\u964d\u7ef4\u8fc7\u7a0b\u4e2d\u7684\u5e45\u5ea6\u53d8\u5316\uff0c\u4ece\u800c\u4fdd\u6301k-NN\u5173\u7cfb\u3002", "result": "RAE\u5728\u9002\u5ea6\u8bad\u7ec3\u5f00\u9500\u4e0b\uff0c\u76f8\u6bd4\u73b0\u6709\u964d\u7ef4\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684k-NN\u53ec\u56de\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5feb\u901f\u7684\u68c0\u7d22\u6548\u7387\u3002", "conclusion": "\u901a\u8fc7\u4e25\u683c\u7684\u6570\u5b66\u5206\u6790\u8bc1\u660e\u6b63\u5219\u5316\u5efa\u7acb\u4e86\u53d8\u6362\u5411\u91cf\u8303\u6570\u5931\u771f\u7387\u7684\u4e0a\u754c\uff0c\u4e3ak-NN\u4fdd\u6301\u63d0\u4f9b\u4e86\u53ef\u8bc1\u660e\u7684\u4fdd\u8bc1\uff0cRAE\u662f\u4e00\u79cd\u6709\u6548\u7684k-NN\u4fdd\u6301\u964d\u7ef4\u65b9\u6cd5\u3002"}}
{"id": "2509.26063", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.26063", "abs": "https://arxiv.org/abs/2509.26063", "authors": ["Guoqing Hu", "An Zhang. Shuchang Liu", "Wenyu Mao", "Jiancan Wu", "Xun Yang", "Xiang Li", "Lantao Hu", "Han Li", "Kun Gai", "Xiang Wang"], "title": "Fading to Grow: Growing Preference Ratios via Preference Fading Discrete Diffusion for Recommendation", "comment": null, "summary": "Recommenders aim to rank items from a discrete item corpus in line with user\ninterests, yet suffer from extremely sparse user preference data. Recent\nadvances in diffusion models have inspired diffusion-based recommenders, which\nalleviate sparsity by injecting noise during a forward process to prevent the\ncollapse of perturbed preference distributions. However, current\ndiffusion-based recommenders predominantly rely on continuous Gaussian noise,\nwhich is intrinsically mismatched with the discrete nature of user preference\ndata in recommendation. In this paper, building upon recent advances in\ndiscrete diffusion, we propose PreferGrow, a discrete diffusion-based\nrecommender system that models preference ratios by fading and growing user\npreferences over the discrete item corpus. PreferGrow differs from existing\ndiffusion-based recommenders in three core aspects: (1) Discrete modeling of\npreference ratios: PreferGrow models relative preference ratios between item\npairs, rather than operating in the item representation or raw score simplex.\nThis formulation aligns naturally with the discrete and ranking-oriented nature\nof recommendation tasks. (2) Perturbing via preference fading: Instead of\ninjecting continuous noise, PreferGrow fades user preferences by replacing the\npreferred item with alternatives -- physically akin to negative sampling --\nthereby eliminating the need for any prior noise assumption. (3) Preference\nreconstruction via growing: PreferGrow reconstructs user preferences by\niteratively growing the preference signals from the estimated ratios.\nPreferGrow offers a well-defined matrix-based formulation with theoretical\nguarantees on Markovianity and reversibility, and it demonstrates consistent\nperformance gains over state-of-the-art diffusion-based recommenders across\nfive benchmark datasets, highlighting both its theoretical soundness and\nempirical effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86PreferGrow\uff0c\u4e00\u79cd\u57fa\u4e8e\u79bb\u6563\u6269\u6563\u7684\u63a8\u8350\u7cfb\u7edf\uff0c\u901a\u8fc7\u504f\u597d\u6de1\u5316\u548c\u589e\u957f\u6765\u5efa\u6a21\u7528\u6237\u504f\u597d\u6bd4\u7387\uff0c\u89e3\u51b3\u4e86\u8fde\u7eed\u9ad8\u65af\u566a\u58f0\u4e0e\u79bb\u6563\u7528\u6237\u504f\u597d\u6570\u636e\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u63a8\u8350\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u8fde\u7eed\u9ad8\u65af\u566a\u58f0\uff0c\u8fd9\u4e0e\u63a8\u8350\u4e2d\u7528\u6237\u504f\u597d\u6570\u636e\u7684\u79bb\u6563\u6027\u8d28\u4e0d\u5339\u914d\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7b26\u5408\u63a8\u8350\u4efb\u52a1\u79bb\u6563\u548c\u6392\u5e8f\u5bfc\u5411\u7279\u6027\u7684\u65b9\u6cd5\u3002", "method": "PreferGrow\u91c7\u7528\u79bb\u6563\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u504f\u597d\u6de1\u5316\uff08\u7528\u66ff\u4ee3\u9879\u66ff\u6362\u9996\u9009\u9879\uff09\u548c\u504f\u597d\u589e\u957f\uff08\u4ece\u4f30\u8ba1\u6bd4\u7387\u8fed\u4ee3\u91cd\u5efa\u504f\u597d\uff09\u6765\u5efa\u6a21\u9879\u76ee\u5bf9\u4e4b\u95f4\u7684\u76f8\u5bf9\u504f\u597d\u6bd4\u7387\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cPreferGrow\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u6269\u6563\u7684\u63a8\u8350\u7cfb\u7edf\u8868\u73b0\u51fa\u6301\u7eed\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u5176\u7406\u8bba\u4e25\u8c28\u6027\u548c\u5b9e\u8bc1\u6709\u6548\u6027\u3002", "conclusion": "PreferGrow\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9a\u4e49\u826f\u597d\u7684\u57fa\u4e8e\u77e9\u9635\u7684\u516c\u5f0f\uff0c\u5177\u6709\u9a6c\u5c14\u53ef\u592b\u6027\u548c\u53ef\u9006\u6027\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u4e3a\u79bb\u6563\u63a8\u8350\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u5408\u9002\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.26107", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.26107", "abs": "https://arxiv.org/abs/2509.26107", "authors": ["Huanyu Zhang", "Xiaoxuan Shen", "Yu Lei", "Baolin Yi", "Jianfang Liu", "Yinao xie"], "title": "Items Proxy Bridging: Enabling Frictionless Critiquing in Knowledge Graph Recommendations", "comment": null, "summary": "Modern recommender systems place great inclination towards facilitating user\nexperience, as more applications enabling users to critique and then refine\nrecommendations immediately. Considering the real-time requirements,\ncritique-able recommender systems typically straight modify the model\nparameters and update the recommend list through analyzing the user critiquing\nkeyphrases in the inference phase. Current critiquing methods require first\nconstructing a specially designated model which establish direct correlations\nbetween users and keyphrases during the training phase allowing for innovative\nrecommendations upon the critiquing,restricting the applicable scenarios.\nAdditionally, all these approaches ignore the catastrophic forgetting problem,\nwhere the cumulative changes in parameters during continuous multi-step\ncritiquing may lead to a collapse in model performance. Thus, We conceptualize\na proxy bridging users and keyphrases, proposing a streamlined yet potent Items\nProxy Generic Critiquing Framework (IPGC) framework, which can serve as a\nuniversal plugin for most knowledge graph recommender models based on\ncollaborative filtering (CF) strategies. IPGC provides a new paradigm for\nfrictionless integration of critique mechanisms to enable iterative\nrecommendation refinement in mainstream recommendation scenarios. IPGC\ndescribes the items proxy mechanism for transforming the critiquing\noptimization objective of user-keyphrase pairs into user-item pairs, adapting\nit for general CF recommender models without the necessity of specifically\ndesigned user-keyphrase correlation module. Furthermore, an anti-forgetting\nregularizer is introduced in order to efficiently mitigate the catastrophic\nforgetting problem of the model as a prior for critiquing optimization.", "AI": {"tldr": "\u63d0\u51fa\u4e86IPGC\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u901a\u7528\u7684\u53ef\u6279\u5224\u63a8\u8350\u7cfb\u7edf\u63d2\u4ef6\uff0c\u901a\u8fc7\u7269\u54c1\u4ee3\u7406\u673a\u5236\u5c06\u7528\u6237-\u5173\u952e\u8bcd\u5bf9\u7684\u6279\u5224\u4f18\u5316\u76ee\u6807\u8f6c\u6362\u4e3a\u7528\u6237-\u7269\u54c1\u5bf9\uff0c\u9002\u7528\u4e8e\u5927\u591a\u6570\u57fa\u4e8e\u534f\u540c\u8fc7\u6ee4\u7684\u77e5\u8bc6\u56fe\u8c31\u63a8\u8350\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u6297\u9057\u5fd8\u6b63\u5219\u5316\u5668\u89e3\u51b3\u8fde\u7eed\u591a\u6b65\u6279\u5224\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u53ef\u6279\u5224\u63a8\u8350\u7cfb\u7edf\u9700\u8981\u4e13\u95e8\u6784\u5efa\u7528\u6237\u4e0e\u5173\u952e\u8bcd\u76f4\u63a5\u5173\u8054\u7684\u6a21\u578b\uff0c\u9650\u5236\u4e86\u5e94\u7528\u573a\u666f\uff0c\u4e14\u5ffd\u89c6\u4e86\u8fde\u7eed\u591a\u6b65\u6279\u5224\u4e2d\u53c2\u6570\u7d2f\u79ef\u53d8\u5316\u5bfc\u81f4\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "method": "IPGC\u6846\u67b6\u91c7\u7528\u7269\u54c1\u4ee3\u7406\u673a\u5236\uff0c\u5c06\u7528\u6237-\u5173\u952e\u8bcd\u5bf9\u7684\u6279\u5224\u4f18\u5316\u8f6c\u6362\u4e3a\u7528\u6237-\u7269\u54c1\u5bf9\u4f18\u5316\uff0c\u4f7f\u5176\u80fd\u4f5c\u4e3a\u901a\u7528\u63d2\u4ef6\u96c6\u6210\u5230\u4e3b\u6d41\u63a8\u8350\u6a21\u578b\u4e2d\uff0c\u540c\u65f6\u5f15\u5165\u6297\u9057\u5fd8\u6b63\u5219\u5316\u5668\u9632\u6b62\u6a21\u578b\u6027\u80fd\u5d29\u6e83\u3002", "result": "IPGC\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65e0\u7f1d\u96c6\u6210\u6279\u5224\u673a\u5236\u7684\u65b0\u8303\u5f0f\uff0c\u652f\u6301\u8fed\u4ee3\u63a8\u8350\u4f18\u5316\uff0c\u9002\u7528\u4e8e\u4e3b\u6d41\u63a8\u8350\u573a\u666f\uff0c\u65e0\u9700\u4e13\u95e8\u8bbe\u8ba1\u7684\u7528\u6237-\u5173\u952e\u8bcd\u5173\u8054\u6a21\u5757\u3002", "conclusion": "IPGC\u6846\u67b6\u89e3\u51b3\u4e86\u73b0\u6709\u53ef\u6279\u5224\u63a8\u8350\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u901a\u7528\u63d2\u4ef6\u8bbe\u8ba1\u548c\u6297\u9057\u5fd8\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5728\u4e3b\u6d41\u63a8\u8350\u6a21\u578b\u4e2d\u9ad8\u6548\u96c6\u6210\u6279\u5224\u529f\u80fd\u7684\u76ee\u6807\u3002"}}
{"id": "2509.26172", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.26172", "abs": "https://arxiv.org/abs/2509.26172", "authors": ["Xu Chen", "Yunmeng Shu", "Yuangang Pan", "Jinsong Lan", "Xiaoyong Zhu", "Shuai Xiao", "Haojin Zhu", "Ivor W. Tsang", "Bo Zheng"], "title": "Leveraging Scene Context with Dual Networks for Sequential User Behavior Modeling", "comment": "12pages", "summary": "Modeling sequential user behaviors for future behavior prediction is crucial\nin improving user's information retrieval experience. Recent studies highlight\nthe importance of incorporating contextual information to enhance prediction\nperformance. One crucial but usually neglected contextual information is the\nscene feature which we define as sub-interfaces within an app, created by\ndevelopers to provide specific functionalities, such as ``text2product search\"\nand ``live\" modules in e-commence apps. Different scenes exhibit distinct\nfunctionalities and usage habits, leading to significant distribution gap in\nuser engagement across them. Popular sequential behavior models either ignore\nthe scene feature or merely use it as attribute embeddings, which cannot\neffectively capture the dynamic interests and interplay between scenes and\nitems when modeling user sequences. In this work, we propose a novel Dual\nSequence Prediction networks (DSPnet) to effectively capture the dynamic\ninterests and interplay between scenes and items for future behavior\nprediction. DSPnet consists of two parallel networks dedicated to learn users'\ndynamic interests over items and scenes, and a sequence feature enhancement\nmodule to capture the interplay for enhanced future behavior prediction.\nFurther, we introduce a Conditional Contrastive Regularization (CCR) loss to\ncapture the invariance of similar historical sequences. Theoretical analysis\nsuggests that DSPnet is a principled way to learn the joint relationships\nbetween scene and item sequences. Extensive experiments are conducted on one\npublic benchmark and two collected industrial datasets. The method has been\ndeployed online in our system, bringing a 0.04 point increase in CTR, 0.78\\%\ngrowth in deals, and 0.64\\% rise in GMV. The codes are available at this\nanonymous github:\n\\textcolor{blue}{https://anonymous.4open.science/r/DSPNet-ForPublish-2506/}.", "AI": {"tldr": "\u63d0\u51faDSPnet\u6a21\u578b\uff0c\u901a\u8fc7\u53cc\u5e8f\u5217\u7f51\u7edc\u5206\u522b\u5b66\u4e60\u7528\u6237\u5bf9\u7269\u54c1\u548c\u573a\u666f\u7684\u52a8\u6001\u5174\u8da3\uff0c\u7ed3\u5408\u5e8f\u5217\u7279\u5f81\u589e\u5f3a\u6a21\u5757\u548c\u5bf9\u6bd4\u6b63\u5219\u5316\u635f\u5931\uff0c\u6709\u6548\u6355\u6349\u573a\u666f\u4e0e\u7269\u54c1\u95f4\u7684\u4ea4\u4e92\u5173\u7cfb\uff0c\u63d0\u5347\u672a\u6765\u884c\u4e3a\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5e8f\u5217\u884c\u4e3a\u6a21\u578b\u8981\u4e48\u5ffd\u7565\u573a\u666f\u7279\u5f81\uff0c\u8981\u4e48\u4ec5\u5c06\u5176\u4f5c\u4e3a\u5c5e\u6027\u5d4c\u5165\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u573a\u666f\u4e0e\u7269\u54c1\u4e4b\u95f4\u7684\u52a8\u6001\u5174\u8da3\u548c\u4ea4\u4e92\u5173\u7cfb\u3002\u4e0d\u540c\u573a\u666f\u5177\u6709\u4e0d\u540c\u7684\u529f\u80fd\u548c\u4f7f\u7528\u4e60\u60ef\uff0c\u5bfc\u81f4\u7528\u6237\u53c2\u4e0e\u5ea6\u5206\u5e03\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "method": "\u63d0\u51faDual Sequence Prediction networks (DSPnet)\uff0c\u5305\u542b\u4e24\u4e2a\u5e76\u884c\u7f51\u7edc\u5206\u522b\u5b66\u4e60\u7528\u6237\u5bf9\u7269\u54c1\u548c\u573a\u666f\u7684\u52a8\u6001\u5174\u8da3\uff0c\u4ee5\u53ca\u5e8f\u5217\u7279\u5f81\u589e\u5f3a\u6a21\u5757\u6355\u6349\u573a\u666f\u4e0e\u7269\u54c1\u95f4\u7684\u4ea4\u4e92\u5173\u7cfb\u3002\u5f15\u5165Conditional Contrastive Regularization (CCR)\u635f\u5931\u6765\u6355\u6349\u76f8\u4f3c\u5386\u53f2\u5e8f\u5217\u7684\u4e0d\u53d8\u6027\u3002", "result": "\u5728\u4e00\u4e2a\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\u548c\u4e24\u4e2a\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\u3002\u8be5\u65b9\u6cd5\u5df2\u5728\u7cfb\u7edf\u4e2d\u5728\u7ebf\u90e8\u7f72\uff0c\u5e26\u6765CTR\u63d0\u53470.04\u70b9\uff0c\u4ea4\u6613\u91cf\u589e\u957f0.78%\uff0cGMV\u589e\u957f0.64%\u3002", "conclusion": "DSPnet\u662f\u5b66\u4e60\u573a\u666f\u548c\u7269\u54c1\u5e8f\u5217\u8054\u5408\u5173\u7cfb\u7684\u539f\u7406\u6027\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u6355\u6349\u52a8\u6001\u5174\u8da3\u548c\u573a\u666f-\u7269\u54c1\u4ea4\u4e92\uff0c\u663e\u8457\u63d0\u5347\u672a\u6765\u884c\u4e3a\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2509.26184", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26184", "abs": "https://arxiv.org/abs/2509.26184", "authors": ["William Walden", "Marc Mason", "Orion Weller", "Laura Dietz", "Hannah Recknor", "Bryan Li", "Gabrielle Kaili-May Liu", "Yu Hou", "James Mayfield", "Eugene Yang"], "title": "Auto-ARGUE: LLM-Based Report Generation Evaluation", "comment": "ECIR 2025 demo format", "summary": "Generation of long-form, citation-backed reports is a primary use case for\nretrieval augmented generation (RAG) systems. While open-source evaluation\ntools exist for various RAG tasks, ones tailored to report generation are\nlacking. Accordingly, we introduce Auto-ARGUE, a robust LLM-based\nimplementation of the recent ARGUE framework for report generation evaluation.\nWe present analysis of Auto-ARGUE on the report generation pilot task from the\nTREC 2024 NeuCLIR track, showing good system-level correlations with human\njudgments. We further release a web app for visualization of Auto-ARGUE\noutputs.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86Auto-ARGUE\uff0c\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u62a5\u544a\u751f\u6210\u8bc4\u4f30\u5de5\u5177\uff0c\u5728TREC 2024 NeuCLIR\u4efb\u52a1\u4e2d\u4e0e\u4eba\u5de5\u8bc4\u4f30\u6709\u826f\u597d\u76f8\u5173\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5f00\u6e90RAG\u8bc4\u4f30\u5de5\u5177\u7f3a\u4e4f\u9488\u5bf9\u62a5\u544a\u751f\u6210\u4efb\u52a1\u7684\u4e13\u95e8\u5de5\u5177\uff0c\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u7528\u4e8e\u62a5\u544a\u751f\u6210\u8bc4\u4f30\u7684\u5de5\u5177\u3002", "method": "\u57fa\u4e8eARGUE\u6846\u67b6\u5f00\u53d1\u4e86Auto-ARGUE\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9c81\u68d2\u5b9e\u73b0\uff0c\u7528\u4e8e\u62a5\u544a\u751f\u6210\u8bc4\u4f30\u3002", "result": "\u5728TREC 2024 NeuCLIR track\u7684\u62a5\u544a\u751f\u6210\u8bd5\u70b9\u4efb\u52a1\u4e2d\uff0cAuto-ARGUE\u5728\u7cfb\u7edf\u7ea7\u522b\u4e0e\u4eba\u5de5\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\u51fa\u826f\u597d\u7684\u76f8\u5173\u6027\u3002", "conclusion": "Auto-ARGUE\u662f\u4e00\u4e2a\u6709\u6548\u7684\u62a5\u544a\u751f\u6210\u8bc4\u4f30\u5de5\u5177\uff0c\u5e76\u53d1\u5e03\u4e86\u7528\u4e8e\u53ef\u89c6\u5316\u8f93\u51fa\u7684Web\u5e94\u7528\u7a0b\u5e8f\u3002"}}
{"id": "2509.26203", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26203", "abs": "https://arxiv.org/abs/2509.26203", "authors": ["Victor Sechaud", "Patrice Abry", "Laurent Jacques", "Juli\u00e1n Tachella"], "title": "Self-supervised learning for phase retrieval", "comment": "in French language. GRETSI, Aug 2025, Strasboug, France", "summary": "In recent years, deep neural networks have emerged as a solution for inverse\nimaging problems. These networks are generally trained using pairs of images:\none degraded and the other of high quality, the latter being called 'ground\ntruth'. However, in medical and scientific imaging, the lack of fully sampled\ndata limits supervised learning. Recent advances have made it possible to\nreconstruct images from measurement data alone, eliminating the need for\nreferences. However, these methods remain limited to linear problems, excluding\nnon-linear problems such as phase retrieval. We propose a self-supervised\nmethod that overcomes this limitation in the case of phase retrieval by using\nthe natural invariance of images to translations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u7684\u76f8\u4f4d\u6062\u590d\u65b9\u6cd5\uff0c\u5229\u7528\u56fe\u50cf\u5e73\u79fb\u4e0d\u53d8\u6027\uff0c\u65e0\u9700\u53c2\u8003\u56fe\u50cf\u5373\u53ef\u4ece\u6d4b\u91cf\u6570\u636e\u91cd\u5efa\u56fe\u50cf\u3002", "motivation": "\u5728\u533b\u5b66\u548c\u79d1\u5b66\u6210\u50cf\u4e2d\uff0c\u7f3a\u4e4f\u5168\u91c7\u6837\u6570\u636e\u9650\u5236\u4e86\u76d1\u7763\u5b66\u4e60\u3002\u73b0\u6709\u65b9\u6cd5\u4ec5\u9650\u4e8e\u7ebf\u6027\u95ee\u9898\uff0c\u65e0\u6cd5\u5904\u7406\u975e\u7ebf\u6027\u95ee\u9898\u5982\u76f8\u4f4d\u6062\u590d\u3002", "method": "\u5229\u7528\u56fe\u50cf\u5e73\u79fb\u4e0d\u53d8\u6027\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d4b\u91cf\u6570\u636e\u672c\u8eab\u8fdb\u884c\u91cd\u5efa\uff0c\u65e0\u9700\u53c2\u8003\u56fe\u50cf\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u6210\u529f\u5904\u7406\u975e\u7ebf\u6027\u76f8\u4f4d\u6062\u590d\u95ee\u9898\u3002", "conclusion": "\u63d0\u51fa\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\u4e3a\u975e\u7ebf\u6027\u9006\u6210\u50cf\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u7f3a\u4e4f\u53c2\u8003\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2509.26262", "categories": ["cs.IR", "cs.CE"], "pdf": "https://arxiv.org/pdf/2509.26262", "abs": "https://arxiv.org/abs/2509.26262", "authors": ["Homa Jamalof", "Luca Vassio", "Danilo Giordano", "Marco Mellia", "Claudio De Tommasi"], "title": "Analyzing BEV Suitability and Charging Strategies Using Italian Driving Data", "comment": "Accepted at 2025 IEEE Transportation Electrification Conference and\n  Expo, Asia-Pacific (ITEC-AP 2025)", "summary": "Battery Electric Vehicles (BEVs) are rapidly evolving from a niche\nalternative to an established option for private transportation, often\nreplacing Internal Combustion Engine (ICE) vehicles. Despite growing interest,\nsignificant barriers remain, including range anxiety, the inconvenience\nassociated with public charging stations, and higher costs. This study analyses\nextensive telemetry data collected from 10,441 users using ICE vehicles in an\nItalian province to assess the potential for switching to BEVs without changing\ncurrent travel behaviour. We evaluate to what extent the BEV models can fulfil\ntheir mobility needs under different charging scenarios. To do so, we replicate\ntrips and parking events, simulating and monitoring the battery state of\ncharge. The analysis reveals the compromises between charging behaviours and\nlimited BEV autonomy. Assuming access to overnight charging, at least 35% of\nthe users could already adopt even low-capacity BEVs.", "AI": {"tldr": "\u57fa\u4e8e\u610f\u5927\u522910,441\u540d\u71c3\u6cb9\u8f66\u7528\u6237\u7684\u51fa\u884c\u6570\u636e\u5206\u6790\uff0c\u7814\u7a76\u8bc4\u4f30\u5728\u4e0d\u6539\u53d8\u51fa\u884c\u4e60\u60ef\u7684\u60c5\u51b5\u4e0b\u8f6c\u5411\u7535\u52a8\u8f66\u7684\u53ef\u884c\u6027\uff0c\u53d1\u73b0\u81f3\u5c1135%\u7684\u7528\u6237\u5728\u591c\u95f4\u5145\u7535\u6761\u4ef6\u4e0b\u53ef\u6539\u7528\u4f4e\u5bb9\u91cf\u7535\u52a8\u8f66\u3002", "motivation": "\u7535\u52a8\u8f66\u6b63\u4ece\u66ff\u4ee3\u9009\u62e9\u53d1\u5c55\u4e3a\u4e3b\u6d41\u4ea4\u901a\u5de5\u5177\uff0c\u4f46\u4ecd\u9762\u4e34\u7eed\u822a\u7126\u8651\u3001\u5145\u7535\u4e0d\u4fbf\u548c\u6210\u672c\u9ad8\u7b49\u969c\u788d\uff0c\u9700\u8981\u8bc4\u4f30\u7528\u6237\u5728\u4e0d\u6539\u53d8\u51fa\u884c\u884c\u4e3a\u60c5\u51b5\u4e0b\u8f6c\u5411\u7535\u52a8\u8f66\u7684\u5b9e\u9645\u53ef\u884c\u6027\u3002", "method": "\u6536\u96c6\u71c3\u6cb9\u8f66\u7528\u6237\u7684\u51fa\u884c\u6570\u636e\uff0c\u6a21\u62df\u590d\u5236\u884c\u7a0b\u548c\u505c\u8f66\u4e8b\u4ef6\uff0c\u5728\u4e0d\u540c\u5145\u7535\u573a\u666f\u4e0b\u76d1\u6d4b\u7535\u6c60\u7535\u91cf\u72b6\u6001\uff0c\u8bc4\u4f30\u7535\u52a8\u8f66\u6ee1\u8db3\u51fa\u884c\u9700\u6c42\u7684\u80fd\u529b\u3002", "result": "\u5206\u6790\u663e\u793a\u5145\u7535\u884c\u4e3a\u4e0e\u6709\u9650\u7eed\u822a\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002\u5047\u8bbe\u5177\u5907\u591c\u95f4\u5145\u7535\u6761\u4ef6\uff0c\u81f3\u5c1135%\u7684\u7528\u6237\u53ef\u6539\u7528\u4f4e\u5bb9\u91cf\u7535\u52a8\u8f66\u3002", "conclusion": "\u76f8\u5f53\u5927\u6bd4\u4f8b\u7684\u7528\u6237\u5728\u73b0\u6709\u51fa\u884c\u6a21\u5f0f\u4e0b\u5df2\u5177\u5907\u8f6c\u5411\u7535\u52a8\u8f66\u7684\u6761\u4ef6\uff0c\u591c\u95f4\u5145\u7535\u57fa\u7840\u8bbe\u65bd\u5bf9\u7535\u52a8\u8f66\u666e\u53ca\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.26378", "categories": ["cs.IR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.26378", "abs": "https://arxiv.org/abs/2509.26378", "authors": ["Junjie Zhou", "Ze Liu", "Lei Xiong", "Jin-Ge Yao", "Yueze Wang", "Shitao Xiao", "Fenfen Lin", "Miguel Hu Chen", "Zhicheng Dou", "Siqi Bao", "Defu Lian", "Yongping Xiong", "Zheng Liu"], "title": "MR$^2$-Bench: Going Beyond Matching to Reasoning in Multimodal Retrieval", "comment": null, "summary": "Multimodal retrieval is becoming a crucial component of modern AI\napplications, yet its evaluation lags behind the demands of more realistic and\nchallenging scenarios. Existing benchmarks primarily probe surface-level\nsemantic correspondence (e.g., object-text matching) while failing to assess\nthe deeper reasoning required to capture complex relationships between visual\nand textual information. To address this gap, we introduce MR$^2$-Bench, a\nreasoning-intensive benchmark for multimodal retrieval. MR$^2$-Bench presents\nthe following critical values: 1) all tasks are reasoning-driven, going beyond\nshallow matching to effectively assess models' capacity for logical, spatial,\nand causal inference; 2) it features diverse multimodal data, such as natural\nimages, diagrams, and visual puzzles, enabling comprehensive evaluation across\ncontent types; 3) it supports complex queries and documents containing multiple\nimages and covers diverse retrieval scenarios, more accurately reflecting\nreal-world applications. Our benchmark contains 1,309 curated queries, derived\neither from manual collection and annotation or from selective consolidation of\npublic datasets. Despite achieving strong results on existing benchmarks,\ncurrent state-of-the-art models still struggle on MR$^2$-Bench: for example,\nthe leading Seed1.6-Embedding model attains a Recall@1 of 77.78 on MMEB, but\nonly 9.91 on MR$^2$-Bench. This substantial performance gap highlights both the\nincreased challenge posed by our benchmark and the pressing need for further\nadvances in reasoning-intensive multimodal retrieval. The dataset and\nevaluation code will be made publicly available at\nhttps://github.com/VectorSpaceLab/MR2-Bench.", "AI": {"tldr": "MR\u00b2-Bench\u662f\u4e00\u4e2a\u63a8\u7406\u5bc6\u96c6\u7684\u591a\u6a21\u6001\u68c0\u7d22\u57fa\u51c6\uff0c\u8d85\u8d8a\u8868\u9762\u8bed\u4e49\u5339\u914d\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u903b\u8f91\u3001\u7a7a\u95f4\u548c\u56e0\u679c\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u6d4b\u8bd5\u8868\u9762\u8bed\u4e49\u5bf9\u5e94\uff08\u5982\u7269\u4f53-\u6587\u672c\u5339\u914d\uff09\uff0c\u65e0\u6cd5\u8bc4\u4f30\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\u4e4b\u95f4\u590d\u6742\u5173\u7cfb\u6240\u9700\u7684\u6df1\u5ea6\u63a8\u7406\u80fd\u529b\u3002", "method": "\u6784\u5efa\u5305\u542b1,309\u4e2a\u7cbe\u5fc3\u7b56\u5212\u67e5\u8be2\u7684\u57fa\u51c6\uff0c\u6db5\u76d6\u81ea\u7136\u56fe\u50cf\u3001\u56fe\u8868\u548c\u89c6\u89c9\u8c1c\u9898\u7b49\u591a\u6837\u5316\u591a\u6a21\u6001\u6570\u636e\uff0c\u652f\u6301\u5305\u542b\u591a\u4e2a\u56fe\u50cf\u7684\u590d\u6742\u67e5\u8be2\u548c\u6587\u6863\u3002", "result": "\u5f53\u524d\u6700\u5148\u8fdb\u6a21\u578b\u5728MR\u00b2-Bench\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4f8b\u5982Seed1.6-Embedding\u6a21\u578b\u5728MMEB\u4e0aRecall@1\u4e3a77.78\uff0c\u4f46\u5728MR\u00b2-Bench\u4e0a\u4ec5\u4e3a9.91\u3002", "conclusion": "MR\u00b2-Bench\u63ed\u793a\u4e86\u591a\u6a21\u6001\u68c0\u7d22\u4e2d\u63a8\u7406\u80fd\u529b\u7684\u91cd\u5927\u6311\u6218\uff0c\u5f3a\u8c03\u4e86\u5728\u63a8\u7406\u5bc6\u96c6\u578b\u591a\u6a21\u6001\u68c0\u7d22\u65b9\u9762\u8fdb\u4e00\u6b65\u53d1\u5c55\u7684\u8feb\u5207\u9700\u6c42\u3002"}}
{"id": "2509.26448", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.26448", "abs": "https://arxiv.org/abs/2509.26448", "authors": ["Abdullah Abbas", "Michael Heep", "Theodor Sperle"], "title": "Informed Dataset Selection", "comment": "45 pages, 4 figures", "summary": "The selection of datasets in recommender systems research lacks a systematic\nmethodology. Researchers often select datasets based on popularity rather than\nempirical suitability. We developed the APS Explorer, a web application that\nimplements the Algorithm Performance Space (APS) framework for informed dataset\nselection. The system analyzes 96 datasets using 28 algorithms across three\nmetrics (nDCG, Hit Ratio, Recall) at five K-values. We extend the APS framework\nwith a statistical based classification system that categorizes datasets into\nfive difficulty levels based on quintiles. We also introduce a\nvariance-normalized distance metric based on Mahalanobis distance to measure\nsimilarity. The APS Explorer was successfully developed with three interactive\nmodules for visualizing algorithm performance, direct comparing algorithms, and\nanalyzing dataset metadata. This tool shifts the process of selecting datasets\nfrom intuition-based to evidence-based practices, and it is publicly available\nat datasets.recommender-systems.com.", "AI": {"tldr": "\u5f00\u53d1\u4e86APS Explorer\u7f51\u7edc\u5e94\u7528\uff0c\u57fa\u4e8e\u7b97\u6cd5\u6027\u80fd\u7a7a\u95f4\u6846\u67b6\u4e3a\u63a8\u8350\u7cfb\u7edf\u7814\u7a76\u63d0\u4f9b\u6570\u636e\u9a71\u52a8\u7684\u6570\u636e\u96c6\u9009\u62e9\u65b9\u6cd5\uff0c\u5305\u542b\u53ef\u89c6\u5316\u3001\u7b97\u6cd5\u6bd4\u8f83\u548c\u5143\u6570\u636e\u5206\u6790\u529f\u80fd\u3002", "motivation": "\u63a8\u8350\u7cfb\u7edf\u7814\u7a76\u4e2d\u6570\u636e\u96c6\u9009\u62e9\u7f3a\u4e4f\u7cfb\u7edf\u6027\u65b9\u6cd5\uff0c\u7814\u7a76\u8005\u901a\u5e38\u57fa\u4e8e\u6d41\u884c\u5ea6\u800c\u975e\u5b9e\u8bc1\u9002\u7528\u6027\u9009\u62e9\u6570\u636e\u96c6\u3002", "method": "\u5b9e\u73b0\u7b97\u6cd5\u6027\u80fd\u7a7a\u95f4\u6846\u67b6\uff0c\u5206\u679096\u4e2a\u6570\u636e\u96c6\u300128\u79cd\u7b97\u6cd5\u5728\u4e09\u4e2a\u6307\u6807\u548c\u4e94\u4e2aK\u503c\u4e0b\u7684\u8868\u73b0\uff0c\u6269\u5c55\u4e86\u57fa\u4e8e\u7edf\u8ba1\u7684\u4e94\u7ea7\u96be\u5ea6\u5206\u7c7b\u7cfb\u7edf\u548c\u57fa\u4e8e\u9a6c\u6c0f\u8ddd\u79bb\u7684\u65b9\u5dee\u5f52\u4e00\u5316\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u3002", "result": "\u6210\u529f\u5f00\u53d1APS Explorer\uff0c\u5305\u542b\u4e09\u4e2a\u4ea4\u4e92\u6a21\u5757\uff1a\u7b97\u6cd5\u6027\u80fd\u53ef\u89c6\u5316\u3001\u76f4\u63a5\u7b97\u6cd5\u6bd4\u8f83\u548c\u6570\u636e\u96c6\u5143\u6570\u636e\u5206\u6790\u3002", "conclusion": "\u8be5\u5de5\u5177\u5c06\u6570\u636e\u96c6\u9009\u62e9\u8fc7\u7a0b\u4ece\u57fa\u4e8e\u76f4\u89c9\u8f6c\u53d8\u4e3a\u57fa\u4e8e\u8bc1\u636e\u7684\u5b9e\u8df5\uff0c\u5e76\u5df2\u516c\u5f00\u53ef\u7528\u3002"}}
