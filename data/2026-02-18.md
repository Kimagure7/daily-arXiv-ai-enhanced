<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 8]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [ScrapeGraphAI-100k: A Large-Scale Dataset for LLM-Based Web Information Extraction](https://arxiv.org/abs/2602.15189)
*William Brach,Francesco Zuppichini,Marco Vinciguerra,Lorenzo Padoan*

Main category: cs.IR

TL;DR: ScrapeGraphAI-100k是一个包含10万真实世界LLM提取事件的大规模数据集，用于网页信息提取研究，支持小模型微调、结构化提取基准测试和模式归纳研究。


<details>
  <summary>Details</summary>
Motivation: 现有网页信息提取数据集规模小、合成或仅含文本，无法捕捉网页的结构化上下文。需要真实、大规模、包含结构上下文的数据集来改进网页信息检索。

Method: 通过ScrapeGraphAI的opt-in遥测数据收集2025年Q2-Q3期间的900万事件，经过去重和模式平衡处理后得到93,695个实例。每个实例包含Markdown内容、提示、JSON模式、LLM响应以及复杂度/验证元数据。

Result: 数据集涵盖多样化领域和语言，分析了模式复杂度增加时的失败模式。微调实验显示，在数据集子集上训练的1.7B小模型能缩小与30B大基线的差距，证明了数据集对高效提取的实用性。

Conclusion: ScrapeGraphAI-100k为小模型微调、结构化提取基准测试和网页信息检索索引的模式归纳研究提供了有价值的资源，已在HuggingFace公开可用。

Abstract: The use of large language models for web information extraction is becoming increasingly fundamental to modern web information retrieval pipelines. However, existing datasets tend to be small, synthetic or text-only, failing to capture the structural context of the web. We introduce ScrapeGraphAI-100k, a large-scale dataset comprising real-world LLM extraction events, collected via opt-in ScrapeGraphAI telemetry during Q2 and Q3 of 2025. Starting from 9M events, we deduplicate and balance by schema to produce 93,695 examples spanning diverse domains and languages. Each instance includes Markdown content, a prompt, a JSON schema, the LLM response, and complexity/validation metadata. We characterize the datasets structural diversity and its failure modes as schema complexity increases. We also provide a fine-tuning experiment showing that a small language model (1.7B) trained on a subset narrows the gap to larger baselines (30B), underscoring the datasets utility for efficient extraction. ScrapeGraphAI-100k enables fine-tuning small models, benchmarking structured extraction, and studying schema induction for web IR indexing, and is publicly available on HuggingFace.

</details>


### [2] [Semantics-Aware Denoising: A PLM-Guided Sample Reweighting Strategy for Robust Recommendation](https://arxiv.org/abs/2602.15359)
*Xikai Yang,Yang Wang,Yilin Li,Sebastian Sun*

Main category: cs.IR

TL;DR: SAID框架利用语义一致性识别和降权隐式反馈中的噪声点击，通过PLM编码器计算用户兴趣与物品内容的语义相似度，将其转化为样本权重调节训练损失，提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 隐式反馈（如用户点击）包含大量噪声（误点击、诱导点击、探索性浏览），这些噪声不能反映真实用户偏好，导致推荐模型预测准确性下降和推荐不可靠。

Method: SAID框架：1）从历史行为构建文本用户兴趣画像；2）使用预训练语言模型编码器计算用户兴趣与目标物品描述的语义相似度；3）将相似度分数转化为样本权重，调节训练损失，降低语义不一致点击的影响。该方法仅修改损失函数，保持主干推荐模型不变。

Result: 在两个真实世界数据集上的实验表明，SAID能持续提升推荐性能，在AUC指标上相对强基线提升高达2.2%，在高噪声条件下表现出特别显著的鲁棒性。

Conclusion: SAID是一个简单有效的隐式反馈去噪框架，通过语义一致性识别噪声点击，仅需修改损失函数即可显著提升推荐性能，特别适用于高噪声场景。

Abstract: Implicit feedback, such as user clicks, serves as the primary data source for modern recommender systems. However, click interactions inherently contain substantial noise, including accidental clicks, clickbait-induced interactions, and exploratory browsing behaviors that do not reflect genuine user preferences. Training recommendation models with such noisy positive samples leads to degraded prediction accuracy and unreliable recommendations. In this paper, we propose SAID (Semantics-Aware Implicit Denoising), a simple yet effective framework that leverages semantic consistency between user interests and item content to identify and downweight potentially noisy interactions. Our approach constructs textual user interest profiles from historical behaviors and computes semantic similarity with target item descriptions using pre-trained language model (PLM) based text encoders. The similarity scores are then transformed into sample weights that modulate the training loss, effectively reducing the impact of semantically inconsistent clicks. Unlike existing denoising methods that require complex auxiliary networks or multi-stage training procedures, SAID only modifies the loss function while keeping the backbone recommendation model unchanged. Extensive experiments on two real-world datasets demonstrate that SAID consistently improves recommendation performance, achieving up to 2.2% relative improvement in AUC over strong baselines, with particularly notable robustness under high noise conditions.

</details>


### [3] [Automatic Funny Scene Extraction from Long-form Cinematic Videos](https://arxiv.org/abs/2602.15381)
*Sibendu Paul,Haotian Jiang,Caren Chen*

Main category: cs.IR

TL;DR: 提出端到端系统自动从长电影中识别和排名幽默场景，结合镜头检测、多模态场景定位和幽默标签，显著提升场景检测和幽默识别性能。


<details>
  <summary>Details</summary>
Motivation: 从电影中自动提取高质量幽默场景对于创建吸引人的视频预览和短视频内容至关重要，但长电影时长和复杂叙事、幽默的多模态依赖和微妙风格增加了识别难度。

Method: 端到端系统包含镜头检测、多模态场景定位和幽默标签。创新包括：结合视觉和文本线索的场景分割方法、通过引导三元组挖掘改进镜头表示、利用音频和文本的多模态幽默标签框架。

Result: 在OVSD数据集上场景检测AP提升18.3%，长文本幽默检测F1分数0.834。在五部电影评估中，87%提取的片段是幽默的，98%场景准确定位，系统能泛化到预告片。

Conclusion: 系统能有效提升内容创作流程、改善用户参与度，并简化各种电影媒体格式的短视频内容生成，展示了在流媒体平台应用的潜力。

Abstract: Automatically extracting engaging and high-quality humorous scenes from cinematic titles is pivotal for creating captivating video previews and snackable content, boosting user engagement on streaming platforms. Long-form cinematic titles, with their extended duration and complex narratives, challenge scene localization, while humor's reliance on diverse modalities and its nuanced style add further complexity. This paper introduces an end-to-end system for automatically identifying and ranking humorous scenes from long-form cinematic titles, featuring shot detection, multimodal scene localization, and humor tagging optimized for cinematic content. Key innovations include a novel scene segmentation approach combining visual and textual cues, improved shot representations via guided triplet mining, and a multimodal humor tagging framework leveraging both audio and text. Our system achieves an 18.3% AP improvement over state-of-the-art scene detection on the OVSD dataset and an F1 score of 0.834 for detecting humor in long text. Extensive evaluations across five cinematic titles demonstrate 87% of clips extracted by our pipeline are intended to be funny, while 98% of scenes are accurately localized. With successful generalization to trailers, these results showcase the pipeline's potential to enhance content creation workflows, improve user engagement, and streamline snackable content generation for diverse cinematic media formats.

</details>


### [4] [GaiaFlow: Semantic-Guided Diffusion Tuning for Carbon-Frugal Search](https://arxiv.org/abs/2602.15423)
*Rong Fu,Wenxin Zhang,Jia Yee Tan,Chunlei Meng,Shuo Yin,Xiaowen Ma,Wangyu Wu,Muge Qi,Guangzhen Yao,Zhaolu Kang,Zeli Su,Simon Fong*

Main category: cs.IR

TL;DR: GaiaFlow是一个用于碳节约搜索的创新框架，通过语义引导的扩散调优来平衡搜索精度和环境保护。


<details>
  <summary>Details</summary>
Motivation: 随着复杂神经架构的功耗需求不断增长，信息检索社区认识到生态可持续性是关键优先事项，需要在模型设计中进行根本性的范式转变。虽然当代神经排序器达到了前所未有的准确性，但其计算强度带来的重大环境外部性在大规模部署中常被忽视。

Method: GaiaFlow框架采用语义引导的扩散调优，结合检索引导的Langevin动力学和硬件无关的性能建模策略来优化搜索精度与环境保护之间的权衡。通过自适应早期退出协议和精度感知的量化推理，该架构显著减少了操作碳足迹。

Result: 广泛的实验评估表明，GaiaFlow在有效性和能源效率之间实现了优越的平衡，为下一代神经搜索系统提供了可扩展且可持续的路径。

Conclusion: GaiaFlow为碳节约搜索提供了一个创新框架，通过语义引导的扩散调优在保持检索质量的同时显著降低环境足迹，为可持续的神经搜索系统开辟了新途径。

Abstract: As the burgeoning power requirements of sophisticated neural architectures escalate, the information retrieval community has recognized ecological sustainability as a pivotal priority that necessitates a fundamental paradigm shift in model design. While contemporary neural rankers have attained unprecedented accuracy, the substantial environmental externalities associated with their computational intensity often remain overlooked in large-scale deployments. We present GaiaFlow, an innovative framework engineered to facilitate carbon-frugal search by operationalizing semantic-guided diffusion tuning. Our methodology orchestrates the convergence of retrieval-guided Langevin dynamics and a hardware-independent performance modeling strategy to optimize the trade-off between search precision and environmental preservation. By incorporating adaptive early exit protocols and precision-aware quantized inference, the proposed architecture significantly mitigates operational carbon footprints while maintaining robust retrieval quality across heterogeneous computing infrastructures. Extensive experimental evaluations demonstrate that GaiaFlow achieves a superior equilibrium between effectiveness and energy efficiency, offering a scalable and sustainable pathway for next-generation neural search systems.

</details>


### [5] [Binge Watch: Reproducible Multimodal Benchmarks Datasets for Large-Scale Movie Recommendation on MovieLens-10M and 20M](https://arxiv.org/abs/2602.15505)
*Giuseppe Spillo,Alessandro Petruzzelli,Cataldo Musto,Marco de Gemmis,Pasquale Lops,Giovanni Semeraro*

Main category: cs.IR

TL;DR: 该论文发布了两个大规模多模态电影推荐数据集M3L-10M和M3L-20M，通过为流行的MovieLens数据集添加多模态特征（剧情文本、海报、预告片），并采用标准化流程提取文本、视觉、音频和视频特征，旨在促进多模态推荐系统的可复现性研究。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推荐系统领域缺乏大规模、高质量、可公开获取的数据集，现有数据集要么规模较小，要么构建过程不透明，这阻碍了该领域的研究进展和结果可复现性。

Method: 基于MovieLens-10M和MovieLens-20M数据集，通过标准化流程收集电影的剧情文本、海报和预告片，使用多种最先进的编码器提取文本、视觉、音频和视频特征，构建了M3L-10M和M3L-20M两个数据集。

Result: 成功构建并公开发布了两个大规模多模态电影推荐数据集，包含原始数据映射、提取的特征以及多种格式的完整数据集，同时进行了定性和定量分析验证数据集质量。

Conclusion: 这项工作为大规模多模态电影推荐领域提供了重要的基础资源，通过提供标准化、可复现的数据集，将推动多模态推荐系统研究的进展和结果可比性。

Abstract: With the growing interest in Multimodal Recommender Systems (MRSs), collecting high-quality datasets provided with multimedia side information (text, images, audio, video) has become a fundamental step. However, most of the current literature in the field relies on small- or medium-scale datasets that are either not publicly released or built using undocumented processes.
  In this paper, we aim to fill this gap by releasing M3L-10M and M3L-20M, two large-scale, reproducible, multimodal datasets for the movie domain, obtained by enriching with multimodal features the popular MovieLens-10M and MovieLens-20M, respectively. By following a fully documented pipeline, we collect movie plots, posters, and trailers, from which textual, visual, acoustic, and video features are extracted using several state-of-the-art encoders. We publicly release mappings to download the original raw data, the extracted features, and the complete datasets in multiple formats, fostering reproducibility and advancing the field of MRSs. In addition, we conduct qualitative and quantitative analyses that showcase our datasets across several perspectives.
  This work represents a foundational step to ensure reproducibility and replicability in the large-scale, multimodal movie recommendation domain. Our resource can be fully accessed at the following link: https://zenodo.org/records/18499145, while the source code is accessible at https://github.com/giuspillo/M3L_10M_20M.

</details>


### [6] [Eco-Amazon: Enriching E-commerce Datasets with Product Carbon Footprint for Sustainable Recommendations](https://arxiv.org/abs/2602.15508)
*Giuseppe Spillo,Allegra De Filippo,Cataldo Musto,Michela Milano,Giovanni Semeraro*

Main category: cs.IR

TL;DR: Eco-Amazon：一个为亚马逊数据集添加产品碳足迹（PCF）元数据的新资源，使用LLM零样本框架估计碳排放，支持可持续信息检索和推荐系统研究。


<details>
  <summary>Details</summary>
Motivation: 在负责任和可持续AI时代，信息检索和推荐系统需要超越传统准确性指标，纳入环境可持续性考量。然而，现有标准基准缺乏项目级环境影响数据，严重限制了这一研究方向的发展。

Method: 提出Eco-Amazon资源，包含三个广泛使用的亚马逊数据集（家居、服装、电子产品）的增强版本，通过零样本框架利用大型语言模型（LLMs）基于产品属性估计项目级产品碳足迹（PCF）元数据。

Result: 发布了Eco-Amazon数据集，包含PCF信号增强的项目元数据；提供了LLM-based PCF估计脚本，允许研究人员丰富任何产品目录并复现结果；展示了利用PCF估计促进更可持续产品的用例。

Conclusion: Eco-Amazon通过提供环境信号，使研究社区能够开发、基准测试和评估下一代可持续检索和推荐模型，填补了可持续AI研究中项目级环境影响数据的空白。

Abstract: In the era of responsible and sustainable AI, information retrieval and recommender systems must expand their scope beyond traditional accuracy metrics to incorporate environmental sustainability. However, this research line is severely limited by the lack of item-level environmental impact data in standard benchmarks. This paper introduces Eco-Amazon, a novel resource designed to bridge this gap. Our resource consists of an enriched version of three widely used Amazon datasets (i.e., Home, Clothing, and Electronics) augmented with Product Carbon Footprint (PCF) metadata. CO2e emission scores were generated using a zero-shot framework that leverages Large Language Models (LLMs) to estimate item-level PCF based on product attributes. Our contribution is three-fold: (i) the release of the Eco-Amazon datasets, enriching item metadata with PCF signals; (ii) the LLM-based PCF estimation script, which allows researchers to enrich any product catalogue and reproduce our results; (iii) a use case demonstrating how PCF estimates can be exploited to promote more sustainable products. By providing these environmental signals, Eco-Amazon enables the community to develop, benchmark, and evaluate the next generation of sustainable retrieval and recommendation models. Our resource is available at https://doi.org/10.5281/zenodo.18549130, while our source code is available at: http://github.com/giuspillo/EcoAmazon/.

</details>


### [7] [Can Recommender Systems Teach Themselves? A Recursive Self-Improving Framework with Fidelity Control](https://arxiv.org/abs/2602.15659)
*Luankang Zhang,Hao Wang,Zhongzhou Liu,Mingjia Yin,Yonghao Huang,Jiaqi Li,Wei Guo,Yong Liu,Huifeng Guo,Defu Lian,Enhong Chen*

Main category: cs.IR

TL;DR: 提出RSIR框架，让推荐模型通过生成可信的用户交互序列来自我增强训练数据，无需外部数据或教师模型，解决数据稀疏性问题。


<details>
  <summary>Details</summary>
Motivation: 高质量训练数据的稀缺是机器学习模型扩展的根本瓶颈，推荐系统中用户交互的极端稀疏性导致优化地形崎岖和泛化能力差。

Method: RSIR框架采用闭环操作：当前模型生成可信的用户交互序列，基于保真度的质量控制机制筛选与用户偏好流形一致的序列，然后在增强的数据集上训练后继模型。

Result: RSIR在多个基准测试和架构中产生一致、累积的增益，即使较小的模型也能受益，弱模型可以为强模型生成有效的训练课程。

Conclusion: 递归自我改进是克服数据稀疏性的通用、模型无关方法，为推荐系统及其他领域提供了可扩展的前进路径。

Abstract: The scarcity of high-quality training data presents a fundamental bottleneck to scaling machine learning models. This challenge is particularly acute in recommendation systems, where extreme sparsity in user interactions leads to rugged optimization landscapes and poor generalization. We propose the Recursive Self-Improving Recommendation (RSIR) framework, a paradigm in which a model bootstraps its own performance without reliance on external data or teacher models. RSIR operates in a closed loop: the current model generates plausible user interaction sequences, a fidelity-based quality control mechanism filters them for consistency with user's approximate preference manifold, and a successor model is augmented on the enriched dataset. Our theoretical analysis shows that RSIR acts as a data-driven implicit regularizer, smoothing the optimization landscape and guiding models toward more robust solutions. Empirically, RSIR yields consistent, cumulative gains across multiple benchmarks and architectures. Notably, even smaller models benefit, and weak models can generate effective training curricula for stronger ones. These results demonstrate that recursive self-improvement is a general, model-agnostic approach to overcoming data sparsity, suggesting a scalable path forward for recommender systems and beyond. Our anonymized code is available at https://anonymous.4open.science/r/RSIR-7C5B .

</details>


### [8] [The Next Paradigm Is User-Centric Agent, Not Platform-Centric Service](https://arxiv.org/abs/2602.15682)
*Luankang Zhang,Hang Lv,Qiushi Pan,Kefen Wang,Yonghao Huang,Xinrui Miao,Yin Xu,Wei Guo,Yong Liu,Hao Wang,Enhong Chen*

Main category: cs.IR

TL;DR: 论文主张数字服务应从平台中心转向用户中心，利用LLM和端侧智能实现真正以用户利益为优先的智能代理。


<details>
  <summary>Details</summary>
Motivation: 当前平台中心的服务模型以平台指标（如参与度、转化率）为导向，往往与用户的真实需求不符，甚至与用户利益相冲突。平台技术的进步（特别是LLM）并未真正转化为用户利益，而是优先考虑服务提供者的目标。

Method: 提出向用户中心智能代理的转型，探讨实现这一愿景的机遇与挑战，设计实用的设备-云端管道实施方案，并讨论必要的治理和生态系统结构。

Result: 论证了用户中心智能代理的可行性，强调其应优先考虑隐私保护、与用户定义的目标对齐，并赋予用户对偏好和行为的控制权。通过LLM和端侧智能的进步，这一愿景现在可以实现。

Conclusion: 数字服务的未来应从平台中心转向用户中心代理，这需要技术实现、治理结构和生态系统支持的综合变革，以实现真正以用户利益为核心的数字服务。

Abstract: Modern digital services have evolved into indispensable tools, driving the present large-scale information systems. Yet, the prevailing platform-centric model, where services are optimized for platform-driven metrics such as engagement and conversion, often fails to align with users' true needs. While platform technologies have advanced significantly-especially with the integration of large language models (LLMs)-we argue that improvements in platform service quality do not necessarily translate to genuine user benefit. Instead, platform-centric services prioritize provider objectives over user welfare, resulting in conflicts against user interests. This paper argues that the future of digital services should shift from a platform-centric to a user-centric agent. These user-centric agents prioritize privacy, align with user-defined goals, and grant users control over their preferences and actions. With advancements in LLMs and on-device intelligence, the realization of this vision is now feasible. This paper explores the opportunities and challenges in transitioning to user-centric intelligence, presents a practical device-cloud pipeline for its implementation, and discusses the necessary governance and ecosystem structures for its adoption.

</details>
