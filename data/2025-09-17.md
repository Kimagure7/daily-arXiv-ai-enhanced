<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 45]
- [cs.IR](#cs.IR) [Total: 9]
- [cs.LG](#cs.LG) [Total: 98]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MTEB-NL and E5-NL: Embedding Benchmark and Models for Dutch](https://arxiv.org/abs/2509.12340)
*Nikolay Banar,Ehsan Lotfi,Jens Van Nooten,Cristina Arhiliuc,Marija Kliocaite,Walter Daelemans*

Main category: cs.CL

TL;DR: 这篇论文为了解决荷兰语在多语言嵌入资源中的代表性不足问题，提供了新的评测基准MTEB-NL、训练数据集以及高效的E5-NL嵌入模型系列。


<details>
  <summary>Details</summary>
Motivation: 荷兰语在现有的多语言嵌入资源（模型、基准测试和数据集）中占比较小，代表性不足，需要提供专门的资源来支持其进一步发展。

Method: 1）构建MTEB-NL评测基准，包含现有和新创建的荷兰语数据集，涵盖多种任务类型
2）编诒荷兰语检索数据集，并使用大语言模型生成合成数据来扩大任务覆盖范围
3）发布E5-NL模型系列，这是一组简洁但高效的嵌入模型

Result: 开发了全面的荷兰语嵌入评测基准和训练资源，E5-NL模型在多个任务上都表现出强劲的性能。

Conclusion: 该研究有效补充了荷兰语在嵌入技术领域的空白，为该语言的嵌入模型开发和评估提供了完整的资源支持，所有资源已公开发布。

Abstract: Recently, embedding resources, including models, benchmarks, and datasets,
have been widely released to support a variety of languages. However, the Dutch
language remains underrepresented, typically comprising only a small fraction
of the published multilingual resources. To address this gap and encourage the
further development of Dutch embeddings, we introduce new resources for their
evaluation and generation. First, we introduce the Massive Text Embedding
Benchmark for Dutch (MTEB-NL), which includes both existing Dutch datasets and
newly created ones, covering a wide range of tasks. Second, we provide a
training dataset compiled from available Dutch retrieval datasets, complemented
with synthetic data generated by large language models to expand task coverage
beyond retrieval. Finally, we release a series of E5-NL models compact yet
efficient embedding models that demonstrate strong performance across multiple
tasks. We make our resources publicly available through the Hugging Face Hub
and the MTEB package.

</details>


### [2] [MORABLES: A Benchmark for Assessing Abstract Moral Reasoning in LLMs with Fables](https://arxiv.org/abs/2509.12371)
*Matteo Marcuzzo,Alessandro Zangari,Andrea Albarelli,Jose Camacho-Collados,Mohammad Taher Pilehvar*

Main category: cs.CL

TL;DR: MORABLES是一个基于寓言和短篇故事构建的人类验证基准，用于评估LLM的道德推理能力，发现即使大型模型也容易受到对抗性攻击，存在显著的自相矛盾问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在标准阅读理解基准上的优异表现，研究重点转向评估其复杂抽象推理和推断能力，特别是基于文学作品的深度理解技能。

Method: 构建MORABLES基准，包含从历史文学中提取的寓言和短篇故事，采用多项选择题形式测试道德推理，并设计对抗性变体来测试模型鲁棒性。

Result: 大型模型表现优于小型模型，但对对抗性操作敏感，约20%的情况下会自相矛盾，推理增强模型未能弥补这一差距。

Conclusion: 规模而非推理能力是性能的主要驱动力，当前LLM在道德推理方面仍存在脆弱性，依赖表面模式而非真正的道德推理。

Abstract: As LLMs excel on standard reading comprehension benchmarks, attention is
shifting toward evaluating their capacity for complex abstract reasoning and
inference. Literature-based benchmarks, with their rich narrative and moral
depth, provide a compelling framework for evaluating such deeper comprehension
skills. Here, we present MORABLES, a human-verified benchmark built from fables
and short stories drawn from historical literature. The main task is structured
as multiple-choice questions targeting moral inference, with carefully crafted
distractors that challenge models to go beyond shallow, extractive question
answering. To further stress-test model robustness, we introduce adversarial
variants designed to surface LLM vulnerabilities and shortcuts due to issues
such as data contamination. Our findings show that, while larger models
outperform smaller ones, they remain susceptible to adversarial manipulation
and often rely on superficial patterns rather than true moral reasoning. This
brittleness results in significant self-contradiction, with the best models
refuting their own answers in roughly 20% of cases depending on the framing of
the moral choice. Interestingly, reasoning-enhanced models fail to bridge this
gap, suggesting that scale - not reasoning ability - is the primary driver of
performance.

</details>


### [3] [LLM-as-a-Judge: Rapid Evaluation of Legal Document Recommendation for Retrieval-Augmented Generation](https://arxiv.org/abs/2509.12382)
*Anu Pradhan,Alexandra Ortan,Apurv Verma,Madhavan Seshadri*

Main category: cs.CL

TL;DR: 本文研究在法学推荐系统中使用LLM作为评估者的可行性，发现传统评估指标在AI系统评估中可能误导，提出Gwet's AC2和秩相关系数作为更可靠的评估指标，结合Wilcoxon符号秩检验实现自动化但统计严谨的评估框架。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的兴起，传统评估指标在法学等专业领域的推荐系统评估中已不足以捕捉细微的质量维度，需要探索LLM作为评估者的可靠性来解决评估瓶颈问题。

Method: 通过系统实验研究两个关键问题：1）哪种评估者间可靠性指标最能捕捉LLM与人类评估的一致性；2）如何进行统计上可靠的系统比较。使用Krippendorff's alpha、Gwet's AC2和秩相关系数等指标进行对比分析。

Result: 发现传统的一致性指标如Krippendorff's alpha在AI系统评估的偏态分布中可能产生误导，而Gwet's AC2和秩相关系数更适合作为评估者选择的稳健指标。Wilcoxon符号秩检验结合Benjamini-Hochberg校正提供了可靠的系统比较统计严谨性。

Conclusion: 研究为法学应用提供了一条可扩展、成本效益高的评估路径，将原本人力密集的评估瓶颈转变为自动化但统计严谨的评估框架，满足了法律应用对精度的要求。

Abstract: The evaluation bottleneck in recommendation systems has become particularly
acute with the rise of Generative AI, where traditional metrics fall short of
capturing nuanced quality dimensions that matter in specialized domains like
legal research. Can we trust Large Language Models to serve as reliable judges
of their own kind? This paper investigates LLM-as-a-Judge as a principled
approach to evaluating Retrieval-Augmented Generation systems in legal
contexts, where the stakes of recommendation quality are exceptionally high.
  We tackle two fundamental questions that determine practical viability: which
inter-rater reliability metrics best capture the alignment between LLM and
human assessments, and how do we conduct statistically sound comparisons
between competing systems? Through systematic experimentation, we discover that
traditional agreement metrics like Krippendorff's alpha can be misleading in
the skewed distributions typical of AI system evaluations. Instead, Gwet's AC2
and rank correlation coefficients emerge as more robust indicators for judge
selection, while the Wilcoxon Signed-Rank Test with Benjamini-Hochberg
corrections provides the statistical rigor needed for reliable system
comparisons.
  Our findings suggest a path toward scalable, cost-effective evaluation that
maintains the precision demanded by legal applications, transforming what was
once a human-intensive bottleneck into an automated, yet statistically
principled, evaluation framework.

</details>


### [4] [SENTRA: Selected-Next-Token Transformer for LLM Text Detection](https://arxiv.org/abs/2509.12385)
*Mitchell Plyler,Yilun Zhang,Alexander Tuzhilin,Saoud Khalifah,Sen Tian*

Main category: cs.CL

TL;DR: 提出SENTRA检测器，基于Transformer编码器和对比预训练，在跨域检测LLM生成文本方面显著优于基线方法


<details>
  <summary>Details</summary>
Motivation: 随着LLM能力增强和广泛使用，其被滥用的风险也在增加，需要检测未明确声明的LLM生成文本

Method: 使用基于Transformer的编码器，利用选择的下一个token概率序列，在大规模无标注数据上进行对比预训练

Result: 在3个公开数据集、24个文本领域的实验中，SENTRA在跨域设置下显著优于流行基线方法

Conclusion: SENTRA是一个通用的分类器，能够有效检测LLM生成的文本，特别是在跨域场景下表现优异

Abstract: LLMs are becoming increasingly capable and widespread. Consequently, the
potential and reality of their misuse is also growing. In this work, we address
the problem of detecting LLM-generated text that is not explicitly declared as
such. We present a novel, general-purpose, and supervised LLM text detector,
SElected-Next-Token tRAnsformer (SENTRA). SENTRA is a Transformer-based encoder
leveraging selected-next-token-probability sequences and utilizing contrastive
pre-training on large amounts of unlabeled data. Our experiments on three
popular public datasets across 24 domains of text demonstrate SENTRA is a
general-purpose classifier that significantly outperforms popular baselines in
the out-of-domain setting.

</details>


### [5] [MORQA: Benchmarking Evaluation Metrics for Medical Open-Ended Question Answering](https://arxiv.org/abs/2509.12405)
*Wen-wai Yim,Asma Ben Abacha,Zixuan Yu,Robert Doerning,Fei Xia,Meliha Yetisgen*

Main category: cs.CL

TL;DR: MORQA是一个新的多语言医疗问答基准测试，用于评估自然语言生成系统在医疗领域的表现，发现基于大语言模型的评估方法显著优于传统指标


<details>
  <summary>Details</summary>
Motivation: 医疗领域的自然语言生成评估面临独特挑战，传统自动评估指标（如BLEU、ROUGE等）在区分高质量输出方面存在不足，特别是在开放式医疗问答任务中

Method: 构建MORQA多语言基准测试，包含英语和中文的医疗视觉和文本问答数据集，每个问题有2-4个以上医学专家编写的标准答案，并使用专家人工评分。对比传统指标和基于LLM（如GPT-4、Gemini）的评估方法

Result: 基于大语言模型的评估方法在相关性方面显著优于传统指标，LLM对语义细微差别敏感且对参考答案的变异性具有鲁棒性

Conclusion: 这是医疗领域首个全面的多语言NLG评估定性研究，强调了需要与人类判断对齐的评估方法，所有数据集和标注将公开发布以支持未来研究

Abstract: Evaluating natural language generation (NLG) systems in the medical domain
presents unique challenges due to the critical demands for accuracy, relevance,
and domain-specific expertise. Traditional automatic evaluation metrics, such
as BLEU, ROUGE, and BERTScore, often fall short in distinguishing between
high-quality outputs, especially given the open-ended nature of medical
question answering (QA) tasks where multiple valid responses may exist. In this
work, we introduce MORQA (Medical Open-Response QA), a new multilingual
benchmark designed to assess the effectiveness of NLG evaluation metrics across
three medical visual and text-based QA datasets in English and Chinese. Unlike
prior resources, our datasets feature 2-4+ gold-standard answers authored by
medical professionals, along with expert human ratings for three English and
Chinese subsets. We benchmark both traditional metrics and large language model
(LLM)-based evaluators, such as GPT-4 and Gemini, finding that LLM-based
approaches significantly outperform traditional metrics in correlating with
expert judgments. We further analyze factors driving this improvement,
including LLMs' sensitivity to semantic nuances and robustness to variability
among reference answers. Our results provide the first comprehensive,
multilingual qualitative study of NLG evaluation in the medical domain,
highlighting the need for human-aligned evaluation methods. All datasets and
annotations will be publicly released to support future research.

</details>


### [6] [MedFact: Benchmarking the Fact-Checking Capabilities of Large Language Models on Chinese Medical Texts](https://arxiv.org/abs/2509.12440)
*Jiayi He,Yangmin Huang,Qianyun Du,Xiangying Zhou,Zhiyang He,Jiaxue Hu,Xiaodong Tao,Lixian Lai*

Main category: cs.CL

TL;DR: MedFact是一个新的中文医疗事实核查基准，包含2116个专家标注实例，涵盖13个医学专业和8种错误类型，用于评估LLMs在医疗领域的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有基准在医疗领域的数据范围有限，无法捕捉真实世界医疗信息的复杂性，需要更严格的评估标准来确保LLMs在医疗应用中的事实可靠性。

Method: 采用混合AI-人类框架构建基准，通过迭代专家反馈优化AI驱动的多标准过滤过程，确保数据质量和难度。评估了20个领先LLMs在真实性分类和错误定位方面的表现。

Result: 模型通常能判断文本是否包含错误，但精确定位错误仍是重大挑战，即使表现最好的模型也达不到人类水平。发现模型存在"过度批评"现象，常将正确信息误判为错误。

Conclusion: MedFact揭示了LLMs在医疗应用中面临的关键挑战，为开发更事实可靠和医学感知的模型提供了强大资源。

Abstract: The increasing deployment of Large Language Models (LLMs) in healthcare
necessitates a rigorous evaluation of their factual reliability. However,
existing benchmarks are often limited by narrow domains of data, failing to
capture the complexity of real-world medical information. To address this
critical gap, we introduce MedFact, a new and challenging benchmark for Chinese
medical fact-checking. MedFact comprises 2,116 expert-annotated instances
curated from diverse real-world texts, spanning 13 medical specialties, 8
fine-grained error types, 4 writing styles, and multiple difficulty levels. Its
construction employs a hybrid AI-human framework where iterative expert
feedback refines an AI-driven, multi-criteria filtering process, ensuring both
high data quality and difficulty. We conduct a comprehensive evaluation of 20
leading LLMs, benchmarking their performance on veracity classification and
error localization against a human expert baseline. Our results reveal that
while models can often determine if a text contains an error, precisely
localizing it remains a substantial challenge, with even top-performing models
falling short of human performance. Furthermore, our analysis uncovers a
frequent ``over-criticism'' phenomenon, a tendency for models to misidentify
correct information as erroneous, which is exacerbated by advanced reasoning
techniques such as multi-agent collaboration and inference-time scaling. By
highlighting these critical challenges for deploying LLMs in medical
applications, MedFact provides a robust resource to drive the development of
more factually reliable and medically aware models.

</details>


### [7] [Topic Coverage-based Demonstration Retrieval for In-Context Learning](https://arxiv.org/abs/2509.12451)
*Wonbin Kweon,SeongKu Kang,Runchu Tian,Pengcheng Jiang,Jiawei Han,Hwanjo Yu*

Main category: cs.CL

TL;DR: TopicK是一个基于主题覆盖的检索框架，通过识别测试输入所需的细粒度知识主题并评估模型在这些主题上的知识水平，迭代选择能够覆盖模型知识不足主题的演示样本，从而提升上下文学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有的上下文学习方法通常仅基于嵌入相似度或生成概率来检索演示样本，这可能导致选择不相关或冗余的示例。为了提供测试输入所需的所有必要信息，需要识别和覆盖细粒度的知识需求。

Method: 提出TopicK框架：1）估计测试输入所需的主题；2）评估模型在这些主题上的知识水平；3）迭代选择能够引入先前未覆盖且模型知识水平低的主题的演示样本。

Result: 在多个数据集和各种开源及闭源大语言模型上进行了广泛实验，验证了TopicK的有效性。

Conclusion: TopicK通过主题覆盖的方法有效提升了上下文学习中演示样本的选择质量，能够更全面地覆盖测试输入和模型所需的相关知识主题。

Abstract: The effectiveness of in-context learning relies heavily on selecting
demonstrations that provide all the necessary information for a given test
input. To achieve this, it is crucial to identify and cover fine-grained
knowledge requirements. However, prior methods often retrieve demonstrations
based solely on embedding similarity or generation probability, resulting in
irrelevant or redundant examples. In this paper, we propose TopicK, a topic
coverage-based retrieval framework that selects demonstrations to
comprehensively cover topic-level knowledge relevant to both the test input and
the model. Specifically, TopicK estimates the topics required by the input and
assesses the model's knowledge on those topics. TopicK then iteratively selects
demonstrations that introduce previously uncovered required topics, in which
the model exhibits low topical knowledge. We validate the effectiveness of
TopicK through extensive experiments across various datasets and both open- and
closed-source LLMs. Our source code is available at
https://github.com/WonbinKweon/TopicK_EMNLP2025.

</details>


### [8] [Automated Generation of Research Workflows from Academic Papers: A Full-text Mining Framework](https://arxiv.org/abs/2509.12955)
*Heng Zhang,Chengzhi Zhang*

Main category: cs.CL

TL;DR: 提出端到端框架，通过挖掘全文学术论文生成结构化研究流程图，在NLP领域实现高精度工作流识别和可视化


<details>
  <summary>Details</summary>
Motivation: 现有方法只能提取碎片化流程组件，无法捕捉完整研究流程，需要自动化生成研究流程以提高研究可重复性和推动"AI for Science"范式

Method: 段落中心方法：1) PU学习和SciBERT识别工作流描述段落 2) Flan-T5生成工作流短语 3) ChatGPT少样本学习分类短语到数据准备、处理、分析阶段 4) 映射到文档位置生成可视化流程图

Result: 工作流段落识别F1-score 0.9772；工作流短语生成ROUGE-1/2/L分别为0.4543/0.2877/0.4427；分类精度0.958；成功分析NLP领域过去20年方法论转变

Conclusion: 提供了经过验证的自动化工作流生成技术框架，为实证研究演化科学范式提供了新颖的过程导向视角

Abstract: The automated generation of research workflows is essential for improving the
reproducibility of research and accelerating the paradigm of "AI for Science".
However, existing methods typically extract merely fragmented procedural
components and thus fail to capture complete research workflows. To address
this gap, we propose an end-to-end framework that generates comprehensive,
structured research workflows by mining full-text academic papers. As a case
study in the Natural Language Processing (NLP) domain, our paragraph-centric
approach first employs Positive-Unlabeled (PU) Learning with SciBERT to
identify workflow-descriptive paragraphs, achieving an F1-score of 0.9772.
Subsequently, we utilize Flan-T5 with prompt learning to generate workflow
phrases from these paragraphs, yielding ROUGE-1, ROUGE-2, and ROUGE-L scores of
0.4543, 0.2877, and 0.4427, respectively. These phrases are then systematically
categorized into data preparation, data processing, and data analysis stages
using ChatGPT with few-shot learning, achieving a classification precision of
0.958. By mapping categorized phrases to their document locations in the
documents, we finally generate readable visual flowcharts of the entire
research workflows. This approach facilitates the analysis of workflows derived
from an NLP corpus and reveals key methodological shifts over the past two
decades, including the increasing emphasis on data analysis and the transition
from feature engineering to ablation studies. Our work offers a validated
technical framework for automated workflow generation, along with a novel,
process-oriented perspective for the empirical investigation of evolving
scientific paradigms. Source code and data are available at:
https://github.com/ZH-heng/research_workflow.

</details>


### [9] [Does Language Model Understand Language?](https://arxiv.org/abs/2509.12459)
*Suvojit Acharjee,Utathya Aich,Asfak Ali*

Main category: cs.CL

TL;DR: 这篇论文通过新的LUCID数据集和RECISE指南，评估了多个SOTA语言模型在英语和孜加拉语中对语言细粒度现象（否定、时态、语态等）的理解能力，发现Compound-Beta模型在与人类判断对齐方面表现最优。


<details>
  <summary>Details</summary>
Motivation: 语言模型在教育技术中的应用需要关注其对语言细粒度现象的理解能力，确保语言清晰性以支持有效学习，特别是在联合国可持续发展目标4的背景下。

Method: 构建LUCID数据集（英语和孜加拉语的细致句子对），提出RECISE评估指南，使用Pearson相关、Spearman相关、MAE和新的HCE准确率指标，评估多个SOTA模型。

Result: Compound-Beta模型表现最优，在英语中获得最高Pearson相关系数，并在混合语言数据上体现出健壮性能，显示出与人类判断的强对齐。

Conclusion: 该研究强调了语言模型在细粒度语言现象理解方面的挑战，并提供了一个结构化评估框架，Compound-Beta模型的优异表现为教育技术中的模型部署提供了重要参考。

Abstract: Despite advances in natural language generation and understanding, LM still
struggle with fine grained linguistic phenomena such as tense, negation, voice,
and modality which are the elements central to effective human communication.
In the context of the United Nations SDG 4, where linguistic clarity is
critical, the deployment of LMs in educational technologies demands careful
scrutiny. As LMs are increasingly powering applications like tutoring systems,
automated grading, and translation, their alignment with human linguistic
interpretation becomes essential for effective learning. In this study, we
conduct a evaluation of SOTA language models across these challenging contexts
in both English and Bengali. To ensure a structured assessment, we introduce a
new Route for Evaluation of Cognitive Inference in Systematic Environments
guidelines. Our proposed LUCID dataset, composed of carefully crafted sentence
pairs in English and Bengali, specifically challenges these models on critical
aspects of language comprehension, including negation, tense, voice variations.
We assess the performance of SOTA models including MISTRAL-SABA-24B,
LLaMA-4-Scout-17B, LLaMA-3.3-70B, Gemma2-9B, and Compound-Beta using standard
metrics like Pearson correlation, Spearman correlation, and Mean Absolute
Error, as well as novel, linguistically inspired metric the HCE accuracy. The
HCE accuracy measures how often model predictions fall within one standard
deviation of the mean human rating, thus capturing human like tolerance for
variability in language interpretation. Our findings highlight Compound-Beta as
the most balanced model, consistently achieving high correlations and low MAEs
across diverse language conditions. It records the highest Pearson correlation
in English and demonstrates robust performance on mixed-language data,
indicating a strong alignment with human judgments in cross lingual scenarios.

</details>


### [10] [Audited Reasoning Refinement: Fine-Tuning Language Models via LLM-Guided Step-Wise Evaluation and Correction](https://arxiv.org/abs/2509.12476)
*Sumanta Bhattacharyya,Sara Riaz,Pedram Rooshenas*

Main category: cs.CL

TL;DR: R2tA方法通过提炼大语言模型的推理轨迹来训练任务专用的小型推理模型，在数据稀缺领域提供了一种实用且成本效益高的LLM适应方案


<details>
  <summary>Details</summary>
Motivation: 在直接人工监督或高质量标签稀缺的情况下，训练任务专用的小型推理模型具有挑战性。但具备推理能力的大语言模型能产生丰富的中间推理轨迹，可以系统性地提炼这些轨迹来创建有效的监督信号

Method: 提出Reason-Refine-then-Align (R2tA)方法：1) 从开源基础模型生成初始推理和响应；2) 提炼这些轨迹，修复幻觉和不一致性，形成高质量数据集；3) 进行两阶段对齐：监督微调(SFT)和直接偏好优化(DPO)，使模型的中间推理与人类验证的概念偏好对齐

Result: 在评估数据库系统设计中的扩展实体关系图(EERDs)案例研究中，构建了包含600个EERD变体的数据集（450/150训练/测试分割），涵盖11个错误类别。实证评估表明R2tA方法有效

Conclusion: R2tA为数据稀缺领域的可扩展LLM适应提供了实用且成本效益高的路径，能够为教育等领域开发可复现的AI工具

Abstract: Training a task-specific small reasoning model is challenging when direct
human supervision or high-quality labels are scarce. However, LLMs with
reasoning capabilities produce abundant intermediate reasoning traces that can
be systematically refined to create effective supervision signals. We propose
Reason-Refine-then-Align (R2tA), which turns refined model rationales into
supervision for training task-specific reasoning models. Our method generates
initial reasoning and responses from an open-source base model on task-specific
inputs, then refines these traces, fixing hallucinations and inconsistencies,
to form a high-fidelity dataset. We perform a two-stage alignment, supervised
fine-tuning (SFT), followed by direct preference optimization (DPO) to
calibrate the model's intermediate reasoning with human-validated conceptual
preferences and then condition the final output on that aligned reasoning. As a
case study, we apply R2tA to evaluate extended entity relationship diagrams
(EERDs) in database system design, a structurally complex task where
prompt-only methods miss or hallucinate errors. We curated a dataset of 600
EERD variants (train/test split of 450/150, respectively) with induced mistakes
spanning 11 categories. Empirical evaluation suggests R2tA provides a
practical, cost-effective path to scalable LLM adaptation in data-scarce
domains, enabling reproducible AI tools for education and beyond.

</details>


### [11] [FunAudio-ASR Technical Report](https://arxiv.org/abs/2509.12508)
*Keyu An,Yanni Chen,Chong Deng,Changfeng Gao,Zhifu Gao,Bo Gong,Xiangang Li,Yabin Li,Xiang Lv,Yunjie Ji,Yiheng Jiang,Bin Ma,Haoneng Luo,Chongjia Ni,Zexu Pan,Yiping Peng,Zhendong Peng,Peiyao Wang,Hao Wang,Wen Wang,Wupeng Wang,Biao Tian,Zhentao Tan,Nan Yang,Bin Yuan,Jieping Ye,Jixing Yu,Qinglin Zhang,Kun Zou,Han Zhao,Shengkui Zhao,Jingren Zhou*

Main category: cs.CL

TL;DR: FunAudio-ASR是一个基于大语言模型的大规模语音识别系统，通过数据扩展、模型规模扩展、LLM深度集成和强化学习相结合，在实际应用场景中实现最先进性能，特别针对实际部署进行了优化。


<details>
  <summary>Details</summary>
Motivation: 虽然基于大语言模型的ASR系统在开放基准测试中表现良好，但在实际工业评估中往往表现不佳，且LLM容易产生幻觉问题，影响用户体验。

Method: 结合海量数据、大模型容量、LLM集成和强化学习，针对流式处理、噪声鲁棒性、代码切换、热词定制等实际需求进行专门优化。

Result: FunAudio-ASR在实际应用数据集上实现了SOTA性能，证明了其在实践环境中的有效性和鲁棒性。

Conclusion: 该研究展示了如何通过生产导向的优化，使基于LLM的ASR系统在实际工业应用中达到卓越性能，解决了现有系统在真实场景中的局限性。

Abstract: In recent years, automatic speech recognition (ASR) has witnessed
transformative advancements driven by three complementary paradigms: data
scaling, model size scaling, and deep integration with large language models
(LLMs). However, LLMs are prone to hallucination, which can significantly
degrade user experience in real-world ASR applications. In this paper, we
present FunAudio-ASR, a large-scale, LLM-based ASR system that synergistically
combines massive data, large model capacity, LLM integration, and reinforcement
learning to achieve state-of-the-art performance across diverse and complex
speech recognition scenarios. Moreover, FunAudio-ASR is specifically optimized
for practical deployment, with enhancements in streaming capability, noise
robustness, code-switching, hotword customization, and satisfying other
real-world application requirements. Experimental results show that while most
LLM-based ASR systems achieve strong performance on open-source benchmarks,
they often underperform on real industry evaluation sets. Thanks to
production-oriented optimizations, FunAudio-ASR achieves SOTA performance on
real application datasets, demonstrating its effectiveness and robustness in
practical settings.

</details>


### [12] [A comparison of pipelines for the translation of a low resource language based on transformers](https://arxiv.org/abs/2509.12514)
*Chiara Bonfanti,Michele Colombino,Giulia Coucourde,Faeze Memari,Stefano Pinardi,Rosa Meo*

Main category: cs.CL

TL;DR: 本文比较了三种训练Transformer模型进行法语-班巴拉语机器翻译的流水线，发现简单Transformer模型在低资源翻译任务中表现最佳，达到10% BLEU和21% chrF分数


<details>
  <summary>Details</summary>
Motivation: 为非洲曼德语系的班巴拉语（约1418万使用者）开发有效的机器翻译系统，解决低资源语言翻译的挑战

Method: 1) 训练简单Transformer进行法-班翻译；2) 微调LLaMA3指导模型；3) 使用语言蒸馏技术将班巴拉语集成到预训练LaBSE模型中

Result: 简单Transformer流水线表现最好：在Bayelemagaba数据集上达到10% BLEU和21% chrF，在Yiri数据集上达到33.81% BLEU和41% chrF。指导模型在单一数据集上表现更好

Conclusion: 对于低资源语言翻译，简单的Transformer架构比复杂的指导模型和语言蒸馏方法更有效，指导模型倾向于捕获数据集特定模式

Abstract: This work compares three pipelines for training transformer-based neural
networks to produce machine translators for Bambara, a Mand\`e language spoken
in Africa by about 14,188,850 people. The first pipeline trains a simple
transformer to translate sentences from French into Bambara. The second
fine-tunes LLaMA3 (3B-8B) instructor models using decoder-only architectures
for French-to-Bambara translation. Models from the first two pipelines were
trained with different hyperparameter combinations to improve BLEU and chrF
scores, evaluated on both test sentences and official Bambara benchmarks. The
third pipeline uses language distillation with a student-teacher dual neural
network to integrate Bambara into a pre-trained LaBSE model, which provides
language-agnostic embeddings. A BERT extension is then applied to LaBSE to
generate translations. All pipelines were tested on Dokotoro (medical) and
Bayelemagaba (mixed domains). Results show that the first pipeline, although
simpler, achieves the best translation accuracy (10% BLEU, 21% chrF on
Bayelemagaba), consistent with low-resource translation results. On the Yiri
dataset, created for this work, it achieves 33.81% BLEU and 41% chrF.
Instructor-based models perform better on single datasets than on aggregated
collections, suggesting they capture dataset-specific patterns more
effectively.

</details>


### [13] [MAGIC-Enhanced Keyword Prompting for Zero-Shot Audio Captioning with CLIP Models](https://arxiv.org/abs/2509.12591)
*Vijay Govindarajan,Pratik Patel,Sahil Tripathi,Md Azizul Hoque,Gautam Siddharth Kashyap*

Main category: cs.CL

TL;DR: 提出零样本自动音频字幕生成系统，利用预训练模型避免大量训练，通过音频CLIP提取特征并生成结构化提示，引导大语言模型生成字幕，相比传统方法在NLG评分上提升35%


<details>
  <summary>Details</summary>
Motivation: 自动音频字幕生成面临数据集有限的问题，相比图像字幕数据量不足，需要克服训练数据稀缺的挑战

Method: 使用预训练音频CLIP模型提取听觉特征并生成结构化提示，通过大语言模型进行字幕生成，采用MAGIC搜索优化token选择确保与音频内容对齐

Result: 实验显示NLG平均分从4.7提升至7.3，提升35%，性能受音频-文本匹配模型和关键词选择影响显著，使用单关键词提示效果最佳，无关键词列表时性能下降50%

Conclusion: 零样本方法有效解决了音频字幕生成的数据稀缺问题，预训练模型结合结构化提示和优化搜索策略能显著提升字幕生成质量

Abstract: Automated Audio Captioning (AAC) generates captions for audio clips but faces
challenges due to limited datasets compared to image captioning. To overcome
this, we propose the zero-shot AAC system that leverages pre-trained models,
eliminating the need for extensive training. Our approach uses a pre-trained
audio CLIP model to extract auditory features and generate a structured prompt,
which guides a Large Language Model (LLM) in caption generation. Unlike
traditional greedy decoding, our method refines token selection through the
audio CLIP model, ensuring alignment with the audio content. Experimental
results demonstrate a 35% improvement in NLG mean score (from 4.7 to 7.3) using
MAGIC search with the WavCaps model. The performance is heavily influenced by
the audio-text matching model and keyword selection, with optimal results
achieved using a single keyword prompt, and a 50% performance drop when no
keyword list is used.

</details>


### [14] [EconProver: Towards More Economical Test-Time Scaling for Automated Theorem Proving](https://arxiv.org/abs/2509.12603)
*Mukai Li,Linfeng Song,Zhenwen Liang,Jiahao Xu,Shansan Gong,Qi Liu,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: 本文提出EconProver方法，通过动态CoT切换机制和多样化并行强化学习，显著降低自动定理证明的计算成本，仅用12%的计算资源就能达到基线方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在自动定理证明中采用测试时扩展策略（如反射式CoT推理和增加采样次数）带来显著性能提升，但也引入了巨大的计算开销，且现有成本分析忽略了不同扩展策略带来的采样成本差异。

Method: 提出两种互补方法：1）动态CoT切换机制减少不必要的token消耗；2）多样化并行扩展强化学习，使用可训练前缀在有限采样次数下提高通过率。这两种方法可集成到统一的EconRL流程中。

Result: 在miniF2F和ProofNet数据集上的实验表明，EconProver仅用12%的计算成本就达到了基线方法的可比性能。

Conclusion: 这项工作为部署轻量级自动定理证明模型提供了可行的见解，在不牺牲性能的前提下显著降低了计算成本。

Abstract: Large Language Models (LLMs) have recently advanced the field of Automated
Theorem Proving (ATP), attaining substantial performance gains through widely
adopted test-time scaling strategies, notably reflective Chain-of-Thought (CoT)
reasoning and increased sampling passes. However, they both introduce
significant computational overhead for inference. Moreover, existing cost
analyses typically regulate only the number of sampling passes, while
neglecting the substantial disparities in sampling costs introduced by
different scaling strategies. In this paper, we systematically compare the
efficiency of different test-time scaling strategies for ATP models and
demonstrate the inefficiency of the current state-of-the-art (SOTA) open-source
approaches. We then investigate approaches to significantly reduce token usage
and sample passes while maintaining the original performance. Specifically, we
propose two complementary methods that can be integrated into a unified EconRL
pipeline for amplified benefits: (1) a dynamic Chain-of-Thought (CoT) switching
mechanism designed to mitigate unnecessary token consumption, and (2) Diverse
parallel-scaled reinforcement learning (RL) with trainable prefixes to enhance
pass rates under constrained sampling passes. Experiments on miniF2F and
ProofNet demonstrate that our EconProver achieves comparable performance to
baseline methods with only 12% of the computational cost. This work provides
actionable insights for deploying lightweight ATP models without sacrificing
performance.

</details>


### [15] [Positional Encoding via Token-Aware Phase Attention](https://arxiv.org/abs/2509.12635)
*Yu,Wang,Sheng Shen,Rémi Munos,Hongyuan Zhan,Yuandong Tian*

Main category: cs.CL

TL;DR: 这篇论文提出了Token-Aware Phase Attention (TAPA)新的位置编码方法，解决了RoPE在长上下文序列中存在的距离依赖偏置问题，无需后置调整即可扩展到更长上下文长度。


<details>
  <summary>Details</summary>
Motivation: 识别到RoPE位置嵌入在模型长上下文时存在内在的距离依赖偏置，而现有扩展方法通常需要调整超参数或重新尚定，需要一种更有效的解决方案。

Method: 提出Token-Aware Phase Attention (TAPA)方法，将可学习的相位函数集成到注意力机制中，保持令牌间在长距离上的交互能力。

Result: TAPA能够通过直接轻量细调扩展到更长上下文，外推到未见长度，并在长上下文上获得显著更低的浪费率。

Conclusion: TAPA作为一种新的位置编码方法，有效解决了RoPE在长上下文序列建模中的限制，具有更好的扩展性和性能。

Abstract: We prove under practical assumptions that Rotary Positional Embedding (RoPE)
introduces an intrinsic distance-dependent bias in attention scores that limits
RoPE's ability to model long-context. RoPE extension methods may alleviate this
issue, but they typically require post-hoc adjustments after pretraining, such
as rescaling or hyperparameters retuning. This paper introduces Token-Aware
Phase Attention (TAPA), a new positional encoding method that incorporates a
learnable phase function into the attention mechanism. TAPA preserves token
interactions over long range, extends to longer contexts with direct and light
fine-tuning, extrapolates to unseen lengths, and attains significantly lower
perplexity on long-context than RoPE families.

</details>


### [16] [PAC: Pronunciation-Aware Contextualized Large Language Model-based Automatic Speech Recognition](https://arxiv.org/abs/2509.12647)
*Li Fu,Yu Xin,Sunlu Zeng,Lu Fan,Youzheng Wu,Xiaodong He*

Main category: cs.CL

TL;DR: 提出发音感知上下文框架(PAC)，通过两阶段学习解决LLM-ASR系统中的发音建模和同音词区分问题，显著降低词错误率和长尾词偏置错误率


<details>
  <summary>Details</summary>
Motivation: 解决基于大语言模型的自动语音识别系统中有效发音建模和鲁棒同音词区分两个关键挑战，这对原始词或长尾词识别至关重要

Method: 采用两阶段学习范式：1)发音引导的上下文学习方法，使用交错字素-音素上下文建模策略；2)发音区分性强化学习方法，通过扰动标签采样增强模型区分上下文同音词的能力

Result: 在英文Librispeech和中文AISHELL-1数据集上，相比预训练LLM-ASR模型，相对词错误率分别降低30.2%和53.8%；相比强基线，长尾词偏置错误率相对降低31.8%和60.5%

Conclusion: PAC框架有效解决了LLM-ASR系统中的发音建模和同音词区分问题，显著提升了识别性能，特别是在长尾词识别方面表现突出

Abstract: This paper presents a Pronunciation-Aware Contextualized (PAC) framework to
address two key challenges in Large Language Model (LLM)-based Automatic Speech
Recognition (ASR) systems: effective pronunciation modeling and robust
homophone discrimination. Both are essential for raw or long-tail word
recognition. The proposed approach adopts a two-stage learning paradigm. First,
we introduce a pronunciation-guided context learning method. It employs an
interleaved grapheme-phoneme context modeling strategy that incorporates
grapheme-only distractors, encouraging the model to leverage phonemic cues for
accurate recognition. Then, we propose a pronunciation-discriminative
reinforcement learning method with perturbed label sampling to further enhance
the model\'s ability to distinguish contextualized homophones. Experimental
results on the public English Librispeech and Mandarin AISHELL-1 datasets
indicate that PAC: (1) reduces relative Word Error Rate (WER) by 30.2% and
53.8% compared to pre-trained LLM-based ASR models, and (2) achieves 31.8% and
60.5% relative reductions in biased WER for long-tail words compared to strong
baselines, respectively.

</details>


### [17] [Don't Change My View: Ideological Bias Auditing in Large Language Models](https://arxiv.org/abs/2509.12652)
*Paul Kröger,Emilio Barkett*

Main category: cs.CL

TL;DR: 本文提出了一种检测大型语言模型意识形态偏向的方法，通过分析模型输出在相关主题提示下的分布变化来识别潜在的意识形态操纵，适用于对专有黑盒系统的审计。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在数百万用户产品中的广泛应用，其输出可能影响个人信念并塑造公众舆论。如果LLMs可以被有意引导向特定意识形态立场，控制这些系统的实体可能对公共话语产生不成比例的影响。

Method: 采用先前提出的统计方法，适应意识形态偏见审计的新背景。该方法具有模型无关的设计，不需要访问语言模型内部结构，而是通过分析模型在选定主题相关提示下输出的分布变化来识别潜在的意识形态操纵。

Result: 通过一系列实验验证了该方法的有效性，证明了其实际适用性以及支持对LLM行为进行独立事后审计的潜力。

Conclusion: 该方法为检测大型语言模型意识形态偏向提供了有效的工具，特别适合对专有黑盒系统进行审计，有助于确保AI系统的透明性和问责制。

Abstract: As large language models (LLMs) become increasingly embedded in products used
by millions, their outputs may influence individual beliefs and, cumulatively,
shape public opinion. If the behavior of LLMs can be intentionally steered
toward specific ideological positions, such as political or religious views,
then those who control these systems could gain disproportionate influence over
public discourse. Although it remains an open question whether LLMs can
reliably be guided toward coherent ideological stances and whether such
steering can be effectively prevented, a crucial first step is to develop
methods for detecting when such steering attempts occur. In this work, we adapt
a previously proposed statistical method to the new context of ideological bias
auditing. Our approach carries over the model-agnostic design of the original
framework, which does not require access to the internals of the language
model. Instead, it identifies potential ideological steering by analyzing
distributional shifts in model outputs across prompts that are thematically
related to a chosen topic. This design makes the method particularly suitable
for auditing proprietary black-box systems. We validate our approach through a
series of experiments, demonstrating its practical applicability and its
potential to support independent post hoc audits of LLM behavior.

</details>


### [18] [Mitigating Strategy Preference Bias in Emotional Support Conversation via Uncertainty Estimations](https://arxiv.org/abs/2509.12661)
*Yougen Zhou,Qin Chen,Ningning Zhou,Jie Zhou,Xingjiao Wu,Liang He*

Main category: cs.CL

TL;DR: 本文提出了一种通过强化学习双奖励函数来缓解大语言模型在情感支持对话中策略规划偏好偏差的方法，通过识别知识边界并优化准确性和置信度来改善策略规划效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在情感支持对话中存在策略规划准确性低和明显偏好偏差的问题，现有方法虽然通过微调策略规划器显示了一定潜力，但对偏好偏差的根本原因研究不足。

Method: 首先揭示LLMs策略规划中的知识边界，然后提出基于强化学习的双奖励函数方法，根据知识边界通过准确性和基于熵的置信度来优化策略规划。

Result: 在ESCov和ExTES数据集上的多个LLM骨干网络实验表明，该方法优于基线方法，证实了其有效性。

Conclusion: 该方法成功缓解了LLMs在情感支持对话中的策略规划偏好偏差，通过知识边界识别和双奖励优化机制提高了策略规划的质量和公平性。

Abstract: Emotional support conversation (ESC) aims to alleviate distress through
empathetic dialogue, yet large language models (LLMs) face persistent
challenges in delivering effective ESC due to low accuracy in strategy
planning. Moreover, there is a considerable preference bias towards specific
strategies. Prior methods using fine-tuned strategy planners have shown
potential in reducing such bias, while the underlying causes of the preference
bias in LLMs have not well been studied. To address these issues, we first
reveal the fundamental causes of the bias by identifying the knowledge
boundaries of LLMs in strategy planning. Then, we propose an approach to
mitigate the bias by reinforcement learning with a dual reward function, which
optimizes strategy planning via both accuracy and entropy-based confidence for
each region according to the knowledge boundaries. Experiments on the ESCov and
ExTES datasets with multiple LLM backbones show that our approach outperforms
the baselines, confirming the effectiveness of our approach.

</details>


### [19] [Chat-Driven Text Generation and Interaction for Person Retrieval](https://arxiv.org/abs/2509.12662)
*Zequn Xie,Chuxin Wang,Sihang Cai,Yeqiang Wang,Shulei Wang,Tao Jin*

Main category: cs.CL

TL;DR: 提出了MTG和MTI两个模块，构建无需人工标注的文本行人搜索框架，通过多轮对话生成伪标签和优化查询，显著提升检索性能


<details>
  <summary>Details</summary>
Motivation: 解决文本行人搜索中高质量文本标注获取困难、成本高昂的问题，提升系统的可扩展性和实际部署能力

Method: Multi-Turn Text Generation (MTG)模块通过与大语言模型模拟对话生成丰富的伪标签；Multi-Turn Text Interaction (MTI)模块在推理时通过动态对话优化用户查询

Result: 在广泛评估中实现了竞争性或更优的结果，同时消除了对人工标注的需求

Conclusion: 该方法为文本行人搜索系统的可扩展和实际部署铺平了道路，显著提高了检索准确性、鲁棒性和可用性

Abstract: Text-based person search (TBPS) enables the retrieval of person images from
large-scale databases using natural language descriptions, offering critical
value in surveillance applications. However, a major challenge lies in the
labor-intensive process of obtaining high-quality textual annotations, which
limits scalability and practical deployment. To address this, we introduce two
complementary modules: Multi-Turn Text Generation (MTG) and Multi-Turn Text
Interaction (MTI). MTG generates rich pseudo-labels through simulated dialogues
with MLLMs, producing fine-grained and diverse visual descriptions without
manual supervision. MTI refines user queries at inference time through dynamic,
dialogue-based reasoning, enabling the system to interpret and resolve vague,
incomplete, or ambiguous descriptions - characteristics often seen in
real-world search scenarios. Together, MTG and MTI form a unified and
annotation-free framework that significantly improves retrieval accuracy,
robustness, and usability. Extensive evaluations demonstrate that our method
achieves competitive or superior results while eliminating the need for manual
captions, paving the way for scalable and practical deployment of TBPS systems.

</details>


### [20] [Towards Inclusive Toxic Content Moderation: Addressing Vulnerabilities to Adversarial Attacks in Toxicity Classifiers Tackling LLM-generated Content](https://arxiv.org/abs/2509.12672)
*Shaz Furniturewala,Arkaitz Zubiaga*

Main category: cs.CL

TL;DR: 本文提出基于机制解释性技术的新策略，识别毒性分类器中导致误分类的脆弱组件，并通过抑制这些脆弱电路来提高对抗攻击的性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型生成内容激增，传统基于人类文本训练的内容审核分类器面临误分类问题，现有防御策略被动且效果有限，需要主动识别和修复模型脆弱性。

Method: 使用微调的BERT和RoBERTa分类器，通过对抗攻击技术识别脆弱电路，并在多样化数据集上进行测试，涵盖不同少数群体。

Result: 发现模型存在对性能关键或易受攻击的特定头部，抑制脆弱头部可提升对抗输入性能，不同人口群体间的脆弱性由不同头部负责。

Conclusion: 该方法不仅能提高模型鲁棒性，还能揭示模型训练中的公平性和鲁棒性差距，为开发更具包容性的毒性检测模型提供指导。

Abstract: The volume of machine-generated content online has grown dramatically due to
the widespread use of Large Language Models (LLMs), leading to new challenges
for content moderation systems. Conventional content moderation classifiers,
which are usually trained on text produced by humans, suffer from
misclassifications due to LLM-generated text deviating from their training data
and adversarial attacks that aim to avoid detection. Present-day defence
tactics are reactive rather than proactive, since they rely on adversarial
training or external detection models to identify attacks. In this work, we aim
to identify the vulnerable components of toxicity classifiers that contribute
to misclassification, proposing a novel strategy based on mechanistic
interpretability techniques. Our study focuses on fine-tuned BERT and RoBERTa
classifiers, testing on diverse datasets spanning a variety of minority groups.
We use adversarial attacking techniques to identify vulnerable circuits.
Finally, we suppress these vulnerable circuits, improving performance against
adversarial attacks. We also provide demographic-level insights into these
vulnerable circuits, exposing fairness and robustness gaps in model training.
We find that models have distinct heads that are either crucial for performance
or vulnerable to attack and suppressing the vulnerable heads improves
performance on adversarial input. We also find that different heads are
responsible for vulnerability across different demographic groups, which can
inform more inclusive development of toxicity detection models.

</details>


### [21] [Case-Based Decision-Theoretic Decoding with Quality Memories](https://arxiv.org/abs/2509.12677)
*Hiroyuki Deguchi,Masaaki Nagata*

Main category: cs.CL

TL;DR: 提出CBDT解码方法，结合案例推理和决策理论，通过领域数据样本来估计期望效用，在机器翻译和图像描述任务中优于传统MBR和MAP解码方法。


<details>
  <summary>Details</summary>
Motivation: 传统MBR解码依赖于从文本生成模型中采样的文本，难以准确捕捉域外知识或信息，需要改进以更好地处理域外数据。

Method: 提出基于案例的决策理论（CBDT）解码，利用领域数据样本来估计期望效用，结合MBR和CBDT解码方法。

Result: 在7个域De-En和Ja↔En翻译任务以及MSCOCO和nocaps数据集的图像描述任务中，MBR+CBDT组合方法优于单独的MBR解码。

Conclusion: CBDT解码不仅能生成比MAP解码更高质量的文本，而且与MBR结合后能进一步提升性能，特别是在处理域外知识方面表现更优。

Abstract: Minimum Bayes risk (MBR) decoding is a decision rule of text generation,
which selects the hypothesis that maximizes the expected utility and robustly
generates higher-quality texts than maximum a posteriori (MAP) decoding.
However, it depends on sample texts drawn from the text generation model; thus,
it is difficult to find a hypothesis that correctly captures the knowledge or
information of out-of-domain. To tackle this issue, we propose case-based
decision-theoretic (CBDT) decoding, another method to estimate the expected
utility using examples of domain data. CBDT decoding not only generates
higher-quality texts than MAP decoding, but also the combination of MBR and
CBDT decoding outperformed MBR decoding in seven domain De--En and
Ja$\leftrightarrow$En translation tasks and image captioning tasks on MSCOCO
and nocaps datasets.

</details>


### [22] [HistoryBankQA: Multilingual Temporal Question Answering on Historical Events](https://arxiv.org/abs/2509.12720)
*Biswadip Mandal,Anant Khandelwal,Manish Gupta*

Main category: cs.CL

TL;DR: 提出了HistoryBank，一个包含1000万+历史事件的多语言数据库，并构建了涵盖6种时间推理任务的QA基准测试，评估了多个主流大语言模型在历史事件时间推理方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有时间推理数据集规模有限、缺乏多语言覆盖且更关注当代事件，需要构建一个全面的多语言历史事件数据库和基准测试来评估LLM的时间推理能力。

Method: 从维基百科时间线页面和文章信息框中提取历史事件，构建包含10种语言的10M+事件数据库，并设计涵盖6种时间推理任务的QA基准。

Result: GPT4o在所有答案类型和语言中表现最佳；Gemma-2在小型语言模型中表现最优。数据库提供了前所未有的历史深度和语言广度覆盖。

Conclusion: HistoryBank为推进多语言和时间感知的历史事件自然语言理解提供了全面资源，代码和数据集将在论文接受后公开以促进进一步研究。

Abstract: Temporal reasoning about historical events is a critical skill for NLP tasks
like event extraction, historical entity linking, temporal question answering,
timeline summarization, temporal event clustering and temporal natural language
inference. Yet efforts on benchmarking temporal reasoning capabilities of large
language models (LLMs) are rather limited. Existing temporal reasoning datasets
are limited in scale, lack multilingual coverage and focus more on contemporary
events. To address these limitations, we present HistoryBank, a multilingual
database of 10M+ historical events extracted from Wikipedia timeline pages and
article infoboxes. Our database provides unprecedented coverage in both
historical depth and linguistic breadth with 10 languages. Additionally, we
construct a comprehensive question answering benchmark for temporal reasoning
across all languages. This benchmark covers a diverse set of 6 temporal QA
reasoning tasks, and we evaluate a suite of popular language models
(LLaMA-3-8B, Mistral-7B, Gemma-2-9b, Qwen3-8B, GPT4o) to assess their
performance on these tasks. As expected GPT4o performs best across all answer
types and languages; Gemma-2 outperforms the other small language models. Our
work aims to provide a comprehensive resource for advancing multilingual and
temporally-aware natural language understanding of historical events. To
facilitate further research, we will make our code and datasets publicly
available upon acceptance of this paper.

</details>


### [23] [Contrastive Learning with Enhanced Abstract Representations using Grouped Loss of Abstract Semantic Supervision](https://arxiv.org/abs/2509.12771)
*Omri Suissa,Muhiim Ali,Shengmai Chen,Yinuo Cai,Shekhar Pradhan*

Main category: cs.CL

TL;DR: 本文提出CLEAR GLASS模型，通过分组对比学习技术提升视觉语言模型的概念抽象能力，使用MAGIC数据集和新型对比损失函数，使模型能够识别图像中的高层次概念。


<details>
  <summary>Details</summary>
Motivation: 人类能够识别图像中的抽象概念，而不仅仅是对象和关系。研究旨在探索视觉语言模型是否具备这种概念抽象能力，并开发策略来编码图像中的高层次概念信息。

Method: 引入分组图像-标题数据集MAGIC，使用新型对比损失技术：外部对比损失（文本-图像对比组）和内部损失（组内实例距离）。训练迫使模型为每个图像-标题组创建接近高层次概念语义表示的潜在表示。

Result: 实验表明，该训练方法使模型在抽象概念识别方面相比最先进模型有所改进，概念抽象能力作为涌现能力出现。

Conclusion: 提出的分组对比学习方法有效提升了视觉语言模型的概念抽象能力，模型能够在未接触高层次概念标签的情况下学习到概念级别的语义表示。

Abstract: Humans can recognize an image as an instance of a general concept, beyond
simply identifying its objects and their relationships. In this paper, we
investigate 1. The extent to which VLMs have this concept abstraction capacity,
and 2. Strategies for encoding the sort of higher-concept information in images
that would enable the resulting VLM model (CLEAR GLASS model) to have this
capability to a greater degree. To this end, we introduce a grouped
image-caption dataset (MAGIC), which consists of several groups of image
captions and for each group a set of associated images and higher-level
conceptual labels. We use a novel contrastive loss technique to induce the
model to encode in the representation of each image (caption) in a group the
information that is common to all members of the image-caption group. Our main
contribution is a grouped contrastive loss function based on text-image
contrastive groups (outer contrastive loss) as well as an inner loss which
measures the distances between image-caption instances in the group. Our
training methodology results in the CLEAR GLASS model having the concept
abstraction capacity as an emergent capacity because the model is not exposed
to the higher-level concepts associated with each group. Instead, the training
forces the model to create for each image-caption group a semantic
representation that brings it closer to the semantic representation of the
higher-level concepts in the latent semantic space. Our experiments show that
this training methodology results in a model which shows improvement in
abstract concept recognition compared to SOTA models.

</details>


### [24] [ConvergeWriter: Data-Driven Bottom-Up Article Construction](https://arxiv.org/abs/2509.12811)
*Binquan Ji,Jiaqi Wang,Ruiting Li,Xingchen Han,Yiyang Qi,Shichao Wang,Yifei Lu,Yuantao Han,Feiliang Ren*

Main category: cs.CL

TL;DR: 提出了一种新颖的"自下而上"数据驱动框架，通过"检索优先、聚类构建结构"策略，先建立知识边界再进行生成规划，有效解决长文本生成中的事实准确性和内容碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 现有"自上而下"方法在生成假设或大纲后再检索证据，容易导致模型计划与可用知识脱节，造成内容碎片化和事实不准确。需要一种能够严格约束生成内容、确保可追溯性的方法。

Method: 采用"检索优先获取知识，聚类构建结构"策略：1）从知识库进行详尽迭代检索；2）使用无监督聚类算法将检索文档组织成"知识簇"；3）基于这些簇生成层次化大纲和最终文档内容。

Result: 在14B和32B参数模型上的实验结果表明，该方法达到或超过最先进基线性能，在需要高保真度和结构连贯性的知识约束场景中展现出独特优势。

Conclusion: 该方法为生成可靠、结构化长文档提供了有效范式，为高风险、知识密集型领域中的LLM应用铺平了道路，从根本上减轻了幻觉风险。

Abstract: Large Language Models (LLMs) have shown remarkable prowess in text
generation, yet producing long-form, factual documents grounded in extensive
external knowledge bases remains a significant challenge. Existing "top-down"
methods, which first generate a hypothesis or outline and then retrieve
evidence, often suffer from a disconnect between the model's plan and the
available knowledge, leading to content fragmentation and factual inaccuracies.
To address these limitations, we propose a novel "bottom-up," data-driven
framework that inverts the conventional generation pipeline. Our approach is
predicated on a "Retrieval-First for Knowledge, Clustering for Structure"
strategy, which first establishes the "knowledge boundaries" of the source
corpus before any generative planning occurs. Specifically, we perform
exhaustive iterative retrieval from the knowledge base and then employ an
unsupervised clustering algorithm to organize the retrieved documents into
distinct "knowledge clusters." These clusters form an objective, data-driven
foundation that directly guides the subsequent generation of a hierarchical
outline and the final document content. This bottom-up process ensures that the
generated text is strictly constrained by and fully traceable to the source
material, proactively adapting to the finite scope of the knowledge base and
fundamentally mitigating the risk of hallucination. Experimental results on
both 14B and 32B parameter models demonstrate that our method achieves
performance comparable to or exceeding state-of-the-art baselines, and is
expected to demonstrate unique advantages in knowledge-constrained scenarios
that demand high fidelity and structural coherence. Our work presents an
effective paradigm for generating reliable, structured, long-form documents,
paving the way for more robust LLM applications in high-stakes,
knowledge-intensive domains.

</details>


### [25] [Data Augmentation for Maltese NLP using Transliterated and Machine Translated Arabic Data](https://arxiv.org/abs/2509.12853)
*Kurt Micallef,Nizar Habash,Claudia Borg*

Main category: cs.CL

TL;DR: 探索阿拉伯语资源通过跨语言增强技术能否支持马耳他语NLP，引入新的音译系统并证明阿拉伯语增强能显著提升马耳他语NLP任务性能


<details>
  <summary>Details</summary>
Motivation: 马耳他语作为受罗曼语和日耳曼语影响的独特闪语，其拉丁字母书写与阿拉伯语等近亲存在差距，需要研究能否利用阿拉伯语资源来支持其NLP

Method: 研究多种阿拉伯语文本数据与马耳他语对齐策略，包括不同音译方案和机器翻译方法，并引入新的能更好代表马耳他语正字法的音译系统

Result: 阿拉伯语增强能显著有益于马耳他语NLP任务

Conclusion: 阿拉伯语资源可以通过跨语言增强技术有效支持马耳他语NLP，特别是在单语和多语模型中

Abstract: Maltese is a unique Semitic language that has evolved under extensive
influence from Romance and Germanic languages, particularly Italian and
English. Despite its Semitic roots, its orthography is based on the Latin
script, creating a gap between it and its closest linguistic relatives in
Arabic. In this paper, we explore whether Arabic-language resources can support
Maltese natural language processing (NLP) through cross-lingual augmentation
techniques. We investigate multiple strategies for aligning Arabic textual data
with Maltese, including various transliteration schemes and machine translation
(MT) approaches. As part of this, we also introduce novel transliteration
systems that better represent Maltese orthography. We evaluate the impact of
these augmentations on monolingual and mutlilingual models and demonstrate that
Arabic-based augmentation can significantly benefit Maltese NLP tasks.

</details>


### [26] [Benchmarking and Improving LVLMs on Event Extraction from Multimedia Documents](https://arxiv.org/abs/2509.12876)
*Fuyu Xing,Zimu Wang,Wei Wang,Haiyang Zhang*

Main category: cs.CL

TL;DR: 对DeepSeek-VL2和Qwen-VL系列等大型视觉语言模型在多媒体事件抽取任务上的首次系统性评估，发现few-shot下视觉任务表现更好但文本任务困难，LoRA微调显著提升性能，多模态组合展现强协同效应


<details>
  <summary>Details</summary>
Motivation: 多媒体内容激增需要有效的多媒体事件抽取系统，但大型视觉语言模型在此任务上的应用潜力尚未充分探索

Method: 在M2E2数据集上评估代表性LVLMs，涵盖文本、图像和跨媒体子任务，采用few-shot提示和LoRA微调两种设置

Result: few-shot LVLMs在视觉任务表现更好但文本任务困难；LoRA微调显著提升性能；多模态组合展现强协同效应，在跨模态设置中达到最优性能

Conclusion: LVLMs在多媒体事件抽取中展现潜力，但在语义精度、定位和跨模态基础等关键挑战方面仍需改进

Abstract: The proliferation of multimedia content necessitates the development of
effective Multimedia Event Extraction (M2E2) systems. Though Large
Vision-Language Models (LVLMs) have shown strong cross-modal capabilities,
their utility in the M2E2 task remains underexplored. In this paper, we present
the first systematic evaluation of representative LVLMs, including DeepSeek-VL2
and the Qwen-VL series, on the M2E2 dataset. Our evaluations cover text-only,
image-only, and cross-media subtasks, assessed under both few-shot prompting
and fine-tuning settings. Our key findings highlight the following valuable
insights: (1) Few-shot LVLMs perform notably better on visual tasks but
struggle significantly with textual tasks; (2) Fine-tuning LVLMs with LoRA
substantially enhances model performance; and (3) LVLMs exhibit strong synergy
when combining modalities, achieving superior performance in cross-modal
settings. We further provide a detailed error analysis to reveal persistent
challenges in areas such as semantic precision, localization, and cross-modal
grounding, which remain critical obstacles for advancing M2E2 capabilities.

</details>


### [27] [The LLM Already Knows: Estimating LLM-Perceived Question Difficulty via Hidden Representations](https://arxiv.org/abs/2509.12886)
*Yubo Zhu,Dongrui Liu,Zecheng Lin,Wei Tong,Sheng Zhong,Jing Shao*

Main category: cs.CL

TL;DR: 提出一种基于LLM隐藏状态的无输出token生成难度估计方法，通过马尔可夫链建模token生成过程，仅利用初始隐藏状态即可高效准确估计问题难度


<details>
  <summary>Details</summary>
Motivation: 现有LLM难度估计方法依赖重复响应采样、辅助模型或微调目标模型，计算成本高且可能影响通用性，需要更高效通用的难度估计方案

Method: 将token级生成过程建模为马尔可夫链，定义价值函数来估计给定隐藏状态的预期输出质量，仅基于初始隐藏状态进行难度估计

Result: 在文本和多模态任务上的广泛实验表明，该方法在难度估计方面一致优于现有基线方法

Conclusion: 该方法可有效指导自适应推理策略（如Self-Consistency、Best-of-N、Self-Refine），以更少的生成token实现更高的推理效率

Abstract: Estimating the difficulty of input questions as perceived by large language
models (LLMs) is essential for accurate performance evaluation and adaptive
inference. Existing methods typically rely on repeated response sampling,
auxiliary models, or fine-tuning the target model itself, which may incur
substantial computational costs or compromise generality. In this paper, we
propose a novel approach for difficulty estimation that leverages only the
hidden representations produced by the target LLM. We model the token-level
generation process as a Markov chain and define a value function to estimate
the expected output quality given any hidden state. This allows for efficient
and accurate difficulty estimation based solely on the initial hidden state,
without generating any output tokens. Extensive experiments across both textual
and multimodal tasks demonstrate that our method consistently outperforms
existing baselines in difficulty estimation. Moreover, we apply our difficulty
estimates to guide adaptive reasoning strategies, including Self-Consistency,
Best-of-N, and Self-Refine, achieving higher inference efficiency with fewer
generated tokens.

</details>


### [28] [Conan-Embedding-v2: Training an LLM from Scratch for Text Embeddings](https://arxiv.org/abs/2509.12892)
*Shiyu Li,Yang Tang,Ruijie Liu,Shi-Zhe Chen,Xi Chen*

Main category: cs.CL

TL;DR: Conan-embedding-v2是一个14亿参数的文本嵌入模型，通过从零训练和专门优化，在MTEB基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法使用LoRA微调LLMs时存在的数据和训练差距问题，包括LLMs与嵌入模型之间的数据差异以及因果掩码与双向掩码的训练差异。

Method: 1) 添加新闻数据和多语言对进行LLM预训练；2) 提出跨语言检索数据集；3) 引入软掩码机制逐步过渡因果掩码和双向掩码；4) 提出动态硬负样本挖掘方法。

Result: 仅用14亿参数就在MTEB和中文MTEB基准测试中（截至2025年5月19日）取得了最先进的性能。

Conclusion: 通过从零训练、软掩码机制和动态负样本挖掘，成功构建了高效的文本嵌入模型，证明了专门设计的训练策略的重要性。

Abstract: Large language models (LLMs) have recently demonstrated excellent performance
in text embedding tasks. Previous work usually use LoRA to fine-tune existing
LLMs, which are limited by the data and training gap between LLMs and embedding
models. In this work, we introduce Conan-embedding-v2, a new 1.4B-parameter LLM
trained from scratch and fine-tuned as a text embedder. First, we add news data
and multilingual pairs for LLM pretraining to bridge the data gap. Based on
this, we propose a cross-lingual retrieval dataset that enables the LLM to
better integrate embeddings across different languages. Second, whereas LLMs
use a causal mask with token-level loss, embedding models use a bidirectional
mask with sentence-level loss. This training gap makes full fine-tuning less
effective than LoRA. We introduce a soft-masking mechanism to gradually
transition between these two types of masks, enabling the model to learn more
comprehensive representations. Based on this, we propose a dynamic hard
negative mining method that exposes the model to more difficult negative
examples throughout the training process. Being intuitive and effective, with
only approximately 1.4B parameters, Conan-embedding-v2 achieves SOTA
performance on both the Massive Text Embedding Benchmark (MTEB) and Chinese
MTEB (May 19, 2025).

</details>


### [29] [All Roads Lead to Rome: Graph-Based Confidence Estimation for Large Language Model Reasoning](https://arxiv.org/abs/2509.12908)
*Caiqi Zhang,Chang Shu,Ehsan Shareghi,Nigel Collier*

Main category: cs.CL

TL;DR: 提出基于图的无训练置信度估计方法，专门针对大语言模型的推理任务，通过建模推理路径为有向图并利用图属性来改进置信度估计。


<details>
  <summary>Details</summary>
Motivation: 现有的置信度估计方法主要针对事实问答任务，在推理任务上泛化能力不足，需要专门针对推理任务设计置信度估计方法。

Method: 将推理路径建模为有向图，利用图的中心性、路径收敛性和路径权重等属性进行置信度估计，无需训练。

Result: 在两个大语言模型和三个推理数据集上的实验表明，该方法改进了置信度估计，并在两个下游任务上提升了性能。

Conclusion: 基于图的置信度估计方法能够有效提升大语言模型在推理任务上的可靠性，为模型部署提供更好的置信度评估。

Abstract: Confidence estimation is essential for the reliable deployment of large
language models (LLMs). Existing methods are primarily designed for factual QA
tasks and often fail to generalize to reasoning tasks. To address this gap, we
propose a set of training-free, graph-based confidence estimation methods
tailored to reasoning tasks. Our approach models reasoning paths as directed
graphs and estimates confidence by exploiting graph properties such as
centrality, path convergence, and path weighting. Experiments with two LLMs on
three reasoning datasets demonstrate improved confidence estimation and
enhanced performance on two downstream tasks.

</details>


### [30] [Investigating ReLoRA: Effects on the Learning Dynamics of Small Language Models](https://arxiv.org/abs/2509.12960)
*Yuval Weiss,David Demitri Africa,Paula Buttery,Richard Diehl Martinez*

Main category: cs.CL

TL;DR: ReLoRA在小型语言模型(11M-66M参数)预训练中表现不如标准训练，性能差距随模型增大而扩大，且会加剧小模型的秩缺陷问题。


<details>
  <summary>Details</summary>
Motivation: 研究ReLoRA在小语言模型预训练中的效果，因为小模型具有更低的计算和环境成本，但参数高效方法在预训练阶段的扩展尚不明确。

Method: 通过消融实验系统评估ReLoRA在11M-66M参数小语言模型上的性能表现和学习动态，包括损失、Paloma困惑度和BLiMP等指标。

Result: ReLoRA在所有评估指标上都比标准训练表现更差，且随着模型规模增大，性能差距更加明显。分析显示ReLoRA会强化小模型中已有的秩缺陷。

Conclusion: 低秩更新策略可能不适用于小语言模型的预训练，需要在低计算资源环境下进行更多研究来改进参数高效方法。

Abstract: Parameter-efficient methods such as LoRA have revolutionised the fine-tuning
of LLMs. Still, their extension to pretraining via ReLoRA is less well
understood, especially for small language models (SLMs), which offer lower
computational and environmental costs. This work is the first systematic study
of ReLoRA in SLMs (11M-66M parameters), evaluating both performance and
learning dynamics. Through ablation experiments, we find that ReLoRA generally
performs worse than standard training on loss, Paloma perplexity and BLiMP,
with the gap widening for the larger models. Further analysis of the learning
dynamics of the models indicates that ReLoRA reinforces the rank deficiencies
found in smaller models. These results indicate that low-rank update strategies
may not transfer easily to SLM pretraining, highlighting the need for more
research in the low-compute regime.

</details>


### [31] [Do LLMs Understand Wine Descriptors Across Cultures? A Benchmark for Cultural Adaptations of Wine Reviews](https://arxiv.org/abs/2509.12961)
*Chenye Zou,Xingyue Wen,Tianyi Hu,Qian Janice Wang,Daniel Hershcovich*

Main category: cs.CL

TL;DR: 本文研究跨文化葡萄酒评论的适应性翻译，提出文化导向的评估标准，发现现有模型在捕捉文化细微差别方面存在困难


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的发展为文化感知语言任务提供了可能，需要研究超越字面翻译的文化适应性转换，特别是在葡萄酒评论这种富含文化特定描述符的领域

Method: 构建首个包含8k中文和16k英文评论的平行语料库，使用神经机器翻译基线和最先进的大语言模型进行基准测试，提出文化接近度、文化中立性和文化真实性三个文化导向评估标准

Result: 分析表明当前模型在捕捉文化细微差别方面存在困难，特别是在跨文化翻译葡萄酒描述时，突显了翻译模型在处理文化内容时的挑战和局限性

Conclusion: 跨文化葡萄酒评论适应是一个具有挑战性的问题，现有翻译模型在文化内容处理方面仍有明显局限，需要开发更有效的文化感知翻译方法

Abstract: Recent advances in large language models (LLMs) have opened the door to
culture-aware language tasks. We introduce the novel problem of adapting wine
reviews across Chinese and English, which goes beyond literal translation by
incorporating regional taste preferences and culture-specific flavor
descriptors. In a case study on cross-cultural wine review adaptation, we
compile the first parallel corpus of professional reviews, containing 8k
Chinese and 16k Anglophone reviews. We benchmark both
neural-machine-translation baselines and state-of-the-art LLMs with automatic
metrics and human evaluation. For the latter, we propose three culture-oriented
criteria -- Cultural Proximity, Cultural Neutrality, and Cultural Genuineness
-- to assess how naturally a translated review resonates with target-culture
readers. Our analysis shows that current models struggle to capture cultural
nuances, especially in translating wine descriptions across different cultures.
This highlights the challenges and limitations of translation models in
handling cultural content.

</details>


### [32] [SitLLM: Large Language Models for Sitting Posture Health Understanding via Pressure Sensor Data](https://arxiv.org/abs/2509.12994)
*Jian Gao,Fufangchen Zhao,Yiyang Zhang,Danfeng Yan*

Main category: cs.CL

TL;DR: SitLLM是一个轻量级多模态框架，结合压力传感和大型语言模型，实现细粒度坐姿理解和个性化健康反馈生成。


<details>
  <summary>Details</summary>
Motivation: 现有坐姿监测系统识别粒度粗糙，缺乏语义表达能力，无法提供个性化反馈。不良坐姿是导致长期肌肉骨骼疾病和生理功能障碍的关键因素。

Method: 包含三个核心组件：1) 高斯鲁棒传感器嵌入模块，对压力图进行空间分块并注入局部噪声扰动；2) 提示驱动的跨模态对齐模块，通过多头交叉注意力将传感器嵌入重编程到LLM语义空间；3) 多上下文提示模块，融合特征级、结构级、统计级和语义级上下文信息。

Result: 论文提出了SitLLM框架，但摘要中未明确说明具体实验结果。

Conclusion: SitLLM通过结合压力传感和LLM技术，能够实现更精细的坐姿理解和个性化的健康导向响应生成，解决了现有系统的局限性。

Abstract: Poor sitting posture is a critical yet often overlooked factor contributing
to long-term musculoskeletal disorders and physiological dysfunctions. Existing
sitting posture monitoring systems, although leveraging visual, IMU, or
pressure-based modalities, often suffer from coarse-grained recognition and
lack the semantic expressiveness necessary for personalized feedback. In this
paper, we propose \textbf{SitLLM}, a lightweight multimodal framework that
integrates flexible pressure sensing with large language models (LLMs) to
enable fine-grained posture understanding and personalized health-oriented
response generation. SitLLM comprises three key components: (1) a
\textit{Gaussian-Robust Sensor Embedding Module} that partitions pressure maps
into spatial patches and injects local noise perturbations for robust feature
extraction; (2) a \textit{Prompt-Driven Cross-Modal Alignment Module} that
reprograms sensor embeddings into the LLM's semantic space via multi-head
cross-attention using the pre-trained vocabulary embeddings; and (3) a
\textit{Multi-Context Prompt Module} that fuses feature-level, structure-level,
statistical-level, and semantic-level contextual information to guide
instruction comprehension.

</details>


### [33] [Multi-Model Synthetic Training for Mission-Critical Small Language Models](https://arxiv.org/abs/2509.13047)
*Nolan Platt,Pragyansmita Nayak*

Main category: cs.CL

TL;DR: 使用LLM作为一次性教师生成合成数据，而非直接推理，实现261倍成本降低，在海上情报任务中达到75%准确率


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在专业领域应用受限，因为领域特定训练数据稀缺且复杂，需要寻找更经济高效的解决方案

Method: 通过GPT-4o和o3-mini多模型生成，将32亿条AIS船舶跟踪记录转换为21,543个合成问答对，用于微调Qwen2.5-7B模型

Result: 微调后的模型在海上任务中达到75%准确率，相比使用更大模型进行推理成本大幅降低

Conclusion: 经过适当微调的更小、更便宜的模型可以提供与昂贵大模型相似的准确性，为专业AI应用的合成数据集生成提供了可复现框架

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
many domains, yet their application to specialized fields remains constrained
by the scarcity and complexity of domain-specific training data. We present a
novel approach that achieves a 261x cost reduction for maritime intelligence by
using LLMs as one-time teachers rather than using them directly for inference.
Our method transforms 3.2 billion Automatic Identification System (AIS) vessel
tracking records into 21,543 synthetic question and answer pairs through
multi-model generation (GPT-4o and o3-mini), preventing overfitting and
ensuring accurate reasoning. The resulting fine-tuned Qwen2.5-7B model achieves
75% accuracy on maritime tasks, while being substantially cheaper than using a
larger model for inference. We show that smaller, cheaper models -- when fine
tuned properly -- can provide similar accuracy compared to larger models that
are prohibitively expensive. Our work contributes to the growing field of
synthetic dataset generation for specialized AI applications and presents a
highly reproducible framework for domains where manual annotation is
infeasible. Beyond expanding research in the growing field of specialized small
language models, our approach has immediate applications in maritime safety,
security operations, and vessel traffic management systems in various
industries.

</details>


### [34] [Shaping Explanations: Semantic Reward Modeling with Encoder-Only Transformers for GRPO](https://arxiv.org/abs/2509.13081)
*Francesco Pappone,Ruggero Marino Lazzaroni,Federico Califano,Niccolò Gentile,Roberto Marras*

Main category: cs.CL

TL;DR: 本文提出在GRPO框架中使用小型编码器变换器作为语义奖励模型，通过余弦相似度提供密集的语义奖励信号，显著改善了意大利医学入学考试解释生成任务的忠实性和清晰度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成符合复杂定性目标（如教学合理性）的输出方面存在挑战，传统强化学习技术依赖缓慢昂贵的LLM评估或脆弱的基于关键词的指标，无法捕捉高质量解释的语义本质。

Method: 在GRPO框架中引入小型高效的编码器变换器作为语义奖励模型，基于生成解释与真实参考之间的余弦相似度提供密集语义奖励信号，指导策略生成不仅事实正确且在结构和概念上与专家推理一致的解释。

Result: 在意大利医学入学考试训练任务中，使用语义奖励的GRPO方法相比强SFT基线显著提高了解释的忠实性和清晰度。

Conclusion: 使用轻量级编码器模型进行细致奖励塑造在复杂生成任务中具有强大效果，为语义对齐提供了有效解决方案。

Abstract: While Large Language Models (LLMs) excel at generating human-like text,
aligning their outputs with complex, qualitative goals like pedagogical
soundness remains a significant challenge. Standard reinforcement learning
techniques often rely on slow and expensive LLM-as-a-judge evaluations or on
brittle, keyword-based metrics like ROUGE, which fail to capture the semantic
essence of a high-quality explanation. In this work, we introduce a novel
approach to reward shaping within the Group Relative Policy Optimisation (GRPO)
framework. Our central contribution is the use of a small, efficient
encoder-only transformer as a semantic reward model. This model provides a
dense, semantically rich reward signal based on the cosine similarity between a
generated explanation and a ground-truth reference, guiding the policy towards
explanations that are not just factually correct but also structurally and
conceptually aligned with expert reasoning. We apply this method to the task of
training a model for the Italian medical-school entrance examinations,
following standard domain-adaptive continued pre-training (CPT) and supervised
fine-tuning (SFT). Our results demonstrate that GRPO with our proposed semantic
reward significantly improves explanation faithfulness and clarity over a
strong SFT baseline, showcasing the power of using lightweight encoder models
for nuanced reward shaping in complex generation tasks

</details>


### [35] [Empowering LLMs with Parameterized Skills for Adversarial Long-Horizon Planning](https://arxiv.org/abs/2509.13127)
*Sijia Cui,Shuai Xu,Aiyao He,Yanna Wang,Bo Xu*

Main category: cs.CL

TL;DR: PLAP框架通过语言规划与参数化执行相结合，解决了LLM智能体在长时域对抗环境中的落地难题，在MicroRTS游戏中表现优异，超越了多数基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体方法存在两个主要问题：直接生成底层动作不可靠，而高层任务规划又依赖专家经验来转换为具体动作序列。需要一种既能利用LLM规划能力又能可靠执行的方法。

Method: 提出PLAP（Plan with Language, Act with Parameter）框架，包含三个核心组件：环境特定的参数化技能库、LLM驱动的技能规划器、将参数化技能转换为可执行动作序列的技能执行器。

Result: 在MicroRTS实时策略游戏中，GPT-4o驱动的零样本PLAP超越80%基线智能体，Qwen2-72B驱动的少样本PLAP甚至超越了顶级脚本智能体CoacAI。还建立了LLM长时域技能规划能力排行榜。

Conclusion: PLAP框架有效解决了LLM智能体在复杂长时域环境中的落地问题，通过分离语言规划和参数化执行，既发挥了LLM的规划优势，又保证了动作执行的可靠性。

Abstract: Recent advancements in Large Language Models(LLMs) have led to the
development of LLM-based AI agents. A key challenge is the creation of agents
that can effectively ground themselves in complex, adversarial long-horizon
environments. Existing methods mainly focus on (1) using LLMs as policies to
interact with the environment through generating low-level feasible actions,
and (2) utilizing LLMs to generate high-level tasks or language guides to
stimulate action generation. However, the former struggles to generate reliable
actions, while the latter relies heavily on expert experience to translate
high-level tasks into specific action sequences. To address these challenges,
we introduce the Plan with Language, Act with Parameter (PLAP) planning
framework that facilitates the grounding of LLM-based agents in long-horizon
environments. The PLAP method comprises three key components: (1) a skill
library containing environment-specific parameterized skills, (2) a skill
planner powered by LLMs, and (3) a skill executor converting the parameterized
skills into executable action sequences. We implement PLAP in MicroRTS, a
long-horizon real-time strategy game that provides an unfamiliar and
challenging environment for LLMs. The experimental results demonstrate the
effectiveness of PLAP. In particular, GPT-4o-driven PLAP in a zero-shot setting
outperforms 80% of baseline agents, and Qwen2-72B-driven PLAP, with carefully
crafted few-shot examples, surpasses the top-tier scripted agent, CoacAI.
Additionally, we design comprehensive evaluation metrics and test 6
closed-source and 2 open-source LLMs within the PLAP framework, ultimately
releasing an LLM leaderboard ranking long-horizon skill planning ability. Our
code is available at https://github.com/AI-Research-TeamX/PLAP.

</details>


### [36] [LLM Hallucination Detection: A Fast Fourier Transform Method Based on Hidden Layer Temporal Signals](https://arxiv.org/abs/2509.13154)
*Jinxin Li,Gang Tu,ShengYu Cheng,Junjie Hu,Jinting Wang,Rui Chen,Zhilong Zhou,Dongbo Shan*

Main category: cs.CL

TL;DR: HSAD是一种基于隐藏信号分析的幻觉检测框架，通过分析自回归生成过程中隐藏表示的时序动态，在频域提取特征，相比现有方法在多个基准测试中提升超过10个百分点。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法存在局限性：事实性检查受外部知识覆盖限制，静态隐藏状态分析无法捕捉推理动态偏差。需要更有效和鲁棒的检测方法。

Method: HSAD框架通过采样各层激活构建隐藏层信号，应用快速傅里叶变换获得频域表示，提取最强非直流频率分量作为频谱特征，并利用自回归特性确定最佳观测点。

Result: 在包括TruthfulQA在内的多个基准测试中，HSAD相比现有最先进方法实现了超过10个百分点的改进。

Conclusion: 通过将推理过程建模与频域分析相结合，HSAD为LLMs中的鲁棒幻觉检测建立了新范式。

Abstract: Hallucination remains a critical barrier for deploying large language models
(LLMs) in reliability-sensitive applications. Existing detection methods
largely fall into two categories: factuality checking, which is fundamentally
constrained by external knowledge coverage, and static hidden-state analysis,
that fails to capture deviations in reasoning dynamics. As a result, their
effectiveness and robustness remain limited. We propose HSAD (Hidden Signal
Analysis-based Detection), a novel hallucination detection framework that
models the temporal dynamics of hidden representations during autoregressive
generation. HSAD constructs hidden-layer signals by sampling activations across
layers, applies Fast Fourier Transform (FFT) to obtain frequency-domain
representations, and extracts the strongest non-DC frequency component as
spectral features. Furthermore, by leveraging the autoregressive nature of
LLMs, HSAD identifies optimal observation points for effective and reliable
detection. Across multiple benchmarks, including TruthfulQA, HSAD achieves over
10 percentage points improvement compared to prior state-of-the-art methods. By
integrating reasoning-process modeling with frequency-domain analysis, HSAD
establishes a new paradigm for robust hallucination detection in LLMs.

</details>


### [37] [The Few-shot Dilemma: Over-prompting Large Language Models](https://arxiv.org/abs/2509.13196)
*Yongjian Tang,Doruk Tuncel,Christian Koerner,Thomas Runkler*

Main category: cs.CL

TL;DR: 研究发现大语言模型存在过度提示现象，过多的领域特定示例反而会降低性能，通过TF-IDF和分层抽样方法确定了各模型的最佳示例数量，在软件需求分类任务上实现了1%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 挑战传统few-shot学习的认知，研究过度提示现象对大语言模型性能的影响，特别是在软件工程和需求分析领域的应用。

Method: 使用随机抽样、语义嵌入和TF-IDF向量三种标准few-shot选择方法，在多个LLM上进行评估，通过逐步增加TF-IDF选择的分层few-shot示例数量来确定最优数量。

Result: 实验发现过度添加领域特定示例会降低某些LLM性能，确定了各模型的最佳示例数量，在功能和非功能需求分类任务上超越了现有最佳方法1%。

Conclusion: 过度提示是一个真实存在的问题，需要谨慎选择few-shot示例数量，TF-IDF和分层抽样相结合的方法能以更少示例获得更好性能，避免过度提示问题。

Abstract: Over-prompting, a phenomenon where excessive examples in prompts lead to
diminished performance in Large Language Models (LLMs), challenges the
conventional wisdom about in-context few-shot learning. To investigate this
few-shot dilemma, we outline a prompting framework that leverages three
standard few-shot selection methods - random sampling, semantic embedding, and
TF-IDF vectors - and evaluate these methods across multiple LLMs, including
GPT-4o, GPT-3.5-turbo, DeepSeek-V3, Gemma-3, LLaMA-3.1, LLaMA-3.2, and Mistral.
Our experimental results reveal that incorporating excessive domain-specific
examples into prompts can paradoxically degrade performance in certain LLMs,
which contradicts the prior empirical conclusion that more relevant few-shot
examples universally benefit LLMs. Given the trend of LLM-assisted software
engineering and requirement analysis, we experiment with two real-world
software requirement classification datasets. By gradually increasing the
number of TF-IDF-selected and stratified few-shot examples, we identify their
optimal quantity for each LLM. This combined approach achieves superior
performance with fewer examples, avoiding the over-prompting problem, thus
surpassing the state-of-the-art by 1% in classifying functional and
non-functional requirements.

</details>


### [38] [Evaluating LLM Alignment on Personality Inference from Real-World Interview Data](https://arxiv.org/abs/2509.13244)
*Jianfeng Zhu,Julina Maharjan,Xinyu Li,Karin G. Coifman,Ruoming Jin*

Main category: cs.CL

TL;DR: LLM在人格特质评估方面表现有限，所有模型预测与真实人格特质的Pearson相关系数均低于0.26，表明当前LLM与验证心理学构念的对齐度不足。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在需要心理理解的角色中部署增多，如情感支持代理和决策助手，但其在生态有效对话环境中解释人类人格特质的能力尚未被探索。现有研究主要在社交媒体数据上使用离散的大五人格标签模拟LLM"角色"，而LLM与基于自然互动的连续真实人格评估的对齐程度基本未被检验。

Method: 引入包含半结构化访谈记录和验证连续大五特质分数的新基准。系统评估三种范式：(1)使用GPT-4.1 Mini的零样本和思维链提示；(2)对RoBERTa和Meta-LLaMA架构应用基于LoRA的微调；(3)使用预训练BERT和OpenAI text-embedding-3-small的静态嵌入进行回归分析。

Result: 所有模型预测与真实人格特质之间的Pearson相关系数均低于0.26。思维链提示相比零样本提示只有微小改进，表明人格推断更依赖于潜在语义表示而非显式推理。

Conclusion: 研究结果强调了将LLM与复杂人类属性对齐的挑战，并推动未来在特质特定提示、上下文感知建模和对齐导向微调方面的工作。

Abstract: Large Language Models (LLMs) are increasingly deployed in roles requiring
nuanced psychological understanding, such as emotional support agents,
counselors, and decision-making assistants. However, their ability to interpret
human personality traits, a critical aspect of such applications, remains
unexplored, particularly in ecologically valid conversational settings. While
prior work has simulated LLM "personas" using discrete Big Five labels on
social media data, the alignment of LLMs with continuous, ground-truth
personality assessments derived from natural interactions is largely
unexamined. To address this gap, we introduce a novel benchmark comprising
semi-structured interview transcripts paired with validated continuous Big Five
trait scores. Using this dataset, we systematically evaluate LLM performance
across three paradigms: (1) zero-shot and chain-of-thought prompting with
GPT-4.1 Mini, (2) LoRA-based fine-tuning applied to both RoBERTa and Meta-LLaMA
architectures, and (3) regression using static embeddings from pretrained BERT
and OpenAI's text-embedding-3-small. Our results reveal that all Pearson
correlations between model predictions and ground-truth personality traits
remain below 0.26, highlighting the limited alignment of current LLMs with
validated psychological constructs. Chain-of-thought prompting offers minimal
gains over zero-shot, suggesting that personality inference relies more on
latent semantic representation than explicit reasoning. These findings
underscore the challenges of aligning LLMs with complex human attributes and
motivate future work on trait-specific prompting, context-aware modeling, and
alignment-oriented fine-tuning.

</details>


### [39] [ChartGaze: Enhancing Chart Understanding in LVLMs with Eye-Tracking Guided Attention Refinement](https://arxiv.org/abs/2509.13282)
*Ali Salamatian,Amirhossein Abaskohi,Wan-Cyuan Fan,Mir Rayat Imtiaz Hossain,Leonid Sigal,Giuseppe Carenini*

Main category: cs.CL

TL;DR: ChartGaze是一个新的眼动追踪数据集，通过比较人类和模型注意力模式，发现LVLMs在图表问答任务中注意力与人类注视存在差异，提出基于人类注视的注意力优化方法，显著提升了模型准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在图表问答任务中仍面临挑战，特别是在注意力集中在图表无关区域时，导致准确性和可解释性下降。通过分析人类注视模式来改进模型注意力机制。

Method: 构建ChartGaze眼动追踪数据集，捕获人类在图表推理任务中的注视模式；提出基于人类注视的注意力优化方法，将图像-文本注意力与人类注视点对齐。

Result: 提出的注视引导注意力优化方法在多个模型上实现了最高2.56个百分点的准确率提升，同时改善了注意力对齐效果。

Conclusion: 融入人类注视信息可以有效提升图表导向的大型视觉语言模型的推理质量和可解释性，展示了人类认知模式在改进AI模型中的价值。

Abstract: Charts are a crucial visual medium for communicating and representing
information. While Large Vision-Language Models (LVLMs) have made progress on
chart question answering (CQA), the task remains challenging, particularly when
models attend to irrelevant regions of the chart. In this work, we present
ChartGaze, a new eye-tracking dataset that captures human gaze patterns during
chart reasoning tasks. Through a systematic comparison of human and model
attention, we find that LVLMs often diverge from human gaze, leading to reduced
interpretability and accuracy. To address this, we propose a gaze-guided
attention refinement that aligns image-text attention with human fixations. Our
approach improves both answer accuracy and attention alignment, yielding gains
of up to 2.56 percentage points across multiple models. These results
demonstrate the promise of incorporating human gaze to enhance both the
reasoning quality and interpretability of chart-focused LVLMs.

</details>


### [40] [WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents](https://arxiv.org/abs/2509.13309)
*Zile Qiao,Guoxin Chen,Xuanzhong Chen,Donglei Yu,Wenbiao Yin,Xinyu Wang,Zhen Zhang,Baixuan Li,Huifeng Yin,Kuan Li,Rui Min,Minpeng Liao,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: WebResearcher是一个创新的深度研究框架，通过迭代式研究范式和可扩展数据合成引擎，解决了现有单上下文方法的上下文窒息和噪声污染问题，在6个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理在自主发现和合成外部知识时面临上下文窒息和噪声污染的问题，需要一种新的框架来克服这些限制，实现更有效的深度研究。

Method: 提出两个核心组件：(1) WebResearcher迭代深度研究范式，将深度研究建模为马尔可夫决策过程，定期整合发现到演化报告中；(2) WebFrontier可扩展数据合成引擎，通过工具增强的复杂性升级生成高质量训练数据。

Result: 在6个具有挑战性的基准测试中实现了最先进的性能，甚至超越了前沿专有系统。训练数据显著提升了传统单上下文方法的工具使用能力，并通过并行思维实现自然扩展。

Conclusion: WebResearcher框架通过创新的迭代研究范式和数据合成方法，有效解决了深度研究中的关键挑战，为构建自主知识发现AI代理提供了强有力的解决方案。

Abstract: Recent advances in deep-research systems have demonstrated the potential for
AI agents to autonomously discover and synthesize knowledge from external
sources. In this paper, we introduce WebResearcher, a novel framework for
building such agents through two key components: (1) WebResearcher, an
iterative deep-research paradigm that reformulates deep research as a Markov
Decision Process, where agents periodically consolidate findings into evolving
reports while maintaining focused workspaces, overcoming the context
suffocation and noise contamination that plague existing mono-contextual
approaches; and (2) WebFrontier, a scalable data synthesis engine that
generates high-quality training data through tool-augmented complexity
escalation, enabling systematic creation of research tasks that bridge the gap
between passive knowledge recall and active knowledge construction. Notably, we
find that the training data from our paradigm significantly enhances tool-use
capabilities even for traditional mono-contextual methods. Furthermore, our
paradigm naturally scales through parallel thinking, enabling concurrent
multi-agent exploration for more comprehensive conclusions. Extensive
experiments across 6 challenging benchmarks demonstrate that WebResearcher
achieves state-of-the-art performance, even surpassing frontier proprietary
systems.

</details>


### [41] [Scaling Agents via Continual Pre-training](https://arxiv.org/abs/2509.13310)
*Liangcai Su,Zhen Zhang,Guangyu Li,Zhuo Chen,Chenxi Wang,Maojia Song,Xinyu Wang,Kuan Li,Jialong Wu,Xuanzhong Chen,Zile Qiao,Zhongwang Zhang,Huifeng Yin,Shihao Cai,Runnan Fang,Zhengwei Tao,Wenbiao Yin,Chenxiong Qian,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: 论文提出Agentic CPT方法，通过持续预训练构建强大的智能体基础模型AgentFounder-30B，在10个基准测试中达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有基于通用基础模型的训练方法在智能体任务中表现不佳，主要原因是缺乏强大的智能体基础模型，导致模型需要同时学习多样化智能体行为并与专家演示对齐，造成优化冲突

Method: 提出Agentic Continual Pre-training (Agentic CPT)方法，将其整合到深度研究智能体训练流程中，构建智能体基础模型AgentFounder

Result: AgentFounder-30B在10个基准测试中取得最先进性能：BrowseComp-en 39.9%、BrowseComp-zh 43.3%、HLE Pass@1 31.5%，同时保持强大的工具使用能力

Conclusion: Agentic CPT方法成功解决了智能体基础模型缺失的问题，通过持续预训练构建了性能优异的智能体基础模型，为智能体系统发展提供了有效解决方案

Abstract: Large language models (LLMs) have evolved into agentic systems capable of
autonomous tool use and multi-step reasoning for complex problem-solving.
However, post-training approaches building upon general-purpose foundation
models consistently underperform in agentic tasks, particularly in open-source
implementations. We identify the root cause: the absence of robust agentic
foundation models forces models during post-training to simultaneously learn
diverse agentic behaviors while aligning them to expert demonstrations, thereby
creating fundamental optimization tensions. To this end, we are the first to
propose incorporating Agentic Continual Pre-training (Agentic CPT) into the
deep research agents training pipeline to build powerful agentic foundational
models. Based on this approach, we develop a deep research agent model named
AgentFounder. We evaluate our AgentFounder-30B on 10 benchmarks and achieve
state-of-the-art performance while retains strong tool-use ability, notably
39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE.

</details>


### [42] [Towards General Agentic Intelligence via Environment Scaling](https://arxiv.org/abs/2509.13311)
*Runnan Fang,Shihao Cai,Baixuan Li,Jialong Wu,Guangyu Li,Wenbiao Yin,Xinyu Wang,Xiaobin Wang,Liangcai Su,Zhen Zhang,Shibin Wu,Zhengwei Tao,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: 提出了一个可扩展的环境构建框架AgentScaler，通过自动创建异构模拟环境来增强大语言模型的函数调用能力，采用两阶段微调策略显著提升了智能体性能


<details>
  <summary>Details</summary>
Motivation: 现实世界API需要精确、鲁棒的函数调用智能，而智能体能力的广度与其训练环境的多样性密切相关。当前面临两个核心挑战：如何规模化构建环境以及如何从环境交互中有效训练智能体能力

Method: 设计了一个可扩展框架自动构建完全模拟的异构环境，系统性地拓宽函数调用场景空间。采用两阶段微调策略：首先赋予智能体基础能力，然后针对特定领域进行专业化训练

Result: 在tau-bench、tau2-Bench和ACEBench等智能体基准测试上的广泛实验表明，训练出的AgentScaler模型显著增强了模型的函数调用能力

Conclusion: 通过规模化环境构建和两阶段训练策略，成功提升了智能体的函数调用智能，为部署大语言模型到实际应用奠定了基础

Abstract: Advanced agentic intelligence is a prerequisite for deploying Large Language
Models in practical, real-world applications. Diverse real-world APIs demand
precise, robust function-calling intelligence, which needs agents to develop
these capabilities through interaction in varied environments. The breadth of
function-calling competence is closely tied to the diversity of environments in
which agents are trained. In this work, we scale up environments as a step
towards advancing general agentic intelligence. This gives rise to two central
challenges: (i) how to scale environments in a principled manner, and (ii) how
to effectively train agentic capabilities from experiences derived through
interactions with these environments. To address these, we design a scalable
framework that automatically constructs heterogeneous environments that are
fully simulated, systematically broadening the space of function-calling
scenarios. We further adapt a two-phase agent fine-tuning strategy: first
endowing agents with fundamental agentic capabilities, then specializing them
for domain-specific contexts. Extensive experiments on agentic benchmarks,
tau-bench, tau2-Bench, and ACEBench, demonstrate that our trained model,
AgentScaler, significantly enhances the function-calling capability of models.

</details>


### [43] [WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research](https://arxiv.org/abs/2509.13312)
*Zijian Li,Xin Guan,Bo Zhang,Shen Huang,Houquan Zhou,Shaopeng Lai,Ming Yan,Yong Jiang,Pengjun Xie,Fei Huang,Jun Zhang,Jingren Zhou*

Main category: cs.CL

TL;DR: WebWeaver是一个双智能体框架，通过规划器和写作者协同工作，模拟人类研究过程，解决开放深度研究中的长上下文问题和幻觉问题，在多个基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 当前开放深度研究(OEDR)方法存在静态研究流程和一次性生成范式的问题，导致长上下文失败和幻觉现象。需要模拟人类研究过程来解决这些挑战。

Method: 采用双智能体框架：规划器动态循环迭代获取证据并优化大纲；写作者执行分层检索和写作过程，逐部分撰写报告，针对性检索必要证据。

Result: 在DeepResearch Bench、DeepConsult和DeepResearchGym等主要OEDR基准测试中建立了新的最先进水平。

Conclusion: 自适应规划和聚焦式合成对于生成高质量、可靠且结构良好的报告至关重要，验证了以人为中心的迭代方法学的有效性。

Abstract: This paper tackles open-ended deep research (OEDR), a complex challenge where
AI agents must synthesize vast web-scale information into insightful reports.
Current approaches are plagued by dual-fold limitations: static research
pipelines that decouple planning from evidence acquisition and one-shot
generation paradigms that easily suffer from long-context failure issues like
"loss in the middle" and hallucinations. To address these challenges, we
introduce WebWeaver, a novel dual-agent framework that emulates the human
research process. The planner operates in a dynamic cycle, iteratively
interleaving evidence acquisition with outline optimization to produce a
comprehensive, source-grounded outline linking to a memory bank of evidence.
The writer then executes a hierarchical retrieval and writing process,
composing the report section by section. By performing targeted retrieval of
only the necessary evidence from the memory bank for each part, it effectively
mitigates long-context issues. Our framework establishes a new state-of-the-art
across major OEDR benchmarks, including DeepResearch Bench, DeepConsult, and
DeepResearchGym. These results validate our human-centric, iterative
methodology, demonstrating that adaptive planning and focused synthesis are
crucial for producing high-quality, reliable, and well-structured reports.

</details>


### [44] [ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization](https://arxiv.org/abs/2509.13313)
*Xixi Wu,Kuan Li,Yida Zhao,Liwen Zhang,Litu Ou,Huifeng Yin,Zhongwang Zhang,Yong Jiang,Pengjun Xie,Fei Huang,Minhao Cheng,Shuai Wang,Hong Cheng,Jingren Zhou*

Main category: cs.CL

TL;DR: ReSum是一种新的LLM网络代理范式，通过周期性上下文摘要来克服上下文窗口限制，实现无限探索，在多个基准测试中平均比ReAct提升4.5%


<details>
  <summary>Details</summary>
Motivation: 基于LLM的网络代理在知识密集型任务中表现良好，但受到ReAct等范式中上下文窗口限制的阻碍。复杂查询涉及多个实体、交织关系和高不确定性，需要大量搜索周期，在达到完整解决方案之前迅速耗尽上下文预算

Method: 引入ReSum范式，通过定期上下文摘要将不断增长的交互历史转换为紧凑的推理状态，保持对先前发现的感知同时绕过上下文约束。提出ReSum-GRPO，整合GRPO与分段轨迹训练和优势广播，使代理熟悉基于摘要的推理

Result: 在三个基准测试中，ReSum相比ReAct平均绝对提升4.5%，经过ReSum-GRPO训练后进一步提升达8.2%。WebResummer-30B仅用1K训练样本就在BrowseComp-zh上达到33.3% Pass@1，在BrowseComp-en上达到18.3%，超越现有开源网络代理

Conclusion: ReSum通过上下文摘要有效解决了LLM网络代理的上下文窗口限制问题，实现了更好的性能和无限探索能力，为复杂网络任务提供了有效的解决方案

Abstract: Large Language Model (LLM)-based web agents demonstrate strong performance on
knowledge-intensive tasks but are hindered by context window limitations in
paradigms like ReAct. Complex queries involving multiple entities, intertwined
relationships, and high uncertainty demand extensive search cycles that rapidly
exhaust context budgets before reaching complete solutions. To overcome this
challenge, we introduce ReSum, a novel paradigm that enables indefinite
exploration through periodic context summarization. ReSum converts growing
interaction histories into compact reasoning states, maintaining awareness of
prior discoveries while bypassing context constraints. For paradigm adaptation,
we propose ReSum-GRPO, integrating GRPO with segmented trajectory training and
advantage broadcasting to familiarize agents with summary-conditioned
reasoning. Extensive experiments on web agents of varying scales across three
benchmarks demonstrate that ReSum delivers an average absolute improvement of
4.5\% over ReAct, with further gains of up to 8.2\% following ReSum-GRPO
training. Notably, with only 1K training samples, our WebResummer-30B (a
ReSum-GRPO-trained version of WebSailor-30B) achieves 33.3\% Pass@1 on
BrowseComp-zh and 18.3\% on BrowseComp-en, surpassing existing open-source web
agents.

</details>


### [45] [Do Natural Language Descriptions of Model Activations Convey Privileged Information?](https://arxiv.org/abs/2509.13316)
*Millicent Li,Alberto Mario Ceballos Arroyo,Giordano Rogers,Naomi Saphra,Byron C. Wallace*

Main category: cs.CL

TL;DR: 该研究批判性评估了LLM激活解释方法，发现现有基准测试存在缺陷，解释结果往往反映了解释器LLM的参数知识而非目标模型的真实激活模式。


<details>
  <summary>Details</summary>
Motivation: 验证激活解释方法是否能真正揭示目标LLM的内部工作机制，还是仅仅反映了输入信息或解释器LLM的偏见。

Method: 在先前工作使用的数据集上评估流行解释方法，进行控制实验分析解释结果与目标模型激活的关系。

Result: 解释方法在无目标模型内部访问的情况下也能在基准测试中成功，解释结果主要反映解释器LLM的参数知识而非目标模型激活。

Conclusion: 需要针对性基准测试和实验控制来严格评估解释方法是否能提供对LLM操作的有意义洞察。

Abstract: Recent interpretability methods have proposed to translate LLM internal
representations into natural language descriptions using a second verbalizer
LLM. This is intended to illuminate how the target model represents and
operates on inputs. But do such activation verbalization approaches actually
provide privileged knowledge about the internal workings of the target model,
or do they merely convey information about its inputs? We critically evaluate
popular verbalization methods across datasets used in prior work and find that
they succeed at benchmarks without any access to target model internals,
suggesting that these datasets are not ideal for evaluating verbalization
methods. We then run controlled experiments which reveal that verbalizations
often reflect the parametric knowledge of the verbalizer LLM which generated
them, rather than the activations of the target LLM being decoded. Taken
together, our results indicate a need for targeted benchmarks and experimental
controls to rigorously assess whether verbalization methods provide meaningful
insights into the operations of LLMs.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [46] [Knowledge Graph Tokenization for Behavior-Aware Generative Next POI Recommendation](https://arxiv.org/abs/2509.12350)
*Ke Sun,Mayi Xu*

Main category: cs.IR

TL;DR: KGTB提出了一种基于知识图谱的令牌化方法，通过多行为学习解决现有生成式POI推荐中的信息丢失和行为理解不足问题


<details>
  <summary>Details</summary>
Motivation: 现有生成式POI推荐方法存在两个主要问题：(1)令牌化器难以编码推荐数据中的异构信号，导致信息丢失；(2)指令微调任务仅关注用户POI访问行为，忽略其他行为类型，对移动性理解不足

Method: 使用知识图谱格式组织推荐数据以保留异构信息，开发基于KG的令牌化器将节点量化为结构ID，提出多行为学习引入多种行为特定的预测任务（如POI、类别和区域访问行为）进行LLM微调

Result: 在四个真实城市数据集上的实验证明了KGTB的优越性能

Conclusion: KGTB通过知识图谱令牌化和多行为学习有效解决了生成式POI推荐中的信息丢失和行为理解不足问题，显著提升了推荐性能

Abstract: Generative paradigm, especially powered by Large Language Models (LLMs), has
emerged as a new solution to the next point-of-interest (POI) recommendation.
Pioneering studies usually adopt a two-stage pipeline, starting with a
tokenizer converting POIs into discrete identifiers that can be processed by
LLMs, followed by POI behavior prediction tasks to instruction-tune LLM for
next POI recommendation. Despite of remarkable progress, they still face two
limitations: (1) existing tokenizers struggle to encode heterogeneous signals
in the recommendation data, suffering from information loss issue, and (2)
previous instruction-tuning tasks only focus on users' POI visit behavior while
ignore other behavior types, resulting in insufficient understanding of
mobility. To address these limitations, we propose KGTB (Knowledge Graph
Tokenization for Behavior-aware generative next POI recommendation).
Specifically, KGTB organizes the recommendation data in a knowledge graph (KG)
format, of which the structure can seamlessly preserve the heterogeneous
information. Then, a KG-based tokenizer is developed to quantize each node into
an individual structural ID. This process is supervised by the KG's structure,
thus reducing the loss of heterogeneous information. Using generated IDs, KGTB
proposes multi-behavior learning that introduces multiple behavior-specific
prediction tasks for LLM fine-tuning, e.g., POI, category, and region visit
behaviors. Learning on these behavior tasks provides LLMs with comprehensive
insights on the target POI visit behavior. Experiments on four real-world city
datasets demonstrate the superior performance of KGTB.

</details>


### [47] [What News Recommendation Research Did (But Mostly Didn't) Teach Us About Building A News Recommender](https://arxiv.org/abs/2509.12361)
*Karl Higley,Robin Burke,Michael D. Ekstrand,Bart P. Knijnenburg*

Main category: cs.IR

TL;DR: 论文报告了构建新闻推荐系统POPROX的实际经验，发现当前研究文献与真实系统构建之间存在显著差距，并提出了使未来研究更具实践应用性的建议。


<details>
  <summary>Details</summary>
Motivation: 旨在将新闻推荐文献应用于构建真实的新闻推荐系统，检验当前研究对实际系统构建的支持程度，并识别文献中的空白。

Method: 通过构建和运营POPROX这个实时新闻推荐研究平台，收集实践经验，分析在构建个性化功能时遇到的挑战，并与现有文献进行对比。

Result: 发现了构建新闻推荐系统时遇到的多个意外挑战，这些挑战与文献中的空白相关，表明当前研究对实际系统构建的支持有限。

Conclusion: 需要使新闻推荐研究更加贴近实际应用，提出了从构建实时系统中获得的经验教训，并指出了未来研究可以更具影响力和应用性的机会。

Abstract: One of the goals of recommender systems research is to provide insights and
methods that can be used by practitioners to build real-world systems that
deliver high-quality recommendations to actual people grounded in their genuine
interests and needs. We report on our experience trying to apply the news
recommendation literature to build POPROX, a live platform for news
recommendation research, and reflect on the extent to which the current state
of research supports system-building efforts. Our experience highlights several
unexpected challenges encountered in building personalization features that are
commonly found in products from news aggregators and publishers, and shows how
those difficulties are connected to surprising gaps in the literature. Finally,
we offer a set of lessons learned from building a live system with a persistent
user base and highlight opportunities to make future news recommendation
research more applicable and impactful in practice.

</details>


### [48] [LEAF: Knowledge Distillation of Text Embedding Models with Teacher-Aligned Representations](https://arxiv.org/abs/2509.12539)
*Robin Vujanic,Thomas Rueckstiess*

Main category: cs.IR

TL;DR: LEAF是一个轻量级的知识蒸馏框架，用于文本嵌入模型，其核心特点是学生模型与教师模型对齐，支持信息检索中的非对称架构，并在BEIR和MTEB基准测试中达到同规模模型的最优性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决文本嵌入模型在信息检索等任务中需要高效部署和推理的问题，同时保持与大型教师模型相当的性能，并减少对标注数据和计算资源的需求。

Method: 采用知识蒸馏框架，使学生模型（leaf）与教师模型对齐，支持非对称架构（文档用教师模型编码，查询用学生模型处理），无需显式训练即可继承教师模型的MRL和输出量化鲁棒性。

Result: 提出的leaf-ir模型（23M参数）在BEIR基准测试中排名第一，leaf-mt模型在MTEB v2（英语）排行榜中同样排名第一，且非对称模式下检索性能进一步提升。

Conclusion: LEAF框架有效实现了轻量级文本嵌入模型的蒸馏，在保持高性能的同时降低了数据和计算需求，适用于黑盒模型且具有广泛的适用性。

Abstract: We present LEAF ("Lightweight Embedding Alignment Framework"), a knowledge
distillation framework for text embedding models. A key distinguishing feature
is that our distilled leaf models are aligned to their teacher. In the context
of information retrieval, this allows for flexible asymmetric architectures
where documents are encoded with the larger teacher model, while queries can be
served with the smaller leaf models. We also show that leaf models
automatically inherit MRL and robustness to output quantization whenever these
properties are present in the teacher model, without explicitly training for
them. To demonstrate the capability of our framework we publish leaf-ir, a 23M
parameters information retrieval oriented text embedding model trained using
LEAF, which sets a new state-of-the-art (SOTA) on BEIR, ranking #1 on the
public leaderboard for this benchmark and for models of its size. When run in
asymmetric mode, its retrieval performance is further increased. Our scheme is
however not restricted to the information retrieval setting, and we demonstrate
its wider applicability by synthesizing the multi-task leaf-mt model. This also
sets a new SOTA, ranking #1 on the public MTEB v2 (English) leaderboard for its
size. LEAF is applicable to black-box models and in contrast to other embedding
model training frameworks, it does not require judgments nor hard negatives,
and training can be conducted using small batch sizes. Thus, dataset and
training infrastructure requirements for our framework are modest. We make our
models publicly available under a permissive Apache 2.0 license.

</details>


### [49] [InfoGain-RAG: Boosting Retrieval-Augmented Generation via Document Information Gain-based Reranking and Filtering](https://arxiv.org/abs/2509.12765)
*Zihan Wang,Zihan Liang,Zhou Shao,Yufei Ma,Huangyu Dai,Ben Chen,Lingtao Mao,Chenyi Lei,Yuqing Ding,Han Li*

Main category: cs.IR

TL;DR: 提出了Document Information Gain (DIG)指标来量化检索文档对答案生成的贡献度，并基于此开发了InfoGain-RAG框架，通过训练专门的reranker来优先选择有价值的文档，显著提升了RAG系统的性能。


<details>
  <summary>Details</summary>
Motivation: 当前RAG框架难以识别检索文档是否对答案生成有实质性贡献，无法有效过滤无关或误导性内容，这显著影响最终性能。

Method: 提出DIG指标计算文档的价值（LLM在有/无该文档时的生成置信度差异），并基于DIG分数训练专门的reranker来精确区分和排序检索文档。

Result: 在多个模型和基准测试中，InfoGain-RAG显著优于现有方法，在NaturalQA上相比naive RAG、self-reflective RAG和ranking-based RAG分别提升17.9%、4.5%和12.5%的精确匹配准确率，在GPT-4o上平均提升15.3%。

Conclusion: InfoGain-RAG为RAG系统提供了可靠的解决方案，能够有效过滤无关文档并选择最有价值的文档，在多个应用中具有可行性。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising approach to
address key limitations of Large Language Models (LLMs), such as hallucination,
outdated knowledge, and lacking reference. However, current RAG frameworks
often struggle with identifying whether retrieved documents meaningfully
contribute to answer generation. This shortcoming makes it difficult to filter
out irrelevant or even misleading content, which notably impacts the final
performance. In this paper, we propose Document Information Gain (DIG), a novel
metric designed to quantify the contribution of retrieved documents to correct
answer generation. DIG measures a document's value by computing the difference
of LLM's generation confidence with and without the document augmented.
Further, we introduce InfoGain-RAG, a framework that leverages DIG scores to
train a specialized reranker, which prioritizes each retrieved document from
exact distinguishing and accurate sorting perspectives. This approach can
effectively filter out irrelevant documents and select the most valuable ones
for better answer generation. Extensive experiments across various models and
benchmarks demonstrate that InfoGain-RAG can significantly outperform existing
approaches, on both single and multiple retrievers paradigm. Specifically on
NaturalQA, it achieves the improvements of 17.9%, 4.5%, 12.5% in exact match
accuracy against naive RAG, self-reflective RAG and modern ranking-based RAG
respectively, and even an average of 15.3% increment on advanced proprietary
model GPT-4o across all datasets. These results demonstrate the feasibility of
InfoGain-RAG as it can offer a reliable solution for RAG in multiple
applications.

</details>


### [50] [DiffHash: Text-Guided Targeted Attack via Diffusion Models against Deep Hashing Image Retrieval](https://arxiv.org/abs/2509.12824)
*Zechao Liu,Zheng Zhou,Xiangkun Chen,Tao Liang,Dapeng Lang*

Main category: cs.IR

TL;DR: DiffHash是一种基于扩散模型的深度哈希目标攻击方法，通过LLM生成文本指导优化潜在表示，使用多空间哈希对齐网络和文本引导注意力机制，在保持视觉合理性的同时实现更好的攻击效果。


<details>
  <summary>Details</summary>
Motivation: 深度哈希模型面临严重的安全风险，现有目标攻击方法缺乏多模态指导、依赖标签信息且基于像素级操作，需要更有效的攻击方法。

Method: 提出DiffHash方法：1）使用LLM生成目标图像的文本信息作为指导；2）优化图像的潜在表示而非直接修改像素；3）设计多空间哈希对齐网络对齐图像空间和文本空间到二进制哈希空间；4）在重建过程中加入文本引导注意力机制。

Result: 实验表明该方法优于现有最先进的目标攻击方法，具有更好的黑盒可迁移性和跨数据集的稳定性。

Conclusion: DiffHash通过多模态指导和潜在空间优化，有效解决了深度哈希模型的安全漏洞问题，为对抗攻击提供了新的思路。

Abstract: Deep hashing models have been widely adopted to tackle the challenges of
large-scale image retrieval. However, these approaches face serious security
risks due to their vulnerability to adversarial examples. Despite the
increasing exploration of targeted attacks on deep hashing models, existing
approaches still suffer from a lack of multimodal guidance, reliance on
labeling information and dependence on pixel-level operations for attacks. To
address these limitations, we proposed DiffHash, a novel diffusion-based
targeted attack for deep hashing. Unlike traditional pixel-based attacks that
directly modify specific pixels and lack multimodal guidance, our approach
focuses on optimizing the latent representations of images, guided by text
information generated by a Large Language Model (LLM) for the target image.
Furthermore, we designed a multi-space hash alignment network to align the
high-dimension image space and text space to the low-dimension binary hash
space. During reconstruction, we also incorporated text-guided attention
mechanisms to refine adversarial examples, ensuring them aligned with the
target semantics while maintaining visual plausibility. Extensive experiments
have demonstrated that our method outperforms state-of-the-art (SOTA) targeted
attack methods, achieving better black-box transferability and offering more
excellent stability across datasets.

</details>


### [51] [A Learnable Fully Interacted Two-Tower Model for Pre-Ranking System](https://arxiv.org/abs/2509.12948)
*Chao Xiong,Xianwen Yu,Wei Xu,Lei Cheng,Chuan Yuan,Linjian Mo*

Main category: cs.IR

TL;DR: 提出了一种名为FIT的可学习全交互双塔模型，通过在预排序系统中引入元查询模块和轻量级相似度评分器，实现了用户和物品特征之间的丰富信息交互，同时保持推理效率。


<details>
  <summary>Details</summary>
Motivation: 传统双塔模型在预排序系统中虽然效率高，但由于用户和物品塔独立处理，缺乏信息交互，导致效果受限。需要一种既能保持效率又能增强交互能力的新架构。

Method: FIT模型包含两个核心组件：元查询模块（MQM）通过可学习的物品元矩阵实现早期特征交互；轻量级相似度评分器（LSS）实现后期塔间交互。

Result: 在多个公共数据集上的实验结果表明，FIT模型显著优于现有的最先进预排序基线模型。

Conclusion: FIT模型成功解决了传统双塔模型信息交互不足的问题，在保持推理效率的同时显著提升了预排序效果，为大规模推荐系统提供了更好的解决方案。

Abstract: Pre-ranking plays a crucial role in large-scale recommender systems by
significantly improving the efficiency and scalability within the constraints
of providing high-quality candidate sets in real time. The two-tower model is
widely used in pre-ranking systems due to a good balance between efficiency and
effectiveness with decoupled architecture, which independently processes user
and item inputs before calculating their interaction (e.g. dot product or
similarity measure). However, this independence also leads to the lack of
information interaction between the two towers, resulting in less
effectiveness. In this paper, a novel architecture named learnable Fully
Interacted Two-tower Model (FIT) is proposed, which enables rich information
interactions while ensuring inference efficiency. FIT mainly consists of two
parts: Meta Query Module (MQM) and Lightweight Similarity Scorer (LSS).
Specifically, MQM introduces a learnable item meta matrix to achieve expressive
early interaction between user and item features. Moreover, LSS is designed to
further obtain effective late interaction between the user and item towers.
Finally, experimental results on several public datasets show that our proposed
FIT significantly outperforms the state-of-the-art baseline pre-ranking models.

</details>


### [52] [Protecting participants or population? Comparison of k-anonymous Origin-Destination matrices](https://arxiv.org/abs/2509.12950)
*Pietro Armenante,Kai Huang,Nikhil Jha,Luca Vassio*

Main category: cs.IR

TL;DR: 本文提出了一个基于人口统计的隐私保护方法，用于生成k-匿名的OD矩阵，不仅保护调查参与者，还保护整个实际人口的隐私。


<details>
  <summary>Details</summary>
Motivation: 传统的OD矩阵匿名化方法主要关注数据集本身的隐私保护，但忽略了统计加权代理所代表的实际人口的隐私风险。需要开发既能保护调查参与者又能保护整个推断人口身份的方法。

Method: 提出了ODkAnon贪婪算法，通过地理区域泛化实现k-匿名性。与传统方法（ATG、OIGH和Mondrian）相比，新方法在速度和匿名质量之间取得平衡，并采用基于人口的隐私视角。

Result: 开发了能够生成同时满足调查参与者和整个实际人口k-匿名性要求的OD矩阵的方法，相比传统方法提供了更全面的隐私保护。

Conclusion: 基于人口统计的隐私保护框架是OD矩阵匿名化的根本性转变，通过考虑代表性来调整匿名化框架，不仅保护调查参与者，还保护推断的实际人口身份。

Abstract: Origin-Destination (OD) matrices are a core component of research on users'
mobility and summarize how individuals move between geographical regions. These
regions should be small enough to be representative of user mobility, without
incurring substantial privacy risks. There are two added values of the
NetMob2025 challenge dataset. Firstly, the data is extensive and contains a lot
of socio-demographic information that can be used to create multiple OD
matrices, based on the segments of the population. Secondly, a participant is
not merely a record in the data, but a statistically weighted proxy for a
segment of the real population. This opens the door to a fundamental shift in
the anonymization paradigm. A population-based view of privacy is central to
our contribution. By adjusting our anonymization framework to account for
representativeness, we are also protecting the inferred identity of the actual
population, rather than survey participants alone. The challenge addressed in
this work is to produce and compare OD matrices that are k-anonymous for survey
participants and for the whole population. We compare several traditional
methods of anonymization to k-anonymity by generalizing geographical areas.
These include generalization over a hierarchy (ATG and OIGH) and the classical
Mondrian. To this established toolkit, we add a novel method, i.e., ODkAnon, a
greedy algorithm aiming at balancing speed and quality. Unlike previous
approaches, which primarily address the privacy aspects of the given datasets,
we aim to contribute to the generation of privacy-preserving OD matrices
enriched with socio-demographic segmentation that achieves k-anonymity on the
actual population.

</details>


### [53] [Green Recommender Systems: Understanding and Minimizing the Carbon Footprint of AI-Powered Personalization](https://arxiv.org/abs/2509.13001)
*Lukas Wegmeth,Tobias Vente,Alan Said,Joeran Beel*

Main category: cs.IR

TL;DR: 推荐系统研究中深度学习模型比传统模型产生42倍多的CO2排放，单篇论文排放量相当于纽约到墨尔本的航班或一棵树260年的固碳量


<details>
  <summary>Details</summary>
Motivation: 随着全球变暖加剧，需要评估和减少推荐系统的环境影响，但推荐系统社区对此缺乏认识和评估

Method: 复制2013和2023年ACM RecSys会议的79篇论文实验流程，使用硬件能耗表测量能耗并转换为CO2当量，比较传统AI模型和深度学习模型

Result: 使用深度学习模型的论文排放约42倍于传统模型的CO2当量，平均单篇深度学习论文产生2,909千克CO2当量

Conclusion: 推荐系统和机器学习社区迫切需要采用绿色AI原则，在算法进步和环境责任之间取得平衡，构建可持续的AI个性化未来

Abstract: As global warming soars, the need to assess and reduce the environmental
impact of recommender systems is becoming increasingly urgent. Despite this,
the recommender systems community hardly understands, addresses, and evaluates
the environmental impact of their work. In this study, we examine the
environmental impact of recommender systems research by reproducing typical
experimental pipelines. Based on our results, we provide guidelines for
researchers and practitioners on how to minimize the environmental footprint of
their work and implement green recommender systems - recommender systems
designed to minimize their energy consumption and carbon footprint. Our
analysis covers 79 papers from the 2013 and 2023 ACM RecSys conferences,
comparing traditional "good old-fashioned AI" models with modern deep learning
models. We designed and reproduced representative experimental pipelines for
both years, measuring energy consumption using a hardware energy meter and
converting it into CO2 equivalents. Our results show that papers utilizing deep
learning models emit approximately 42 times more CO2 equivalents than papers
using traditional models. On average, a single deep learning-based paper
generates 2,909 kilograms of CO2 equivalents - more than the carbon emissions
of a person flying from New York City to Melbourne or the amount of CO2
sequestered by one tree over 260 years. This work underscores the urgent need
for the recommender systems and wider machine learning communities to adopt
green AI principles, balancing algorithmic advancements and environmental
responsibility to build a sustainable future with AI-powered personalization.

</details>


### [54] [Efficient Cold-Start Recommendation via BPE Token-Level Embedding Initialization with LLM](https://arxiv.org/abs/2509.13179)
*Yushang Zhao,Xinyue Han,Qian Leng,Qianyi Sun,Haotian Lyu,Chengrui Zhou*

Main category: cs.IR

TL;DR: 提出基于BPE分词和LLM嵌入的冷启动推荐方法，使用细粒度词元级向量作为语义先验，在零样本设置下实现高效推荐


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统中的冷启动问题，特别是在新用户或新物品缺乏历史交互数据时，传统基于内容或混合方法在稀疏元数据环境中只能捕捉浅层模式

Method: 应用Byte Pair Encoding(BPE)分词和预训练大语言模型(LLM)嵌入，获得与BPE词汇表对齐的细粒度词元级向量，作为未见实体的密集语义先验

Result: 在基准数据集上实验显示，BPE-LLM方法在Recall@k、NDCG@k和Hit Rate指标上优于标准基线，具有更好的泛化性和可解释性，计算性能充足

Conclusion: 词元级语义初始化作为轻量级但有效的扩展，可应用于现代推荐系统的零样本设置，特别在多语言和稀疏输入环境中表现优异

Abstract: The cold-start issue is the challenge when we talk about recommender systems,
especially in the case when we do not have the past interaction data of new
users or new items. Content-based features or hybrid solutions are common as
conventional solutions, but they can only work in a sparse metadata environment
with shallow patterns. In this paper, the efficient cold-start recommendation
strategy is presented, which is based on the sub word-level representations by
applying Byte Pair Encoding (BPE) tokenization and pre-trained Large Language
Model (LLM) embedding in the initialization procedure. We obtain fine-grained
token-level vectors that are aligned with the BPE vocabulary as opposed to
using coarse-grained sentence embeddings. Together, these token embeddings can
be used as dense semantic priors on unseen entities, making immediate
recommendation performance possible without user-item interaction history. Our
mechanism can be compared to collaborative filtering systems and tested over
benchmark datasets with stringent cold-start assumptions. Experimental findings
show that the given BPE-LLM method achieves higher Recall@k, NDCG@k, and Hit
Rate measurements compared to the standard baseline and displays the same
capability of sufficient computational performance. Furthermore, we demonstrate
that using subword-aware embeddings yields better generalizability and is more
interpretable, especially within a multilingual and sparse input setting. The
practical application of token-level semantic initialization as a lightweight,
but nevertheless effective extension to modern recommender systems in the
zero-shot setting is indicated within this work.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [55] [PowerGrow: Feasible Co-Growth of Structures and Dynamics for Power Grid Synthesis](https://arxiv.org/abs/2509.12212)
*Xinyu He,Chenhan Xiao,Haoran Li,Ruizhong Qiu,Zhe Xu,Yang Weng,Jingrui He,Hanghang Tong*

Main category: cs.LG

TL;DR: PowerGrow是一个用于生成电力系统测试案例的协同生成框架，通过依赖分解和分层图扩散过程，在保持运行有效性的同时显著降低计算开销


<details>
  <summary>Details</summary>
Motivation: 现代电力系统日益动态化，但公开测试案例稀缺，需要能够联合生成电网结构和节点动态的生成工具，同时保持物理可行性和避免过高计算成本

Method: 采用依赖分解方法将复杂联合分布分解为条件分布链，使用分层图β扩散过程进行结构合成，配合时间自编码器将时间序列数据嵌入紧凑潜在空间

Result: 在基准测试中，PowerGrow在保真度和多样性上优于现有扩散模型，达到98.9%的潮流收敛率和改进的N-1事故恢复能力

Conclusion: PowerGrow能够生成运行有效且现实的电网场景，为解决电力系统测试案例稀缺问题提供了有效解决方案

Abstract: Modern power systems are becoming increasingly dynamic, with changing
topologies and time-varying loads driven by renewable energy variability,
electric vehicle adoption, and active grid reconfiguration. Despite these
changes, publicly available test cases remain scarce, due to security concerns
and the significant effort required to anonymize real systems. Such limitations
call for generative tools that can jointly synthesize grid structure and nodal
dynamics. However, modeling the joint distribution of network topology, branch
attributes, bus properties, and dynamic load profiles remains a major
challenge, while preserving physical feasibility and avoiding prohibitive
computational costs. We present PowerGrow, a co-generative framework that
significantly reduces computational overhead while maintaining operational
validity. The core idea is dependence decomposition: the complex joint
distribution is factorized into a chain of conditional distributions over
feasible grid topologies, time-series bus loads, and other system attributes,
leveraging their mutual dependencies. By constraining the generation process at
each stage, we implement a hierarchical graph beta-diffusion process for
structural synthesis, paired with a temporal autoencoder that embeds
time-series data into a compact latent space, improving both training stability
and sample fidelity. Experiments across benchmark settings show that PowerGrow
not only outperforms prior diffusion models in fidelity and diversity but also
achieves a 98.9\% power flow convergence rate and improved N-1 contingency
resilience. This demonstrates its ability to generate operationally valid and
realistic power grid scenarios.

</details>


### [56] [Scaling Up Data Parallelism in Decentralized Deep Learning](https://arxiv.org/abs/2509.12213)
*Bing Xie,Junqi Yin,Zhenyu Zhou,Sarp Oral,Feiyi Wang*

Main category: cs.LG

TL;DR: 本文提出了DBench基准测试框架和Ada自适应方法，发现在大规模去中心化DNN训练中，模型精度与通信图连接数和参数张量方差相关，Ada方法能够在1008个GPU规模上达到与中心化训练相当的精度。


<details>
  <summary>Details</summary>
Motivation: 尽管去中心化学习在理论上被广泛探索，但由于缺乏稳定性、可扩展性和通用性，尚未在生产环境中广泛应用。本研究旨在探索大规模去中心化数据并行训练的实际应用。

Method: 引入DBench基准测试框架，包含中心化和去中心化DNN训练；提出基准测试方法分析模型精度与参数张量方差的相关性；基于观察结果提出Ada自适应方法，动态调整通信图。

Result: 发现去中心化训练存在可扩展性和通用性问题；模型精度与通信图连接数相关；对参数张量方差异常敏感；Ada方法在1008个GPU规模上训练ResNet50达到与中心化训练相当的收敛速度和精度。

Conclusion: Ada自适应方法能够有效解决大规模去中心化DNN训练的问题，在保持良好收敛性能的同时获得与中心化训练相当的模型精度，为去中心化学习的生产应用提供了可行方案。

Abstract: Although it has been extensively explored in theory, decentralized learning
is not yet green-lighted for production use, largely due to a lack of
stability, scalability, and generality in large scale DNN training. To shed
light on the production use of decentralized learning, this work studies
decentralized data parallel training at scale. To this end, we introduce a
benchmarking framework, namely DBench, to host both centralized and
decentralized DNN training. Building upon DBench, we introduce a benchmarking
methodology to uncover the correlations between model accuracy and the
variances of parameter tensors by varying communication graphs and training
scales. Based on the benchmarking results, we observe that, (1) Similar to
centralized learning, decentralized data parallel training also presents the
issues of scalability and generality when the training scales up; (2) The model
accuracy of decentralized learning is correlated to the number of connections
in a communication graph; (3) The model accuracy of decentralized learning is
surprisingly sensitive to the variance of parameter tensors across model
replicas. Built upon the observations, we propose Ada, a decentralized adaptive
approach that performs large scale DNN training following a decentralized SGD
method and adapting the communication graph in use dynamically throughout
training iterations. We apply Ada on large scale training and observe that Ada
can obtain the best convergence rates consistently in decentralized DNN
training, and delivers equally or comparably good model accuracy for all sample
applications as centralized learning does, even when training ResNet50 for
ImageNet-1K on the scale of 1008 GPUs.

</details>


### [57] [MEUV: Achieving Fine-Grained Capability Activation in Large Language Models via Mutually Exclusive Unlock Vectors](https://arxiv.org/abs/2509.12221)
*Xin Tong,Zhi Lin,Jingya Wang,Meng Han,Bo Jin*

Main category: cs.LG

TL;DR: MEUV框架通过将单一拒绝方向分解为多个主题对齐、近乎正交的向量，实现了细粒度的安全控制，在保持高攻击成功率的同时显著减少跨主题泄漏。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全对齐机制会同时拒绝恶意请求和合法的高风险场景使用，而之前的单向量绕过方法缺乏语义控制，会无差别解锁所有危险主题。

Method: 提出相互排斥解锁向量(MEUV)框架，通过多任务目标学习主题对齐的正交向量，包含差分消融边界、跨主题惩罚、正交性惩罚等辅助项。

Result: 在双语恶意提示基准测试中，MEUV在多个模型上达到不低于87%的攻击成功率，同时将跨主题泄漏减少高达90%，且中英文向量可相互迁移。

Conclusion: MEUV证明了细粒度主题级能力激活的可行性，为安全敏感领域的受控LLM部署铺平了道路，且具有最小效用损失。

Abstract: Large language models (LLMs) enforce safety alignment to reliably refuse
malicious requests, yet the same blanket safeguards also block legitimate uses
in policing, defense, and other high-stakes settings. Earlier
"refusal-direction" edits can bypass those layers, but they rely on a single
vector that indiscriminately unlocks all hazardous topics, offering no semantic
control. We introduce Mutually Exclusive Unlock Vectors (MEUV), a lightweight
framework that factorizes the monolithic refusal direction into topic-aligned,
nearly orthogonal vectors, each dedicated to one sensitive capability. MEUV is
learned in a single epoch with a multi-task objective that blends a
differential-ablation margin, cross-topic and orthogonality penalties, and
several auxiliary terms. On bilingual malicious-prompt benchmarks, MEUV
achieves an attack success rate of no less than 87% on Gemma-2-2B, LLaMA-3-8B,
and Qwen-7B, yet cuts cross-topic leakage by up to 90% compared with the best
single-direction baseline. Vectors trained in Chinese transfer almost unchanged
to English (and vice versa), suggesting a language-agnostic refusal subspace.
The results show that fine-grained, topic-level capability activation is
achievable with minimal utility loss, paving the way for controlled LLMs
deployment in security-sensitive domains.

</details>


### [58] [Accelerating Privacy-Preserving Federated Learning in Large-Scale LEO Satellite Systems](https://arxiv.org/abs/2509.12222)
*Binquan Guo,Junteng Cao,Marie Siew,Binbin Chen,Tony Q. S. Quek,Zhu Han*

Main category: cs.LG

TL;DR: 这篇论文研究了在大规模低赨道卫星系统中调度联邦学习的问题，提出了一种基于离散时序图的按需调度框架，通过动态分配通信资源来加速联邦学习过程。


<details>
  <summary>Details</summary>
Motivation: 大规模低赨道卫星系统能够支持地理分布式AI模型协同训练，但因隐私考虑和监管约束无法集中处理原始数据。联邦学习提供了隐私保护方案，但卫星系统的动态拓扑结构和有限带宽会延长训练时间。

Method: 提出一种离散时序图基础的按需调度框架，动态分配通信资源来加速联邦学习过程，解决参数聚合和分发的急需问题。

Result: 模拟结果显示，该方法比传统统计多寄用策略提高了41.48%的性能，总体训练轮次时间减少14.20%至41.48%。对于更大模型和更多客户端，加速效果更为显著。

Conclusion: 该研究提出的调度框架能够有效加速卫星网络中的联邦学习，具有良好的可扩展性，尤其适用于大规模模型和多客户端场景。

Abstract: Large-scale low-Earth-orbit (LEO) satellite systems are increasingly valued
for their ability to enable rapid and wide-area data exchange, thereby
facilitating the collaborative training of artificial intelligence (AI) models
across geographically distributed regions. Due to privacy concerns and
regulatory constraints, raw data collected at remote clients cannot be
centrally aggregated, posing a major obstacle to traditional AI training
methods. Federated learning offers a privacy-preserving alternative by training
local models on distributed devices and exchanging only model parameters.
However, the dynamic topology and limited bandwidth of satellite systems will
hinder timely parameter aggregation and distribution, resulting in prolonged
training times. To address this challenge, we investigate the problem of
scheduling federated learning over satellite networks and identify key
bottlenecks that impact the overall duration of each training round. We propose
a discrete temporal graph-based on-demand scheduling framework that dynamically
allocates communication resources to accelerate federated learning. Simulation
results demonstrate that the proposed approach achieves significant performance
gains over traditional statistical multiplexing-based model exchange
strategies, reducing overall round times by 14.20% to 41.48%. Moreover, the
acceleration effect becomes more pronounced for larger models and higher
numbers of clients, highlighting the scalability of the proposed approach.

</details>


### [59] [TripOptimizer: Generative 3D Shape Optimization and Drag Prediction using Triplane VAE Networks](https://arxiv.org/abs/2509.12224)
*Parsa Vatani,Mohamed Elrefaie,Farhad Nazarpour,Faez Ahmed*

Main category: cs.LG

TL;DR: TripOptimizer是一个完全可微分的深度学习框架，使用三平面隐式神经表示从点云数据快速进行空气动力学分析和形状优化，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统基于CFD的空气动力学形状优化计算成本过高，严重限制了设计空间探索，需要开发更高效的优化方法。

Method: 采用变分自编码器，结合三平面隐式神经表示进行高保真3D几何重建，使用DrivAerNet++数据集（8000个车辆几何和阻力系数）训练，通过修改编码器参数子集实现形状优化。

Result: 优化设计实现了高达11.8%的阻力系数降低，结果通过独立的高保真CFD模拟（超过1.5亿个网格）验证，对非水密网格具有鲁棒性。

Conclusion: 该框架实现了更敏捷的空气动力学形状优化工作流程，减少了对计算密集型CFD模拟的依赖，特别适用于早期设计阶段。

Abstract: The computational cost of traditional Computational Fluid Dynamics-based
Aerodynamic Shape Optimization severely restricts design space exploration.
This paper introduces TripOptimizer, a fully differentiable deep learning
framework for rapid aerodynamic analysis and shape optimization directly from
vehicle point cloud data. TripOptimizer employs a Variational Autoencoder
featuring a triplane-based implicit neural representation for high-fidelity 3D
geometry reconstruction and a drag coefficient prediction head. Trained on
DrivAerNet++, a large-scale dataset of 8,000 unique vehicle geometries with
corresponding drag coefficients computed via Reynolds-Averaged Navier-Stokes
simulations, the model learns a latent representation that encodes
aerodynamically salient geometric features. We propose an optimization strategy
that modifies a subset of the encoder parameters to steer an initial geometry
towards a target drag value, and demonstrate its efficacy in case studies where
optimized designs achieved drag coefficient reductions up to 11.8\%. These
results were subsequently validated by using independent, high-fidelity
Computational Fluid Dynamics simulations with more than 150 million cells. A
key advantage of the implicit representation is its inherent robustness to
geometric imperfections, enabling optimization of non-watertight meshes, a
significant challenge for traditional adjoint-based methods. The framework
enables a more agile Aerodynamic Shape Optimization workflow, reducing reliance
on computationally intensive CFD simulations, especially during early design
stages.

</details>


### [60] [A Physics-Informed Neural Networks-Based Model Predictive Control Framework for $SIR$ Epidemics](https://arxiv.org/abs/2509.12226)
*Aiping Zhong,Baike She,Philip E. Paré*

Main category: cs.LG

TL;DR: 基于物理信息神经网络(PINNs)的模型预测控制(MPC)框架，用于SIR传染病模型的联合状态和参数实时估计与最优控制


<details>
  <summary>Details</summary>
Motivation: 现有MPC流行病控制研究通常假设状态可测或参数已知，本文旨在解决仅使用噪声感染状态数据同时估计状态和参数的挑战

Method: 提出MPC-PINNs框架，包含三种算法：标准MPC-PINNs、对数尺度MPC-LS-PINNs（提高抗噪性）、分裂积分MPC-SI-PINNs（利用积分算子和状态耦合）。针对不同先验知识（仅知恢复率或基本再生数）设计相应算法

Result: 实验结果表明所提方法在不同设置下均能有效估计流行病状态和参数，并生成最优控制策略

Conclusion: 该框架成功实现了SIR模型的联合状态参数估计与最优控制，为流行病控制提供了有效的计算工具

Abstract: This work introduces a physics-informed neural networks (PINNs)-based model
predictive control (MPC) framework for susceptible-infected-recovered ($SIR$)
spreading models. Existing studies in MPC design for epidemic control often
assume either 1) measurable states of the dynamics, where the parameters are
learned, or 2) known parameters of the model, where the states are learned. In
this work, we address the joint real-time estimation of states and parameters
within the MPC framework using only noisy infected states, under the assumption
that 1) only the recovery rate is known, or 2) only the basic reproduction
number is known. Under the first assumption, we propose MPC-PINNs and two novel
PINNs algorithms, all of which are integrated into the MPC framework. First, we
introduce MPC-PINNs, which are designed for $SIR$ models with control. We then
propose log-scaled PINNs (MPC-LS-PINNs), which incorporate a log-scaled loss
function to improve robustness against noise. Next, we present split-integral
PINNs (MPC-SI-PINNs), which leverage integral operators and state coupling in
the neural network training process to effectively reconstruct the complete
epidemic state information. Building upon these methods, we further extend our
framework for the second assumption. We establish the necessary conditions and
extend our PINNs algorithms, where MPC-SI-PINNs are simplified as split-PINNs
(MPC-S-PINNs). By incorporating these algorithms into the MPC framework, we
simultaneously estimate the epidemic states and parameters while generating
optimal control strategies. Experiment results demonstrate the effectiveness of
the proposed methods under different settings.

</details>


### [61] [Research on Short-Video Platform User Decision-Making via Multimodal Temporal Modeling and Reinforcement Learning](https://arxiv.org/abs/2509.12269)
*Jinmeiyang Wang,Jing Dong,Li Zhou*

Main category: cs.LG

TL;DR: MT-DQN模型整合Transformer、时序图神经网络和深度Q网络，在短视频推荐中显著提升用户行为预测和策略优化性能，但存在计算成本和延迟敏感性问题。


<details>
  <summary>Details</summary>
Motivation: 解决短视频环境中用户行为预测和推荐策略优化的挑战，传统模型在性能上存在不足，需要更先进的集成方法来提升推荐效果。

Method: 提出MT-DQN模型，集成Transformer、时序图神经网络(TGNN)和深度Q网络(DQN)三种技术，用于用户行为建模和推荐策略优化。

Result: 相比传统拼接模型(Concat-Modal)，F1分数平均提升10.97%，NDCG@5平均提升8.3%；相比经典强化学习模型(Vanilla-DQN)，MSE降低34.8%，MAE降低26.5%。

Conclusion: MT-DQN在推荐性能上显著优于现有方法，但实际部署面临计算成本和在线推理延迟的挑战，需要通过未来架构优化来解决。

Abstract: This paper proposes the MT-DQN model, which integrates a Transformer,
Temporal Graph Neural Network (TGNN), and Deep Q-Network (DQN) to address the
challenges of predicting user behavior and optimizing recommendation strategies
in short-video environments. Experiments demonstrated that MT-DQN consistently
outperforms traditional concatenated models, such as Concat-Modal, achieving an
average F1-score improvement of 10.97% and an average NDCG@5 improvement of
8.3%. Compared to the classic reinforcement learning model Vanilla-DQN, MT-DQN
reduces MSE by 34.8% and MAE by 26.5%. Nonetheless, we also recognize
challenges in deploying MT-DQN in real-world scenarios, such as its
computational cost and latency sensitivity during online inference, which will
be addressed through future architectural optimization.

</details>


### [62] [Learning to Route: Per-Sample Adaptive Routing for Multimodal Multitask Prediction](https://arxiv.org/abs/2509.12227)
*Marzieh Ajirak,Oded Bein,Ellen Rose Bowen,Dora Kanellopoulos,Avital Falk,Faith M. Gunning,Nili Solomonov,Logan Grosenick*

Main category: cs.LG

TL;DR: 提出了一种用于多任务多模态预测的自适应路由框架，能够根据样本特性动态选择模态处理路径和任务共享策略，在心理治疗应用中表现出色。


<details>
  <summary>Details</summary>
Motivation: 针对心理治疗领域存在结构化评估和非结构化临床笔记共存、数据部分缺失且结果相关的多模态多任务预测问题，需要处理数据异质性和任务交互的样本间差异。

Method: 基于路由的架构，定义多个模态路径（包括文本和数值特征的原始和融合表示），学习为每个输入选择最信息丰富的专家组合，根据路由决策使用共享或独立头生成任务特定预测，端到端训练。

Result: 在合成数据和真实心理治疗笔记数据上评估，该方法始终优于固定的多任务或单任务基线，学习到的路由策略提供了模态相关性和任务结构的可解释性洞察。

Conclusion: 该框架通过实现考虑数据异质性和任务相关性的个性化信息处理，解决了个性化医疗中的关键挑战，在心理治疗应用中可改善心理健康结果、提高治疗分配精度和临床成本效益。

Abstract: We propose a unified framework for adaptive routing in multitask, multimodal
prediction settings where data heterogeneity and task interactions vary across
samples. Motivated by applications in psychotherapy where structured
assessments and unstructured clinician notes coexist with partially missing
data and correlated outcomes, we introduce a routing-based architecture that
dynamically selects modality processing pathways and task-sharing strategies on
a per-sample basis. Our model defines multiple modality paths, including raw
and fused representations of text and numeric features and learns to route each
input through the most informative expert combination. Task-specific
predictions are produced by shared or independent heads depending on the
routing decision, and the entire system is trained end-to-end. We evaluate the
model on both synthetic data and real-world psychotherapy notes predicting
depression and anxiety outcomes. Our experiments show that our method
consistently outperforms fixed multitask or single-task baselines, and that the
learned routing policy provides interpretable insights into modality relevance
and task structure. This addresses critical challenges in personalized
healthcare by enabling per-subject adaptive information processing that
accounts for data heterogeneity and task correlations. Applied to
psychotherapy, this framework could improve mental health outcomes, enhance
treatment assignment precision, and increase clinical cost-effectiveness
through personalized intervention strategies.

</details>


### [63] [Profiling LoRA/QLoRA Fine-Tuning Efficiency on Consumer GPUs: An RTX 4060 Case Study](https://arxiv.org/abs/2509.12229)
*MSR Avinash*

Main category: cs.LG

TL;DR: 在8GB显存的消费级GPU上对Qwen2.5-1.5B模型进行LoRA/QLoRA微调的系统性能分析，显示分页优化器可提升25%吞吐量，bf16相比fp16效率更低，2048序列长度可行


<details>
  <summary>Details</summary>
Motivation: 探索在严格8GB显存限制的消费级GPU上进行参数高效微调的实际效率，为资源受限的研究者和从业者提供实用指导

Method: 使用NVIDIA RTX 4060对Qwen2.5-1.5B-Instruct模型进行控制性性能分析，系统变化批次大小、序列长度、优化器选择(AdamW vs PagedAdamW)和精度(fp16 vs bf16)

Result: 分页优化器提升吞吐量达25%(628 tok/s vs 500 tok/s基准)，bf16相比fp16效率下降，在8GB限制下2048序列长度可行

Conclusion: 这是首个在消费级GPU上系统研究LLM微调效率的案例研究，提供了可复现的基准测试和实用指南，证明参数高效策略在资源受限环境下的可行性

Abstract: Fine-tuning large language models (LLMs) with parameter-efficient techniques
such as LoRA and QLoRA has enabled adaptation of foundation models on modest
hardware. Yet the efficiency of such training on consumer-grade GPUs,
especially under strict 8 GB VRAM limits, remains underexplored. We present a
controlled profiling study of LoRA/QLoRA fine-tuning using the
Qwen2.5-1.5B-Instruct model on a single NVIDIA RTX 4060. Across three
representative configurations, we systematically vary batch size, sequence
length, optimizer choice (AdamW vs. PagedAdamW), and precision (fp16 vs. bf16).
We report throughput (tokens/s), time per 10k tokens, and VRAM footprint,
alongside energy estimates derived from GPU board power limits. Our results
show that paged optimizers improve throughput by up to 25% (628 tok/s vs. 500
tok/s baseline), while bf16 degrades efficiency relative to fp16. Despite 8 GB
constraints, sequence lengths up to 2048 tokens were feasible using
parameter-efficient strategies. To our knowledge, this is the first systematic
case study of LLM fine-tuning efficiency on consumer GPUs, providing
reproducible benchmarks and practical guidelines for resource-constrained
researchers and practitioners.

</details>


### [64] [Flexible Multimodal Neuroimaging Fusion for Alzheimer's Disease Progression Prediction](https://arxiv.org/abs/2509.12234)
*Benjamin Burns,Yuan Xue,Douglas W. Scharre,Xia Ning*

Main category: cs.LG

TL;DR: PerM-MoE是一种新型稀疏混合专家方法，使用独立的路由器处理每种神经影像模态，在阿尔茨海默病进展预测中，特别是在高模态缺失的临床场景下，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病进展预测需要整合多种神经影像模态，但现有多模态模型在推理时遇到模态缺失时预测准确性下降，而临床实践中模态缺失很常见。

Method: 提出PerM-MoE方法，为每种模态使用独立的路由器，而不是传统的单一路由器，提高了在高模态缺失情况下的模型灵活性。

Result: 在ADNI数据集上评估显示，PerM-MoE在大多数模态缺失变化情况下优于最先进的Flex-MoE和单模态模型，能更有效地利用专家网络。

Conclusion: PerM-MoE方法通过独立模态路由设计，显著提升了多模态模型在临床实际场景中的实用性和预测性能。

Abstract: Alzheimer's disease (AD) is a progressive neurodegenerative disease with high
inter-patient variance in rate of cognitive decline. AD progression prediction
aims to forecast patient cognitive decline and benefits from incorporating
multiple neuroimaging modalities. However, existing multimodal models fail to
make accurate predictions when many modalities are missing during inference, as
is often the case in clinical settings. To increase multimodal model
flexibility under high modality missingness, we introduce PerM-MoE, a novel
sparse mixture-of-experts method that uses independent routers for each
modality in place of the conventional, single router. Using T1-weighted MRI,
FLAIR, amyloid beta PET, and tau PET neuroimaging data from the Alzheimer's
Disease Neuroimaging Initiative (ADNI), we evaluate PerM-MoE, state-of-the-art
Flex-MoE, and unimodal neuroimaging models on predicting two-year change in
Clinical Dementia Rating-Sum of Boxes (CDR-SB) scores under varying levels of
modality missingness. PerM-MoE outperforms the state of the art in most
variations of modality missingness and demonstrates more effective utility of
experts than Flex-MoE.

</details>


### [65] [RL Fine-Tuning Heals OOD Forgetting in SFT](https://arxiv.org/abs/2509.12235)
*Hangzhan Jin,Sitao Luan,Sicheng Lyu,Guillaume Rabusseau,Reihaneh Rabbany,Doina Precup,Mohammad Hamdaqa*

Main category: cs.LG

TL;DR: 研究发现两阶段微调中SFT和RL的协同机制：SFT早期OOD性能最佳但随后下降，RL主要起OOD恢复作用而非创造新能力，奇异向量旋转是关键机制


<details>
  <summary>Details</summary>
Motivation: 探索SFT和RL两阶段微调范式中协同作用的演化机制，挑战"SFT记忆、RL泛化"的简化观点

Method: 使用SVD分析参数矩阵，手动编辑参数并观察性能影响，分析不同训练阶段的OOD表现

Result: 发现OOD性能在SFT早期达到峰值后下降，RL主要恢复SFT丢失的能力而非创造新能力，奇异向量旋转而非奇异值变化是主要机制

Conclusion: 重新定义了SFT和RL在两阶段微调中的角色，发现奇异向量旋转是影响OOD行为的关键机制，为理解微调过程提供了新视角

Abstract: The two-stage fine-tuning paradigm of Supervised Fine-Tuning (SFT) followed
by Reinforcement Learning (RL) has empirically shown better reasoning
performance than one-stage SFT for the post-training of Large Language Models
(LLMs). However, the evolution and mechanism behind the synergy of SFT and RL
are still under-explored and inconclusive. In our study, we find the well-known
claim "SFT memorizes, RL generalizes" is over-simplified, and discover that:
(1) OOD performance peaks at the early stage of SFT and then declines (OOD
forgetting), the best SFT checkpoint cannot be captured by training/test loss;
(2) the subsequent RL stage does not generate fundamentally better OOD
capability, instead it plays an \textbf{OOD restoration} role, recovering the
lost reasoning ability during SFT; (3) The recovery ability has boundaries,
\ie{} \textbf{if SFT trains for too short or too long, RL cannot recover the
lost OOD ability;} (4) To uncover the underlying mechanisms behind the
forgetting and restoration process, we employ SVD analysis on parameter
matrices, manually edit them, and observe their impacts on model performance.
Unlike the common belief that the shift of model capacity mainly results from
the changes of singular values, we find that they are actually quite stable
throughout fine-tuning. Instead, the OOD behavior strongly correlates with the
\textbf{rotation of singular vectors}. Our findings re-identify the roles of
SFT and RL in the two-stage fine-tuning and discover the rotation of singular
vectors as the key mechanism. %reversing the rotations induced by SFT, which
shows recovery from forgetting, whereas imposing the SFT parameter directions
onto a RL-tuned model results in performance degradation. Code is available at
https://github.com/xiaodanguoguo/RL_Heals_SFT

</details>


### [66] [Neural Diffeomorphic-Neural Operator for Residual Stress-Induced Deformation Prediction](https://arxiv.org/abs/2509.12237)
*Changqing Liu,Kaining Dai,Zhiwei Zhao,Tianyi Wu,Yingguang Li*

Main category: cs.LG

TL;DR: 提出了一种基于微分同胚嵌入神经算子的新框架NDNO，用于高效预测不同几何形状结构件的加工变形，解决了传统数值方法计算成本高和神经算子直接应用于变化几何域的限制问题。


<details>
  <summary>Details</summary>
Motivation: 结构件加工变形预测对保证尺寸精度和可靠性至关重要，但传统数值方法计算成本高，特别是处理不同几何形状时。神经算子虽能高效求解偏微分方程，但直接应用于变化几何域存在理论和实践限制。

Method: 通过受平滑性和可逆性约束的微分同胚神经网络，将复杂三维几何显式映射到公共参考域，然后在参考域上训练神经算子学习残余应力引起的变形场。

Result: 该方法能够准确预测主方向和多方向变形场，在不同几何形状（包括组件类型、尺寸和特征）的零件上实现了高精度和高效率。

Conclusion: NDNO框架为变化几何形状结构件的变形预测提供了有效且计算高效的解决方案，实现了对多样几何零件的快速适应和准确预测。

Abstract: Accurate prediction of machining deformation in structural components is
essential for ensuring dimensional precision and reliability. Such deformation
often originates from residual stress fields, whose distribution and influence
vary significantly with geometric complexity. Conventional numerical methods
for modeling the coupling between residual stresses and deformation are
computationally expensive, particularly when diverse geometries are considered.
Neural operators have recently emerged as a powerful paradigm for efficiently
solving partial differential equations, offering notable advantages in
accelerating residual stress-deformation analysis. However, their direct
application across changing geometric domains faces theoretical and practical
limitations. To address this challenge, a novel framework based on
diffeomorphic embedding neural operators named neural diffeomorphic-neural
operator (NDNO) is introduced. Complex three-dimensional geometries are
explicitly mapped to a common reference domain through a diffeomorphic neural
network constrained by smoothness and invertibility. The neural operator is
then trained on this reference domain, enabling efficient learning of
deformation fields induced by residual stresses. Once trained, both the
diffeomorphic neural network and the neural operator demonstrate efficient
prediction capabilities, allowing rapid adaptation to varying geometries. The
proposed method thus provides an effective and computationally efficient
solution for deformation prediction in structural components subject to varying
geometries. The proposed method is validated to predict both main-direction and
multi-direction deformation fields, achieving high accuracy and efficiency
across parts with diverse geometries including component types, dimensions and
features.

</details>


### [67] [Interpretable Data Mining of Follicular Thyroid Cancer Ultrasound Features Using Enhanced Association Rules](https://arxiv.org/abs/2509.12238)
*Songlin Zhou,Tao Zhou,Xin Li,Stephen Shing-Toung Yau*

Main category: cs.LG

TL;DR: 本研究通过改进关联规则挖掘方法，结合可解释机器学习思想，分析了滤泡性甲状腺癌的临床数据，发现了多个与恶性相关的临床指标，为术前诊断提供参考。


<details>
  <summary>Details</summary>
Motivation: 滤泡性甲状腺癌缺乏特征性超声表现，术前诊断比乳头状甲状腺癌更困难，相关临床研究较少，需要寻找有助于术前诊断的临床指征。

Method: 基于2010-2023年北京大学第三医院收集的病例数据进行回顾性分析，改进传统关联规则挖掘方法，结合SHAP方法的可解释机器学习思想，提出新的分析指标来反映临床指征与癌症之间的恶性关联。

Result: 数据集包含1673个结节（1414个良性，259个恶性），分析发现除了常见指标外，结节内结节模式、小梁模式、低TSH评分等指标具有强恶性关联，同时合并桥本甲状腺炎也可能有强恶性关联。

Conclusion: 在疑似滤泡性甲状腺癌结节的术前诊断中，应考虑多个临床指征以提高诊断准确性，研究中发现的多样化恶性关联可为临床医生提供参考。

Abstract: Purpose: Thyroid cancer has been a common cancer. Papillary thyroid cancer
and follicular thyroid cancer are the two most common types of thyroid cancer.
Follicular thyroid cancer lacks distinctive ultrasound signs and is more
difficult to diagnose preoperatively than the more prevalent papillary thyroid
cancer, and the clinical studies associated with it are less well established.
We aimed to analyze the clinical data of follicular thyroid cancer based on a
novel data mining tool to identify some clinical indications that may help in
preoperative diagnosis. Methods: We performed a retrospective analysis based on
case data collected by the Department of General Surgery of Peking University
Third Hospital between 2010 and 2023. Unlike traditional statistical methods,
we improved the association rule mining, a classical data mining method, and
proposed new analytical metrics reflecting the malignant association between
clinical indications and cancer with the help of the idea of SHAP method in
interpretable machine learning. Results: The dataset was preprocessed to
contain 1673 cases (in terms of nodes rather than patients), of which 1414 were
benign and 259 were malignant nodes. Our analysis pointed out that in addition
to some common indicators (e.g., irregular or lobulated nodal margins, uneven
thickness halo, hypoechogenicity), there were also some indicators with strong
malignant associations, such as nodule-in-nodule pattern, trabecular pattern,
and low TSH scores. In addition, our results suggest that the combination of
Hashimoto's thyroiditis may also have a strong malignant association.
Conclusion: In the preoperative diagnosis of nodules suspected of follicular
thyroid cancer, multiple clinical indications should be considered for a more
accurate diagnosis. The diverse malignant associations identified in our study
may serve as a reference for clinicians in related fields.

</details>


### [68] [InJecteD: Analyzing Trajectories and Drift Dynamics in Denoising Diffusion Probabilistic Models for 2D Point Cloud Generation](https://arxiv.org/abs/2509.12239)
*Sanyam Jain,Khuram Naveed,Illia Oleksiienko,Alexandros Iosifidis,Ruben Pauwels*

Main category: cs.LG

TL;DR: InJecteD是一个用于解释去噪扩散概率模型(DDPMs)的框架，通过分析2D点云生成过程中的样本轨迹来增强模型透明度，支持人机协作调试生成模型。


<details>
  <summary>Details</summary>
Motivation: 提高DDPMs的可解释性，通过分析去噪过程中的样本轨迹来理解模型行为，支持从业者调试和改进生成模型。

Method: 使用简化的DDPM架构，分析三个数据集（牛眼、恐龙、圆形）的去噪轨迹，量化位移、速度、聚类和漂移场动态等特性，采用Wasserstein距离和余弦相似度等统计指标。

Result: 实验揭示了不同的去噪阶段：初始噪声探索、快速形状形成和最终细化，不同数据集表现出特定行为（如牛眼的同心收敛vs恐龙的复杂轮廓形成）。傅里叶基嵌入提高了轨迹稳定性和重建质量。

Conclusion: InJecteD框架成功增强了DDPMs的可解释性，通过轨迹分析揭示了去噪过程的内在机制，傅里叶嵌入方法显示出更好的性能，为人机协作的生成模型调试提供了有效工具。

Abstract: This work introduces InJecteD, a framework for interpreting Denoising
Diffusion Probabilistic Models (DDPMs) by analyzing sample trajectories during
the denoising process of 2D point cloud generation. We apply this framework to
three datasets from the Datasaurus Dozen bullseye, dino, and circle using a
simplified DDPM architecture with customizable input and time embeddings. Our
approach quantifies trajectory properties, including displacement, velocity,
clustering, and drift field dynamics, using statistical metrics such as
Wasserstein distance and cosine similarity. By enhancing model transparency,
InJecteD supports human AI collaboration by enabling practitioners to debug and
refine generative models. Experiments reveal distinct denoising phases: initial
noise exploration, rapid shape formation, and final refinement, with
dataset-specific behaviors example, bullseyes concentric convergence vs. dinos
complex contour formation. We evaluate four model configurations, varying
embeddings and noise schedules, demonstrating that Fourier based embeddings
improve trajectory stability and reconstruction quality

</details>


### [69] [Why and How Auxiliary Tasks Improve JEPA Representations](https://arxiv.org/abs/2509.12249)
*Jiacan Yu,Siyi Chen,Mingrui Liu,Nono Horiuchi,Vladimir Braverman,Zicheng Xu,Dan Haramati,Randall Balestriero*

Main category: cs.LG

TL;DR: 本文为JEPA架构提供了理论分析，证明在确定性MDP中，当潜在转移一致性损失和辅助回归损失都趋近于零时，非等价观测必须映射到不同的潜在表示，辅助任务锚定了表示必须保留的区别。


<details>
  <summary>Details</summary>
Motivation: Joint-Embedding Predictive Architecture (JEPA)在视觉表示学习和基于模型的强化学习中应用日益广泛，但其行为机制仍缺乏深入理解，需要理论分析来指导改进。

Method: 理论分析一个简单的JEPA变体，该变体具有与潜在动态联合训练的辅助回归头。在确定性MDP中证明"无病态表示坍塌"定理，并通过计数环境中的受控消融实验验证理论。

Result: 证明了辅助回归头与JEPA联合训练比分开训练能产生更丰富的表示。辅助任务与转移动态共同编码了正确的等价关系。

Conclusion: 研究指出了改进JEPA编码器的路径：通过与转移动态共同编码正确等价关系的辅助函数来训练编码器，辅助任务锚定了表示必须保留的关键区别。

Abstract: Joint-Embedding Predictive Architecture (JEPA) is increasingly used for
visual representation learning and as a component in model-based RL, but its
behavior remains poorly understood. We provide a theoretical characterization
of a simple, practical JEPA variant that has an auxiliary regression head
trained jointly with latent dynamics. We prove a No Unhealthy Representation
Collapse theorem: in deterministic MDPs, if training drives both the
latent-transition consistency loss and the auxiliary regression loss to zero,
then any pair of non-equivalent observations, i.e., those that do not have the
same transition dynamics or auxiliary label, must map to distinct latent
representations. Thus, the auxiliary task anchors which distinctions the
representation must preserve. Controlled ablations in a counting environment
corroborate the theory and show that training the JEPA model jointly with the
auxiliary head generates a richer representation than training them separately.
Our work indicates a path to improve JEPA encoders: training them with an
auxiliary function that, together with the transition dynamics, encodes the
right equivalence relations.

</details>


### [70] [Representation Learning on Large Non-Bipartite Transaction Networks using GraphSAGE](https://arxiv.org/abs/2509.12255)
*Mihir Tare,Clemens Rattasits,Yiming Wu,Euan Wielewski*

Main category: cs.LG

TL;DR: 本文展示了GraphSAGE图神经网络在银行交易网络中的实际应用，该方法能够处理动态异构交易数据，生成可解释的节点嵌入，并在洗钱检测任务中提升高风险账户识别效果。


<details>
  <summary>Details</summary>
Motivation: 传统图嵌入方法难以处理银行动态交易数据，金融机构需要可扩展的工具来分析复杂的交易网络，特别是能够泛化到未见节点的归纳式方法。

Method: 使用GraphSAGE归纳式图神经网络框架，构建匿名客户和商户交易网络，生成节点嵌入，并应用于洗钱检测分类任务。

Result: 嵌入结果显示与地理和人口统计属性对齐的可解释聚类，在洗钱检测模型中显著改善了高风险账户的优先级排序。

Conclusion: GraphSAGE框架具有出色的适应性、可扩展性和可解释性，为金融机构利用图机器学习在交易生态系统中获得可操作洞察提供了蓝图。

Abstract: Financial institutions increasingly require scalable tools to analyse complex
transactional networks, yet traditional graph embedding methods struggle with
dynamic, real-world banking data. This paper demonstrates the practical
application of GraphSAGE, an inductive Graph Neural Network framework, to
non-bipartite heterogeneous transaction networks within a banking context.
Unlike transductive approaches, GraphSAGE scales well to large networks and can
generalise to unseen nodes which is critical for institutions working with
temporally evolving transactional data. We construct a transaction network
using anonymised customer and merchant transactions and train a GraphSAGE model
to generate node embeddings. Our exploratory work on the embeddings reveals
interpretable clusters aligned with geographic and demographic attributes.
Additionally, we illustrate their utility in downstream classification tasks by
applying them to a money mule detection model where using these embeddings
improves the prioritisation of high-risk accounts. Beyond fraud detection, our
work highlights the adaptability of this framework to banking-scale networks,
emphasising its inductive capability, scalability, and interpretability. This
study provides a blueprint for financial organisations to harness graph machine
learning for actionable insights in transactional ecosystems.

</details>


### [71] [Quantum-Inspired Stacked Integrated Concept Graph Model (QISICGM) for Diabetes Risk Prediction](https://arxiv.org/abs/2509.12259)
*Kenneth G. Young II*

Main category: cs.LG

TL;DR: QISICGM是一个量子启发的机器学习框架，通过集成概念图和堆叠集成方法，在糖尿病风险预测中实现了0.8933的F1分数和0.8699的AUC，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决糖尿病风险预测中的准确性和效率问题，同时应对类别不平衡的挑战，开发一个结合量子启发技术和集成学习的创新框架。

Method: 使用PIMA印第安人糖尿病数据集（含2000个合成样本，共2768个样本），集成自改进概念图和堆叠集成模型（包括随机森林、额外树、变换器、CNN和FFNN），采用量子启发的相位特征映射和邻域序列建模技术。

Result: 实现了0.8933的OOF F1分数和0.8699的AUC，推理效率达到8.5行/秒，在CPU上高效运行，性能超越传统方法。

Conclusion: QISICGM为AI辅助临床分诊提供了潜在基准，通过校准、可解释性和开源可复现性强调了可信AI的重要性，开源代码已发布在GitHub上。

Abstract: The Quantum-Inspired Stacked Integrated Concept Graph Model (QISICGM) is an
innovative machine learning framework that harnesses quantum-inspired
techniques to predict diabetes risk with exceptional accuracy and efficiency.
Utilizing the PIMA Indians Diabetes dataset augmented with 2,000 synthetic
samples to mitigate class imbalance (total: 2,768 samples, 1,949 positives),
QISICGM integrates a self-improving concept graph with a stacked ensemble
comprising Random Forests (RF), Extra Trees (ET), transformers, convolutional
neural networks (CNNs), and feed-forward neural networks (FFNNs). This approach
achieves an out-of-fold (OOF) F1 score of 0.8933 and an AUC of 0.8699,
outperforming traditional methods. Quantum inspired elements, such as phase
feature mapping and neighborhood sequence modeling, enrich feature
representations, enabling CPU-efficient inference at 8.5 rows per second. This
paper presents a detailed architecture, theoretical foundations, code insights,
and performance evaluations, including visualizations from the outputs
subfolder. The open-source implementation (v1.0.0) is available at
https://github.com/keninayoung/QISICGM, positioning QISICGM as a potential
benchmark for AI-assisted clinical triage in diabetes and beyond. Ultimately,
this work emphasizes trustworthy AI through calibration, interpretability, and
open-source reproducibility.

</details>


### [72] [Explainable Fraud Detection with GNNExplainer and Shapley Values](https://arxiv.org/abs/2509.12262)
*Ngoc Hieu Dao*

Main category: cs.LG

TL;DR: 开发可解释的欺诈检测系统以满足透明度和可靠性验证需求


<details>
  <summary>Details</summary>
Motivation: 随着数字支付的普及，金融欺诈风险增加，社会和监管机构对AI系统的透明度要求提高，欺诈分析师也需要简洁易懂的解释来提高调查效率

Method: 专注于开发可解释的欺诈检测器

Result: 未在摘要中明确说明具体结果

Conclusion: 需要开发可解释的欺诈检测系统来解决透明度和可解释性挑战

Abstract: The risk of financial fraud is increasing as digital payments are used more
and more frequently. Although the use of artificial intelligence systems for
fraud detection is widespread, society and regulators have raised the standards
for these systems' transparency for reliability verification purposes. To
increase their effectiveness in conducting fraud investigations, fraud analysts
also profit from having concise and understandable explanations. To solve these
challenges, the paper will concentrate on developing an explainable fraud
detector.

</details>


### [73] [Deriving the Scaled-Dot-Function via Maximum Likelihood Estimation and Maximum Entropy Approach](https://arxiv.org/abs/2509.12285)
*Jiyong Ma*

Main category: cs.LG

TL;DR: 本文提出了一种基于最大似然估计的方法来确定transformer模型中的value向量，通过将value向量、key向量和query向量序列建模为高斯分布序列。


<details>
  <summary>Details</summary>
Motivation: 为transformer架构中的scaled-dot-product函数或softmax函数提供新的理论解释，从概率建模的角度理解注意力机制。

Method: 采用最大似然估计方法，将序列建模为高斯分布序列，其中方差依赖于时间步、key向量和query向量，均值依赖于时间步和value向量。同时借鉴最大熵方法，使用query和key向量推导最大熵模型的特征函数。

Result: 提出了transformer模型中value向量确定的新概率解释框架，为注意力机制提供了基于高斯分布和最大熵的理论基础。

Conclusion: 该方法为transformer架构的核心组件提供了新的概率视角解释，可能有助于更好地理解和改进注意力机制的设计。

Abstract: In this paper, we present a maximum likelihood estimation approach to
determine the value vector in transformer models. We model the sequence of
value vectors, key vectors, and the query vector as a sequence of Gaussian
distributions. The variance in each Gaussian distribution depends on the time
step, the corresponding key vector, and the query vector. The mean value in
each Gaussian distribution depends on the time step, and the corresponding
value vector. This analysis may offer a new explanation of the
scaled-dot-product function or softmax function used in transformer
architectures [1]. Another explanation, inspired by [4], is based on the
maximum entropy approach in natural language processing [5]. In this approach,
a query vector and key vectors are used to derive the feature functions for the
maximum entropy model.

</details>


### [74] [Prediction of Stocks Index Price using Quantum GANs](https://arxiv.org/abs/2509.12286)
*Sangram Deshpande,Gopal Ramesh Dahale,Sai Nandan Morapakula,Uday Wad*

Main category: cs.LG

TL;DR: 本文研究量子生成对抗网络(QGANs)在股票价格预测中的应用，通过量子计算技术提升金融预测的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 金融市场具有高度复杂性和波动性，传统模型难以有效捕捉其复杂模式。量子计算与生成模型的结合为金融预测提供了新的解决方案。

Method: 使用定制化的QGAN模型，基于历史股票市场数据进行训练，采用AWS Braket SV1模拟器训练量子电路。

Result: QGAN能够生成与真实市场行为高度相似的合成数据，在收敛速度和预测准确性方面均优于传统的LSTM和GAN模型。

Conclusion: 量子计算在金融预测领域展现出巨大潜力，为交易员、金融分析师和研究人员提供了更先进的市场分析工具，在速度和精度方面具有显著优势。

Abstract: This paper investigates the application of Quantum Generative Adversarial
Networks (QGANs) for stock price prediction. Financial markets are inherently
complex, marked by high volatility and intricate patterns that traditional
models often fail to capture. QGANs, leveraging the power of quantum computing,
offer a novel approach by combining the strengths of generative models with
quantum machine learning techniques. We implement a QGAN model tailored for
stock price prediction and evaluate its performance using historical stock
market data. Our results demonstrate that QGANs can generate synthetic data
closely resembling actual market behavior, leading to enhanced prediction
accuracy. The experiment was conducted using the Stocks index price data and
the AWS Braket SV1 simulator for training the QGAN circuits. The
quantum-enhanced model outperforms classical Long Short-Term Memory (LSTM) and
GAN models in terms of convergence speed and prediction accuracy. This research
represents a key step toward integrating quantum computing in financial
forecasting, offering potential advantages in speed and precision over
traditional methods. The findings suggest important implications for traders,
financial analysts, and researchers seeking advanced tools for market analysis.

</details>


### [75] [C3DE: Causal-Aware Collaborative Neural Controlled Differential Equation for Long-Term Urban Crowd Flow Prediction](https://arxiv.org/abs/2509.12289)
*Yuting Liu,Qiang Zhou,Hanzhe Li,Chenqi Gong,Jingjing Gu*

Main category: cs.LG

TL;DR: 提出了C3DE模型，使用神经控制微分方程解决长期城市人流预测中的累积采样误差问题，通过双路径结构和因果效应估计器处理POI与人群流动的多时间尺度异步动态关系


<details>
  <summary>Details</summary>
Motivation: 长期城市人流预测面临累积采样误差的挑战，同时POI演化对人群流动的重要影响以及两者间的多时间尺度异步动态关系和潜在伪因果关系，使得传统NCDE方法难以有效应用

Method: 提出Causal-aware Collaborative neural CDE (C3DE)模型：1) 使用双路径NCDE作为主干网络捕捉多时间尺度的异步演化信号；2) 设计基于反事实的因果效应估计器的动态校正机制；3) 利用预测器融合POI和人群流动的协同信号进行长期预测

Result: 在三个真实世界数据集上的广泛实验表明C3DE具有优越性能，特别是在人流波动显著的城市中表现突出

Conclusion: C3DE通过结合神经控制微分方程和因果推理，有效解决了长期城市人流预测中的累积误差和伪相关性问题，为城市智能交通管理提供了有效解决方案

Abstract: Long-term urban crowd flow prediction suffers significantly from cumulative
sampling errors, due to increased sequence lengths and sampling intervals,
which inspired us to leverage Neural Controlled Differential Equations (NCDEs)
to mitigate this issue. However, regarding the crucial influence of Points of
Interest (POIs) evolution on long-term crowd flow, the multi-timescale
asynchronous dynamics between crowd flow and POI distribution, coupled with
latent spurious causality, poses challenges to applying NCDEs for long-term
urban crowd flow prediction. To this end, we propose Causal-aware Collaborative
neural CDE (C3DE) to model the long-term dynamic of crowd flow. Specifically,
we introduce a dual-path NCDE as the backbone to effectively capture the
asynchronous evolution of collaborative signals across multiple time scales.
Then, we design a dynamic correction mechanism with the counterfactual-based
causal effect estimator to quantify the causal impact of POIs on crowd flow and
minimize the accumulation of spurious correlations. Finally, we leverage a
predictor for long-term prediction with the fused collaborative signals of POI
and crowd flow. Extensive experiments on three real-world datasets demonstrate
the superior performance of C3DE, particularly in cities with notable flow
fluctuations.

</details>


### [76] [Spontaneous Kolmogorov-Arnold Geometry in Shallow MLPs](https://arxiv.org/abs/2509.12326)
*Michael Freedman,Michael Mulligan*

Main category: cs.LG

TL;DR: 研究发现传统单隐藏层神经网络训练过程中会自发产生Kolmogorov-Arnold几何结构，通过分析Jacobian矩阵的外幂统计特性来量化这种几何特征。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络如何有机地学习准备输入数据以供下游处理，并研究KA几何结构的出现规律以通过及时调整超参数来加速学习。

Method: 通过分析Jacobian矩阵J(x)的外幂统计特性，包括零行数量和子式统计量，来量化和表征KA几何结构。

Result: 发现KA几何结构经常在训练普通单隐藏层神经网络时产生，并对其在函数复杂度和模型超参数空间中的出现位置有了初步理解。

Conclusion: 这项研究是KA-Networks的"另一面"，不是将KA工程化到神经网络中，而是观察KA在浅层MLP中的自然涌现。

Abstract: The Kolmogorov-Arnold (KA) representation theorem constructs universal, but
highly non-smooth inner functions (the first layer map) in a single
(non-linear) hidden layer neural network. Such universal functions have a
distinctive local geometry, a "texture," which can be characterized by the
inner function's Jacobian $J({\mathbf{x}})$, as $\mathbf{x}$ varies over the
data. It is natural to ask if this distinctive KA geometry emerges through
conventional neural network optimization. We find that indeed KA geometry often
is produced when training vanilla single hidden layer neural networks. We
quantify KA geometry through the statistical properties of the exterior powers
of $J(\mathbf{x})$: number of zero rows and various observables for the minor
statistics of $J(\mathbf{x})$, which measure the scale and axis alignment of
$J(\mathbf{x})$. This leads to a rough understanding for where KA geometry
occurs in the space of function complexity and model hyperparameters. The
motivation is first to understand how neural networks organically learn to
prepare input data for later downstream processing and, second, to learn enough
about the emergence of KA geometry to accelerate learning through a timely
intervention in network hyperparameters. This research is the "flip side" of
KA-Networks (KANs). We do not engineer KA into the neural network, but rather
watch KA emerge in shallow MLPs.

</details>


### [77] [Integrating Attention-Enhanced LSTM and Particle Swarm Optimization for Dynamic Pricing and Replenishment Strategies in Fresh Food Supermarkets](https://arxiv.org/abs/2509.12339)
*Xianchen Liu,Tianhui Zhang,Xinyu Zhang,Lingmin Hou,Zhen Guo,Yuanhao Tian,Yang Liu*

Main category: cs.LG

TL;DR: 结合LSTM和PSO的新鲜食品超市定价补货优化方法，通过注意力机制增强预测准确性，实现利润最大化和食品浪费减少


<details>
  <summary>Details</summary>
Motivation: 解决新鲜食品零售中动态定价和库存管理的挑战，平衡利润最大化和减少食品浪费的需求，提供可扩展的解决方案

Method: 使用带注意力机制的LSTM网络预测7天内的销量、价格趋势和损耗率，然后通过PSO算法迭代优化定价和补货策略，结合成本加成定价实现动态调整

Result: 该方法不仅最大化利润，还减少食品浪费，提高决策准确性，为易腐商品行业提供可持续的运营方案

Conclusion: 该框架成功弥合了预测建模与优化之间的差距，为新鲜食品零售和其他易腐商品行业提供了有效的动态定价和库存管理解决方案

Abstract: This paper presents a novel approach to optimizing pricing and replenishment
strategies in fresh food supermarkets by combining Long Short-Term Memory
(LSTM) networks with Particle Swarm Optimization (PSO). The LSTM model,
enhanced with an attention mechanism, is used to predict sales volumes, pricing
trends, and spoilage rates over a seven-day period. The predictions generated
by the LSTM model serve as inputs for the PSO algorithm, which iteratively
optimizes pricing and replenishment strategies to maximize profitability while
adhering to inventory constraints. The integration of cost-plus pricing allows
for dynamic adjustments based on fixed and variable costs, ensuring real-time
adaptability to market fluctuations. The framework not only maximizes profits
but also reduces food waste, contributing to more sustainable supermarket
operations. The attention mechanism enhances the interpretability of the LSTM
model by identifying key time points and factors influencing sales, improving
decision-making accuracy. This methodology bridges the gap between predictive
modeling and optimization, offering a scalable solution for dynamic pricing and
inventory management in fresh food retail and other industries dealing with
perishable goods.

</details>


### [78] [FEDONet : Fourier-Embedded DeepONet for Spectrally Accurate Operator Learning](https://arxiv.org/abs/2509.12344)
*Arth Sojitra,Mrigank Dhingra,Omer San*

Main category: cs.LG

TL;DR: 本文提出了FEDONet，一种在DeepONet架构中嵌入傅里叶特征的改进方法，通过随机傅里叶特征映射增强空间表示能力，在多个PDE数据集上相比传统DeepONet获得2-3倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 标准DeepONet使用全连接线性层在主干网络中，难以捕捉复杂PDE中固有的空间结构，需要改进空间表示能力。

Method: 在DeepONet架构中引入傅里叶嵌入主干网络，利用随机傅里叶特征映射来丰富空间表示能力。

Result: 在Poisson方程、Burgers方程、Lorenz-63混沌系统、Eikonal方程、Allen-Cahn方程、Kuramoto-Sivashinsky方程和Lorenz-96系统等PDE数据集上，FEDONet相比传统DeepONet平均相对L2性能提升2-3倍。

Conclusion: 傅里叶嵌入能有效增强神经算子学习，为PDE代理建模提供了强大且广泛适用的方法。

Abstract: Deep Operator Networks (DeepONets) have recently emerged as powerful
data-driven frameworks for learning nonlinear operators, particularly suited
for approximating solutions to partial differential equations (PDEs). Despite
their promising capabilities, the standard implementation of DeepONets, which
typically employs fully connected linear layers in the trunk network, can
encounter limitations in capturing complex spatial structures inherent to
various PDEs. To address this, we introduce Fourier-embedded trunk networks
within the DeepONet architecture, leveraging random Fourier feature mappings to
enrich spatial representation capabilities. Our proposed Fourier-embedded
DeepONet, FEDONet demonstrates superior performance compared to the traditional
DeepONet across a comprehensive suite of PDE-driven datasets, including the
two-dimensional Poisson equation, Burgers' equation, the Lorenz-63 chaotic
system, Eikonal equation, Allen-Cahn equation, Kuramoto-Sivashinsky equation,
and the Lorenz-96 system. Empirical evaluations of FEDONet consistently show
significant improvements in solution reconstruction accuracy, with average
relative L2 performance gains ranging between 2-3x compared to the DeepONet
baseline. This study highlights the effectiveness of Fourier embeddings in
enhancing neural operator learning, offering a robust and broadly applicable
methodology for PDE surrogate modeling.

</details>


### [79] [Linear Dimensionality Reduction for Word Embeddings in Tabular Data Classification](https://arxiv.org/abs/2509.12346)
*Liam Ressel,Hamza A. A. Gardi*

Main category: cs.LG

TL;DR: 本文研究在工程师薪资预测挑战中，针对高维词嵌入特征和有限训练样本的分类问题，提出了基于PCA和LDA的线性降维方法，特别是提出了分块LDA方法，在竞赛中取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 工程师薪资预测任务中，300维词嵌入特征导致维度灾难，且训练样本有限，使得分类具有挑战性。线性降维方法在表格数据分类中的应用尚未充分探索。

Method: 研究PCA和LDA两种线性降维方法。提出Partitioned-LDA方法，将词嵌入分成等大小的块，在每个块上分别进行LDA，减小协方差矩阵规模。结合收缩正则化提升性能。

Result: PCA在适当子空间维度下优于原始嵌入；无正则化的LDA性能较差，但应用收缩后性能显著提升；Partitioned-LDA优于常规LDA，结合收缩后在竞赛公开排行榜上达到top-10准确率。

Conclusion: 提出的分块LDA方法能有效提升有限训练样本下表格数据分类中词嵌入特征的性能，为解决高维小样本分类问题提供了有效方案。

Abstract: The Engineers' Salary Prediction Challenge requires classifying salary
categories into three classes based on tabular data. The job description is
represented as a 300-dimensional word embedding incorporated into the tabular
features, drastically increasing dimensionality. Additionally, the limited
number of training samples makes classification challenging. Linear
dimensionality reduction of word embeddings for tabular data classification
remains underexplored. This paper studies Principal Component Analysis (PCA)
and Linear Discriminant Analysis (LDA). We show that PCA, with an appropriate
subspace dimension, can outperform raw embeddings. LDA without regularization
performs poorly due to covariance estimation errors, but applying shrinkage
improves performance significantly, even with only two dimensions. We propose
Partitioned-LDA, which splits embeddings into equal-sized blocks and performs
LDA separately on each, thereby reducing the size of the covariance matrices.
Partitioned-LDA outperforms regular LDA and, combined with shrinkage, achieves
top-10 accuracy on the competition public leaderboard. This method effectively
enhances word embedding performance in tabular data classification with limited
training samples.

</details>


### [80] [Unsupervised Atomic Data Mining via Multi-Kernel Graph Autoencoders for Machine Learning Force Fields](https://arxiv.org/abs/2509.12358)
*Hong Sun,Joshua A. Vita,Amit Samanta,Vincenzo Lordi*

Main category: cs.LG

TL;DR: 提出MEAGraph模型，一种无监督方法，用于分析原子数据集并有效去除采样偏差，通过多核变换和注意力机制实现原子环境的有效聚类和剪枝。


<details>
  <summary>Details</summary>
Motivation: 在计算化学和材料科学中，数据集生成技术容易在势能面上过度采样特定区域，传统聚类和剪枝方法在高维原子描述符下难以有效识别不同区域，导致信息丢失或偏差去除不彻底。

Method: 开发Multi-kernel Edge Attention-based Graph Autoencoder (MEAGraph)模型，结合多线性核变换和基于注意力的消息传递，无需标签或大量训练即可捕获几何敏感性，实现有效的原子环境聚类。

Result: 在铌、钽和铁数据集上的应用表明，MEAGraph能有效分组相似原子环境，使基础剪枝技术能够成功去除采样偏差。

Conclusion: MEAGraph为表示学习、聚类和数据优化提供了有效方法，可用于数据分析、异常检测和数据集优化，解决了高维原子描述符下的采样偏差问题。

Abstract: Constructing a chemically diverse dataset while avoiding sampling bias is
critical to training efficient and generalizable force fields. However, in
computational chemistry and materials science, many common dataset generation
techniques are prone to oversampling regions of the potential energy surface.
Furthermore, these regions can be difficult to identify and isolate from each
other or may not align well with human intuition, making it challenging to
systematically remove bias in the dataset. While traditional clustering and
pruning (down-sampling) approaches can be useful for this, they can often lead
to information loss or a failure to properly identify distinct regions of the
potential energy surface due to difficulties associated with the high
dimensionality of atomic descriptors. In this work, we introduce the
Multi-kernel Edge Attention-based Graph Autoencoder (MEAGraph) model, an
unsupervised approach for analyzing atomic datasets. MEAGraph combines multiple
linear kernel transformations with attention-based message passing to capture
geometric sensitivity and enable effective dataset pruning without relying on
labels or extensive training. Demonstrated applications on niobium, tantalum,
and iron datasets show that MEAGraph efficiently groups similar atomic
environments, allowing for the use of basic pruning techniques for removing
sampling bias. This approach provides an effective method for representation
learning and clustering that can be used for data analysis, outlier detection,
and dataset optimization.

</details>


### [81] [Enhancing Smart Farming Through Federated Learning: A Secure, Scalable, and Efficient Approach for AI-Driven Agriculture](https://arxiv.org/abs/2509.12363)
*Ritesh Janga,Rushit Dave*

Main category: cs.LG

TL;DR: 提出基于联邦学习的智能农业框架，用于明尼苏达州农场的作物病害检测，在保护数据隐私的同时实现高精度分类


<details>
  <summary>Details</summary>
Motivation: 农业领域对数据驱动决策的需求日益增长，但农场对数据隐私的担忧限制了数据共享。需要一种既能利用先进机器学习技术又能保护敏感农场数据隐私的解决方案

Method: 采用联邦学习框架，包括：从明尼苏达州农场收集数据、应用本地深度学习算法、迁移学习、中央聚合服务器进行模型精炼

Result: 预期实现：病害检测精度提升、良好的农业场景泛化能力、降低通信和训练时间成本、早期病害识别和干预

Conclusion: 该框架填补了先进机器学习技术与农民实际隐私需求之间的空白，利用联邦学习优势为智能农业系统提供安全高效的病害检测方法

Abstract: The agricultural sector is undergoing a transformation with the integration
of advanced technologies, particularly in data-driven decision-making. This
work proposes a federated learning framework for smart farming, aiming to
develop a scalable, efficient, and secure solution for crop disease detection
tailored to the environmental and operational conditions of Minnesota farms. By
maintaining sensitive farm data locally and enabling collaborative model
updates, our proposed framework seeks to achieve high accuracy in crop disease
classification without compromising data privacy. We outline a methodology
involving data collection from Minnesota farms, application of local deep
learning algorithms, transfer learning, and a central aggregation server for
model refinement, aiming to achieve improved accuracy in disease detection,
good generalization across agricultural scenarios, lower costs in communication
and training time, and earlier identification and intervention against diseases
in future implementations. We outline a methodology and anticipated outcomes,
setting the stage for empirical validation in subsequent studies. This work
comes in a context where more and more demand for data-driven interpretations
in agriculture has to be weighed with concerns about privacy from farms that
are hesitant to share their operational data. This will be important to provide
a secure and efficient disease detection method that can finally revolutionize
smart farming systems and solve local agricultural problems with data
confidentiality. In doing so, this paper bridges the gap between advanced
machine learning techniques and the practical, privacy-sensitive needs of
farmers in Minnesota and beyond, leveraging the benefits of federated learning.

</details>


### [82] [Explainable Unsupervised Multi-Anomaly Detection and Temporal Localization in Nuclear Times Series Data with a Dual Attention-Based Autoencoder](https://arxiv.org/abs/2509.12372)
*Konstantinos Vasili,Zachery T. Dahm,Stylianos Chatzidakis*

Main category: cs.LG

TL;DR: 提出基于LSTM自编码器和双重注意力机制的无监督方法，用于核反应堆辐射监测系统中的异常事件检测和定位


<details>
  <summary>Details</summary>
Motivation: 下一代核反应堆规模更小但会产生大量多元时间序列数据，需要开发远程自主控制系统，而准确诊断模块是关键第一步。现有ML/DL方法存在可解释性差、缺乏真实数据、异常事件稀缺等问题

Method: 使用LSTM自编码器结合特征注意力和时间注意力双重机制，特征注意力为异常模式的辐射传感器分配权重，时间注意力突出异常发生的时间步，实现异常检测和定位

Result: 在PUR-1研究反应堆的真实数据集上进行评估，能够识别受影响的传感器和异常持续时间

Conclusion: 该框架通过统一的网络实现了异常事件的检测、定位和特征化，解决了核安全关键领域ML/DL方法可解释性不足的问题

Abstract: The nuclear industry is advancing toward more new reactor designs, with
next-generation reactors expected to be smaller in scale and power output.
These systems have the potential to produce large volumes of information in the
form of multivariate time-series data, which could be used for enhanced
real-time monitoring and control. In this context, the development of remote
autonomous or semi-autonomous control systems for reactor operation has gained
significant interest. A critical first step toward such systems is an accurate
diagnostics module capable of detecting and localizing anomalies within the
reactor system. Recent studies have proposed various ML and DL approaches for
anomaly detection in the nuclear domain. Despite promising results, key
challenges remain, including limited to no explainability, lack of access to
real-world data, and scarcity of abnormal events, which impedes benchmarking
and characterization. Most existing studies treat these methods as black boxes,
while recent work highlights the need for greater interpretability of ML/DL
outputs in safety-critical domains. Here, we propose an unsupervised
methodology based on an LSTM autoencoder with a dual attention mechanism for
characterization of abnormal events in a real-world reactor radiation area
monitoring system. The framework includes not only detection but also
localization of the event and was evaluated using real-world datasets of
increasing complexity from the PUR-1 research reactor. The attention mechanisms
operate in both the feature and temporal dimensions, where the feature
attention assigns weights to radiation sensors exhibiting abnormal patterns,
while time attention highlights the specific timesteps where irregularities
occur, thus enabling localization. By combining the results, the framework can
identify both the affected sensors and the duration of each anomaly within a
single unified network.

</details>


### [83] [Diffusion-Based Generation and Imputation of Driving Scenarios from Limited Vehicle CAN Data](https://arxiv.org/abs/2509.12375)
*Julian Ripper,Ousama Esbel,Rafael Fietzek,Max Mühlhäuser,Thomas Kreutz*

Main category: cs.LG

TL;DR: 本文提出了一种结合自回归和非自回归技术的混合生成方法，使用去噪扩散概率模型(DDPM)在小型汽车CAN时间序列数据集上生成物理正确的合成数据，并能有效修复损坏样本。


<details>
  <summary>Details</summary>
Motivation: 在包含损坏样本的小型时间序列数据集上训练深度学习模型具有挑战性，需要生成真实且物理正确的合成数据来改善数据质量。

Method: 提出了混合生成方法，结合自回归和非自回归技术，使用两种DDPM架构进行时间序列生成，并提出了多项改进。使用三个新指标评估物理正确性和测试轨道遵循度。

Result: 最佳模型在物理正确性方面甚至优于训练数据，显示出合理的驾驶行为，并成功修复了训练数据中物理上不合理的区域。

Conclusion: DDPM能有效生成高质量的时间序列合成数据，改善小型数据集的质量问题，在汽车CAN数据生成和修复方面表现出色。

Abstract: Training deep learning methods on small time series datasets that also
include corrupted samples is challenging. Diffusion models have shown to be
effective to generate realistic and synthetic data, and correct corrupted
samples through imputation. In this context, this paper focuses on generating
synthetic yet realistic samples of automotive time series data. We show that
denoising diffusion probabilistic models (DDPMs) can effectively solve this
task by applying them to a challenging vehicle CAN-dataset with long-term data
and a limited number of samples. Therefore, we propose a hybrid generative
approach that combines autoregressive and non-autoregressive techniques. We
evaluate our approach with two recently proposed DDPM architectures for time
series generation, for which we propose several improvements. To evaluate the
generated samples, we propose three metrics that quantify physical correctness
and test track adherence. Our best model is able to outperform even the
training data in terms of physical correctness, while showing plausible driving
behavior. Finally, we use our best model to successfully impute physically
implausible regions in the training data, thereby improving the data quality.

</details>


### [84] [Causal-Symbolic Meta-Learning (CSML): Inducing Causal World Models for Few-Shot Generalization](https://arxiv.org/abs/2509.12387)
*Mohamed Zayaan S*

Main category: cs.LG

TL;DR: CSML是一个新颖的因果符号元学习框架，通过感知、因果归纳和推理三个模块学习任务分布的潜在因果结构，在需要真正因果推理的任务上显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现代深度学习模型依赖虚假相关性，泛化能力差且需要大量数据。人类智能的关键在于理解因果机制，实现鲁棒且样本高效的学习

Method: 包含三个核心模块：感知模块（将原始输入映射到解耦的符号表示）、可微分因果归纳模块（发现符号间的因果图）、基于图的推理模块（利用因果图进行预测）。通过元学习跨任务分布的共享因果世界模型

Result: 在CausalWorld新基准测试中，CSML显著优于最先进的元学习和神经符号基线方法，特别是在需要真正因果推理的任务上表现突出

Conclusion: CSML框架通过整合因果推理和元学习，能够从少量样本快速适应新任务，包括需要干预和反事实推理的任务，为实现人类水平的样本高效学习提供了有前景的方向

Abstract: Modern deep learning models excel at pattern recognition but remain
fundamentally limited by their reliance on spurious correlations, leading to
poor generalization and a demand for massive datasets. We argue that a key
ingredient for human-like intelligence-robust, sample-efficient learning-stems
from an understanding of causal mechanisms. In this work, we introduce
Causal-Symbolic Meta-Learning (CSML), a novel framework that learns to infer
the latent causal structure of a task distribution. CSML comprises three key
modules: a perception module that maps raw inputs to disentangled symbolic
representations; a differentiable causal induction module that discovers the
underlying causal graph governing these symbols and a graph-based reasoning
module that leverages this graph to make predictions. By meta-learning a shared
causal world model across a distribution of tasks, CSML can rapidly adapt to
novel tasks, including those requiring reasoning about interventions and
counterfactuals, from only a handful of examples. We introduce CausalWorld, a
new physics-based benchmark designed to test these capabilities. Our
experiments show that CSML dramatically outperforms state-of-the-art
meta-learning and neuro-symbolic baselines, particularly on tasks demanding
true causal inference.

</details>


### [85] [Evaluating the printability of stl files with ML](https://arxiv.org/abs/2509.12392)
*Janik Henn,Adrian Hauptmannl,Hamza A. A. Gardi*

Main category: cs.LG

TL;DR: 开发AI模型检测3D模型中可能导致打印失败的几何问题，帮助非专业用户避免打印失败


<details>
  <summary>Details</summary>
Motivation: 3D打印技术正从专业用户向大众市场扩展，但非专业用户缺乏识别模型几何问题的经验，容易导致打印失败。需要开发自动化工具来帮助用户提前发现问题

Method: 训练AI模型来检测3D模型中常见的几何问题，特别是那些难以打印的几何特征

Result: 提出了一种新颖的AI支持层，能够在打印开始前识别可能导致打印失败的模型特征

Conclusion: 该方法为3D打印软件增加了智能检测功能，有助于降低非专业用户的打印失败率，推动3D打印技术的大众化应用

Abstract: 3D printing has long been a technology for industry professionals and
enthusiasts willing to tinker or even build their own machines. This stands in
stark contrast to today's market, where recent developments have prioritized
ease of use to attract a broader audience. Slicing software nowadays has a few
ways to sanity check the input file as well as the output gcode. Our approach
introduces a novel layer of support by training an AI model to detect common
issues in 3D models. The goal is to assist less experienced users by
identifying features that are likely to cause print failures due to difficult
to print geometries before printing even begins.

</details>


### [86] [Adaptive Spatial Goodness Encoding: Advancing and Scaling Forward-Forward Learning Without Backpropagation](https://arxiv.org/abs/2509.12394)
*Qingchun Gong,Robert Bogdan Staszewski,Kai Xu*

Main category: cs.LG

TL;DR: 提出了自适应空间优良性编码(ASGE)框架，用于解决前向-前向算法在CNN中的通道维度爆炸问题，实现了无反向传播的CNN训练，在多个数据集上达到竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 现有的前向-前向算法扩展虽然改进了原始算法并适应了卷积神经网络，但存在表示能力有限和在大规模数据集上扩展性差的问题，主要原因是通道维度爆炸。

Method: 提出自适应空间优良性编码(ASGE)框架，利用特征图计算每层的空间感知优良性表示，实现逐层监督，将分类复杂度与通道维度解耦。

Result: ASGE在多个基准测试中优于其他前向-前向方法：MNIST 99.65%、FashionMNIST 93.41%、CIFAR-10 90.62%、CIFAR-100 65.42%，并首次在ImageNet上实现前向-前向训练（Top-1 26.21%、Top-5 47.49%）。

Conclusion: ASGE框架完全消除了反向传播，显著缩小了与BP训练模型的性能差距，为可扩展的无反向传播CNN训练奠定了可行基础。

Abstract: The Forward-Forward (FF) algorithm offers a promising alternative to
backpropagation (BP). Despite advancements in recent FF-based extensions, which
have enhanced the original algorithm and adapted it to convolutional neural
networks (CNNs), they often suffer from limited representational capacity and
poor scalability to large-scale datasets, primarily due to exploding channel
dimensionality. In this work, we propose adaptive spatial goodness encoding
(ASGE), a new FF-based training framework tailored for CNNs. ASGE leverages
feature maps to compute spatially-aware goodness representations at each layer,
enabling layer-wise supervision. Crucially, this approach decouples
classification complexity from channel dimensionality, thereby addressing the
issue of channel explosion and achieving competitive performance compared to
other BP-free methods. ASGE outperforms all other FF-based approaches across
multiple benchmarks, delivering test accuracies of 99.65% on MNIST, 93.41% on
FashionMNIST, 90.62% on CIFAR-10, and 65.42% on CIFAR-100. Moreover, we present
the first successful application of FF-based training to ImageNet, with Top-1
and Top-5 accuracies of 26.21% and 47.49%. By entirely eliminating BP and
significantly narrowing the performance gap with BP-trained models, the ASGE
framework establishes a viable foundation toward scalable BP-free CNN training.

</details>


### [87] [Bayesian Parametric Matrix Models: Principled Uncertainty Quantification for Spectral Learning](https://arxiv.org/abs/2509.12406)
*Mohammad Nooraiepour*

Main category: cs.LG

TL;DR: 该论文提出了贝叶斯参数矩阵模型（B-PMMs），为科学机器学习中的谱方法提供不确定性量化，解决了传统确定性方法在安全关键应用中缺乏置信度估计的问题。


<details>
  <summary>Details</summary>
Motivation: 当前谱学习方法仅提供点估计而无不确定性量化，限制了其在需要预测置信度的安全关键应用中的使用。参数矩阵模型虽然性能优异，但其确定性本质阻碍了在不确定性量化应用中的部署。

Method: 提出了贝叶斯参数矩阵模型框架，包括：(i) 具有正则化矩阵扰动界的自适应谱分解，(ii) 使用流形感知矩阵变量高斯后验的结构化变分推理算法，(iii) 具有谱间隙和问题条件显式依赖的有限样本校准保证。

Result: 在5x5到500x500矩阵维度上的实验验证显示完美收敛率，B-PMMs实现了优异的不确定性校准（ECE < 0.05），同时保持良好的扩展性，在谱病态条件下表现优雅退化。

Conclusion: 该框架支持在不确定性关键领域进行稳健谱学习，为更广泛的贝叶斯谱机器学习奠定了基础，提供了即使在近退化状态下也能可靠估计不确定性的方法。

Abstract: Scientific machine learning increasingly uses spectral methods to understand
physical systems. Current spectral learning approaches provide only point
estimates without uncertainty quantification, limiting their use in
safety-critical applications where prediction confidence is essential.
Parametric matrix models have emerged as powerful tools for scientific machine
learning, achieving exceptional performance by learning governing equations.
However, their deterministic nature limits deployment in uncertainty
quantification applications. We introduce Bayesian parametric matrix models
(B-PMMs), a principled framework that extends PMMs to provide uncertainty
estimates while preserving their spectral structure and computational
efficiency. B-PMM addresses the fundamental challenge of quantifying
uncertainty in matrix eigenvalue problems where standard Bayesian methods fail
due to the geometric constraints of spectral decomposition. The theoretical
contributions include: (i) adaptive spectral decomposition with regularized
matrix perturbation bounds that characterize eigenvalue uncertainty
propagation, (ii) structured variational inference algorithms using
manifold-aware matrix-variate Gaussian posteriors that respect Hermitian
constraints, and (iii) finite-sample calibration guarantees with explicit
dependence on spectral gaps and problem conditioning. Experimental validation
across matrix dimensions from 5x5 to 500x500 with perfect convergence rates
demonstrates that B-PMMs achieve exceptional uncertainty calibration (ECE <
0.05) while maintaining favorable scaling. The framework exhibits graceful
degradation under spectral ill-conditioning and provides reliable uncertainty
estimates even in near-degenerate regimes. The proposed framework supports
robust spectral learning in uncertainty-critical domains and lays the
groundwork for broader Bayesian spectral machine learning.

</details>


### [88] [Surrogate Representation Inference for Noisy Text and Image Annotations](https://arxiv.org/abs/2509.12416)
*Kentaro Nakamura*

Main category: cs.LG

TL;DR: 提出Surrogate Representation Inference (SRI)方法，通过神经网络学习低维表示来校正机器学习标注中的偏差，在中等预测精度下可将标准误差降低50%以上，并能处理人工标注中的非差分测量误差。


<details>
  <summary>Details</summary>
Motivation: 现有方法在机器学习模型和LLMs标注非结构化数据时存在标准误差大、需要无误差人工标注的问题，需要开发更有效的偏差校正方法。

Method: 提出SRI方法，假设非结构化数据完全中介人工标注与结构化变量之间的关系，通过神经网络架构学习满足代理假设的低维表示，可校正人工标注中的非差分测量误差。

Result: 模拟研究和实际应用表明，当机器学习预测精度中等时，SRI可将标准误差降低50%以上，即使在人工标注存在非差分测量误差时也能提供有效推断。

Conclusion: SRI方法通过低维表示学习有效解决了机器学习标注中的偏差问题，显著提高了统计推断的精度和鲁棒性。

Abstract: As researchers increasingly rely on machine learning models and LLMs to
annotate unstructured data, such as texts or images, various approaches have
been proposed to correct bias in downstream statistical analysis. However,
existing methods tend to yield large standard errors and require some
error-free human annotation. In this paper, I introduce Surrogate
Representation Inference (SRI), which assumes that unstructured data fully
mediate the relationship between human annotations and structured variables.
The assumption is guaranteed by design provided that human coders rely only on
unstructured data for annotation. Under this setting, I propose a neural
network architecture that learns a low-dimensional representation of
unstructured data such that the surrogate assumption remains to be satisfied.
When multiple human annotations are available, SRI can further correct
non-differential measurement errors that may exist in human annotations.
Focusing on text-as-outcome settings, I formally establish the identification
conditions and semiparametric efficient estimation strategies that enable
learning and leveraging such a low-dimensional representation. Simulation
studies and a real-world application demonstrate that SRI reduces standard
errors by over 50% when machine learning prediction accuracy is moderate and
provides valid inference even when human annotations contain non-differential
measurement errors.

</details>


### [89] [On the Regularity and Fairness of Combinatorial Multi-Armed Bandit](https://arxiv.org/abs/2509.12457)
*Xiaoyi Wu,Bin Li*

Main category: cs.LG

TL;DR: 提出了一种参数化的正则公平学习算法，用于组合多臂老虎机问题，同时优化累积奖励、保证公平性和奖励规律性。


<details>
  <summary>Details</summary>
Motivation: 受无线网络应用启发，需要在最大化累积奖励的同时保证各臂的公平性（最小平均奖励要求）和奖励规律性（奖励频率）。

Method: 算法线性组合虚拟队列长度（跟踪公平性违规）、时间-自-上次-奖励（TSLR）指标和上置信界（UCB）估计，利用Lyapunov函数进行理论分析。

Result: 理论分析表明算法实现了零累积公平性违规、良好的奖励规律性和累积遗憾性能，并通过两个真实数据集验证。

Conclusion: 所提算法能有效同时解决组合多臂老虎机中的三个关键目标：最大化奖励、保证公平性和奖励规律性。

Abstract: The combinatorial multi-armed bandit model is designed to maximize cumulative
rewards in the presence of uncertainty by activating a subset of arms in each
round. This paper is inspired by two critical applications in wireless
networks, where it's not only essential to maximize cumulative rewards but also
to guarantee fairness among arms (i.e., the minimum average reward required by
each arm) and ensure reward regularity (i.e., how often each arm receives the
reward). In this paper, we propose a parameterized regular and fair learning
algorithm to achieve these three objectives. In particular, the proposed
algorithm linearly combines virtual queue-lengths (tracking the fairness
violations), Time-Since-Last-Reward (TSLR) metrics, and Upper Confidence Bound
(UCB) estimates in its weight measure. Here, TSLR is similar to
age-of-information and measures the elapsed number of rounds since the last
time an arm received a reward, capturing the reward regularity performance, and
UCB estimates are utilized to balance the tradeoff between exploration and
exploitation in online learning. By exploring a key relationship between
virtual queue-lengths and TSLR metrics and utilizing several non-trivial
Lyapunov functions, we analytically characterize zero cumulative fairness
violation, reward regularity, and cumulative regret performance under our
proposed algorithm. These theoretical outcomes are verified by simulations
based on two real-world datasets.

</details>


### [90] [Nonlocal Neural Tangent Kernels via Parameter-Space Interactions](https://arxiv.org/abs/2509.12467)
*Sriram Nagaraj,Vishakh Hari*

Main category: cs.LG

TL;DR: 提出了非局部神经正切核(NNTK)框架，用参数空间中的非局部交互近似替代局部梯度，从而将NTK理论扩展到非光滑函数、随机估计器和更广泛的模型族


<details>
  <summary>Details</summary>
Motivation: 传统NTK框架依赖于网络对参数可微的假设，这一假设在处理非光滑目标函数或表现出不可微行为的参数化模型时会失效，限制了NTK理论的应用范围

Method: 使用非局部梯度替代标准梯度，提出了固定核和基于注意力的两种非局部算子公式，通过参数空间中的非局部交互近似来实现

Result: 成功扩展了NTK理论的应用范围，使其能够处理非光滑函数和更广泛的模型族，并通过数值研究验证了新公式的有效性

Conclusion: 非局部神经正切核框架为分析非光滑神经网络训练动力学提供了新的理论工具，扩展了传统NTK方法的适用范围

Abstract: The Neural Tangent Kernel (NTK) framework has provided deep insights into the
training dynamics of neural networks under gradient flow. However, it relies on
the assumption that the network is differentiable with respect to its
parameters, an assumption that breaks down when considering non-smooth target
functions or parameterized models exhibiting non-differentiable behavior. In
this work, we propose a Nonlocal Neural Tangent Kernel (NNTK) that replaces the
local gradient with a nonlocal interaction-based approximation in parameter
space. Nonlocal gradients are known to exist for a wider class of functions
than the standard gradient. This allows NTK theory to be extended to nonsmooth
functions, stochastic estimators, and broader families of models. We explore
both fixed-kernel and attention-based formulations of this nonlocal operator.
We illustrate the new formulation with numerical studies.

</details>


### [91] [Comparative Analysis of Wave Scattering Numerical Modeling Using the Boundary Element Method and Physics-Informed Neural Networks](https://arxiv.org/abs/2509.12483)
*Oscar Rincón-Cardeno,Gregorio Pérez Bernal,Silvana Montoya Noguera,Nicolás Guarín-Zapata*

Main category: cs.LG

TL;DR: 本研究比较了边界元法(BEM)和物理信息神经网络(PINNs)在二维亥姆霍兹方程波散射问题中的性能，发现在相同精度下PINNs训练时间比BEM长42倍，但评估速度快204倍，且泛化能力较差。


<details>
  <summary>Details</summary>
Motivation: 比较BEM和PINNs两种方法在相同条件下解决波散射问题的性能，为未来波传播问题研究提供方法选择的定量依据。

Method: 使用BEM进行边界离散化，使用PINNs通过最小化控制方程和边界条件的残差来训练神经网络，并通过超参数优化确定PINNs配置，比较两种方法的精度、计算时间和泛化能力。

Result: 在相同精度下，PINNs训练时间比BEM长42倍，但评估速度快204倍。在训练域外泛化时，PINNs相对误差从7.46×10⁻²增加到8.22，而BEM在扩展区域保持相似误差水平。

Conclusion: PINNs在评估速度上有优势但训练耗时且泛化能力有限，BEM在计算效率和泛化性方面表现更稳定，研究为波传播问题的方法选择提供了重要参考。

Abstract: Purpose - This study compares the Boundary Element Method (BEM) and
Physics-Informed Neural Networks (PINNs) for solving the two-dimensional
Helmholtz equation in wave scattering problems. The objective is to evaluate
the performance of both methods under the same conditions.
  Design/methodology/approach - We solve the Helmholtz equation using BEM and
PINNs for the same scattering problem. The PINNs are trained by minimizing the
residual of the governing equations and boundary conditions, with their
configuration determined through hyperparameter optimization, while the BEM is
applied using boundary discretization. Both methods are evaluated in terms of
solution accuracy, computation time, and generalization capacity.
  Findings - Numerical experiments were conducted by varying the number of
integration points for BEM and the number of layers and neurons per layer for
PINNs. Hyperparameter tuning provided further insight into suitable
configurations for wave scattering problems. At comparable accuracy, PINNs
produced consistent solutions but required training times approximately 42
times longer than BEM. However, once trained, PINNs achieved evaluation times
up to 204 times faster. The generalization capacity was also assessed outside
the PINN training domain, where the relative error increased from $7.46 \times
10^{-2}$ to 8.22, while BEM maintained a similar error level in the extended
region.
  Originality/value - This work presents a direct comparison between PINNs and
BEM for the Helmholtz equation. The analysis provides quantitative data on the
performance of both methods, supporting their selection in future research on
wave propagation problems and establishing future challenges and directions.

</details>


### [92] [Finite-Agent Stochastic Differential Games on Large Graphs: II. Graph-Based Architectures](https://arxiv.org/abs/2509.12484)
*Ruimeng Hu,Jihao Long,Haosheng Zhou*

Main category: cs.LG

TL;DR: 提出了一种名为NTM的新型神经网络架构，用于计算图结构随机微分博弈中的纳什均衡，通过嵌入非可训练组件减少参数数量并提高计算效率


<details>
  <summary>Details</summary>
Motivation: 解决图结构多智能体系统中随机微分博弈的计算挑战，这些系统在金融、机器人、能源和社会动力学等领域普遍存在，传统方法在大规模稀疏设置下参数过多且计算效率低

Method: 设计NTM架构，在图引导的稀疏化前馈神经网络中嵌入固定的非可训练组件，与底层图拓扑对齐，然后将其集成到Direct Parameterization和Deep BSDE两种游戏求解器中

Result: 理论证明NTM在静态图博弈中具有通用逼近性质，数值实验表明基于NTM的方法在三种随机微分博弈中达到与全可训练方法相当的性能，同时计算效率更高

Conclusion: NTM架构通过减少可训练参数数量，在保持性能的同时显著提高了图结构随机微分博弈求解的计算效率，为大规模多智能体系统提供了有效的解决方案

Abstract: We propose a novel neural network architecture, called Non-Trainable
Modification (NTM), for computing Nash equilibria in stochastic differential
games (SDGs) on graphs. These games model a broad class of graph-structured
multi-agent systems arising in finance, robotics, energy, and social dynamics,
where agents interact locally under uncertainty. The NTM architecture imposes a
graph-guided sparsification on feedforward neural networks, embedding fixed,
non-trainable components aligned with the underlying graph topology. This
design enhances interpretability and stability, while significantly reducing
the number of trainable parameters in large-scale, sparse settings. We
theoretically establish a universal approximation property for NTM in static
games on graphs and numerically validate its expressivity and robustness
through supervised learning tasks. Building on this foundation, we incorporate
NTM into two state-of-the-art game solvers, Direct Parameterization and Deep
BSDE, yielding their sparse variants (NTM-DP and NTM-DBSDE). Numerical
experiments on three SDGs across various graph structures demonstrate that
NTM-based methods achieve performance comparable to their fully trainable
counterparts, while offering improved computational efficiency.

</details>


### [93] [Prediction and Causality of functional MRI and synthetic signal using a Zero-Shot Time-Series Foundation Model](https://arxiv.org/abs/2509.12497)
*Alessandro Crimi,Andrea Brovelli*

Main category: cs.LG

TL;DR: 本文评估了基础模型在fMRI脑信号预测和因果发现中的表现，与传统Wiener-Granger因果方法对比，发现基础模型在零样本设置下具有竞争力，并能更精确检测因果交互


<details>
  <summary>Details</summary>
Motivation: 随着基础模型的兴起，需要了解其在脑信号预测和因果分析方面与传统方法的比较，以及是否能在零样本设置下应用

Method: 使用基础模型进行零样本和微调预测，通过Granger-like估计与标准Granger因果性比较，使用逻辑映射耦合和Ornstein-Uhlenbeck过程生成合成时间序列进行验证

Result: 基础模型在零样本预测fMRI时间序列方面表现竞争性（对照组MAPE 0.55，患者组0.27），虽然标准Granger因果性未显示明显差异，但基础模型能更精确检测因果交互

Conclusion: 基础模型具有多功能性、强大的零样本性能，在时间序列数据的预测和因果发现方面具有潜在应用价值

Abstract: Time-series forecasting and causal discovery are central in neuroscience, as
predicting brain activity and identifying causal relationships between neural
populations and circuits can shed light on the mechanisms underlying cognition
and disease. With the rise of foundation models, an open question is how they
compare to traditional methods for brain signal forecasting and causality
analysis, and whether they can be applied in a zero-shot setting. In this work,
we evaluate a foundation model against classical methods for inferring
directional interactions from spontaneous brain activity measured with
functional magnetic resonance imaging (fMRI) in humans. Traditional approaches
often rely on Wiener-Granger causality. We tested the forecasting ability of
the foundation model in both zero-shot and fine-tuned settings, and assessed
causality by comparing Granger-like estimates from the model with standard
Granger causality. We validated the approach using synthetic time series
generated from ground-truth causal models, including logistic map coupling and
Ornstein-Uhlenbeck processes. The foundation model achieved competitive
zero-shot forecasting fMRI time series (mean absolute percentage error of 0.55
in controls and 0.27 in patients). Although standard Granger causality did not
show clear quantitative differences between models, the foundation model
provided a more precise detection of causal interactions.
  Overall, these findings suggest that foundation models offer versatility,
strong zero-shot performance, and potential utility for forecasting and causal
discovery in time-series data.

</details>


### [94] [Phi: Preference Hijacking in Multi-modal Large Language Models at Inference Time](https://arxiv.org/abs/2509.12521)
*Yifan Lan,Yuanpu Cao,Weitong Zhang,Lu Lin,Jinghui Chen*

Main category: cs.LG

TL;DR: 本文提出了一种名为Preference Hijacking (Phi)的新方法，通过精心优化的图像在推理时任意操纵多模态大语言模型的输出偏好，生成具有偏见但难以检测的响应。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型(MLLMs)的广泛应用引发了严重的安全担忧，研究发现其输出偏好可以通过优化图像被任意操纵，这种攻击生成的相关但带有偏见的响应既不明显有害也不违反伦理，难以检测。

Method: 提出Preference Hijacking (Phi)方法，在推理时使用偏好劫持图像来操纵MLLM响应偏好，无需修改模型。还引入了通用劫持扰动，可嵌入不同图像中将MLLM响应导向攻击者指定的偏好。

Result: 在各种任务上的实验结果表明该方法有效，能够成功劫持MLLM的输出偏好。

Conclusion: 该研究揭示了MLLMs存在新的安全风险，即输出偏好可通过优化图像被操纵，需要开发相应的防御机制来应对这种隐蔽的攻击方式。

Abstract: Recently, Multimodal Large Language Models (MLLMs) have gained significant
attention across various domains. However, their widespread adoption has also
raised serious safety concerns. In this paper, we uncover a new safety risk of
MLLMs: the output preference of MLLMs can be arbitrarily manipulated by
carefully optimized images. Such attacks often generate contextually relevant
yet biased responses that are neither overtly harmful nor unethical, making
them difficult to detect. Specifically, we introduce a novel method, Preference
Hijacking (Phi), for manipulating the MLLM response preferences using a
preference hijacked image. Our method works at inference time and requires no
model modifications. Additionally, we introduce a universal hijacking
perturbation -- a transferable component that can be embedded into different
images to hijack MLLM responses toward any attacker-specified preferences.
Experimental results across various tasks demonstrate the effectiveness of our
approach. The code for Phi is accessible at https://github.com/Yifan-Lan/Phi.

</details>


### [95] [Selective Risk Certification for LLM Outputs via Information-Lift Statistics: PAC-Bayes, Robustness, and Skeleton Design](https://arxiv.org/abs/2509.12527)
*Sanjeda Akter,Ibne Farabi Shihab,Anuj Sharma*

Main category: cs.LG

TL;DR: 本文提出了信息提升证书理论，通过PAC-Bayes子伽马分析和骨架敏感性定理，为选择性分类提供形式化保证，在多个数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常产生看似合理但错误的输出，现有启发式方法缺乏形式化保证，需要开发具有理论保证的选择性分类方法。

Method: 开发了信息提升证书理论，包括PAC-Bayes子伽马分析、骨架敏感性定理、假设违反下的故障模式保证，以及骨架构建的变分方法。

Result: 在六个数据集和多个模型家族上验证了假设，在相同风险下减少12-15%的弃权率，运行时开销保持在20%以下（通过批处理进一步降低）。

Conclusion: 该理论为选择性分类提供了首个全面的形式化保证框架，在保持低运行时开销的同时显著提升了模型性能。

Abstract: Large language models often produce plausible but incorrect outputs. Existing
heuristics such as HallBayes lack formal guarantees. We develop the first
comprehensive theory of \emph{information-lift certificates} under selective
classification. Our contributions are: (i) a PAC-Bayes \emph{sub-gamma}
analysis extending beyond standard Bernstein bounds; (ii) explicit skeleton
sensitivity theorems quantifying robustness to misspecification; (iii)
failure-mode guarantees under assumption violations; and (iv) a principled
variational method for skeleton construction. Across six datasets and multiple
model families, we validate assumptions empirically, reduce abstention by
12--15\% at the same risk, and maintain runtime overhead below 20\% (further
reduced via batching).

</details>


### [96] [Graph Homophily Booster: Rethinking the Role of Discrete Features on Heterophilic Graphs](https://arxiv.org/abs/2509.12530)
*Ruizhong Qiu,Ting-Wei Li,Gaotang Li,Hanghang Tong*

Main category: cs.LG

TL;DR: GRAPHITE是一个通过图变换直接提高图同质性的新框架，专门解决图神经网络在异质图上的性能问题，在异质图上显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有GNN在异质图上表现不佳，甚至不如简单的MLP，需要超越架构设计的新方法来直接解决异质性问题

Method: 通过创建特征节点来促进具有相似特征节点间的同质性消息传递，直接转换图结构来提高图同质性

Result: 在挑战性数据集上显著优于最先进方法，在异质图上性能大幅提升，在同质图上也能达到可比精度

Conclusion: GRAPHITE是首个显式转换图结构来直接提高同质性的方法，为解决图异质性提供了创新范式

Abstract: Graph neural networks (GNNs) have emerged as a powerful tool for modeling
graph-structured data. However, existing GNNs often struggle with heterophilic
graphs, where connected nodes tend to have dissimilar features or labels. While
numerous methods have been proposed to address this challenge, they primarily
focus on architectural designs without directly targeting the root cause of the
heterophily problem. These approaches still perform even worse than the
simplest MLPs on challenging heterophilic datasets. For instance, our
experiments show that 21 latest GNNs still fall behind the MLP on the Actor
dataset. This critical challenge calls for an innovative approach to addressing
graph heterophily beyond architectural designs. To bridge this gap, we propose
and study a new and unexplored paradigm: directly increasing the graph
homophily via a carefully designed graph transformation. In this work, we
present a simple yet effective framework called GRAPHITE to address graph
heterophily. To the best of our knowledge, this work is the first method that
explicitly transforms the graph to directly improve the graph homophily.
Stemmed from the exact definition of homophily, our proposed GRAPHITE creates
feature nodes to facilitate homophilic message passing between nodes that share
similar features. Furthermore, we both theoretically and empirically show that
our proposed GRAPHITE significantly increases the homophily of originally
heterophilic graphs, with only a slight increase in the graph size. Extensive
experiments on challenging datasets demonstrate that our proposed GRAPHITE
significantly outperforms state-of-the-art methods on heterophilic graphs while
achieving comparable accuracy with state-of-the-art methods on homophilic
graphs.

</details>


### [97] [Cross-Modal Deep Metric Learning for Time Series Anomaly Detection](https://arxiv.org/abs/2509.12540)
*Wei Li,Zheze Yang*

Main category: cs.LG

TL;DR: 提出基于跨模态深度度量学习的时间序列异常检测方法，通过特征聚类模型和vMF分布提高检测灵敏度和效率


<details>
  <summary>Details</summary>
Motivation: 解决时间序列异常检测中灵敏度低和耗时高的问题

Method: 构建跨模态深度度量学习特征聚类模型，包含输入层、三元组选择层和损失函数计算层；使用平方欧氏距离计算聚类中心距离，采用随机梯度下降优化；利用主成分方向向量内积作为异常度量，应用vMF分布描述时间序列数据方向特征

Result: 实验结果表明该方法能准确分类不同属性的时间序列数据，对异常具有高灵敏度，检测精度高、速度快、鲁棒性强

Conclusion: 所提出的跨模态深度度量学习方法有效解决了时间序列异常检测的灵敏度和效率问题，具有很好的实用价值

Abstract: To effectively address the issues of low sensitivity and high time
consumption in time series anomaly detection, we propose an anomaly detection
method based on cross-modal deep metric learning. A cross-modal deep metric
learning feature clustering model is constructed, composed of an input layer, a
triplet selection layer, and a loss function computation layer. The squared
Euclidean distances between cluster centers are calculated, and a stochastic
gradient descent strategy is employed to optimize the model and classify
different time series features. The inner product of principal component
direction vectors is used as a metric for anomaly measurement. The von
Mises-Fisher (vMF) distribution is applied to describe the directional
characteristics of time series data, and historical data is used to train and
obtain evaluation parameters. By comparing the principal component direction
vector of actual time series data with the threshold, anomaly detection is
performed. Experimental results demonstrate that the proposed method accurately
classifies time series data with different attributes, exhibits high
sensitivity to anomalies, and achieves high detection accuracy, fast detection
speed, and strong robustness.

</details>


### [98] [iCD: A Implicit Clustering Distillation Mathod for Structural Information Mining](https://arxiv.org/abs/2509.12553)
*Xiang Xue,Yatu Ji,Qing-dao-er-ji Ren,Bao Shi,Min Lu,Nier Wu,Xufei Zhuang,Haiteng Xu,Gan-qi-qi-ge Cha*

Main category: cs.LG

TL;DR: 提出iCD方法，通过Gram矩阵挖掘logits中的可解释结构知识，无需标签或特征对齐，在细粒度分类任务中表现优异


<details>
  <summary>Details</summary>
Motivation: Logit知识蒸馏虽然简单但缺乏可解释性，需要一种能够挖掘logits中结构知识的方法

Method: 使用解耦的局部logit表示的Gram矩阵，让学生模型学习潜在语义结构模式

Result: 在基准数据集上验证有效性，特别是在细粒度分类任务中达到+5.08%的峰值提升

Conclusion: iCD是一种简单有效的可解释知识蒸馏方法，能够从logits中挖掘结构知识并实现优异性能

Abstract: Logit Knowledge Distillation has gained substantial research interest in
recent years due to its simplicity and lack of requirement for intermediate
feature alignment; however, it suffers from limited interpretability in its
decision-making process. To address this, we propose implicit Clustering
Distillation (iCD): a simple and effective method that mines and transfers
interpretable structural knowledge from logits, without requiring ground-truth
labels or feature-space alignment. iCD leverages Gram matrices over decoupled
local logit representations to enable student models to learn latent semantic
structural patterns. Extensive experiments on benchmark datasets demonstrate
the effectiveness of iCD across diverse teacher-student architectures, with
particularly strong performance in fine-grained classification tasks --
achieving a peak improvement of +5.08% over the baseline. The code is available
at: https://github.com/maomaochongaa/iCD.

</details>


### [99] [No Need for "Learning" to Defer? A Training Free Deferral Framework to Multiple Experts through Conformal Prediction](https://arxiv.org/abs/2509.12573)
*Tim Bary,Benoît Macq,Louis Petit*

Main category: cs.LG

TL;DR: 提出基于共形预测的训练免费、模型无关的专家延迟框架，通过预测集识别标签不确定性并选择最具区分性的专家，在CIFAR10-H和ImageNet16-H上达到99%+准确率，减少专家工作量11倍。


<details>
  <summary>Details</summary>
Motivation: 现有学习延迟(L2D)方法对专家组成变化敏感且需要大量重新训练，需要一种无需训练、模型无关的专家延迟解决方案。

Method: 使用共形预测器生成预测集来识别标签特定不确定性，通过可分离性准则选择最能区分剩余可能标签的专家。

Result: 在CIFAR10-H和ImageNet16-H上分别达到99.57±0.10%和99.40±0.52%准确率，专家工作量减少最多11倍，在专家性能下降和低信息设置下保持稳健。

Conclusion: 该方法为现实世界人机协作提供了可扩展、无需重新训练的L2D替代方案，显著提升预测可靠性并降低人工成本。

Abstract: AI systems often fail to deliver reliable predictions across all inputs,
prompting the need for hybrid human-AI decision-making. Existing Learning to
Defer (L2D) approaches address this by training deferral models, but these are
sensitive to changes in expert composition and require significant retraining
if experts change. We propose a training-free, model- and expert-agnostic
framework for expert deferral based on conformal prediction. Our method uses
the prediction set generated by a conformal predictor to identify
label-specific uncertainty and selects the most discriminative expert using a
segregativity criterion, measuring how well an expert distinguishes between the
remaining plausible labels. Experiments on CIFAR10-H and ImageNet16-H show that
our method consistently outperforms both the standalone model and the strongest
expert, with accuracies attaining $99.57\pm0.10\%$ and $99.40\pm0.52\%$, while
reducing expert workload by up to a factor of $11$. The method remains robust
under degraded expert performance and shows a gradual performance drop in
low-information settings. These results suggest a scalable, retraining-free
alternative to L2D for real-world human-AI collaboration.

</details>


### [100] [Exploring Training Data Attribution under Limited Access Constraints](https://arxiv.org/abs/2509.12581)
*Shiyuan Zhang,Junwei Deng,Juhan Bae,Jiaqi Ma*

Main category: cs.LG

TL;DR: 本研究系统分析了在模型访问受限和计算资源有限条件下训练数据归因(TDA)方法的可行性，提出了使用代理模型等解决方案，并证明未在目标数据集上训练的模型也能获得有意义的归因分数。


<details>
  <summary>Details</summary>
Motivation: 现有的梯度基TDA方法(如影响函数)在实际应用中面临两大挑战：商业模型不可公开访问和计算资源有限，这限制了TDA方法的广泛应用。

Method: 通过系统研究不同访问约束级别下的TDA方法，利用代理模型等适当设计的解决方案，并验证未在目标数据集上训练的模型获取归因分数的有效性。

Result: 研究发现即使在模型访问受限和计算资源有限的情况下，通过适当方法仍能获得有信息量的归因分数，这些分数在一系列任务中保持有效性。

Conclusion: 该研究为在现实环境中部署TDA提供了实用指导，旨在提高在有限访问条件下的可行性和效率，推动TDA方法在数据选择、数据清理等实际应用中的更广泛采用。

Abstract: Training data attribution (TDA) plays a critical role in understanding the
influence of individual training data points on model predictions.
Gradient-based TDA methods, popularized by \textit{influence function} for
their superior performance, have been widely applied in data selection, data
cleaning, data economics, and fact tracing. However, in real-world scenarios
where commercial models are not publicly accessible and computational resources
are limited, existing TDA methods are often constrained by their reliance on
full model access and high computational costs. This poses significant
challenges to the broader adoption of TDA in practical applications.
  In this work, we present a systematic study of TDA methods under various
access and resource constraints. We investigate the feasibility of performing
TDA under varying levels of access constraints by leveraging appropriately
designed solutions such as proxy models. Besides, we demonstrate that
attribution scores obtained from models without prior training on the target
dataset remain informative across a range of tasks, which is useful for
scenarios where computational resources are limited. Our findings provide
practical guidance for deploying TDA in real-world environments, aiming to
improve feasibility and efficiency under limited access.

</details>


### [101] [A Multimodal Foundation Model to Enhance Generalizability and Data Efficiency for Pan-cancer Prognosis Prediction](https://arxiv.org/abs/2509.12600)
*Huajun Zhou,Fengtao Zhou,Jiabo Ma,Yingxue Xu,Xi Wang,Xiuming Zhang,Li Liang,Zhenhui Li,Hao Chen*

Main category: cs.LG

TL;DR: MICE是一个多模态基础模型，通过协作专家机制整合病理图像、临床报告和基因组数据，在泛癌预后预测中显著优于现有方法，提高了泛化性和数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型难以充分利用多模态数据的丰富信息，提取的表示泛化性差，需要开发能有效整合异质信息的多模态模型来改善肿瘤微环境的整体理解。

Method: 采用功能多样化的多专家模块替代传统多专家模块，全面捕捉跨癌症和癌症特异性见解；结合对比学习和监督学习，使用11,799名患者的数据增强模型泛化性。

Result: MICE在内部队列中C-index提升3.8%-11.2%，在独立队列中提升5.8%-8.8%，显著优于单模态和最先进的多专家多模态模型，并展现出卓越的数据效率。

Conclusion: MICE为泛癌预后预测建立了有效且可扩展的基础，具有个性化定制治疗和改善治疗结果的强大潜力。

Abstract: Multimodal data provides heterogeneous information for a holistic
understanding of the tumor microenvironment. However, existing AI models often
struggle to harness the rich information within multimodal data and extract
poorly generalizable representations. Here we present MICE (Multimodal data
Integration via Collaborative Experts), a multimodal foundation model that
effectively integrates pathology images, clinical reports, and genomics data
for precise pan-cancer prognosis prediction. Instead of conventional
multi-expert modules, MICE employs multiple functionally diverse experts to
comprehensively capture both cross-cancer and cancer-specific insights.
Leveraging data from 11,799 patients across 30 cancer types, we enhanced MICE's
generalizability by coupling contrastive and supervised learning. MICE
outperformed both unimodal and state-of-the-art multi-expert-based multimodal
models, demonstrating substantial improvements in C-index ranging from 3.8% to
11.2% on internal cohorts and 5.8% to 8.8% on independent cohorts,
respectively. Moreover, it exhibited remarkable data efficiency across diverse
clinical scenarios. With its enhanced generalizability and data efficiency,
MICE establishes an effective and scalable foundation for pan-cancer prognosis
prediction, holding strong potential to personalize tailored therapies and
improve treatment outcomes.

</details>


### [102] [A Novel Recurrent Neural Network Framework for Prediction and Treatment of Oncogenic Mutation Progression](https://arxiv.org/abs/2509.12732)
*Rishab Parthasarathy,Achintya Bhowmik*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于人工智能的效率高、端到端的疾病递录分析框架，能够预测癌病严重程度和突变进展，并推荐治疗方案，无需依赖传统的耐耗时激光实验数据。


<details>
  <summary>Details</summary>
Motivation: 癌病仍是美国第二大死因，但现有的递录分析方法依靠手工获取的实验室数据，效率低下。需要开发更高效的人工智能方法来预测癌病进展和推荐治疗方案。

Method: 采用时间序列机器学习模型结合递录分析。首先从TCGA数据库中分离突变序列，使用新的预处理算法根据突变频率筛选关键突变，然后用递归神经网络(RNN)预测癌病严重程度，最后结合多个药物目标数据库预测未来突变和推荐治疗方案。

Result: 框架达到了稳健的结果，ROC曲线准确度超过60%，与现有癌病诊断方法相似。预处理算法成功分离出了各癌病阶段的几百个关键驱动突变，并生成了基因频率热力图。

Conclusion: 这是首个无需依赖耐耗时激光实验的高效、成本效益高的端到端框架，能够预测癌病进展并提供治疗建议，为癌病诊疗开启了新方向。

Abstract: Despite significant medical advancements, cancer remains the second leading
cause of death, with over 600,000 deaths per year in the US. One emerging
field, pathway analysis, is promising but still relies on manually derived wet
lab data, which is time-consuming to acquire. This work proposes an efficient,
effective end-to-end framework for Artificial Intelligence (AI) based pathway
analysis that predicts both cancer severity and mutation progression, thus
recommending possible treatments. The proposed technique involves a novel
combination of time-series machine learning models and pathway analysis. First,
mutation sequences were isolated from The Cancer Genome Atlas (TCGA) Database.
Then, a novel preprocessing algorithm was used to filter key mutations by
mutation frequency. This data was fed into a Recurrent Neural Network (RNN)
that predicted cancer severity. Then, the model probabilistically used the RNN
predictions, information from the preprocessing algorithm, and multiple
drug-target databases to predict future mutations and recommend possible
treatments. This framework achieved robust results and Receiver Operating
Characteristic (ROC) curves (a key statistical metric) with accuracies greater
than 60%, similar to existing cancer diagnostics. In addition, preprocessing
played an instrumental role in isolating important mutations, demonstrating
that each cancer stage studied may contain on the order of a few-hundred key
driver mutations, consistent with current research. Heatmaps based on predicted
gene frequency were also generated, highlighting key mutations in each cancer.
Overall, this work is the first to propose an efficient, cost-effective
end-to-end framework for projecting cancer progression and providing possible
treatments without relying on expensive, time-consuming wet lab work.

</details>


### [103] [High-Energy Concentration for Federated Learning in Frequency Domain](https://arxiv.org/abs/2509.12630)
*Haozhi Shi,Weiying Xie,Hangyu Ye,Daixun Li,Jitao Ma,Leyuan Fang*

Main category: cs.LG

TL;DR: 提出FedFD方法，通过频域高能量集中特性过滤冗余高频信息，在联邦学习中降低通信成本并提升性能


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习中使用数据集蒸馏方法存在空间域冗余信息和噪声问题，增加了通信负担，需要更高效的频域处理方法

Method: 基于离散余弦变换的高能量集中特性，设计二进制掩码保留低频成分，通过频域分布对齐和真实数据驱动的合成分类损失来优化低频分量质量

Result: 在5个图像和语音数据集上，FedFD在CIFAR-10数据集上通信成本降低37.78%，性能提升10.88%，优于现有最优方法

Conclusion: 频域高能量集中方法能有效减少联邦学习中的通信负担，同时提升模型性能，为隐私保护下的协同优化提供了新思路

Abstract: Federated Learning (FL) presents significant potential for collaborative
optimization without data sharing. Since synthetic data is sent to the server,
leveraging the popular concept of dataset distillation, this FL framework
protects real data privacy while alleviating data heterogeneity. However, such
methods are still challenged by the redundant information and noise in entire
spatial-domain designs, which inevitably increases the communication burden. In
this paper, we propose a novel Frequency-Domain aware FL method with
high-energy concentration (FedFD) to address this problem. Our FedFD is
inspired by the discovery that the discrete cosine transform predominantly
distributes energy to specific regions, referred to as high-energy
concentration. The principle behind FedFD is that low-energy like
high-frequency components usually contain redundant information and noise, thus
filtering them helps reduce communication costs and optimize performance. Our
FedFD is mathematically formulated to preserve the low-frequency components
using a binary mask, facilitating an optimal solution through frequency-domain
distribution alignment. In particular, real data-driven synthetic
classification is imposed into the loss to enhance the quality of the
low-frequency components. On five image and speech datasets, FedFD achieves
superior performance than state-of-the-art methods while reducing communication
costs. For example, on the CIFAR-10 dataset with Dirichlet coefficient $\alpha
= 0.01$, FedFD achieves a minimum reduction of 37.78\% in the communication
cost, while attaining a 10.88\% performance gain.

</details>


### [104] [Similarity-Distance-Magnitude Activations](https://arxiv.org/abs/2509.12760)
*Allen Schmaltz*

Main category: cs.LG

TL;DR: 提出了一种新的SDM激活函数，在标准softmax基础上增加了相似性和距离感知，提高了神经网络对协变量偏移和分布外输入的鲁棒性，并为选择性分类提供了更好的可解释性和性能。


<details>
  <summary>Details</summary>
Motivation: 标准softmax激活函数在处理协变量偏移和分布外输入时缺乏鲁棒性，且可解释性有限。需要一种能够同时考虑输出幅度、相似性和距离信息的激活函数来提高模型性能。

Method: 在softmax激活函数基础上引入相似性感知（正确预测的深度匹配）和距离感知（到训练分布的距离），形成相似性-距离-幅度（SDM）激活函数。

Result: SDM激活函数比softmax对高概率区域的协变量偏移和分布外输入更鲁棒，通过密集匹配提供基于范例的可解释性，并能对类别经验CDF进行分区以保护选择性分类中的类别召回率。

Conclusion: SDM激活函数在选择性分类任务中优于softmax，即使考虑后校准方法，也提供了更好的鲁棒性、可解释性和性能保证。

Abstract: We introduce a more robust and interpretable formulation of the standard
softmax activation function commonly used with neural networks by adding
Similarity (i.e., correctly predicted depth-matches into training) awareness
and Distance-to-training-distribution awareness to the existing output
Magnitude (i.e., decision-boundary) awareness. When used as the final-layer
activation with language models, the resulting Similarity-Distance-Magnitude
(SDM) activation function is more robust than the softmax function to
co-variate shifts and out-of-distribution inputs in high-probability regions,
and provides interpretability-by-exemplar via dense matching. Complementing the
prediction-conditional estimates, the SDM activation enables a partitioning of
the class-wise empirical CDFs to guard against low class-wise recall among
selective classifications. These properties make it preferable for selective
classification, even when considering post-hoc calibration methods over the
softmax.

</details>


### [105] [Leveraging Intermediate Representations of Time Series Foundation Models for Anomaly Detection](https://arxiv.org/abs/2509.12650)
*Chan Sik Han,Keon Myung Lee*

Main category: cs.LG

TL;DR: TimeRep是一种新颖的时间序列异常检测方法，利用时间序列基础模型的中间层表示来计算异常分数，通过距离度量而非传统的重构或预测误差，在UCR异常档案数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列异常检测方法通常依赖基础模型的最终层表示，通过重构或预测误差计算异常分数，但忽略了中间层可能包含的更丰富信息。

Method: 选择预训练TSFM中最具信息量的中间层和补丁标记位置，构建参考表示集合并应用核心集策略缩减规模，通过距离度量计算异常分数，并集成适应机制处理概念漂移。

Result: 在包含250个单变量时间序列的UCR异常档案数据集上，TimeRep持续优于包括非深度学习、深度学习和基础模型方法在内的多种最先进基线方法。

Conclusion: TimeRep通过利用时间序列基础模型的中间层表示和创新的距离度量方法，为时间序列异常检测提供了更有效的解决方案，特别是在处理概念漂移方面表现出色。

Abstract: Detecting anomalies in time series data is essential for the reliable
operation of many real-world systems. Recently, time series foundation models
(TSFMs) have emerged as a powerful tool for anomaly detection. However,
existing methods typically rely on the final layer's representations of TSFMs,
computing the anomaly score as a reconstruction or forecasting error via a
task-specific head. Instead, we propose TimeRep, a novel anomaly detection
approach that leverages the intermediate layer's representations of TSFMs,
computing the anomaly score as the distance between these representations.
Given a pre-trained TSFM, TimeRep selects the intermediate layer and
patch-token position that yield the most informative representation. TimeRep
forms a reference collection of intermediate representations from the training
data and applies a core-set strategy to reduce its size while maintaining
distributional coverage. During inference, TimeRep computes the anomaly score
for incoming data by measuring the distance between its intermediate
representations and those of the collection. To address concept drift, TimeRep
integrates an adaptation mechanism that, at inference time, augments the
collection exclusively with non-redundant intermediate representations from
incoming data. We conducted extensive experiments on the UCR Anomaly Archive,
which contains 250 univariate time series. TimeRep consistently outperforms a
broad spectrum of state-of-the-art baselines, including non-DL, DL, and
foundation model-based methods.

</details>


### [106] [Instance-level Randomization: Toward More Stable LLM Evaluations](https://arxiv.org/abs/2509.12678)
*Yiyang Li,Yonghuang Wu,Ying Luo,Liangtai Sun,Zishu Qin,Lin Qiu,Xuezhi Cao,Xunliang Cai*

Main category: cs.LG

TL;DR: 本文针对大语言模型评估中的不稳定性问题，提出了实例级随机化(ILR)方法，通过在每次实验中为每个实例随机化所有影响因素并多次实验取平均，有效减少评估方差并提高模型比较的公平性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型评估存在不稳定性问题，随机因素（如few-shot示例）的微小变化会导致分数剧烈波动甚至模型排名变化。不同LLM对特定随机因素设置有不同偏好，固定设置可能导致不公平比较。

Method: 提出实例级随机化(ILR)方法：不再在整个基准测试中使用固定设置，而是为每个实例随机化所有影响评估分数的因素，进行多次实验并报告平均分数。

Result: 理论分析和实证结果表明，ILR能够减少随机因素引起的方差和不公平比较，并且以不到一半的计算成本达到与先前方法相似的鲁棒性水平。

Conclusion: ILR方法有效解决了LLM评估中的不稳定性问题，通过实例级随机化和多次平均的方式，在降低计算成本的同时提高了评估的稳定性和公平性。

Abstract: Evaluations of large language models (LLMs) suffer from instability, where
small changes of random factors such as few-shot examples can lead to drastic
fluctuations of scores and even model rankings. Moreover, different LLMs can
have different preferences for a certain setting of random factors. As a
result, using a fixed setting of random factors, which is often adopted as the
paradigm of current evaluations, can lead to potential unfair comparisons
between LLMs. To mitigate the volatility of evaluations, we first theoretically
analyze the sources of variance induced by changes in random factors. Targeting
these specific sources, we then propose the instance-level randomization (ILR)
method to reduce variance and enhance fairness in model comparisons. Instead of
using a fixed setting across the whole benchmark in a single experiment, we
randomize all factors that affect evaluation scores for every single instance,
run multiple experiments and report the averaged score. Theoretical analyses
and empirical results demonstrate that ILR can reduce the variance and unfair
comparisons caused by random factors, as well as achieve similar robustness
level with less than half computational cost compared with previous methods.

</details>


### [107] [Rethinking the Evaluation of Alignment Methods: Insights into Diversity, Generalisation, and Safety](https://arxiv.org/abs/2509.12936)
*Denis Janiak,Julia Moska,Dawid Motyka,Karolina Seweryn,Paweł Walkowiak,Bartosz Żuk,Arkadiusz Janz*

Main category: cs.LG

TL;DR: 提出了一个统一的评估框架，比较PPO、DPO、ORPO、KTO等LLM对齐方法在事实性、安全性、简洁性、主动性和多样性五个维度上的表现，发现不同方法在不同维度各有优势。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单一技术或特定维度，缺乏对LLM对齐方法内在权衡的整体评估，需要开发一个全面的评估框架来指导更平衡可靠的LLM开发。

Method: 使用统一的评估框架，在分布内和分布外数据集上比较PPO、DPO、ORPO、KTO等对齐方法，采用经过人工研究验证的LLM-as-Judge专用提示进行多维度评估。

Result: DPO和KTO在事实准确性方面表现最佳，PPO和DPO在安全性方面领先，PPO在简洁性和主动性之间取得最佳平衡。

Conclusion: 研究结果揭示了常见对齐方法的权衡关系，为开发更平衡可靠的LLM提供了指导，不同对齐方法在不同维度上各有优势，需要根据具体需求选择合适的方法。

Abstract: Large language models (LLMs) require careful alignment to balance competing
objectives - factuality, safety, conciseness, proactivity, and diversity.
Existing studies focus on individual techniques or specific dimensions, lacking
a holistic assessment of the inherent trade-offs. We propose a unified
evaluation framework that compares LLM alignment methods (PPO, DPO, ORPO, KTO)
across these five axes, using both in-distribution and out-of-distribution
datasets. Leveraging a specialized LLM-as-Judge prompt, validated through human
studies, we reveal that DPO and KTO excel in factual accuracy, PPO and DPO lead
in safety, and PPO best balances conciseness with proactivity. Our findings
provide insights into trade-offs of common alignment methods, guiding the
development of more balanced and reliable LLMs.

</details>


### [108] [Large Language Model Scaling Laws for Neural Quantum States in Quantum Chemistry](https://arxiv.org/abs/2509.12679)
*Oliver Knitter,Dan Zhao,Stefan Leichenauer,Shravan Veerapaneni*

Main category: cs.LG

TL;DR: 该论文研究了基于transformer的神经量子态(NQS)在量子化学应用中的缩放规律，发现模型大小与训练时间的关系高度依赖于损失度量和ansatz选择，与语言模型的线性关系不同。


<details>
  <summary>Details</summary>
Motivation: 随着神经量子态(NQS)越来越多地采用基于大语言模型(LLM)的组件，作者希望理解NQS的缩放规律，从而揭示NQS ansatze的可扩展性和最优性能-资源权衡。

Method: 通过识别基于transformer的NQS在二次量子化量子化学应用中的缩放规律，测量绝对误差和V-score作为性能指标，并进行计算约束的优化分析。

Result: 研究发现模型大小与训练时间的关系高度依赖于损失度量和ansatz选择，与语言模型中发现的近似线性关系不同。

Conclusion: NQS的缩放规律与语言模型存在显著差异，表明需要针对量子计算应用开发专门的缩放理论和优化策略。

Abstract: Scaling laws have been used to describe how large language model (LLM)
performance scales with model size, training data size, or amount of
computational resources. Motivated by the fact that neural quantum states (NQS)
has increasingly adopted LLM-based components, we seek to understand NQS
scaling laws, thereby shedding light on the scalability and optimal
performance--resource trade-offs of NQS ansatze. In particular, we identify
scaling laws that predict the performance, as measured by absolute error and
V-score, for transformer-based NQS as a function of problem size in
second-quantized quantum chemistry applications. By performing analogous
compute-constrained optimization of the obtained parametric curves, we find
that the relationship between model size and training time is highly dependent
on loss metric and ansatz, and does not follow the approximately linear
relationship found for language models.

</details>


### [109] [When Inverse Data Outperforms: Exploring the Pitfalls of Mixed Data in Multi-Stage Fine-Tuning](https://arxiv.org/abs/2509.13079)
*Mengyi Deng,Xin Li,Tingyu Zhu,Zhicheng Yang,Zhijiang Guo,Wei Wang*

Main category: cs.LG

TL;DR: 通过反转1000个正向推理样本构建高质量逆向推理数据集r1k，研究双向推理目标下SFT和DPO对模型对齐的影响。SFT在r1k上相比s1k提升1.6%-6.8%准确率，但混合训练会削弱方向区分度，DPO只能部分恢复区分度但会抑制次要推理路径。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注单向监督微调，忽略了不同推理模式之间的复杂相互作用，需要研究双向推理对模型对齐的影响。

Method: 构建逆向推理数据集r1k（通过反转s1k中的1000个正向样本），分别在SFT和DPO框架下研究正向和逆向数据的混合训练效果。

Result: SFT在r1k上表现优于s1k（提升1.6%-6.8%），但混合训练会削弱方向区分度。DPO能部分恢复区分度，但会通过将概率质量转移到无关输出来抑制较不受偏好的推理路径。

Conclusion: 混合推理数据会引入冲突的监督信号，需要开发具有方向感知能力的鲁棒对齐策略。

Abstract: Existing work has shown that o1-level performance can be achieved with
limited data distillation, but most existing methods focus on unidirectional
supervised fine-tuning (SFT), overlooking the intricate interplay between
diverse reasoning patterns. In this paper, we construct r1k, a high-quality
reverse reasoning dataset derived by inverting 1,000 forward examples from s1k,
and examine how SFT and Direct Preference Optimization (DPO) affect alignment
under bidirectional reasoning objectives. SFT on r1k yields a 1.6%--6.8%
accuracy improvement over s1k across evaluated benchmarks. However, naively
mixing forward and reverse data during SFT weakens the directional distinction.
Although DPO can partially recover this distinction, it also suppresses less
preferred reasoning paths by shifting the probability mass toward irrelevant
outputs. These findings suggest that mixed reasoning data introduce conflicting
supervision signals, underscoring the need for robust and direction-aware
alignment strategies.

</details>


### [110] [ZTree: A Subgroup Identification Based Decision Tree Learning Framework](https://arxiv.org/abs/2509.12688)
*Eric Cheng,Jie Cheng*

Main category: cs.LG

TL;DR: ZTree是一种基于假设检验的新型决策树学习框架，用统计显著的子群识别替代传统的纯度分裂方法，通过交叉验证控制多重检验，仅需一个参数（z阈值）即可调节树复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统CART决策树基于纯度分裂缺乏统计理论基础，ZTree旨在提供统计上更严谨的分裂准则，通过假设检验确保每个分裂都有统计显著性。

Method: 在每个节点应用假设检验（z检验、t检验、Mann-Whitney U、log-rank等）评估候选子群与补集的差异显著性，使用交叉验证方法处理多重检验问题，z阈值（本质上是p值）作为唯一参数控制树复杂度。

Result: 在五个大规模UCI数据集上的实证评估显示，ZTree在低数据量情况下表现优异，相比CART能生成更简单的树而不牺牲性能。

Conclusion: ZTree提供了基于假设检验的统计严谨决策树框架，参数调节直观高效，支持多种统计检验方法，为传统决策树分裂提供了统计基础更强的替代方案。

Abstract: Decision trees are a commonly used class of machine learning models valued
for their interpretability and versatility, capable of both classification and
regression. We propose ZTree, a novel decision tree learning framework that
replaces CART's traditional purity based splitting with statistically
principled subgroup identification. At each node, ZTree applies hypothesis
testing (e.g., z-tests, t-tests, Mann-Whitney U, log-rank) to assess whether a
candidate subgroup differs meaningfully from the complement. To adjust for the
complication of multiple testing, we employ a cross-validation-based approach
to determine if further node splitting is needed. This robust stopping
criterion eliminates the need for post-pruning and makes the test threshold
(z-threshold) the only parameter for controlling tree complexity. Because of
the simplicity of the tree growing procedure, once a detailed tree is learned
using the most lenient z-threshold, all simpler trees can be derived by simply
removing nodes that do not meet the larger z-thresholds. This makes parameter
tuning intuitive and efficient. Furthermore, this z-threshold is essentially a
p-value, allowing users to easily plug in appropriate statistical tests into
our framework without adjusting the range of parameter search. Empirical
evaluation on five large-scale UCI datasets demonstrates that ZTree
consistently delivers strong performance, especially at low data regimes.
Compared to CART, ZTree also tends to grow simpler trees without sacrificing
performance. ZTree introduces a statistically grounded alternative to
traditional decision tree splitting by leveraging hypothesis testing and a
cross-validation approach to multiple testing correction, resulting in an
efficient and flexible framework.

</details>


### [111] [WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning](https://arxiv.org/abs/2509.13305)
*Kuan Li,Zhongwang Zhang,Huifeng Yin,Rui Ye,Yida Zhao,Liwen Zhang,Litu Ou,Dingchu Zhang,Xixi Wu,Jialong Wu,Xinyu Wang,Zile Qiao,Zhen Zhang,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.LG

TL;DR: WebSailor是一种后训练方法，通过生成高不确定性任务、RFT冷启动和DUPO算法，使开源LLM在复杂信息搜索任务中达到与专有代理相当的性能


<details>
  <summary>Details</summary>
Motivation: 克服人类认知限制是LLM训练的关键前沿。专有代理系统在复杂信息搜索基准上表现出超人类能力，而开源模型缺乏系统化处理极端不确定性的推理模式

Method: 通过结构化采样和信息模糊化生成新颖的高不确定性任务，采用RFT冷启动和高效的代理RL训练算法DUPO（重复采样策略优化）

Result: WebSailor在复杂信息搜索任务中显著优于所有开源代理，匹配专有代理的性能，缩小了能力差距

Conclusion: 该方法成功地将专有代理的关键能力——系统化处理极端不确定性的推理模式——移植到开源模型中，实现了性能的显著提升

Abstract: Transcending human cognitive limitations represents a critical frontier in
LLM training. Proprietary agentic systems like DeepResearch have demonstrated
superhuman capabilities on extremely complex information-seeking benchmarks
such as BrowseComp, a feat previously unattainable. We posit that their success
hinges on a sophisticated reasoning pattern absent in open-source models: the
ability to systematically reduce extreme uncertainty when navigating vast
information landscapes. Based on this insight, we introduce WebSailor, a
complete post-training methodology designed to instill this crucial capability.
Our approach involves generating novel, high-uncertainty tasks through
structured sampling and information obfuscation, RFT cold start, and an
efficient agentic RL training algorithm, Duplicating Sampling Policy
Optimization (DUPO). With this integrated pipeline, WebSailor significantly
outperforms all open-source agents in complex information-seeking tasks,
matching proprietary agents' performance and closing the capability gap.

</details>


### [112] [Soft Graph Transformer for MIMO Detection](https://arxiv.org/abs/2509.12694)
*Jiadong Hong,Lei Liu,Xinyu Bian,Wenjie Wang,Zhaoyang Zhang*

Main category: cs.LG

TL;DR: 提出了Soft Graph Transformer (SGT)用于MIMO检测，结合了图注意力机制和消息传递，支持软输入软输出，接近最大似然性能。


<details>
  <summary>Details</summary>
Motivation: 传统最大似然检测计算复杂度太高，消息传递算法依赖大系统渐近性和随机矩阵假设，现有Transformer检测器无法利用MIMO因子图结构和解码器软信息。

Method: 将消息传递直接集成到图感知注意力机制中，通过软输入嵌入支持解码器信息更新，实现有效的软输出生成。

Result: 作为独立检测器，SGT接近最大似然性能，超越之前的Transformer方法。

Conclusion: SGT成功克服了现有方法的局限性，在保持计算效率的同时实现了优异的检测性能。

Abstract: We propose the Soft Graph Transformer (SGT), a Soft-Input-Soft-Output neural
architecture tailored for MIMO detection. While Maximum Likelihood (ML)
detection achieves optimal accuracy, its prohibitive exponential complexity
renders it impractical for real-world systems. Conventional message passing
algorithms offer tractable alternatives but rely on large-system asymptotics
and random matrix assumptions, both of which break down under practical
implementations. Prior Transformer-based detectors, on the other hand, fail to
incorporate the MIMO factor graph structure and cannot utilize decoder-side
soft information, limiting their standalone performance and their applicability
in iterative detection-decoding (IDD). To overcome these limitations, SGT
integrates message passing directly into a graph-aware attention mechanism and
supports decoder-informed updates through soft-input embeddings. This design
enables effective soft-output generation while preserving computational
efficiency. As a standalone detector, SGT closely approaches ML performance and
surpasses prior Transformer-based approaches.

</details>


### [113] [Bi-level Personalization for Federated Foundation Models: A Task-vector Aggregation Approach](https://arxiv.org/abs/2509.12697)
*Yiyuan Yang,Guodong Long,Qinghua Lu,Liming Zhu,Jing Jiang*

Main category: cs.LG

TL;DR: 提出了一种双层个性化联邦学习框架，用于在基础模型上进行联邦微调，通过客户端个性化微调和服务器端基于相似用户的个性化聚合来解决小规模用户群体或专业场景下的数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 联邦基础模型需要在小规模新用户或专业场景下进行微调，这些场景数据有限且存在非独立同分布数据，个性化与联邦学习之间的权衡变得更加敏感。

Method: 采用双层个性化框架：客户端使用私有数据进行个性化微调，服务器端使用客户端特定任务向量测量相似用户进行个性化聚合，从而获得群体个性化信息并减少不相关或利益冲突客户端的干扰。

Result: 在基准数据集上的广泛实验分析证明了所提出算法的有效性。

Conclusion: 该双层个性化框架能够有效解决联邦基础模型在小规模数据场景下的微调问题，平衡个性化需求和联邦学习效果。

Abstract: Federated foundation models represent a new paradigm to jointly fine-tune
pre-trained foundation models across clients. It is still a challenge to
fine-tune foundation models for a small group of new users or specialized
scenarios, which typically involve limited data compared to the large-scale
data used in pre-training. In this context, the trade-off between
personalization and federation becomes more sensitive. To tackle these, we
proposed a bi-level personalization framework for federated fine-tuning on
foundation models. Specifically, we conduct personalized fine-tuning on the
client-level using its private data, and then conduct a personalized
aggregation on the server-level using similar users measured by client-specific
task vectors. Given the personalization information gained from client-level
fine-tuning, the server-level personalized aggregation can gain group-wise
personalization information while mitigating the disturbance of irrelevant or
interest-conflict clients with non-IID data. The effectiveness of the proposed
algorithm has been demonstrated by extensive experimental analysis in benchmark
datasets.

</details>


### [114] [NORA: A Nephrology-Oriented Representation Learning Approach Towards Chronic Kidney Disease Classification](https://arxiv.org/abs/2509.12704)
*Mohammad Abdul Hafeez Khan,Twisha Bhattacharyya,Omar Khan,Noorah Khan,Alina Aziz Fatima Khan,Mohammed Qutub Khan,Sujoy Ghosh Hajra*

Main category: cs.LG

TL;DR: 提出NORA方法，结合监督对比学习和随机森林分类器，利用常规非肾脏临床变量进行慢性肾病分类，在早期检测方面表现优异


<details>
  <summary>Details</summary>
Motivation: 慢性肾病早期检测困难，特别是在门诊环境中缺乏实验室肾脏生物标志物，需要探索常规临床变量的预测潜力

Method: NORA方法：监督对比学习+非线性随机森林分类器，从表格化电子健康记录数据中提取判别性患者表征，用于下游CKD分类

Result: NORA提高了类别可分性和整体分类性能，特别是在早期CKD的F1分数方面有显著提升，在不同患者群体中表现出良好的泛化能力

Conclusion: NORA方法有效利用常规非肾脏临床变量进行CKD风险分层，为门诊环境中的早期肾病检测提供了实用解决方案

Abstract: Chronic Kidney Disease (CKD) affects millions of people worldwide, yet its
early detection remains challenging, especially in outpatient settings where
laboratory-based renal biomarkers are often unavailable. In this work, we
investigate the predictive potential of routinely collected non-renal clinical
variables for CKD classification, including sociodemographic factors, comorbid
conditions, and urinalysis findings. We introduce the Nephrology-Oriented
Representation leArning (NORA) approach, which combines supervised contrastive
learning with a nonlinear Random Forest classifier. NORA first derives
discriminative patient representations from tabular EHR data, which are then
used for downstream CKD classification. We evaluated NORA on a clinic-based EHR
dataset from Riverside Nephrology Physicians. Our results demonstrated that
NORA improves class separability and overall classification performance,
particularly enhancing the F1-score for early-stage CKD. Additionally, we
assessed the generalizability of NORA on the UCI CKD dataset, demonstrating its
effectiveness for CKD risk stratification across distinct patient cohorts.

</details>


### [115] [Spatio-temporal DeepKriging in PyTorch: A Supplementary Application to Precipitation Data for Interpolation and Probabilistic Forecasting](https://arxiv.org/abs/2509.12708)
*Pratik Nag*

Main category: cs.LG

TL;DR: 提出了一个基于PyTorch的时空深度克里金框架，用于欧洲降水数据的插值和预测，能够处理时空不规则性并生成高分辨率结果


<details>
  <summary>Details</summary>
Motivation: 需要处理欧洲降水数据的时空不规则性，实现高精度插值和多步预测，为气候数据分析提供有效工具

Method: 开发了基于PyTorch的时空深度克里金（STDK）框架，包含独立的插值和预测模块，能够处理时空不规则数据

Result: 在日降水测量数据上进行了广泛评估，证明了该方法在预测性能和鲁棒性方面的有效性

Conclusion: 该框架为类似气候数据集提供了可复现的解决方案，在降水数据插值和预测方面表现出色

Abstract: A detailed analysis of precipitation data over Europe is presented, with a
focus on interpolation and forecasting applications. A Spatio-temporal
DeepKriging (STDK) framework has been implemented using the PyTorch platform to
achieve these objectives. The proposed model is capable of handling
spatio-temporal irregularities while generating high-resolution interpolations
and multi-step forecasts. Reproducible code modules have been developed as
standalone PyTorch implementations for the
interpolation\footnote[2]{Interpolation -
https://github.com/pratiknag/Spatio-temporalDeepKriging-Pytorch.git} and
forecasting\footnote[3]{Forecasting -
https://github.com/pratiknag/pytorch-convlstm.git}, facilitating broader
application to similar climate datasets. The effectiveness of this approach is
demonstrated through extensive evaluation on daily precipitation measurements,
highlighting predictive performance and robustness.

</details>


### [116] [Unbiased Online Curvature Approximation for Regularized Graph Continual Learning](https://arxiv.org/abs/2509.12727)
*Jie Yin,Ke Sun,Han Wu*

Main category: cs.LG

TL;DR: 提出了一个基于Fisher信息矩阵的图持续学习正则化框架，并开发了一种新的在线曲率近似方法，显著优于现有正则化方法


<details>
  <summary>Details</summary>
Motivation: 解决图持续学习中灾难性遗忘问题，特别是在无回放、类增量设置的挑战性场景下，现有方法如EWC使用对角近似的局限性

Method: 建立基于Fisher信息矩阵诱导的弯曲参数空间的通用正则化框架，提出基于模型当前学习状态的在线无偏曲率近似方法，直接估计正则化项而不显式计算和存储FIM

Result: 在三个图数据集上的大量实验表明，该方法显著优于现有的基于正则化的方法，在稳定性（保留旧知识）和可塑性（获取新知识）之间实现了优越的平衡

Conclusion: 提出的在线曲率近似方法能够更好地捕捉学习新任务时的损失景观，同时保留从先前任务中学到的知识，为图持续学习提供了有效的正则化解决方案

Abstract: Graph continual learning (GCL) aims to learn from a continuous sequence of
graph-based tasks. Regularization methods are vital for preventing catastrophic
forgetting in GCL, particularly in the challenging replay-free,
class-incremental setting, where each task consists of a set of unique classes.
In this work, we first establish a general regularization framework for GCL
based on the curved parameter space induced by the Fisher information matrix
(FIM). We show that the dominant Elastic Weight Consolidation (EWC) and its
variants are a special case within this framework, using a diagonal
approximation of the empirical FIM based on parameters from previous tasks. To
overcome their limitations, we propose a new unbiased online curvature
approximation of the full FIM based on the model's current learning state. Our
method directly estimates the regularization term in an online manner without
explicitly evaluating and storing the FIM itself. This enables the model to
better capture the loss landscape during learning new tasks while retaining the
knowledge learned from previous tasks. Extensive experiments on three graph
datasets demonstrate that our method significantly outperforms existing
regularization-based methods, achieving a superior trade-off between stability
(retaining old knowledge) and plasticity (acquiring new knowledge).

</details>


### [117] [A Graph Machine Learning Approach for Detecting Topological Patterns in Transactional Graphs](https://arxiv.org/abs/2509.12730)
*Francesco Zola,Jon Ander Medina,Andrea Venturi,Amaia Gil,Raul Orduna*

Main category: cs.LG

TL;DR: 提出了一种结合图机器学习和网络分析的方法，通过四步预处理框架和Graph Autoencoders来检测金融交易图中的拓扑模式，以改进复杂金融犯罪行为的检测。


<details>
  <summary>Details</summary>
Motivation: 数字生态系统的发展使金融部门面临不断演变的滥用和犯罪策略，传统基于规则的系统缺乏检测复杂或协同犯罪行为的适应性，需要分析参与者互动来发现可疑活动。

Method: 四步预处理框架：提取图结构、考虑数据时间性管理大节点集、检测社区、应用自动标注策略生成弱标签；然后实现三种不同的Graph Autoencoder变体来区分已知拓扑模式。

Result: 初步结果表明，这种以模式为中心、拓扑驱动的方法能有效检测复杂金融犯罪方案，为传统基于规则的检测系统提供了有前景的替代方案。

Conclusion: 该方法通过图机器学习和网络分析的结合，解决了传统金融数据集稀疏、无标签的挑战，为金融犯罪检测提供了更有效的解决方案。

Abstract: The rise of digital ecosystems has exposed the financial sector to evolving
abuse and criminal tactics that share operational knowledge and techniques both
within and across different environments (fiat-based, crypto-assets, etc.).
Traditional rule-based systems lack the adaptability needed to detect
sophisticated or coordinated criminal behaviors (patterns), highlighting the
need for strategies that analyze actors' interactions to uncover suspicious
activities and extract their modus operandi. For this reason, in this work, we
propose an approach that integrates graph machine learning and network analysis
to improve the detection of well-known topological patterns within
transactional graphs. However, a key challenge lies in the limitations of
traditional financial datasets, which often provide sparse, unlabeled
information that is difficult to use for graph-based pattern analysis.
Therefore, we firstly propose a four-step preprocessing framework that involves
(i) extracting graph structures, (ii) considering data temporality to manage
large node sets, (iii) detecting communities within, and (iv) applying
automatic labeling strategies to generate weak ground-truth labels. Then, once
the data is processed, Graph Autoencoders are implemented to distinguish among
the well-known topological patterns. Specifically, three different GAE variants
are implemented and compared in this analysis. Preliminary results show that
this pattern-focused, topology-driven method is effective for detecting complex
financial crime schemes, offering a promising alternative to conventional
rule-based detection systems.

</details>


### [118] [EmbeddedML: A New Optimized and Fast Machine Learning Library](https://arxiv.org/abs/2509.12774)
*Halil Hüseyin Çalışkan,Talha Koruk*

Main category: cs.LG

TL;DR: EmbeddedML是一个数学优化的机器学习库，通过算法重写显著减少训练时间，在保持精度的同时比scikit-learn快数倍到数百倍


<details>
  <summary>Details</summary>
Motivation: 传统机器学习库在大数据集上训练时间过长，需要优化训练效率

Method: 对Multiple Linear Regression、Logistic Regression和SVM等算法进行数学重写和优化

Result: Multiple Linear Regression速度显著提升，SVM在小数据集上快2倍、大数据集上快800倍，Logistic Regression快4倍，且精度无损失

Conclusion: EmbeddedML提供了训练时间优化的回归、分类、聚类和降维算法，是高效的机器学习解决方案

Abstract: Machine learning models and libraries can train datasets of different sizes
and perform prediction and classification operations, but machine learning
models and libraries cause slow and long training times on large datasets. This
article introduces EmbeddedML, a training-time-optimized and mathematically
enhanced machine learning library. The speed was increased by approximately
times compared to scikit-learn without any loss in terms of accuracy in
regression models such as Multiple Linear Regression. Logistic Regression and
Support Vector Machines (SVM) algorithms have been mathematically rewritten to
reduce training time and increase accuracy in classification models. With the
applied mathematical improvements, training time has been reduced by
approximately 2 times for SVM on small datasets and by around 800 times on
large datasets, and by approximately 4 times for Logistic Regression, compared
to the scikit-learn implementation. In summary, the EmbeddedML library offers
regression, classification, clustering, and dimensionality reduction algorithms
that are mathematically rewritten and optimized to reduce training time.

</details>


### [119] [Energy-Efficient Quantized Federated Learning for Resource-constrained IoT devices](https://arxiv.org/abs/2509.12814)
*Wilfrid Sougrinoma Compaoré,Yaya Etiabi,El Mehdi Amhoud,Mohamad Assaad*

Main category: cs.LG

TL;DR: 提出了一种面向物联网的联邦学习框架，整合有限块长度传输、模型量化和错误感知聚合机制，显著提升能效和通信可靠性


<details>
  <summary>Details</summary>
Motivation: 物联网设备资源受限，面临能量有限、通信信道不可靠以及无限块长度传输不切实际等挑战，需要设计高效的联邦学习方案

Method: 集成有限块长度传输、模型量化和错误感知聚合机制，优化上行传输功率以平衡节能和模型性能

Result: 相比标准联邦学习模型，能耗降低高达75%，同时保持稳健的模型精度

Conclusion: 该框架为实际物联网部署中高效可靠的联邦学习实现铺平了道路，是资源受限物联网场景的可行解决方案

Abstract: Federated Learning (FL) has emerged as a promising paradigm for enabling
collaborative machine learning while preserving data privacy, making it
particularly suitable for Internet of Things (IoT) environments. However,
resource-constrained IoT devices face significant challenges due to limited
energy,unreliable communication channels, and the impracticality of assuming
infinite blocklength transmission. This paper proposes a federated learning
framework for IoT networks that integrates finite blocklength transmission,
model quantization, and an error-aware aggregation mechanism to enhance energy
efficiency and communication reliability. The framework also optimizes uplink
transmission power to balance energy savings and model performance. Simulation
results demonstrate that the proposed approach significantly reduces energy
consumption by up to 75\% compared to a standard FL model, while maintaining
robust model accuracy, making it a viable solution for FL in real-world IoT
scenarios with constrained resources. This work paves the way for efficient and
reliable FL implementations in practical IoT deployments. Index Terms:
Federated learning, IoT, finite blocklength, quantization, energy efficiency.

</details>


### [120] [Safe Reinforcement Learning using Action Projection: Safeguard the Policy or the Environment?](https://arxiv.org/abs/2509.12833)
*Hannah Markgraf,Shamburaj Sawant,Hanna Krasowski,Lukas Schäfer,Sebastien Gros,Matthias Althoff*

Main category: cs.LG

TL;DR: 本文对基于投影的安全滤波器在强化学习中的两种集成策略（SE-RL和SP-RL）进行了理论比较，重点分析了动作别名效应对策略梯度的影响差异，并提出了改进方法。


<details>
  <summary>Details</summary>
Motivation: 尽管基于投影的安全滤波器在安全关键强化学习中被广泛使用，但对其两种主要集成策略（SE-RL和SP-RL）的理论差异缺乏正式理解，需要系统分析它们的不同特性和性能表现。

Method: 提出了统一的actor-critic算法形式化框架，理论分析了SE-RL和SP-RL的策略梯度估计，重点研究了动作别名效应的影响机制，并比较了不同的缓解策略，包括为SP-RL提出的新型基于惩罚的改进方法。

Result: 实证结果支持理论预测，显示动作别名效应对SP-RL的负面影响更大，但通过适当的改进策略，SP-RL可以在各种环境中匹配甚至优于改进后的SE-RL。

Conclusion: 研究结果为根据任务特性选择和优化基于投影的安全RL方法提供了可操作的见解，表明两种方法在适当改进后都能取得良好性能。

Abstract: Projection-based safety filters, which modify unsafe actions by mapping them
to the closest safe alternative, are widely used to enforce safety constraints
in reinforcement learning (RL). Two integration strategies are commonly
considered: Safe environment RL (SE-RL), where the safeguard is treated as part
of the environment, and safe policy RL (SP-RL), where it is embedded within the
policy through differentiable optimization layers. Despite their practical
relevance in safety-critical settings, a formal understanding of their
differences is lacking. In this work, we present a theoretical comparison of
SE-RL and SP-RL. We identify a key distinction in how each approach is affected
by action aliasing, a phenomenon in which multiple unsafe actions are projected
to the same safe action, causing information loss in the policy gradients. In
SE-RL, this effect is implicitly approximated by the critic, while in SP-RL, it
manifests directly as rank-deficient Jacobians during backpropagation through
the safeguard. Our contributions are threefold: (i) a unified formalization of
SE-RL and SP-RL in the context of actor-critic algorithms, (ii) a theoretical
analysis of their respective policy gradient estimates, highlighting the role
of action aliasing, and (iii) a comparative study of mitigation strategies,
including a novel penalty-based improvement for SP-RL that aligns with
established SE-RL practices. Empirical results support our theoretical
predictions, showing that action aliasing is more detrimental for SP-RL than
for SE-RL. However, with appropriate improvement strategies, SP-RL can match or
outperform improved SE-RL across a range of environments. These findings
provide actionable insights for choosing and refining projection-based safe RL
methods based on task characteristics.

</details>


### [121] [Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use](https://arxiv.org/abs/2509.12867)
*Yabo Zhang,Yihan Zeng,Qingyun Li,Zhen Hu,Kavin Han,Wangmeng Zuo*

Main category: cs.LG

TL;DR: Tool-R1是一个基于强化学习的框架，通过生成可执行Python代码使大语言模型能够进行通用、组合和多步骤的工具使用，在GAIA基准测试中比强基线提升约10%的准确率


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理需要最新知识、精确操作或专业工具使用的现实任务时仍存在局限，需要解决工具增强推理的可靠性问题

Method: 提出强化学习框架，支持用户自定义工具和标准库集成，使用基于结果的奖励函数（结合LLM答案判断和代码执行成功），采用动态样本队列缓存和重用高质量轨迹以提高训练效率

Result: 在GAIA基准测试中显著提高了准确性和鲁棒性，比强基线提升约10%，在复杂多步骤任务上提升更大

Conclusion: Tool-R1在实现可靠高效的工具增强推理方面具有潜力，适用于现实世界应用

Abstract: Large language models (LLMs) have demonstrated strong capabilities in
language understanding and reasoning, yet they remain limited when tackling
real-world tasks that require up-to-date knowledge, precise operations, or
specialized tool use. To address this, we propose Tool-R1, a reinforcement
learning framework that enables LLMs to perform general, compositional, and
multi-step tool use by generating executable Python code. Tool-R1 supports
integration of user-defined tools and standard libraries, with variable sharing
across steps to construct coherent workflows. An outcome-based reward function,
combining LLM-based answer judgment and code execution success, guides policy
optimization. To improve training efficiency, we maintain a dynamic sample
queue to cache and reuse high-quality trajectories, reducing the overhead of
costly online sampling. Experiments on the GAIA benchmark show that Tool-R1
substantially improves both accuracy and robustness, achieving about 10\% gain
over strong baselines, with larger improvements on complex multi-step tasks.
These results highlight the potential of Tool-R1 for enabling reliable and
efficient tool-augmented reasoning in real-world applications. Our code will be
available at https://github.com/YBYBZhang/Tool-R1.

</details>


### [122] [TimeCluster with PCA is Equivalent to Subspace Identification of Linear Dynamical Systems](https://arxiv.org/abs/2509.12895)
*Christian L. Hines,Samuel Spillard,Daniel P. Martin*

Main category: cs.LG

TL;DR: TimeCluster是一种可视化分析技术，通过将重叠时间窗口投影到低维空间来发现多元时间序列的结构。研究发现当使用PCA降维时，该方法在数学上等价于经典的线性子空间辨识方法。


<details>
  <summary>Details</summary>
Motivation: 探索TimeCluster可视化技术与传统子空间系统辨识方法之间的数学等价性，为时间序列分析提供新的理论联系和应用可能性。

Method: 通过理论分析证明TimeCluster的滑动窗口矩阵形成Hankel矩阵，应用PCA（通过SVD）可恢复与子空间辨识相同的主方向。在合成和真实动态信号上进行实验验证。

Result: 实验证实两种方法的嵌入结果完全一致，TimeCluster的聚类坐标与子空间辨识方法重合。

Conclusion: 建立了可视化分析与系统辨识之间的数学等价关系，为未来开发包括状态空间预测、流式处理、外部输入整合和鲁棒趋势可视化等新功能提供了理论基础。

Abstract: TimeCluster is a visual analytics technique for discovering structure in long
multivariate time series by projecting overlapping windows of data into a
low-dimensional space. We show that, when Principal Component Analysis (PCA) is
chosen as the dimensionality reduction technique, this procedure is
mathematically equivalent to classical linear subspace identification
(block-Hankel matrix plus Singular Vector Decomposition (SVD)). In both
approaches, the same low-dimensional linear subspace is extracted from the time
series data. We first review the TimeCluster method and the theory of subspace
system identification. Then we show that forming the sliding-window matrix of a
time series yields a Hankel matrix, so applying PCA (via SVD) to this matrix
recovers the same principal directions as subspace identification. Thus the
cluster coordinates from TimeCluster coincide with the subspace identification
methods. We present experiments on synthetic and real dynamical signals
confirming that the two embeddings coincide. Finally, we explore and discuss
future opportunities enabled by this equivalence, including forecasting from
the identified state space, streaming/online extensions, incorporating and
visualising external inputs and robust techniques for displaying underlying
trends in corrupted data.

</details>


### [123] [Reversible Deep Equilibrium Models](https://arxiv.org/abs/2509.12917)
*Sam McCallum,Kamran Arora,James Foster*

Main category: cs.LG

TL;DR: 提出了可逆深度平衡模型（RevDEQs），解决了传统DEQs梯度计算近似的问题，实现了精确梯度计算，在语言建模和图像分类任务上达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 传统深度平衡模型（DEQs）虽然通过单层迭代多次来替代多层深度网络，在大规模任务中表现优异，但其梯度计算是近似的，导致训练不稳定，需要正则化或大量函数评估来修正

Method: 引入可逆深度平衡模型（RevDEQs），允许精确梯度计算，无需正则化，且比DEQs需要更少的函数评估

Result: RevDEQs在语言建模和图像分类任务上实现了最先进的性能，超越了同类隐式和显式模型

Conclusion: 可逆深度平衡模型通过精确梯度计算解决了传统DEQs的训练稳定性问题，在保持性能优势的同时显著减少了计算需求

Abstract: Deep Equilibrium Models (DEQs) are an interesting class of implicit model
where the model output is implicitly defined as the fixed point of a learned
function. These models have been shown to outperform explicit (fixed-depth)
models in large-scale tasks by trading many deep layers for a single layer that
is iterated many times. However, gradient calculation through DEQs is
approximate. This often leads to unstable training dynamics and requires
regularisation or many function evaluations to fix. Here, we introduce
Reversible Deep Equilibrium Models (RevDEQs) that allow for exact gradient
calculation, no regularisation and far fewer function evaluations than DEQs. We
show that RevDEQs achieve state-of-the-art performance on language modelling
and image classification tasks against comparable implicit and explicit models.

</details>


### [124] [Soft Gradient Boosting with Learnable Feature Transforms for Sequential Regression](https://arxiv.org/abs/2509.12920)
*Huseyin Karaca,Suleyman Serdar Kozat*

Main category: cs.LG

TL;DR: 提出了一种软梯度提升框架，在提升过程中嵌入可学习的线性特征变换，特别适用于高维数据稀缺场景，通过端到端优化提升性能并避免过拟合。


<details>
  <summary>Details</summary>
Motivation: 针对高维、数据稀缺场景下传统梯度提升方法可能无法有效发现最相关输入表示的问题，希望通过在提升过程中同时学习特征变换来提升性能。

Method: 在每个提升迭代中训练软决策树并同时学习线性输入特征变换Q，通过端到端优化实现特征选择/变换和提升的联合学习。

Result: 在合成和真实数据集上证明该方法能有效且高效地提升性能，避免过拟合，并可扩展到可微非线性变换。

Conclusion: 该软梯度提升框架通过集成特征变换学习，在高维数据稀缺场景下表现出色，代码已公开以支持可重现性和未来工作。

Abstract: We propose a soft gradient boosting framework for sequential regression that
embeds a learnable linear feature transform within the boosting procedure. At
each boosting iteration, we train a soft decision tree and learn a linear input
feature transform Q together. This approach is particularly advantageous in
high-dimensional, data-scarce scenarios, as it discovers the most relevant
input representations while boosting. We demonstrate, using both synthetic and
real-world datasets, that our method effectively and efficiently increases the
performance by an end-to-end optimization of feature selection/transform and
boosting while avoiding overfitting. We also extend our algorithm to
differentiable non-linear transforms if overfitting is not a problem. To
support reproducibility and future work, we share our code publicly.

</details>


### [125] [Sy-FAR: Symmetry-based Fair Adversarial Robustness](https://arxiv.org/abs/2509.12939)
*Haneen Najjar,Eyal Ronen,Mahmood Sharif*

Main category: cs.LG

TL;DR: 该论文提出了Sy-FAR方法，通过在对抗性鲁棒性优化中引入对称性概念来解决机器学习系统中的公平性问题，相比追求完美公平性，对称性方法更易实现且效果更好。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗性鲁棒性方法往往导致不公平的鲁棒性——某些类别或群体更容易受到攻击。在现实世界的关键任务（如人脸识别）中，实现完美公平性往往不可行，因为某些类别可能高度相似，导致更多误分类。

Method: 提出了Sy-FAR技术，鼓励对称性（即从类别i到j的攻击成功率与从j到i的攻击成功率相同），同时优化对抗鲁棒性。该方法基于对称关系在大多数领域都是对称的这一直觉，并通过理论证明对称性可以诱导子群间的对称性。

Result: 在五个数据集和三种模型架构上的广泛评估显示，Sy-FAR相比最先进方法显著提高了公平对抗鲁棒性。Sy-FAR运行更快、结果更一致，并且还能改善另一种不公平性——对抗样本容易被分类到的目标类别的脆弱性显著降低。

Conclusion: 对称性是一个比完美公平性更易实现且有效的替代方案，Sy-FAR方法在提高对抗鲁棒性的同时有效解决了公平性问题，特别是在安全关键和公平关键的实际应用中。

Abstract: Security-critical machine-learning (ML) systems, such as face-recognition
systems, are susceptible to adversarial examples, including real-world
physically realizable attacks. Various means to boost ML's adversarial
robustness have been proposed; however, they typically induce unfair
robustness: It is often easier to attack from certain classes or groups than
from others. Several techniques have been developed to improve adversarial
robustness while seeking perfect fairness between classes. Yet, prior work has
focused on settings where security and fairness are less critical. Our insight
is that achieving perfect parity in realistic fairness-critical tasks, such as
face recognition, is often infeasible -- some classes may be highly similar,
leading to more misclassifications between them. Instead, we suggest that
seeking symmetry -- i.e., attacks from class $i$ to $j$ would be as successful
as from $j$ to $i$ -- is more tractable. Intuitively, symmetry is a desirable
because class resemblance is a symmetric relation in most domains.
Additionally, as we prove theoretically, symmetry between individuals induces
symmetry between any set of sub-groups, in contrast to other fairness notions
where group-fairness is often elusive. We develop Sy-FAR, a technique to
encourage symmetry while also optimizing adversarial robustness and extensively
evaluate it using five datasets, with three model architectures, including
against targeted and untargeted realistic attacks. The results show Sy-FAR
significantly improves fair adversarial robustness compared to state-of-the-art
methods. Moreover, we find that Sy-FAR is faster and more consistent across
runs. Notably, Sy-FAR also ameliorates another type of unfairness we discover
in this work -- target classes that adversarial examples are likely to be
classified into become significantly less vulnerable after inducing symmetry.

</details>


### [126] [Spatiotemporal graph neural process for reconstruction, extrapolation, and classification of cardiac trajectories](https://arxiv.org/abs/2509.12953)
*Jaume Banus,Augustin C. Ogier,Roger Hullin,Philippe Meyer,Ruud B. van Heeswijk,Jonas Richiardi*

Main category: cs.LG

TL;DR: 提出了一种基于概率框架的时空动态建模方法，整合神经ODE、图神经网络和神经过程，用于从稀疏观测中重建和预测心脏运动轨迹。


<details>
  <summary>Details</summary>
Motivation: 心脏运动具有结构化时空特性，但临床观测往往稀疏且不完整。需要一种能够同时处理不确定性、时间连续性和解剖结构的方法来准确建模和预测心脏动态。

Method: 使用时空多重图表示动态系统，通过GNN参数化的向量场建模潜在轨迹。基于节点和边级别的稀疏上下文观测，推断潜在初始状态和控制变量的分布，实现轨迹插值和外推。

Result: 在三个合成系统（耦合摆、Lorenz吸引子、Kuramoto振荡器）和两个真实心脏数据集（ACDC N=150，UK Biobank N=526）上验证。ACDC分类准确率达99%，房颤检测准确率67%，能够从单周期观测外推未来心脏周期。

Conclusion: 该方法为心脏运动分析提供了灵活框架，为基于图学习的结构化生物医学时空时间序列数据分析奠定了基础。

Abstract: We present a probabilistic framework for modeling structured spatiotemporal
dynamics from sparse observations, focusing on cardiac motion. Our approach
integrates neural ordinary differential equations (NODEs), graph neural
networks (GNNs), and neural processes into a unified model that captures
uncertainty, temporal continuity, and anatomical structure. We represent
dynamic systems as spatiotemporal multiplex graphs and model their latent
trajectories using a GNN-parameterized vector field. Given the sparse context
observations at node and edge levels, the model infers a distribution over
latent initial states and control variables, enabling both interpolation and
extrapolation of trajectories. We validate the method on three synthetic
dynamical systems (coupled pendulum, Lorenz attractor, and Kuramoto
oscillators) and two real-world cardiac imaging datasets - ACDC (N=150) and UK
Biobank (N=526) - demonstrating accurate reconstruction, extrapolation, and
disease classification capabilities. The model accurately reconstructs
trajectories and extrapolates future cardiac cycles from a single observed
cycle. It achieves state-of-the-art results on the ACDC classification task (up
to 99% accuracy), and detects atrial fibrillation in UK Biobank subjects with
competitive performance (up to 67% accuracy). This work introduces a flexible
approach for analyzing cardiac motion and offers a foundation for graph-based
learning in structured biomedical spatiotemporal time-series data.

</details>


### [127] [BAPFL: Exploring Backdoor Attacks Against Prototype-based Federated Learning](https://arxiv.org/abs/2509.12964)
*Honghong Zeng,Jiong Lou,Zhe Wang,Hefeng Zhou,Chentao Wu,Wei Zhao,Jie Li*

Main category: cs.LG

TL;DR: 本文提出了BAPFL，这是首个专门针对原型联邦学习(PFL)框架的后门攻击方法，相比传统攻击在攻击成功率上提升35%-75%，同时保持主任务准确率。


<details>
  <summary>Details</summary>
Motivation: 原型联邦学习(PFL)因其原型学习机制和数据异构性对现有后门攻击具有天然抵抗性，但其安全性尚未被充分探索，需要专门的后门攻击方法来评估其安全性。

Method: BAPFL结合原型投毒策略和触发器优化机制。原型投毒策略操纵全局原型轨迹误导良性客户端的原型训练，使干净样本的原型远离带触发器样本的原型。触发器优化机制为每个目标标签学习独特隐蔽的触发器，并引导带触发器样本的原型与目标标签的全局原型对齐。

Result: 在多个数据集和PFL变体上的实验结果表明，BAPFL相比传统后门攻击在攻击成功率上实现35%-75%的提升，同时保持主任务准确率。

Conclusion: BAPFL在PFL中展现出高效性、隐蔽性和适应性，为PFL框架的安全性评估提供了有效工具。

Abstract: Prototype-based federated learning (PFL) has emerged as a promising paradigm
to address data heterogeneity problems in federated learning, as it leverages
mean feature vectors as prototypes to enhance model generalization. However,
its robustness against backdoor attacks remains largely unexplored. In this
paper, we identify that PFL is inherently resistant to existing backdoor
attacks due to its unique prototype learning mechanism and local data
heterogeneity. To further explore the security of PFL, we propose BAPFL, the
first backdoor attack method specifically designed for PFL frameworks. BAPFL
integrates a prototype poisoning strategy with a trigger optimization
mechanism. The prototype poisoning strategy manipulates the trajectories of
global prototypes to mislead the prototype training of benign clients, pushing
their local prototypes of clean samples away from the prototypes of
trigger-embedded samples. Meanwhile, the trigger optimization mechanism learns
a unique and stealthy trigger for each potential target label, and guides the
prototypes of trigger-embedded samples to align closely with the global
prototype of the target label. Experimental results across multiple datasets
and PFL variants demonstrate that BAPFL achieves a 35\%-75\% improvement in
attack success rate compared to traditional backdoor attacks, while preserving
main task accuracy. These results highlight the effectiveness, stealthiness,
and adaptability of BAPFL in PFL.

</details>


### [128] [Causal Discovery via Quantile Partial Effect](https://arxiv.org/abs/2509.12981)
*Yikang Chen,Xingzhe Sun,Dehui Du*

Main category: cs.LG

TL;DR: 本文提出了基于分位数部分效应(QPE)的因果发现方法，通过分析观测分布的形状不对称性来识别因果方向，无需考虑噪声机制或马尔可夫假设，并在多变量因果发现中利用Fisher信息确定因果顺序。


<details>
  <summary>Details</summary>
Motivation: 传统因果发现方法通常需要假设噪声机制或函数形式，限制了应用范围。本文旨在开发一种直接基于观测分布形状特性的因果识别方法，避免对噪声或机制的强假设。

Method: 利用分位数回归中的QPE统计量，假设因果效应位于有限线性空间中，通过基函数检验估计的QPE来区分因果方向。在多变量情况下，利用QPE与得分函数的紧密联系，使用Fisher信息作为统计量来确定因果顺序。

Result: 在大量双变量因果发现数据集上的实验表明该方法有效。在多变量因果发现中，使用Fisher信息识别因果顺序的方法在合成和真实世界数据集上都得到了验证。

Conclusion: QPE提供了一种新的因果发现框架，直接从观测分布的形状不对称性中识别因果关系，避免了传统方法对噪声机制和函数形式的强假设，为因果发现提供了新的理论和方法基础。

Abstract: Quantile Partial Effect (QPE) is a statistic associated with conditional
quantile regression, measuring the effect of covariates at different levels.
Our theory demonstrates that when the QPE of cause on effect is assumed to lie
in a finite linear span, cause and effect are identifiable from their
observational distribution. This generalizes previous identifiability results
based on Functional Causal Models (FCMs) with additive, heteroscedastic noise,
etc. Meanwhile, since QPE resides entirely at the observational level, this
parametric assumption does not require considering mechanisms, noise, or even
the Markov assumption, but rather directly utilizes the asymmetry of shape
characteristics in the observational distribution. By performing basis function
tests on the estimated QPE, causal directions can be distinguished, which is
empirically shown to be effective in experiments on a large number of bivariate
causal discovery datasets. For multivariate causal discovery, leveraging the
close connection between QPE and score functions, we find that Fisher
Information is sufficient as a statistical measure to determine causal order
when assumptions are made about the second moment of QPE. We validate the
feasibility of using Fisher Information to identify causal order on multiple
synthetic and real-world multivariate causal discovery datasets.

</details>


### [129] [Bridging Performance Gaps for Foundation Models: A Post-Training Strategy for ECGFounder](https://arxiv.org/abs/2509.12991)
*Ya Zhou,Yujie Yang,Xiaohan Fan,Wei Zhao*

Main category: cs.LG

TL;DR: 提出了一种简单有效的后训练方法，显著提升了ECG基础模型ECGFounder在PTB-XL基准测试上的性能表现，特别是在样本效率方面有显著改善


<details>
  <summary>Details</summary>
Motivation: ECG基础模型虽然在各种任务中具有适应性，但其临床适用性往往受到性能限制，即使在大规模ECG数据集上进行预训练和微调后，仍存在与任务特定模型的性能差距，这主要是由于缺乏有效的后训练策略

Method: 提出了一种后训练方法，包含随机深度(stochastic depth)和预览线性探测(preview linear probing)等关键组件，用于增强基于700多万条ECG记录预训练的ECGFounder模型

Result: 在PTB-XL基准测试中，相比基线微调策略，宏观AUROC提升1.2%-3.3%，宏观AUPRC提升5.3%-20.9%；使用仅10%的训练数据时，宏观AUROC提升9.1%，宏观AUPRC提升34.9%；方法更加稳定且样本效率更高

Conclusion: 后训练策略具有提升ECG基础模型的潜力，这项工作有望促进ECG领域基础模型的持续发展

Abstract: ECG foundation models are increasingly popular due to their adaptability
across various tasks. However, their clinical applicability is often limited by
performance gaps compared to task-specific models, even after pre-training on
large ECG datasets and fine-tuning on target data. This limitation is likely
due to the lack of an effective post-training strategy. In this paper, we
propose a simple yet effective post-training approach to enhance ECGFounder, a
state-of-the-art ECG foundation model pre-trained on over 7 million ECG
recordings. Experiments on the PTB-XL benchmark show that our approach improves
the baseline fine-tuning strategy by 1.2%-3.3% in macro AUROC and 5.3%-20.9% in
macro AUPRC. Additionally, our method outperforms several recent
state-of-the-art approaches, including task-specific and advanced
architectures. Further evaluation reveals that our method is more stable and
sample-efficient compared to the baseline, achieving a 9.1% improvement in
macro AUROC and a 34.9% improvement in macro AUPRC using just 10% of the
training data. Ablation studies identify key components, such as stochastic
depth and preview linear probing, that contribute to the enhanced performance.
These findings underscore the potential of post-training strategies to improve
ECG foundation models, and we hope this work will contribute to the continued
development of foundation models in the ECG domain.

</details>


### [130] [Ensemble Visualization With Variational Autoencoder](https://arxiv.org/abs/2509.13000)
*Cenyang Wu,Qinhan Yu,Liang Zhou*

Main category: cs.LG

TL;DR: 提出了一种基于变分自编码器(VAE)的数据集成可视化方法，通过在潜在空间中构建结构化概率表示来分析空间数据特征


<details>
  <summary>Details</summary>
Motivation: 传统的数据集成可视化方法难以有效处理高维空间数据特征，需要一种能够将空间特征转换到低维潜在空间并进行概率分析的方法

Method: 使用变分自编码器(VAE)进行特征空间转换和无监督学习，将空间特征映射到遵循多元标准高斯分布的潜在空间

Result: 初步实验结果表明该方法在天气预报集成数据集上表现出有效性和通用性，能够计算置信区间和进行密度估计

Conclusion: 该方法为数据集成可视化提供了一种有效的概率表示框架，特别适用于分析空间数据特征的统计特性

Abstract: We present a new method to visualize data ensembles by constructing
structured probabilistic representations in latent spaces, i.e.,
lower-dimensional representations of spatial data features. Our approach
transforms the spatial features of an ensemble into a latent space through
feature space conversion and unsupervised learning using a variational
autoencoder (VAE). The resulting latent spaces follow multivariate standard
Gaussian distributions, enabling analytical computation of confidence intervals
and density estimation of the probabilistic distribution that generates the
data ensemble. Preliminary results on a weather forecasting ensemble
demonstrate the effectiveness and versatility of our method.

</details>


### [131] [ReTrack: Data Unlearning in Diffusion Models through Redirecting the Denoising Trajectory](https://arxiv.org/abs/2509.13007)
*Qitan Shi,Cheng Jin,Jiawei Zhang,Yuantao Gu*

Main category: cs.LG

TL;DR: ReTrack是一种针对扩散模型的高效数据遗忘方法，通过重要性采样和保留主导项来构建优化目标，将去噪轨迹重定向到k近邻，在保持生成质量的同时实现有效遗忘。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然能生成高质量多样化图像，但存在训练数据记忆问题，带来隐私和安全风险。数据遗忘技术可以在不重新训练的情况下移除特定数据的影响。

Method: 采用重要性采样构建更高效的微调损失函数，通过保留主导项来近似优化目标，将去噪轨迹重定向到k近邻，实现高效遗忘。

Result: 在MNIST T-Shirt、CelebA-HQ、CIFAR-10和Stable Diffusion等数据集上实验表明，ReTrack达到了最先进的性能，在遗忘强度和生成质量保持之间取得了最佳平衡。

Conclusion: ReTrack是一种快速有效的扩散模型数据遗忘方法，通过创新的重要性采样和轨迹重定向技术，成功解决了数据记忆问题，同时保持了模型的生成能力。

Abstract: Diffusion models excel at generating high-quality, diverse images but suffer
from training data memorization, raising critical privacy and safety concerns.
Data unlearning has emerged to mitigate this issue by removing the influence of
specific data without retraining from scratch. We propose ReTrack, a fast and
effective data unlearning method for diffusion models. ReTrack employs
importance sampling to construct a more efficient fine-tuning loss, which we
approximate by retaining only dominant terms. This yields an interpretable
objective that redirects denoising trajectories toward the $k$-nearest
neighbors, enabling efficient unlearning while preserving generative quality.
Experiments on MNIST T-Shirt, CelebA-HQ, CIFAR-10, and Stable Diffusion show
that ReTrack achieves state-of-the-art performance, striking the best trade-off
between unlearning strength and generation quality preservation.

</details>


### [132] [Spiking Vocos: An Energy-Efficient Neural Vocoder](https://arxiv.org/abs/2509.13049)
*Yukun Chen,Zhaoxi Mu,Andong Li,Peilin Li,Xinyu Yang*

Main category: cs.LG

TL;DR: 提出Spiking Vocos，一种基于SNN的超低能耗神经声码器，通过Spiking ConvNeXt模块和振幅捷径路径解决SNN信息瓶颈，性能接近ANN版本但能耗仅14.7%


<details>
  <summary>Details</summary>
Motivation: 传统神经声码器在边缘设备上能耗过高，而SNN凭借事件驱动特性具有高能效优势，适合低资源场景

Method: 基于Vocos框架构建SNN声码器，设计Spiking ConvNeXt减少MAC操作，加入振幅捷径路径保持信号动态，采用自架构蒸馏策略和轻量级时序移位模块

Result: 性能与ANN相当（UTMOS 3.74，PESQ 3.45），能耗仅为ANN的14.7%

Conclusion: Spiking Vocos成功实现了高性能与超低能耗的平衡，为边缘设备部署提供了可行解决方案

Abstract: Despite the remarkable progress in the synthesis speed and fidelity of neural
vocoders, their high energy consumption remains a critical barrier to practical
deployment on computationally restricted edge devices. Spiking Neural Networks
(SNNs), widely recognized for their high energy efficiency due to their
event-driven nature, offer a promising solution for low-resource scenarios. In
this paper, we propose Spiking Vocos, a novel spiking neural vocoder with
ultra-low energy consumption, built upon the efficient Vocos framework. To
mitigate the inherent information bottleneck in SNNs, we design a Spiking
ConvNeXt module to reduce Multiply-Accumulate (MAC) operations and incorporate
an amplitude shortcut path to preserve crucial signal dynamics. Furthermore, to
bridge the performance gap with its Artificial Neural Network (ANN)
counterpart, we introduce a self-architectural distillation strategy to
effectively transfer knowledge. A lightweight Temporal Shift Module is also
integrated to enhance the model's ability to fuse information across the
temporal dimension with negligible computational overhead. Experiments
demonstrate that our model achieves performance comparable to its ANN
counterpart, with UTMOS and PESQ scores of 3.74 and 3.45 respectively, while
consuming only 14.7% of the energy. The source code is available at
https://github.com/pymaster17/Spiking-Vocos.

</details>


### [133] [Traces Propagation: Memory-Efficient and Scalable Forward-Only Learning in Spiking Neural Networks](https://arxiv.org/abs/2509.13053)
*Lorenzo Pes,Bojian Yin,Sander Stuijk,Federico Corradi*

Main category: cs.LG

TL;DR: 提出了Traces Propagation (TP)方法，一种前向传播、内存高效、可扩展的完全局部学习规则，结合资格迹和分层对比损失，无需辅助矩阵，在SNN训练中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决SNN训练中的时空信用分配问题，BPTT方法不符合生物神经系统的局部性原理且计算内存需求高，现有局部学习规则需要辅助矩阵导致内存开销大和可扩展性差。

Method: 提出TP方法，结合资格迹和分层对比损失，实现完全局部的前向传播学习，无需辅助层间矩阵。

Result: 在NMNIST和SHD数据集上优于其他完全局部学习规则，在DVS-GESTURE和DVS-CIFAR10等复杂数据集上表现竞争性，可扩展到VGG-9等深层架构，内存扩展性优于现有方法。

Conclusion: TP方法为边缘设备上的高效学习提供了可行途径，适用于实际微调任务如关键词识别。

Abstract: Spiking Neural Networks (SNNs) provide an efficient framework for processing
dynamic spatio-temporal signals and for investigating the learning principles
underlying biological neural systems. A key challenge in training SNNs is to
solve both spatial and temporal credit assignment. The dominant approach for
training SNNs is Backpropagation Through Time (BPTT) with surrogate gradients.
However, BPTT is in stark contrast with the spatial and temporal locality
observed in biological neural systems and leads to high computational and
memory demands, limiting efficient training strategies and on-device learning.
Although existing local learning rules achieve local temporal credit assignment
by leveraging eligibility traces, they fail to address the spatial credit
assignment without resorting to auxiliary layer-wise matrices, which increase
memory overhead and hinder scalability, especially on embedded devices. In this
work, we propose Traces Propagation (TP), a forward-only, memory-efficient,
scalable, and fully local learning rule that combines eligibility traces with a
layer-wise contrastive loss without requiring auxiliary layer-wise matrices. TP
outperforms other fully local learning rules on NMNIST and SHD datasets. On
more complex datasets such as DVS-GESTURE and DVS-CIFAR10, TP showcases
competitive performance and scales effectively to deeper SNN architectures such
as VGG-9, while providing favorable memory scaling compared to prior fully
local scalable rules, for datasets with a significant number of classes.
Finally, we show that TP is well suited for practical fine-tuning tasks, such
as keyword spotting on the Google Speech Commands dataset, thus paving the way
for efficient learning at the edge.

</details>


### [134] [Discovering Mathematical Equations with Diffusion Language Model](https://arxiv.org/abs/2509.13136)
*Xiaoxu Han,Chengzhen Ning,Jinghui Zhong,Fubiao Yang,Yu Wang,Xin Mu*

Main category: cs.LG

TL;DR: DiffuSR是一个基于连续状态扩散语言模型的符号回归预训练框架，通过扩散过程将离散数学符号映射到连续潜在空间，利用交叉注意力机制注入数值数据指导，在标准基准测试中达到与最先进自回归方法竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 符号回归在科学发现中至关重要，但由于搜索空间巨大以及准确性与复杂性之间的权衡，该任务仍然具有挑战性。现有方法存在局限性，需要新的框架来更有效地建模方程分布。

Method: DiffuSR采用可训练的嵌入层在扩散过程中将离散数学符号映射到连续潜在空间，通过迭代去噪将初始噪声序列转换为符号方程，并使用交叉注意力机制注入数值数据指导。还设计了有效的推理策略，将logit先验注入遗传编程中。

Result: 在标准符号回归基准测试中，DiffuSR实现了与最先进自回归方法竞争的性能，并生成更具可解释性和多样性的数学表达式。

Conclusion: DiffuSR作为一个基于扩散模型的符号回归框架，通过连续潜在空间建模和有效的推理策略，在保持竞争性能的同时提高了生成表达式的可解释性和多样性，为符号回归任务提供了新的解决方案。

Abstract: Discovering valid and meaningful mathematical equations from observed data
plays a crucial role in scientific discovery. While this task, symbolic
regression, remains challenging due to the vast search space and the trade-off
between accuracy and complexity. In this paper, we introduce DiffuSR, a
pre-training framework for symbolic regression built upon a continuous-state
diffusion language model. DiffuSR employs a trainable embedding layer within
the diffusion process to map discrete mathematical symbols into a continuous
latent space, modeling equation distributions effectively. Through iterative
denoising, DiffuSR converts an initial noisy sequence into a symbolic equation,
guided by numerical data injected via a cross-attention mechanism. We also
design an effective inference strategy to enhance the accuracy of the
diffusion-based equation generator, which injects logit priors into genetic
programming. Experimental results on standard symbolic regression benchmarks
demonstrate that DiffuSR achieves competitive performance with state-of-the-art
autoregressive methods and generates more interpretable and diverse
mathematical expressions.

</details>


### [135] [Curriculum Learning for Mesh-based simulations](https://arxiv.org/abs/2509.13138)
*Paul Garnier,Vincent Lannelongue,Elie Hachem*

Main category: cs.LG

TL;DR: 提出一种从粗到细的课程学习方法，通过在粗网格上先训练再逐步引入高分辨率数据，加速图神经网络在计算流体动力学中的训练过程，减少50%训练时间并突破性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 高分辨率非结构化网格上的图神经网络训练成本过高，需要找到一种方法来加速收敛并降低计算开销。

Method: 采用课程学习策略，先在最粗的网格上训练模型，然后逐步引入中等和高分辨率网格数据，模型架构保持不变。

Result: 在保持相当泛化精度的同时，总训练时间减少高达50%，在模型容量不足时能够突破性能平台。

Conclusion: 粗到细课程学习是加速高分辨率网格上GNN训练的有效方法，无需改变模型架构即可显著提升训练效率。

Abstract: Graph neural networks (GNNs) have emerged as powerful surrogates for
mesh-based computational fluid dynamics (CFD), but training them on
high-resolution unstructured meshes with hundreds of thousands of nodes remains
prohibitively expensive. We study a \emph{coarse-to-fine curriculum} that
accelerates convergence by first training on very coarse meshes and then
progressively introducing medium and high resolutions (up to \(3\times10^5\)
nodes). Unlike multiscale GNN architectures, the model itself is unchanged;
only the fidelity of the training data varies over time. We achieve comparable
generalization accuracy while reducing total wall-clock time by up to 50\%.
Furthermore, on datasets where our model lacks the capacity to learn the
underlying physics, using curriculum learning enables it to break through
plateaus.

</details>


### [136] [Learning from Heterophilic Graphs: A Spectral Theory Perspective on the Impact of Self-Loops and Parallel Edges](https://arxiv.org/abs/2509.13139)
*Kushal Bose,Swagatam Das*

Main category: cs.LG

TL;DR: 本文分析了图异质性对消息传递图神经网络性能的影响，通过添加自循环和平行边来更新异质图，研究GCN在不同异质网络上的性能趋势与图谱特性的关系。


<details>
  <summary>Details</summary>
Motivation: 图异质性对MP-GNNs性能构成挑战，特别是低通滤波器如GCN在异质图上性能下降，需要深入分析其性能与图谱特性的关系。

Method: 通过在异质图上添加自循环和平行边来更新图结构，观察图拉普拉斯特征值的变化，并在多个基准异质网络上测试GCN的性能趋势。

Result: 研究发现GCN在添加自循环和平行边时表现出不同的性能趋势，建立了图谱与低通滤波器性能趋势之间的联系，图谱能够表征图的内在特性。

Conclusion: 通过观察低通滤波器的性能趋势可以无缝评估图谱特性，无需昂贵的特征值分解，理论分析验证了添加自循环和平行边对图谱的影响。

Abstract: Graph heterophily poses a formidable challenge to the performance of
Message-passing Graph Neural Networks (MP-GNNs). The familiar low-pass filters
like Graph Convolutional Networks (GCNs) face performance degradation, which
can be attributed to the blending of the messages from dissimilar neighboring
nodes. The performance of the low-pass filters on heterophilic graphs still
requires an in-depth analysis. In this context, we update the heterophilic
graphs by adding a number of self-loops and parallel edges. We observe that
eigenvalues of the graph Laplacian decrease and increase respectively by
increasing the number of self-loops and parallel edges. We conduct several
studies regarding the performance of GCN on various benchmark heterophilic
networks by adding either self-loops or parallel edges. The studies reveal that
the GCN exhibited either increasing or decreasing performance trends on adding
self-loops and parallel edges. In light of the studies, we established
connections between the graph spectra and the performance trends of the
low-pass filters on the heterophilic graphs. The graph spectra characterize the
essential intrinsic properties of the input graph like the presence of
connected components, sparsity, average degree, cluster structures, etc. Our
work is adept at seamlessly evaluating graph spectrum and properties by
observing the performance trends of the low-pass filters without pursuing the
costly eigenvalue decomposition. The theoretical foundations are also discussed
to validate the impact of adding self-loops and parallel edges on the graph
spectrum.

</details>


### [137] [FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial Search and Reasoning](https://arxiv.org/abs/2509.13160)
*Liang Hu,Jianpeng Jiao,Jiashuo Liu,Yanle Ren,Zhoufutu Wen,Kaiyuan Zhang,Xuanliang Zhang,Xiang Gao,Tianci He,Fei Hu,Yali Liao,Zaiyuan Wang,Chenghao Yang,Qianyu Yang,Mingren Yin,Zhiyuan Zeng,Ge Zhang,Xinyi Zhang,Xiying Zhao,Zhenwei Zhu,Hongseok Namkoong,Wenhao Huang,Yuwen Tang*

Main category: cs.LG

TL;DR: FinSearchComp是首个开源金融搜索代理基准测试，包含三个真实金融分析师工作流程任务，由70位金融专家标注，评估21个模型在635个金融问题上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有金融数据集缺乏对端到端代理数据搜索能力的评估，金融领域需要处理时间敏感、领域特定的复杂多步搜索，是评估搜索能力和知识推理的理想场景。

Method: 构建包含时间敏感数据获取、简单历史查询和复杂历史调查三个任务的基准测试，采用70位金融专家标注和多阶段质量保证流程，涵盖全球和大中华区市场。

Result: Grok 4在全局子集表现最佳接近专家水平，DouBao在大中华区子集领先。实验表明配备网络搜索和金融插件能显著提升性能，模型和工具的国家来源对性能有重要影响。

Conclusion: FinSearchComp通过模拟真实分析师任务和提供端到端评估，为复杂金融搜索和推理提供了专业、高难度的测试平台。

Abstract: Search has emerged as core infrastructure for LLM-based agents and is widely
viewed as critical on the path toward more general intelligence. Finance is a
particularly demanding proving ground: analysts routinely conduct complex,
multi-step searches over time-sensitive, domain-specific data, making it ideal
for assessing both search proficiency and knowledge-grounded reasoning. Yet no
existing open financial datasets evaluate data searching capability of
end-to-end agents, largely because constructing realistic, complicated tasks
requires deep financial expertise and time-sensitive data is hard to evaluate.
We present FinSearchComp, the first fully open-source agent benchmark for
realistic, open-domain financial search and reasoning. FinSearchComp comprises
three tasks -- Time-Sensitive Data Fetching, Simple Historical Lookup, and
Complex Historical Investigation -- closely reproduce real-world financial
analyst workflows. To ensure difficulty and reliability, we engage 70
professional financial experts for annotation and implement a rigorous
multi-stage quality-assurance pipeline. The benchmark includes 635 questions
spanning global and Greater China markets, and we evaluate 21 models (products)
on it. Grok 4 (web) tops the global subset, approaching expert-level accuracy.
DouBao (web) leads on the Greater China subset. Experimental analyses show that
equipping agents with web search and financial plugins substantially improves
results on FinSearchComp, and the country origin of models and tools impact
performance significantly.By aligning with realistic analyst tasks and
providing end-to-end evaluation, FinSearchComp offers a professional,
high-difficulty testbed for complex financial search and reasoning.

</details>


### [138] [On the Correlation between Individual Fairness and Predictive Accuracy in Probabilistic Models](https://arxiv.org/abs/2509.13165)
*Alessandro Antonucci,Eric Rossetto,Ivan Duvnjak*

Main category: cs.LG

TL;DR: 研究生成概率分类器的个体公平性，通过分析后验推断对私有特征扰动的鲁棒性，发现鲁棒性与预测准确性相关，并提出新方法缓解公平性与准确性的传统权衡


<details>
  <summary>Details</summary>
Motivation: 研究生成概率分类器中个体公平性问题，探索后验推断对私有特征扰动的鲁棒性，旨在理解公平性与准确性之间的关系

Method: 使用贝叶斯网络作为生成模型，在14个具有公平性问题的基准数据集上进行实验。为解决多私有特征鲁棒性分析的计算复杂度问题，将问题重新表述为辅助马尔可夫随机场中的最可能解释任务

Result: 实验证实了鲁棒性与预测准确性之间的相关性假设，鲁棒性更强的实例更可能被准确分类

Conclusion: 研究结果为缓解公平性与准确性之间的传统权衡提供了新的方向，表明通过关注鲁棒性可以同时改善公平性和准确性

Abstract: We investigate individual fairness in generative probabilistic classifiers by
analysing the robustness of posterior inferences to perturbations in private
features. Building on established results in robustness analysis, we
hypothesise a correlation between robustness and predictive accuracy,
specifically, instances exhibiting greater robustness are more likely to be
classified accurately. We empirically assess this hypothesis using a benchmark
of fourteen datasets with fairness concerns, employing Bayesian networks as the
underlying generative models. To address the computational complexity
associated with robustness analysis over multiple private features with
Bayesian networks, we reformulate the problem as a most probable explanation
task in an auxiliary Markov random field. Our experiments confirm the
hypothesis about the correlation, suggesting novel directions to mitigate the
traditional trade-off between fairness and accuracy.

</details>


### [139] [CoVariance Filters and Neural Networks over Hilbert Spaces](https://arxiv.org/abs/2509.13178)
*Claudio Battiloro,Andrea Cavallo,Elvin Isufi*

Main category: cs.LG

TL;DR: 该论文提出了针对无限维希尔伯特空间信号的卷积学习框架，基于协方差算子构建希尔伯特协方差滤波器和网络，证明了其能恢复功能主成分分析，并在时间序列分类任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的协方差神经网络(VNNs)主要处理有限维希尔伯特空间信号，但缺乏对无限维空间的扩展研究。本文旨在将协方差神经网络的鲁棒性和可迁移性优势扩展到无限维希尔伯特空间。

Method: 提出了基于协方差算子的希尔伯特协方差滤波器(HVFs)和希尔伯特协方差网络(HVNs)，包含构造性定义、离散化过程和滤波器组堆叠结构。

Result: 证明了经验HVFs能够恢复滤波信号的功能主成分分析(FPCA)，在合成和真实时间序列分类任务中相比MLP和FPCA分类器表现出更鲁棒的性能。

Conclusion: 该工作成功将协方差神经网络扩展到无限维希尔伯特空间，为处理函数型数据提供了新的卷积学习框架，在理论和实验验证方面均取得了积极成果。

Abstract: CoVariance Neural Networks (VNNs) perform graph convolutions on the empirical
covariance matrix of signals defined over finite-dimensional Hilbert spaces,
motivated by robustness and transferability properties. Yet, little is known
about how these arguments extend to infinite-dimensional Hilbert spaces. In
this work, we take a first step by introducing a novel convolutional learning
framework for signals defined over infinite-dimensional Hilbert spaces,
centered on the (empirical) covariance operator. We constructively define
Hilbert coVariance Filters (HVFs) and design Hilbert coVariance Networks (HVNs)
as stacks of HVF filterbanks with nonlinear activations. We propose a
principled discretization procedure, and we prove that empirical HVFs can
recover the Functional PCA (FPCA) of the filtered signals. We then describe the
versatility of our framework with examples ranging from multivariate
real-valued functions to reproducing kernel Hilbert spaces. Finally, we
validate HVNs on both synthetic and real-world time-series classification
tasks, showing robust performance compared to MLP and FPCA-based classifiers.

</details>


### [140] [Is Meta-Learning Out? Rethinking Unsupervised Few-Shot Classification with Limited Entropy](https://arxiv.org/abs/2509.13185)
*Yunchuan Guan,Yu Liu,Ke Zhou,Zhiqi Shen,Jenq-Neng Hwang,Serge Belongie,Lei Li*

Main category: cs.LG

TL;DR: 本文通过理论分析和实验验证，证明了元学习在有限熵监督设置下比全类训练具有更紧的泛化边界，更高效且对标签噪声和异构任务更鲁棒，并提出了MINO元学习框架来提升无监督性能。


<details>
  <summary>Details</summary>
Motivation: 近期研究表明在全类训练策略下训练的模型在少样本分类任务中可以达到与元学习相当的性能，为了证明元学习的价值，需要建立公平的比较设置。

Method: 建立熵限制监督设置进行公平比较，通过理论分析和实验验证元学习的优势，提出MINO框架（使用DBSCAN自适应聚类算法和动态头进行无监督任务构建，以及基于稳定性的元缩放器来抵抗标签噪声）。

Result: 元学习在有限熵下更高效，对标签噪声和异构任务更鲁棒，MINO框架在多个无监督少样本和零样本任务中表现出有效性。

Conclusion: 元学习在有限熵设置下具有理论优势，MINO框架成功地将这些优势应用于无监督任务，证明了元学习在实际应用中的价值。

Abstract: Meta-learning is a powerful paradigm for tackling few-shot tasks. However,
recent studies indicate that models trained with the whole-class training
strategy can achieve comparable performance to those trained with meta-learning
in few-shot classification tasks. To demonstrate the value of meta-learning, we
establish an entropy-limited supervised setting for fair comparisons. Through
both theoretical analysis and experimental validation, we establish that
meta-learning has a tighter generalization bound compared to whole-class
training. We unravel that meta-learning is more efficient with limited entropy
and is more robust to label noise and heterogeneous tasks, making it
well-suited for unsupervised tasks. Based on these insights, We propose MINO, a
meta-learning framework designed to enhance unsupervised performance. MINO
utilizes the adaptive clustering algorithm DBSCAN with a dynamic head for
unsupervised task construction and a stability-based meta-scaler for robustness
against label noise. Extensive experiments confirm its effectiveness in
multiple unsupervised few-shot and zero-shot tasks.

</details>


### [141] [TRUST-FS: Tensorized Reliable Unsupervised Multi-View Feature Selection for Incomplete Data](https://arxiv.org/abs/2509.13192)
*Minghui Lu,Yanyong Huang,Minbo Ma,Dongjie Wang,Xiuwen Yi,Tianrui Li*

Main category: cs.LG

TL;DR: TRUST-FS是一种针对具有缺失变量的不完整多视图数据的无监督特征选择方法，通过张量分解框架同时进行特征选择、缺失值填补和视图权重学习，并利用主观逻辑获取可信的跨视图相似性信息。


<details>
  <summary>Details</summary>
Motivation: 现有的多视图无监督特征选择方法在处理不完整数据时存在三个主要挑战：1) 只能处理缺失视图而无法处理更一般的缺失变量场景；2) 将缺失值填补和特征选择作为独立过程处理，忽略了它们的交互作用；3) 缺失数据导致相似性图不准确，影响特征选择性能。

Method: 提出TRUST-FS方法，采用自适应加权CP分解在统一的张量分解框架中同时进行特征选择、缺失变量填补和视图权重学习。利用主观逻辑获取可信的跨视图相似性信息，构建可靠的相似性图来指导特征选择和填补过程。

Result: 综合实验结果表明，该方法在有效性和性能上优于现有的最先进方法。

Conclusion: TRUST-FS通过统一的张量分解框架成功解决了不完整多视图数据中缺失变量处理、特征选择与填补过程交互、以及相似性图可靠性等关键问题，为多视图无监督特征选择提供了有效的解决方案。

Abstract: Multi-view unsupervised feature selection (MUFS), which selects informative
features from multi-view unlabeled data, has attracted increasing research
interest in recent years. Although great efforts have been devoted to MUFS,
several challenges remain: 1) existing methods for incomplete multi-view data
are limited to handling missing views and are unable to address the more
general scenario of missing variables, where some features have missing values
in certain views; 2) most methods address incomplete data by first imputing
missing values and then performing feature selection, treating these two
processes independently and overlooking their interactions; 3) missing data can
result in an inaccurate similarity graph, which reduces the performance of
feature selection. To solve this dilemma, we propose a novel MUFS method for
incomplete multi-view data with missing variables, termed Tensorized Reliable
UnSupervised mulTi-view Feature Selection (TRUST-FS). TRUST-FS introduces a new
adaptive-weighted CP decomposition that simultaneously performs feature
selection, missing-variable imputation, and view weight learning within a
unified tensor factorization framework. By utilizing Subjective Logic to
acquire trustworthy cross-view similarity information, TRUST-FS facilitates
learning a reliable similarity graph, which subsequently guides feature
selection and imputation. Comprehensive experimental results demonstrate the
effectiveness and superiority of our method over state-of-the-art methods.

</details>


### [142] [B-TGAT: A Bi-directional Temporal Graph Attention Transformer for Clustering Multivariate Spatiotemporal Data](https://arxiv.org/abs/2509.13202)
*Francis Ndikum Nji,Vandana Janaja,Jianwu Wang*

Main category: cs.LG

TL;DR: 提出了一种结合双向时序图注意力变换器(B-TGAT)的时间分布式混合U-Net自编码器，用于高维多变量时空气候数据的高效时序聚类，在三个不同数据集上表现出优越的聚类分离性和时序稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法在处理高维时空气候数据时难以同时捕捉局部和全局时序关系并保持空间上下文，需要新的方法来应对复杂的时序依赖、演化空间交互和非平稳动态。

Method: 使用ConvLSTM2D模块提取联合时空特征，通过U-Net跳跃连接保持多尺度空间细节，在瓶颈处集成基于图的空间建模和注意力驱动的时序编码(B-TGAT)，实现自适应时序邻域加权和长短程依赖捕捉。

Result: 在三个不同的时空气候数据集上实验表明，该方法在聚类分离性、时序稳定性和与已知气候转变的对齐方面优于最先进的基线方法。

Conclusion: ConvLSTM2D、U-Net跳跃连接和B-TGAT的集成提升了时序聚类性能，同时为复杂的时空变异性提供了可解释的见解，推动了方法学发展和气候科学应用。

Abstract: Clustering high-dimensional multivariate spatiotemporal climate data is
challenging due to complex temporal dependencies, evolving spatial
interactions, and non-stationary dynamics. Conventional clustering methods,
including recurrent and convolutional models, often struggle to capture both
local and global temporal relationships while preserving spatial context. We
present a time-distributed hybrid U-Net autoencoder that integrates a
Bi-directional Temporal Graph Attention Transformer (B-TGAT) to guide efficient
temporal clustering of multidimensional spatiotemporal climate datasets. The
encoder and decoder are equipped with ConvLSTM2D modules that extract joint
spatial--temporal features by modeling localized dynamics and spatial
correlations over time, and skip connections that preserve multiscale spatial
details during feature compression and reconstruction. At the bottleneck,
B-TGAT integrates graph-based spatial modeling with attention-driven temporal
encoding, enabling adaptive weighting of temporal neighbors and capturing both
short and long-range dependencies across regions. This architecture produces
discriminative latent embeddings optimized for clustering. Experiments on three
distinct spatiotemporal climate datasets demonstrate superior cluster
separability, temporal stability, and alignment with known climate transitions
compared to state-of-the-art baselines. The integration of ConvLSTM2D, U-Net
skip connections, and B-TGAT enhances temporal clustering performance while
providing interpretable insights into complex spatiotemporal variability,
advancing both methodological development and climate science applications.

</details>


### [143] [HAM: Hierarchical Adapter Merging for Scalable Continual Learning](https://arxiv.org/abs/2509.13211)
*Eric Nuertey Coleman,Luigi Quarantiello,Samrat Mukherjee,Julio Hurtado,Vincenzo Lomonaco*

Main category: cs.LG

TL;DR: HAM是一种新颖的分层适配器合并框架，通过动态合并不同任务的适配器来解决持续学习中的灾难性遗忘问题，在多个视觉基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中新知识干扰旧知识导致的灾难性遗忘问题，特别是当面对新数据分布时，现有参数高效微调方法在动态学习场景和长任务序列中面临扩展性挑战。

Method: 提出分层适配器合并(HAM)框架：维护固定组集进行分层整合，为每个任务训练低秩适配器和重要性标量，基于适配器相似性动态分组，在组内进行修剪、缩放和合并适配器。

Result: 在三个视觉基准测试上的大量实验表明，HAM显著优于最先进的方法，特别是在任务数量增加时表现更加突出。

Conclusion: HAM框架通过动态合并适配器的分层方法，有效解决了持续学习中的扩展性问题，能够管理比竞争基线更多的任务，并提高了效率。

Abstract: Continual learning is an essential capability of human cognition, yet it
poses significant challenges for current deep learning models. The primary
issue is that new knowledge can interfere with previously learned information,
causing the model to forget earlier knowledge in favor of the new, a phenomenon
known as catastrophic forgetting. Although large pre-trained models can
partially mitigate forgetting by leveraging their existing knowledge and
over-parameterization, they often struggle when confronted with novel data
distributions. Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA,
enable efficient adaptation to new knowledge. However, they still face
challenges in scaling to dynamic learning scenarios and long sequences of
tasks, as maintaining one adapter per task introduces complexity and increases
the potential for interference. In this paper, we introduce Hierarchical
Adapters Merging (HAM), a novel framework that dynamically combines adapters
from different tasks during training. This approach enables HAM to scale
effectively, allowing it to manage more tasks than competing baselines with
improved efficiency. To achieve this, HAM maintains a fixed set of groups that
hierarchically consolidate new adapters. For each task, HAM trains a low-rank
adapter along with an importance scalar, then dynamically groups tasks based on
adapter similarity. Within each group, adapters are pruned, scaled and merge,
facilitating transfer learning between related tasks. Extensive experiments on
three vision benchmarks show that HAM significantly outperforms
state-of-the-art methods, particularly as the number of tasks increases.

</details>


### [144] [Density-Aware Farthest Point Sampling](https://arxiv.org/abs/2509.13213)
*Paolo Climaco,Jochen Garcke*

Main category: cs.LG

TL;DR: 提出一种新的密度感知最远点采样方法(DA-FPS)，用于在标注数据有限的情况下选择训练集，通过最小化加权填充距离来降低回归模型的预测误差上界。


<details>
  <summary>Details</summary>
Motivation: 在机器学习回归任务中，由于计算限制或标注成本高昂，标注训练数据有限，需要从无标注数据中选择合适的训练集来平衡性能与效率。

Method: 推导了Lipschitz连续回归模型预测误差的上界，该上界与训练集的加权填充距离线性相关。提出了密度感知最远点采样(DA-FPS)方法，通过近似最小化数据驱动的加权填充距离估计来优化该上界。

Result: 在三个数据集上使用两种回归模型进行实验，结果显示DA-FPS相比其他采样策略显著降低了平均绝对预测误差。

Conclusion: DA-FPS是一种有效的被动且模型无关的采样方法，仅基于数据特征表示就能选择出高质量的训练集，在有限标注数据场景下能显著提升回归模型性能。

Abstract: We focus on training machine learning regression models in scenarios where
the availability of labeled training data is limited due to computational
constraints or high labeling costs. Thus, selecting suitable training sets from
unlabeled data is essential for balancing performance and efficiency. For the
selection of the training data, we focus on passive and model-agnostic sampling
methods that only consider the data feature representations. We derive an upper
bound for the expected prediction error of Lipschitz continuous regression
models that linearly depends on the weighted fill distance of the training set,
a quantity we can estimate simply by considering the data features. We
introduce "Density-Aware Farthest Point Sampling" (DA-FPS), a novel sampling
method. We prove that DA-FPS provides approximate minimizers for a data-driven
estimation of the weighted fill distance, thereby aiming at minimizing our
derived bound. We conduct experiments using two regression models across three
datasets. The results demonstrate that DA-FPS significantly reduces the mean
absolute prediction error compared to other sampling strategies.

</details>


### [145] [FOSSIL: Regret-minimizing weighting for robust learning under imbalance and small data](https://arxiv.org/abs/2509.13218)
*J. Cha,J. Lee,J. Cho,J. Shin*

Main category: cs.LG

TL;DR: FOSSIL是一个统一的样本权重框架，通过单一可解释公式整合类别不平衡校正、难度感知课程、增强惩罚和预热动态，在数据不平衡和小样本场景中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决不平衡和小数据场景（如罕见疾病成像、基因组学）中标注样本稀缺的问题，现有方法如过采样、焦点损失等存在脆弱性或复杂性，需要统一框架。

Method: 提出FOSSIL框架，通过样本敏感重要性学习，将类别不平衡校正、难度感知课程学习、数据增强惩罚和预热机制整合到单一公式中，无需改变模型架构。

Result: 在合成和真实数据集上，FOSSIL consistently优于ERM、课程学习和元加权基线方法，并提供基于遗憾的理论保证。

Conclusion: FOSSIL提供了一个理论保证的统一框架，有效解决了不平衡小数据学习中的多个挑战，具有实际应用价值。

Abstract: Imbalanced and small data regimes are pervasive in domains such as rare
disease imaging, genomics, and disaster response, where labeled samples are
scarce and naive augmentation often introduces artifacts. Existing solutions
such as oversampling, focal loss, or meta-weighting address isolated aspects of
this challenge but remain fragile or complex. We introduce FOSSIL (Flexible
Optimization via Sample Sensitive Importance Learning), a unified weighting
framework that seamlessly integrates class imbalance correction,
difficulty-aware curricula, augmentation penalties, and warmup dynamics into a
single interpretable formula. Unlike prior heuristics, the proposed framework
provides regret-based theoretical guarantees and achieves consistent empirical
gains over ERM, curriculum, and meta-weighting baselines on synthetic and
real-world datasets, while requiring no architectural changes.

</details>


### [146] [On the Out-of-Distribution Backdoor Attack for Federated Learning](https://arxiv.org/abs/2509.13219)
*Jiahao Xu,Zikai Zhang,Rui Hu*

Main category: cs.LG

TL;DR: 提出了一种新型联邦学习后门攻击OBA，使用OOD数据作为毒化样本和触发器，并设计了SoDa方法提高隐蔽性。同时提出了BNGuard防御方法，利用批归一化层统计偏差检测恶意模型更新。


<details>
  <summary>Details</summary>
Motivation: 传统后门攻击依赖可见触发器和物理修改，应用场景受限。需要开发更隐蔽、实用的联邦学习后门攻击方法，并设计相应的防御机制。

Method: OBA攻击使用OOD数据作为毒化样本和触发器；SoDa通过正则化恶意本地模型的幅度和方向来提高隐蔽性；BNGuard防御通过检测批归一化层统计偏差来识别恶意更新。

Result: OBA能有效绕过现有防御机制，在主任务上保持高精度；BNGuard在各种设置下都能有效防御SoDa攻击。

Conclusion: 该工作提出了更实用的联邦学习后门攻击方法，并设计了有效的防御方案，提升了联邦学习系统的安全性。

Abstract: Traditional backdoor attacks in federated learning (FL) operate within
constrained attack scenarios, as they depend on visible triggers and require
physical modifications to the target object, which limits their practicality.
To address this limitation, we introduce a novel backdoor attack prototype for
FL called the out-of-distribution (OOD) backdoor attack ($\mathtt{OBA}$), which
uses OOD data as both poisoned samples and triggers simultaneously. Our
approach significantly broadens the scope of backdoor attack scenarios in FL.
To improve the stealthiness of $\mathtt{OBA}$, we propose $\mathtt{SoDa}$,
which regularizes both the magnitude and direction of malicious local models
during local training, aligning them closely with their benign versions to
evade detection. Empirical results demonstrate that $\mathtt{OBA}$ effectively
circumvents state-of-the-art defenses while maintaining high accuracy on the
main task.
  To address this security vulnerability in the FL system, we introduce
$\mathtt{BNGuard}$, a new server-side defense method tailored against
$\mathtt{SoDa}$. $\mathtt{BNGuard}$ leverages the observation that OOD data
causes significant deviations in the running statistics of batch normalization
layers. This allows $\mathtt{BNGuard}$ to identify malicious model updates and
exclude them from aggregation, thereby enhancing the backdoor robustness of FL.
Extensive experiments across various settings show the effectiveness of
$\mathtt{BNGuard}$ on defending against $\mathtt{SoDa}$. The code is available
at https://github.com/JiiahaoXU/SoDa-BNGuard.

</details>


### [147] [Single-stream Policy Optimization](https://arxiv.org/abs/2509.13232)
*Zhongwen Xu,Zihan Ding*

Main category: cs.LG

TL;DR: SPO是一种单流策略优化方法，通过持久化KL自适应值跟踪器和全局优势归一化，解决了传统分组方法GRPO的退化组问题和可扩展性限制，在数学推理任务上实现了更好的收敛性和准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基于分组的策略优化方法（如GRPO）存在两个关键问题：频繁出现的退化组会消除学习信号，同步障碍限制了可扩展性。需要一种更稳定、高效的优化方法。

Method: SPO采用单流设计，用持久化的KL自适应值跟踪器替代每组的基线，并在整个批次中全局归一化优势值，为每个样本提供稳定、低方差的学习信号。

Result: 在五个困难数学基准测试中，SPO相比GRPO平均maj@32提高了3.4个百分点，在BRUMO 25上提升7.3pp，AIME 25上提升4.4pp，HMMT 25上提升3.3pp。

Conclusion: SPO的成功表明，通过回归基本原理而非增加架构复杂性，可以为LLM推理带来实质性进展，提供了一条更稳健高效的优化路径。

Abstract: We revisit policy-gradient optimization for Large Language Models (LLMs) from
a single-stream perspective. Prevailing group-based methods like GRPO reduce
variance with on-the-fly baselines but suffer from critical flaws: frequent
degenerate groups erase learning signals, and synchronization barriers hinder
scalability. We introduce Single-stream Policy Optimization (SPO), which
eliminates these issues by design. SPO replaces per-group baselines with a
persistent, KL-adaptive value tracker and normalizes advantages globally across
the batch, providing a stable, low-variance learning signal for every sample.
Being group-free, SPO enables higher throughput and scales effectively in
long-horizon or tool-integrated settings where generation times vary.
Furthermore, the persistent value tracker naturally enables an adaptive
curriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO
converges more smoothly and attains higher accuracy than GRPO, while
eliminating computation wasted on degenerate groups. Ablation studies confirm
that SPO's gains stem from its principled approach to baseline estimation and
advantage normalization, offering a more robust and efficient path for LLM
reasoning. Across five hard math benchmarks with Qwen3 8B, SPO improves the
average maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial
absolute point gains on challenging datasets, including +7.3 pp on BRUMO 25,
+4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain
in pass@$k$ across the evaluated $k$ values. SPO's success challenges the
prevailing trend of adding incidental complexity to RL algorithms, highlighting
a path where fundamental principles, not architectural workarounds, drive the
next wave of progress in LLM reasoning.

</details>


### [148] [Metacognitive Reuse: Turning Recurring LLM Reasoning Into Concise Behaviors](https://arxiv.org/abs/2509.13237)
*Aniket Didolkar,Nicolas Ballas,Sanjeev Arora,Anirudh Goyal*

Main category: cs.LG

TL;DR: 该论文提出了一种将重复推理片段转化为可重用"行为"的方法，通过LLM的元认知分析创建行为手册，在推理时提供相关行为以减少token使用并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在解决多步问题时经常重复推导相同的中间步骤，导致token使用和延迟增加，上下文窗口饱和限制了探索能力。

Method: 通过模型的元认知分析将重复推理片段转化为简洁可重用的"行为"（名称+指令），存储在行为手册中，在推理时提供相关行为或通过监督微调蒸馏到参数中。

Result: 在三种设置下均取得改进：1) 行为条件推理减少推理token达46%，同时保持或提高准确性；2) 行为引导自改进无需参数更新即可提高10%准确率；3) 行为条件SFT比普通SFT更有效地将非推理模型转化为推理模型。

Conclusion: 将缓慢推导转化为快速程序提示使LLM能够记住如何推理，而不仅仅是记住结论，提高了推理效率和效果。

Abstract: Large language models (LLMs) now solve multi-step problems by emitting
extended chains of thought. During the process, they often re-derive the same
intermediate steps across problems, inflating token usage and latency. This
saturation of the context window leaves less capacity for exploration. We study
a simple mechanism that converts recurring reasoning fragments into concise,
reusable "behaviors" (name + instruction) via the model's own metacognitive
analysis of prior traces. These behaviors are stored in a "behavior handbook"
which supplies them to the model in-context at inference or distills them into
parameters via supervised fine-tuning. This approach achieves improved
test-time reasoning across three different settings - 1) Behavior-conditioned
inference: Providing the LLM relevant behaviors in-context during reasoning
reduces number of reasoning tokens by up to 46% while matching or improving
baseline accuracy; 2) Behavior-guided self-improvement: Without any parameter
updates, the model improves its own future reasoning by leveraging behaviors
from its own past problem solving attempts. This yields up to 10% higher
accuracy than a naive critique-and-revise baseline; and 3) Behavior-conditioned
SFT: SFT on behavior-conditioned reasoning traces is more effective at
converting non-reasoning models into reasoning models as compared to vanilla
SFT. Together, these results indicate that turning slow derivations into fast
procedural hints enables LLMs to remember how to reason, not just what to
conclude.

</details>


### [149] [Don't Forget the Nonlinearity: Unlocking Activation Functions in Efficient Fine-Tuning](https://arxiv.org/abs/2509.13240)
*Bo Yin,Xingyi Yang,Xinchao Wang*

Main category: cs.LG

TL;DR: NoRA是首个直接调整预训练Transformer模型中非线性激活函数的参数高效微调框架，通过可学习有理函数和结构化低秩更新，仅需0.4%参数就能达到或超越全参数微调效果。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效微调方法主要调整权重矩阵而保持激活函数固定，作者认为激活函数也应该作为模型适配的一等公民进行优化。

Method: 用可学习有理函数替换固定激活函数，对分子和分母系数应用结构化低秩更新，采用分组设计实现局部适配并提高稳定性。

Result: 在CIFAR数据集上达到或超越全微调精度（+0.17%-0.27%），仅更新0.02M参数；与LoRA结合（NoRA++）在相同训练预算下表现更优；在LLaMA3-8B指令微调中持续提升生成质量，MMLU平均提升0.3%-0.8%。

Conclusion: 激活空间调优是权重基PEFT的互补且高度参数高效的替代方案，将激活函数定位为模型适配的一等对象。

Abstract: Existing parameter-efficient fine-tuning (PEFT) methods primarily adapt
weight matrices while keeping activation functions fixed. We introduce
\textbf{NoRA}, the first PEFT framework that directly adapts nonlinear
activation functions in pretrained transformer-based models. NoRA replaces
fixed activations with learnable rational functions and applies structured
low-rank updates to numerator and denominator coefficients, with a group-wise
design that localizes adaptation and improves stability at minimal cost. On
vision transformers trained on CIFAR-10 and CIFAR-100, NoRA matches or exceeds
full fine-tuning while updating only 0.4\% of parameters (0.02M), achieving
accuracy gains of +0.17\% and +0.27\%. When combined with LoRA
(\textbf{NoRA++}), it outperforms LoRA and DoRA under matched training budgets
by adding fewer trainable parameters. On LLaMA3-8B instruction tuning, NoRA++
consistently improves generation quality, yielding average MMLU gains of
+0.3\%--0.8\%, including +1.6\% on STEM (Alpaca) and +1.3\% on OpenOrca. We
further show that NoRA constrains adaptation to a low-dimensional functional
subspace, implicitly regularizing update magnitude and direction. These results
establish activation-space tuning as a complementary and highly
parameter-efficient alternative to weight-based PEFT, positioning activation
functions as first-class objects for model adaptation.

</details>


### [150] [Post-Hoc Split-Point Self-Consistency Verification for Efficient, Unified Quantification of Aleatoric and Epistemic Uncertainty in Deep Learning](https://arxiv.org/abs/2509.13262)
*Zhizhong Zhao,Ke Chen*

Main category: cs.LG

TL;DR: 提出了一种后处理单前向传播框架SPC-UQ，通过分割点分析(SPA)同时捕捉任意性和认知不确定性，无需重新训练预训练模型，在回归和分类任务中达到或超越现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性量化方法要么计算成本高（如贝叶斯或集成方法），要么只能提供部分任务特定的估计（如单前向传播技术），需要一种高效且全面的不确定性量化解决方案。

Method: 使用分割点分析(SPA)将预测残差分解为上下子集，计算每侧的平均绝对残差(MARs)，通过自一致性差异分数(SDS)进行细粒度认知估计。回归任务中使用分位数回归获得预测区间，分类任务中基于SPA校准softmax输出。

Result: 在多个回归和分类基准测试中，该方法匹配或超越了多种最先进的不确定性量化方法，同时计算开销极小。

Conclusion: SPC-UQ框架提供了一种高效、全面的不确定性量化解决方案，能够同时处理任意性和认知不确定性，适用于各种深度学习任务。

Abstract: Uncertainty quantification (UQ) is vital for trustworthy deep learning, yet
existing methods are either computationally intensive, such as Bayesian or
ensemble methods, or provide only partial, task-specific estimates, such as
single-forward-pass techniques. In this paper, we propose a post-hoc
single-forward-pass framework that jointly captures aleatoric and epistemic
uncertainty without modifying or retraining pretrained models. Our method
applies \emph{Split-Point Analysis} (SPA) to decompose predictive residuals
into upper and lower subsets, computing \emph{Mean Absolute Residuals} (MARs)
on each side. We prove that, under ideal conditions, the total MAR equals the
harmonic mean of subset MARs; deviations define a novel \emph{Self-consistency
Discrepancy Score} (SDS) for fine-grained epistemic estimation across
regression and classification. For regression, side-specific quantile
regression yields prediction intervals with improved empirical coverage, which
are further calibrated via SDS. For classification, when calibration data are
available, we apply SPA-based calibration identities to adjust the softmax
outputs and then compute predictive entropy on these calibrated probabilities.
Extensive experiments on diverse regression and classification benchmarks
demonstrate that our framework matches or exceeds several state-of-the-art UQ
methods while incurring minimal overhead.
  Our source code is available at https://github.com/zzz0527/SPC-UQ.

</details>


### [151] [JANUS: A Dual-Constraint Generative Framework for Stealthy Node Injection Attacks](https://arxiv.org/abs/2509.13266)
*Jiahao Zhang,Xiaobing Pei,Zhaokun Zhong,Wenqiang Hao,Zhenghao Tang*

Main category: cs.LG

TL;DR: 提出了JANUS框架，通过局部特征流形对齐和全局语义模式一致性，解决了现有节点注入攻击方法在隐蔽性方面的局限性，显著提升了攻击效果和隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络节点注入攻击方法主要依赖间接代理指标实现隐蔽性，缺乏对注入内容基本特征的考虑，或仅关注局部结构模仿，存在局部短视问题。

Method: 提出双约束隐蔽节点注入框架JANUS：局部层面采用特征流形对齐策略实现几何一致性；全局层面引入结构化潜在变量并通过最大化互信息确保注入结构与原始图语义模式一致；将注入攻击建模为序列决策过程，使用强化学习代理优化。

Result: 在多个标准数据集上的实验表明，JANUS框架在攻击效果和隐蔽性方面均显著优于现有方法。

Conclusion: JANUS框架通过同时考虑局部几何一致性和全局语义一致性，有效解决了节点注入攻击的隐蔽性问题，为图神经网络安全性研究提供了新思路。

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable performance across
various applications, yet they are vulnerable to sophisticated adversarial
attacks, particularly node injection attacks. The success of such attacks
heavily relies on their stealthiness, the ability to blend in with the original
graph and evade detection. However, existing methods often achieve stealthiness
by relying on indirect proxy metrics, lacking consideration for the fundamental
characteristics of the injected content, or focusing only on imitating local
structures, which leads to the problem of local myopia. To overcome these
limitations, we propose a dual-constraint stealthy node injection framework,
called Joint Alignment of Nodal and Universal Structures (JANUS). At the local
level, we introduce a local feature manifold alignment strategy to achieve
geometric consistency in the feature space. At the global level, we incorporate
structured latent variables and maximize the mutual information with the
generated structures, ensuring the injected structures are consistent with the
semantic patterns of the original graph. We model the injection attack as a
sequential decision process, which is optimized by a reinforcement learning
agent. Experiments on multiple standard datasets demonstrate that the JANUS
framework significantly outperforms existing methods in terms of both attack
effectiveness and stealthiness.

</details>


### [152] [LLMs for energy and macronutrients estimation using only text data from 24-hour dietary recalls: a parameter-efficient fine-tuning experiment using a 10-shot prompt](https://arxiv.org/abs/2509.13268)
*Rodrigo M Carrillo-Larco*

Main category: cs.LG

TL;DR: 研究发现，通过参数高效微调(PEFT)和思维链提示，开源大语言模型仅基于文本描述就能准确预测食物能量和宏量营养素含量，为低负担的文本饮食监测提供了可能。


<details>
  <summary>Details</summary>
Motivation: 当前大多数AI营养估算工具依赖图像输入，但大语言模型能否仅通过文本描述准确预测营养值尚不清楚。如果有效，这种方法可以实现无需照片的更简单饮食监测。

Method: 使用NHANES青少年24小时饮食回顾数据，采用10-shot思维链提示的开源量化LLM，仅基于食物和数量的文本字符串估算能量和五种宏量营养素，并应用参数高效微调(PEFT)评估预测准确性。

Result: 在11,281名青少年数据中，原始LLM预测效果差(能量MAE为652.08，Lin's CCC<0.46)。经过微调的模型表现显著改善，能量MAE降至171.34-190.90，所有结果的Lin's CCC均超过0.89。

Conclusion: 通过思维链提示和PEFT微调，开源LLM仅接触文本输入就能准确预测24小时饮食回顾中的能量和宏量营养素值，这为低负担的文本饮食监测工具提供了前景。

Abstract: BACKGROUND: Most artificial intelligence tools used to estimate nutritional
content rely on image input. However, whether large language models (LLMs) can
accurately predict nutritional values based solely on text descriptions of
foods consumed remains unknown. If effective, this approach could enable
simpler dietary monitoring without the need for photographs. METHODS: We used
24-hour dietary recalls from adolescents aged 12-19 years in the National
Health and Nutrition Examination Survey (NHANES). An open-source quantized LLM
was prompted using a 10-shot, chain-of-thought approach to estimate energy and
five macronutrients based solely on text strings listing foods and their
quantities. We then applied parameter-efficient fine-tuning (PEFT) to evaluate
whether predictive accuracy improved. NHANES-calculated values served as the
ground truth for energy, proteins, carbohydrates, total sugar, dietary fiber
and total fat. RESULTS: In a pooled dataset of 11,281 adolescents (49.9% male,
mean age 15.4 years), the vanilla LLM yielded poor predictions. The mean
absolute error (MAE) was 652.08 for energy and the Lin's CCC <0.46 across
endpoints. In contrast, the fine-tuned model performed substantially better,
with energy MAEs ranging from 171.34 to 190.90 across subsets, and Lin's CCC
exceeding 0.89 for all outcomes. CONCLUSIONS: When prompted using a
chain-of-thought approach and fine-tuned with PEFT, open-source LLMs exposed
solely to text input can accurately predict energy and macronutrient values
from 24-hour dietary recalls. This approach holds promise for low-burden,
text-based dietary monitoring tools.

</details>
