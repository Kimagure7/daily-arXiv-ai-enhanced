<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 13]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Towards Building efficient Routed systems for Retrieval](https://arxiv.org/abs/2601.06389)
*Ramnath Kumar,Prateek Jain,Cho-Jui Hsieh*

Main category: cs.IR

TL;DR: FastLane是一种新颖的检索框架，通过动态路由查询到最有效的表示，减少冗余令牌比较，将延迟交互模型与近似最近邻搜索结合，实现30倍计算效率提升。


<details>
  <summary>Details</summary>
Motivation: ColBERT等延迟交互检索模型虽然通过令牌级交互实现了高精度，但计算成本高，难以与近似最近邻搜索（ANNS）集成，限制了其在大规模应用中的可扩展性。

Method: FastLane采用可学习的路由机制，与嵌入模型联合优化，利用自注意力和可微分选择来动态路由查询到最有效的表示，消除冗余令牌比较。

Result: 计算复杂度降低高达30倍，同时保持有竞争力的检索性能，使延迟交互模型能够与ANNS结合，实现可扩展、低延迟的检索。

Conclusion: FastLane为大规模应用如搜索引擎、推荐系统和问答平台提供了可行的解决方案，并为多语言、多模态和长上下文检索开辟了新途径，推动了高效自适应信息检索的发展。

Abstract: Late-interaction retrieval models like ColBERT achieve superior accuracy by enabling token-level interactions, but their computational cost hinders scalability and integration with Approximate Nearest Neighbor Search (ANNS). We introduce FastLane, a novel retrieval framework that dynamically routes queries to their most informative representations, eliminating redundant token comparisons. FastLane employs a learnable routing mechanism optimized alongside the embedding model, leveraging self-attention and differentiable selection to maximize efficiency. Our approach reduces computational complexity by up to 30x while maintaining competitive retrieval performance. By bridging late-interaction models with ANNS, FastLane enables scalable, low-latency retrieval, making it feasible for large-scale applications such as search engines, recommendation systems, and question-answering platforms. This work opens pathways for multi-lingual, multi-modal, and long-context retrieval, pushing the frontier of efficient and adaptive information retrieval.

</details>


### [2] [PixRec: Leveraging Visual Context for Next-Item Prediction in Sequential Recommendation](https://arxiv.org/abs/2601.06458)
*Sayak Chakrabarty,Souradip Pal*

Main category: cs.IR

TL;DR: PixRec是一个视觉语言推荐框架，通过结合文本属性和产品图像，在顺序推荐任务中显著优于纯文本推荐模型


<details>
  <summary>Details</summary>
Motivation: 现有LLM在顺序推荐中主要依赖纯文本方法，忽略了电商等实际场景中丰富的视觉信息。视觉特征可以帮助区分文本描述相似的商品，提升推荐准确性

Method: 提出PixRec框架，采用视觉语言模型骨干网络，联合处理图像-文本序列，保持双塔结构和混合训练目标，对齐多模态特征投影以处理商品-商品和用户-商品交互

Result: 在Amazon Reviews数据集（增强产品图像）上，相比纯文本推荐器，top-rank准确率提升3倍，top-10准确率提升40%，证明视觉特征能有效区分文本相似的商品

Conclusion: 视觉信息在顺序推荐中具有重要价值，为构建利用视觉信息的软件系统（如电商推荐）迈出一步，未来方向包括扩展多模态推荐器训练、增强视觉-文本特征融合和评估推理性能

Abstract: Large Language Models (LLMs) have recently shown strong potential for usage in sequential recommendation tasks through text-only models, which combine advanced prompt design, contrastive alignment, and fine-tuning on downstream domain-specific data. While effective, these approaches overlook the rich visual information present in many real-world recommendation scenarios, particularly in e-commerce. This paper proposes PixRec - a vision-language framework that incorporates both textual attributes and product images into the recommendation pipeline. Our architecture leverages a vision-language model backbone capable of jointly processing image-text sequences, maintaining a dual-tower structure and mixed training objective while aligning multi-modal feature projections for both item-item and user-item interactions. Using the Amazon Reviews dataset augmented with product images, our experiments demonstrate $3\times$ and 40% improvements in top-rank and top-10 rank accuracy over text-only recommenders respectively, indicating that visual features can help distinguish items with similar textual descriptions. Our work outlines future directions for scaling multi-modal recommenders training, enhancing visual-text feature fusion, and evaluating inference-time performance. This work takes a step toward building software systems utilizing visual information in sequential recommendation for real-world applications like e-commerce.

</details>


### [3] [L-RAG: Balancing Context and Retrieval with Entropy-Based Lazy Loading](https://arxiv.org/abs/2601.06551)
*Sergii Voloshyn*

Main category: cs.IR

TL;DR: L-RAG是一种自适应检索增强生成框架，通过基于熵的门控机制实现分层上下文管理，仅在模型预测不确定性高时才触发昂贵的检索操作，从而在保持准确性的同时显著减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统采用"总是检索"的静态策略，无论查询复杂度如何都会查询向量数据库，导致计算开销大、推理延迟高，这在生产部署中尤为严重。需要一种更智能、自适应的方法来平衡准确性和效率。

Method: L-RAG采用两层架构：首先使用紧凑的文档摘要处理查询，只有当模型的预测熵超过校准阈值时才触发昂贵的分块检索。该方法基于熵作为不确定性信号，实现分层上下文管理。

Result: 在SQuAD 2.0数据集上使用Phi-2模型的实验显示：保守阈值(τ=0.5)下，L-RAG达到78.2%准确率（匹配标准RAG的77.8%），检索减少8%；平衡阈值(τ=1.0)下，检索减少26%，准确率略有下降(76.0%)。延迟分析显示每查询节省80-210ms。熵分布分析显示正确预测(H=1.72)和错误(H=2.20)之间存在显著差异(p<0.001)。

Conclusion: L-RAG提供了一种无需训练的实际方法，通过可配置的阈值在准确性和吞吐量之间实现平衡，为系统架构师提供了灵活的效率-准确性权衡工具，适用于高效RAG部署。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as the predominant paradigm for grounding Large Language Model outputs in factual knowledge, effectively mitigating hallucinations. However, conventional RAG systems operate under a "retrieve-always" assumption, querying vector databases for every input regardless of query complexity. This static approach incurs substantial computational overhead and inference latency, particularly problematic for high-throughput production deployments. We introduce L-RAG (Lazy Retrieval-Augmented Generation), an adaptive framework that implements hierarchical context management through entropy-based gating. L-RAG employs a two-tier architecture: queries are first processed with a compact document summary, and expensive chunk retrieval is triggered only when the model's predictive entropy exceeds a calibrated threshold, signaling genuine uncertainty. Through experiments on SQuAD 2.0 (N=500) using the Phi-2 model, we demonstrate that L-RAG provides a tunable accuracy-efficiency trade-off: at a conservative threshold (tau=0.5), L-RAG achieves 78.2% accuracy, matching Standard RAG (77.8%), with 8% retrieval reduction; at a balanced threshold (tau=1.0), retrieval reduction increases to 26% with modest accuracy trade-off (76.0%). Latency analysis shows that L-RAG saves 80-210ms per query when retrieval latency exceeds 500ms. Analysis of entropy distributions reveals statistically significant separation (p < 0.001) between correct predictions (H=1.72) and errors (H=2.20), validating entropy as a reliable uncertainty signal. L-RAG offers a practical, training-free approach toward more efficient RAG deployment, providing system architects with a configurable knob to balance accuracy and throughput requirements.

</details>


### [4] [Industrial Semantics-Aware Digital Twins: A Hybrid Graph Matching Approach for Asset Administration Shells](https://arxiv.org/abs/2601.06613)
*Ariana Metović,Nicolai Maisch,Samed Ajdinović,Armin Lechler,Andreas Wortmann,Oliver Riedel*

Main category: cs.IR

TL;DR: 提出一种结合规则过滤与嵌入相似度的混合图匹配方法，用于解决数字孪生AAS模型语义可比性问题


<details>
  <summary>Details</summary>
Motivation: 尽管AAS标准提供了工业资产的结构化表示，但由于不同词汇和建模实践，其语义可比性仍然是一个挑战。工程实践中需要检索相似AAS模型以重用子模型、参数和元数据，但异构词汇和建模惯例阻碍了自动化内容级比较。

Method: 提出混合图匹配方法：结合基于SPARQL的规则预过滤与基于RDF2vec的嵌入相似度计算，捕捉AAS模型间的结构和语义关系

Result: 该方法能够实现语义感知的数字孪生表示比较，为增强数字孪生网络中的发现、重用和自动配置提供基础

Conclusion: 混合图匹配方法有效解决了AAS模型语义可比性问题，支持工程实践中对现有模型的检索和重用，促进数字孪生网络的互操作性

Abstract: Although the Asset Administration Shell (AAS) standard provides a structured and machine-readable representation of industrial assets, their semantic comparability remains a major challenge, particularly when different vocabularies and modeling practices are used. Engineering would benefit from retrieving existing AAS models that are similar to the target in order to reuse submodels, parameters, and metadata. In practice, however, heterogeneous vocabularies and divergent modeling conventions hinder automated, content-level comparison across AAS. This paper proposes a hybrid graph matching approach to enable semantics-aware comparison of Digital Twin representations. The method combines rule-based pre-filtering using SPARQL with embedding-based similarity calculation leveraging RDF2vec to capture both structural and semantic relationships between AAS models. This contribution provides a foundation for enhanced discovery, reuse, and automated configuration in Digital Twin networks.

</details>


### [5] [Unleashing the Native Recommendation Potential: LLM-Based Generative Recommendation via Structured Term Identifiers](https://arxiv.org/abs/2601.06798)
*Zhiyang Zhang,Junda She,Kuo Cai,Bo Chen,Shiyao Wang,Xinchen Luo,Qiang Luo,Ruiming Tang,Han Li,Kun Gai,Guorui Zhou*

Main category: cs.IR

TL;DR: 提出Term IDs (TIDs)作为语义丰富的标准化文本关键词作为物品标识符，解决现有LLM推荐系统中文本标识符幻觉问题和语义ID与LLM词汇表不匹配的问题，通过GRLM框架实现高性能生成式推荐。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的推荐系统在构建物品标识符方面存在瓶颈：文本方法引入LLM的巨大输出空间导致幻觉问题，而基于语义ID的方法存在语义ID与LLM原生词汇表之间的语义鸿沟，需要昂贵的词汇表扩展和对齐训练。

Method: 提出Term IDs (TIDs)作为语义丰富的标准化文本关键词标识符，构建GRLM框架：1) 上下文感知术语生成将物品元数据转换为标准化TIDs；2) 集成指令微调协同优化术语内化和序列推荐；3) 弹性标识符接地实现鲁棒的物品映射。

Result: 在真实世界数据集上的大量实验表明，GRLM在多个场景下显著优于基线方法，为通用化和高性能的生成式推荐系统指明了有前景的方向。

Conclusion: TIDs作为语义丰富的标准化文本关键词，能够有效解决LLM推荐系统中的标识符问题，GRLM框架通过创新的术语生成、集成微调和弹性接地机制，实现了高性能的生成式推荐，为通用推荐系统的发展提供了新思路。

Abstract: Leveraging the vast open-world knowledge and understanding capabilities of Large Language Models (LLMs) to develop general-purpose, semantically-aware recommender systems has emerged as a pivotal research direction in generative recommendation. However, existing methods face bottlenecks in constructing item identifiers. Text-based methods introduce LLMs' vast output space, leading to hallucination, while methods based on Semantic IDs (SIDs) encounter a semantic gap between SIDs and LLMs' native vocabulary, requiring costly vocabulary expansion and alignment training. To address this, this paper introduces Term IDs (TIDs), defined as a set of semantically rich and standardized textual keywords, to serve as robust item identifiers. We propose GRLM, a novel framework centered on TIDs, employs Context-aware Term Generation to convert item's metadata into standardized TIDs and utilizes Integrative Instruction Fine-tuning to collaboratively optimize term internalization and sequential recommendation. Additionally, Elastic Identifier Grounding is designed for robust item mapping. Extensive experiments on real-world datasets demonstrate that GRLM significantly outperforms baselines across multiple scenarios, pointing a promising direction for generalizable and high-performance generative recommendation systems.

</details>


### [6] [Applying Embedding-Based Retrieval to Airbnb Search](https://arxiv.org/abs/2601.06873)
*Mustafa Abdool,Soumyadip Banerjee,Moutupsi Paul,Do-kyum Kim,Xioawei Liu,Bin Xu,Tracy Yu,Hui Gao,Karen Ouyang,Huiji Gao,Liwei He,Stephanie Moyerman,Sanjeev Katariya*

Main category: cs.IR

TL;DR: Airbnb构建了一个基于嵌入的检索系统来解决海量房源与多样化用户需求的匹配问题，该系统显著提升了预订转化率等关键指标。


<details>
  <summary>Details</summary>
Motivation: Airbnb搜索面临巨大挑战：热门搜索地点有数十万可用房源，用户偏好多样，且新产品功能（如灵活日期搜索）进一步增加了每次查询的合格房源数量，需要低延迟、高质量的检索系统与整体排序栈集成。

Method: 构建基于嵌入的检索系统，针对双边市场的独特挑战（库存动态变化、用户漏斗多阶段、产品界面多样）进行建模，建立鲁棒评估系统，并设计在线服务架构。

Result: EBR系统已投入生产，支持常规搜索、灵活日期搜索和营销推广邮件等多个用例，通过A/B测试证明在预订转化率等关键指标上取得了统计显著的改进。

Conclusion: 成功构建了高效高质量的检索系统，解决了Airbnb搜索中的独特挑战，为双边市场平台提供了有效的嵌入检索解决方案，并验证了其业务价值。

Abstract: The goal of Airbnb search is to match guests with the ideal accommodation that fits their travel needs. This is a challenging problem, as popular search locations can have around a hundred thousand available homes, and guests themselves have a wide variety of preferences. Furthermore, the launch of new product features, such as \textit{flexible date search,} significantly increased the number of eligible homes per search query. As such, there is a need for a sophisticated retrieval system which can provide high-quality candidates with low latency in a way that integrates with the overall ranking stack.
  This paper details our journey to build an efficient and high-quality retrieval system for Airbnb search. We describe the key unique challenges we encountered when implementing an Embedding-Based Retrieval (EBR) system for a two sided marketplace like Airbnb -- such as the dynamic nature of the inventory, a lengthy user funnel with multiple stages, and a variety of product surfaces. We cover unique insights when modeling the retrieval problem, how to build robust evaluation systems, and design choices for online serving. The EBR system was launched to production and powers several use-cases such as regular search, flexible date and promotional emails for marketing campaigns. The system demonstrated statistically-significant improvements in key metrics, such as booking conversion, via A/B testing.

</details>


### [7] [FinCARDS: Card-Based Analyst Reranking for Financial Document Question Answering](https://arxiv.org/abs/2601.06992)
*Yixi Zhou,Fan Zhang,Yu Chen,Haipeng Zhang,Preslav Nakov,Zhuohan Xie*

Main category: cs.IR

TL;DR: FinCards是一个结构化重排框架，将金融证据选择重构为金融感知模式下的约束满足问题，通过确定性字段级匹配和多阶段锦标赛重排，显著提升企业文件问答中的早期检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的重排方法主要优化语义相关性，导致在长文档上的排名不稳定且决策不透明，而金融问答需要满足实体、财务指标、会计期间和数值等严格约束。

Method: FinCards使用对齐的模式字段（实体、指标、期间和数值跨度）表示文件块和问题，通过确定性字段级匹配进行证据选择，采用多阶段锦标赛重排和稳定性感知聚合，产生可审计的决策轨迹。

Result: 在两个企业文件QA基准测试中，FinCards显著提升了早期检索效果，优于词汇和基于LLM的重排基线，同时减少了排名方差，且无需模型微调或不可预测的推理预算。

Conclusion: FinCards通过结构化重排框架有效解决了金融问答中的证据选择问题，提供了稳定、可审计的检索方案，在保持性能的同时降低了不确定性。

Abstract: Financial question answering (QA) over long corporate filings requires evidence to satisfy strict constraints on entities, financial metrics, fiscal periods, and numeric values. However, existing LLM-based rerankers primarily optimize semantic relevance, leading to unstable rankings and opaque decisions on long documents. We propose FinCards, a structured reranking framework that reframes financial evidence selection as constraint satisfaction under a finance-aware schema. FinCards represents filing chunks and questions using aligned schema fields (entities, metrics, periods, and numeric spans), enabling deterministic field-level matching. Evidence is selected via a multi-stage tournament reranking with stability-aware aggregation, producing auditable decision traces. Across two corporate filing QA benchmarks, FinCards substantially improves early-rank retrieval over both lexical and LLM-based reranking baselines, while reducing ranking variance, without requiring model fine-tuning or unpredictable inference budgets. Our code is available at https://github.com/XanderZhou2022/FINCARDS.

</details>


### [8] [ReinPool: Reinforcement Learning Pooling Multi-Vector Embeddings for Retrieval System](https://arxiv.org/abs/2601.07125)
*Sungguk Cha,DongWook Kim,Mintae Kim,Youngsub Han,Byoung-Ki Jeon,Sangyeob Lee*

Main category: cs.IR

TL;DR: ReinPool使用强化学习动态筛选和池化多向量嵌入，将存储成本降低746-1249倍，同时恢复76-81%的检索性能，相比静态池化方法NDCG@3提升22-33%。


<details>
  <summary>Details</summary>
Motivation: 多向量嵌入模型虽然能保留细粒度的视觉和文本细节，但存储每个token的嵌入会使索引大小膨胀1000倍以上，严重限制了可扩展性。需要一种方法在保持检索性能的同时大幅压缩表示。

Method: 提出ReinPool强化学习框架，通过逆检索目标和NDCG奖励训练，动态过滤和池化多向量嵌入为紧凑的检索优化表示，无需手动重要性标注。

Result: 在Vidore V2基准测试中，ReinPool将多向量表示压缩746-1249倍为单向量，恢复76-81%的全多向量检索性能，相比静态平均池化基线NDCG@3提升22-33%。

Conclusion: 学习型选择策略显著优于启发式聚合方法，能够在保持检索性能的同时大幅减少存储开销，为多向量嵌入的实际应用提供了可行的压缩方案。

Abstract: Multi-vector embedding models have emerged as a powerful paradigm for document retrieval, preserving fine-grained visual and textual details through token-level representations. However, this expressiveness comes at a staggering cost: storing embeddings for every token inflates index sizes by over $1000\times$ compared to single-vector approaches, severely limiting scalability. We introduce \textbf{ReinPool}, a reinforcement learning framework that learns to dynamically filter and pool multi-vector embeddings into compact, retrieval-optimized representations. By training with an inverse retrieval objective and NDCG-based rewards, ReinPool identifies and retains only the most discriminative vectors without requiring manual importance annotations. On the Vidore V2 benchmark across three vision-language embedding models, ReinPool compresses multi-vector representations by $746$--$1249\times$ into single vectors while recovering 76--81\% of full multi-vector retrieval performance. Compared to static mean pooling baselines, ReinPool achieves 22--33\% absolute NDCG@3 improvement, demonstrating that learned selection significantly outperforms heuristic aggregation.

</details>


### [9] [Towards Multi-Behavior Multi-Task Recommendation via Behavior-informed Graph Embedding Learning](https://arxiv.org/abs/2601.07294)
*Wenhao Lai,Weike Pan,Zhong Ming*

Main category: cs.IR

TL;DR: 提出BiGEL方法解决多行为多任务推荐问题，通过级联图范式结合三个关键模块提升各行为类型的推荐性能


<details>
  <summary>Details</summary>
Motivation: 现有多行为推荐方法通常采用级联图范式，但主要优化目标行为（如购买）而忽视辅助行为（如点击、收藏）的性能。在多行为多任务推荐场景中，需要为每种行为类型生成个性化推荐列表，现有方法存在不足

Method: 提出行为感知图嵌入学习（BiGEL）方法：1）使用级联图范式获取行为感知嵌入；2）引入级联门控反馈模块，通过目标行为反馈优化辅助行为偏好；3）全局上下文增强模块整合全局上下文保持用户整体偏好；4）对比偏好对齐模块通过对比学习对齐目标行为偏好与全局偏好

Result: 在两个真实世界数据集上的广泛实验表明，BiGEL相比十种竞争方法具有显著有效性

Conclusion: BiGEL方法通过创新的级联反馈、全局上下文整合和对比偏好对齐机制，有效解决了多行为多任务推荐问题，在保持目标行为性能的同时显著提升了辅助行为的推荐质量

Abstract: Multi-behavior recommendation (MBR) aims to improve the performance w.r.t. the target behavior (i.e., purchase) by leveraging auxiliary behaviors (e.g., click, favourite). However, in real-world scenarios, a recommendation method often needs to process different types of behaviors and generate personalized lists for each task (i.e., each behavior type). Such a new recommendation problem is referred to as multi-behavior multi-task recommendation (MMR). So far, the most powerful MBR methods usually model multi-behavior interactions using a cascading graph paradigm. Although significant progress has been made in optimizing the performance of the target behavior, it often neglects the performance of auxiliary behaviors. To compensate for the deficiencies of the cascading paradigm, we propose a novel solution for MMR, i.e., behavior-informed graph embedding learning (BiGEL). Specifically, we first obtain a set of behavior-aware embeddings by using a cascading graph paradigm. Subsequently, we introduce three key modules to improve the performance of the model. The cascading gated feedback (CGF) module enables a feedback-driven optimization process by integrating feedback from the target behavior to refine the auxiliary behaviors preferences. The global context enhancement (GCE) module integrates the global context to maintain the user's overall preferences, preventing the loss of key preferences due to individual behavior graph modeling. Finally, the contrastive preference alignment (CPA) module addresses the potential changes in user preferences during the cascading process by aligning the preferences of the target behaviors with the global preferences through contrastive learning. Extensive experiments on two real-world datasets demonstrate the effectiveness of our BiGEL compared with ten very competitive methods.

</details>


### [10] [RLPO: Residual Listwise Preference Optimization for Long-Context Review Ranking](https://arxiv.org/abs/2601.07449)
*Hao Jiang,Zhi Yang,Annan Wang,Yichi Zhang,Weisi Lin*

Main category: cs.IR

TL;DR: RLPO通过结合点式评分和列表式残差校正，解决了长上下文评论排名中的效率与准确性权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有评论排名方法在长上下文场景中存在权衡：点式评分效率高但忽略列表级交互，导致排名不准；列表式方法能利用全局上下文但计算成本高且不稳定。需要一种兼顾效率与准确性的解决方案。

Method: 提出残差列表偏好优化(RLPO)：先使用强大的点式LLM评分器生成校准后的点式分数和项目表示，然后通过轻量级编码器在表示层面预测列表式分数残差，避免完整的token级列表处理。

Result: 实验表明RLPO在NDCG@k指标上优于强大的点式和列表式基线方法，且在列表长度增加时保持稳健性。

Conclusion: RLPO通过将点式评分的效率与列表式残差校正的准确性相结合，为长上下文评论排名提供了有效的解决方案，平衡了计算效率与排名质量。

Abstract: Review ranking is pivotal in e-commerce for prioritizing diagnostic and authentic feedback from the deluge of user-generated content. While large language models have improved semantic assessment, existing ranking paradigms face a persistent trade-off in long-context settings. Pointwise scoring is efficient but often fails to account for list-level interactions, leading to miscalibrated top-$k$ rankings. Listwise approaches can leverage global context, yet they are computationally expensive and become unstable as candidate lists grow. To address this, we propose Residual Listwise Preference Optimization (RLPO), which formulates ranking as listwise representation-level residual correction over a strong pointwise LLM scorer. RLPO first produces calibrated pointwise scores and item representations, then applies a lightweight encoder over the representations to predict listwise score residuals, avoiding full token-level listwise processing. We also introduce a large-scale benchmark for long-context review ranking with human verification. Experiments show RLPO improves NDCG@k over strong pointwise and listwise baselines and remains robust as list length increases.

</details>


### [11] [Loci Similes: A Benchmark for Extracting Intertextualities in Latin Literature](https://arxiv.org/abs/2601.07533)
*Julian Schelb,Michael Wittweiler,Marie Revellio,Barbara Feichtinger,Andreas Spitz*

Main category: cs.IR

TL;DR: Loci Similes：一个用于拉丁语互文性检测的基准数据集，包含约17.2万文本片段和545个专家验证的平行文本，旨在解决该领域缺乏标准化基准的问题。


<details>
  <summary>Details</summary>
Motivation: 历史文本间的互文连接研究对于重建作者虚拟图书馆和识别创作来源至关重要，但当前缺乏标准化基准和易用数据集阻碍了新方法的发展。

Method: 创建Loci Similes基准数据集，包含约172k文本片段和545个专家验证的平行文本，使用最先进的LLM建立检索和分类的基线方法。

Result: 建立了包含Late Antique作者与古典作者之间互文连接的标准化数据集，为互文性检测提供了可复现的基准。

Conclusion: Loci Similes填补了拉丁语互文性检测领域缺乏标准化基准的空白，为未来研究提供了可靠的数据基础和方法评估框架。

Abstract: Tracing connections between historical texts is an important part of intertextual research, enabling scholars to reconstruct the virtual library of a writer and identify the sources influencing their creative process. These intertextual links manifest in diverse forms, ranging from direct verbatim quotations to subtle allusions and paraphrases disguised by morphological variation. Language models offer a promising path forward due to their capability of capturing semantic similarity beyond lexical overlap. However, the development of new methods for this task is held back by the scarcity of standardized benchmarks and easy-to-use datasets. We address this gap by introducing Loci Similes, a benchmark for Latin intertextuality detection comprising of a curated dataset of ~172k text segments containing 545 expert-verified parallels linking Late Antique authors to a corpus of classical authors. Using this data, we establish baselines for retrieval and classification of intertextualities with state-of-the-art LLMs.

</details>


### [12] [GAP-Net: Calibrating User Intent via Gated Adaptive Progressive Learning for CTR Prediction](https://arxiv.org/abs/2601.07613)
*Ke Shenqiang,Wei Jianxiong,Hua Qingsong*

Main category: cs.IR

TL;DR: GAP-Net提出三重门控架构解决CTR预测中序列行为建模的三个瓶颈：注意力沉没、静态查询假设和刚性视图聚合，通过自适应稀疏门控注意力、门控级联查询校准和上下文门控去噪融合实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: CTR预测中的序列用户行为建模存在三个内在瓶颈：1）注意力沉没现象，标准Softmax迫使模型为噪声行为分配概率质量；2）静态查询假设，忽略了实时上下文驱动的用户意图动态变化；3）刚性视图聚合，无法根据决策上下文自适应加权异构时序信号。

Method: 提出GAP-Net（门控自适应渐进网络），建立"三重门控"架构，通过三个集成机制：1）自适应稀疏门控注意力（ASGA）进行微级门控以强制稀疏性；2）门控级联查询校准（GCQC）通过中观级联通道动态对齐用户意图；3）上下文门控去噪融合（CGDF）进行宏观级调制以协调多视图序列聚合。

Result: 在工业数据集上的大量实验表明，GAP-Net相比最先进的基线方法实现了显著改进，在交互噪声和意图漂移方面表现出卓越的鲁棒性。

Conclusion: GAP-Net通过统一的三重门控框架有效解决了CTR预测中序列行为建模的三个关键瓶颈，为处理噪声交互和动态用户意图提供了强大的解决方案。

Abstract: Sequential user behavior modeling is pivotal for Click-Through Rate (CTR) prediction yet is hindered by three intrinsic bottlenecks: (1) the "Attention Sink" phenomenon, where standard Softmax compels the model to allocate probability mass to noisy behaviors; (2) the Static Query Assumption, which overlooks dynamic shifts in user intent driven by real-time contexts; and (3) Rigid View Aggregation, which fails to adaptively weight heterogeneous temporal signals according to the decision context. To bridge these gaps, we propose GAP-Net (Gated Adaptive Progressive Network), a unified framework establishing a "Triple Gating" architecture to progressively refine information from micro-level features to macro-level views. GAP-Net operates through three integrated mechanisms: (1) Adaptive Sparse-Gated Attention (ASGA) employs micro-level gating to enforce sparsity, effectively suppressing massive noise activations; (2) Gated Cascading Query Calibration (GCQC) dynamically aligns user intent by bridging real-time triggers and long-term memories via a meso-level cascading channel; and (3) Context-Gated Denoising Fusion (CGDF) performs macro-level modulation to orchestrate the aggregation of multi-view sequences. Extensive experiments on industrial datasets demonstrate that GAP-Net achieves substantial improvements over state-of-the-art baselines, exhibiting superior robustness against interaction noise and intent drift.

</details>


### [13] [AptaFind: A lightweight local interface for automated aptamer curation from scientific literature](https://arxiv.org/abs/2601.07684)
*Geoffrey Taghon*

Main category: cs.IR

TL;DR: AptaFind是一个三级智能架构平台，通过结合本地语言模型和确定性算法，为适配体研究人员提供序列提取、研究线索和文献发现功能，显著提高文献挖掘效率。


<details>
  <summary>Details</summary>
Motivation: 适配体研究文献分散在出版物、补充材料和数据库中，每次搜索耗时数小时，研究人员需要更高效的文献挖掘工具来节省时间。

Method: 采用三级智能架构：1) 直接序列提取；2) 当提取失败时提供策划的研究线索；3) 全面的文献发现以增加置信度。结合本地语言模型进行语义理解和确定性算法确保可靠性，无需云依赖或订阅障碍。

Result: 在300个德克萨斯大学适配体数据库目标上验证显示：84%找到相关文献，84%获得策划研究线索，79%实现直接序列提取，处理速度超过900个目标/小时。

Conclusion: AptaFind证明即使直接序列提取失败，自动化仍能通过快速缩小搜索范围到高质量参考文献，为研究人员提供可操作的情报，显著提高适配体文献挖掘效率。

Abstract: Aptamer researchers face a literature landscape scattered across publications, supplements, and databases, with each search consuming hours that could be spent at the bench. AptaFind transforms this navigation problem through a three-tier intelligence architecture that recognizes research mining is a spectrum, not a binary success or failure. The system delivers direct sequence extraction when possible, curated research leads when extraction fails, and exhaustive literature discovery for additional confidence. By combining local language models for semantic understanding with deterministic algorithms for reliability, AptaFind operates without cloud dependencies or subscription barriers. Validation across 300 University of Texas Aptamer Database targets demonstrates 84 % with some literature found, 84 % with curated research leads, and 79 % with a direct sequence extraction, at a laptop-compute rate of over 900 targets an hour. The platform proves that even when direct sequence extraction fails, automation can still deliver the actionable intelligence researchers need by rapidly narrowing the search to high quality references.

</details>
