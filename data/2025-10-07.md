<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 11]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Evaluating High-Resolution Piano Sustain Pedal Depth Estimation with Musically Informed Metrics](https://arxiv.org/abs/2510.03750)
*Hanwen Zhang,Kun Fang,Ziyu Wang,Ichiro Fujinaga*

Main category: cs.IR

TL;DR: 提出了一个钢琴踏板深度估计的评估框架，在传统帧级指标基础上增加了动作级和手势级分析，能够更好地捕捉音乐相关特征。


<details>
  <summary>Details</summary>
Motivation: 传统帧级指标无法充分评估钢琴踏板深度估计任务，因为它们忽略了音乐上重要的特征如方向变化边界和踏板曲线轮廓。

Method: 提出了一个三层次评估框架：帧级指标、动作级评估（测量方向和时序）和手势级分析（评估每个按压-释放周期的轮廓相似性）。应用该框架比较了音频基线模型和两个变体。

Result: 结果显示，结合MIDI符号信息的模型在动作级和手势级表现显著优于其他模型，尽管帧级增益有限。

Conclusion: 该评估框架能够捕捉传统指标无法识别的音乐相关改进，为踏板深度估计模型提供了更实用有效的评估方法。

Abstract: Evaluation for continuous piano pedal depth estimation tasks remains
incomplete when relying only on conventional frame-level metrics, which
overlook musically important features such as direction-change boundaries and
pedal curve contours. To provide more interpretable and musically meaningful
insights, we propose an evaluation framework that augments standard frame-level
metrics with an action-level assessment measuring direction and timing using
segments of press/hold/release states and a gesture-level analysis that
evaluates contour similarity of each press-release cycle. We apply this
framework to compare an audio-only baseline with two variants: one
incorporating symbolic information from MIDI, and another trained in a
binary-valued setting, all within a unified architecture. Results show that the
MIDI-informed model significantly outperforms the others at action and gesture
levels, despite modest frame-level gains. These findings demonstrate that our
framework captures musically relevant improvements indiscernible by traditional
metrics, offering a more practical and effective approach to evaluating pedal
depth estimation models.

</details>


### [2] [Investigating LLM Variability in Personalized Conversational Information Retrieval](https://arxiv.org/abs/2510.03795)
*Simon Lupart,Daniël van Dijk,Eric Langezaal,Ian van Dort,Mohammad Aliannejadi*

Main category: cs.IR

TL;DR: 这篇可重复性研究重新评估了在个性化对话信息检索中使用个人文本知识库的效果，发现人工选择的PTKB能持续提升检索性能，而LLM选择方法不可靠，并强调多轮评估和方差报告的重要性。


<details>
  <summary>Details</summary>
Motivation: 先前研究基于GPT-3.5 Turbo单次实验得出PTKB个性化可能有害的结论，但存在输出变异性和可重复性问题，需要更严谨的验证。

Method: 使用TREC iKAT 2024数据集，评估多种模型（Llama 1B-70B、Qwen-7B、GPT-4o-mini），比较人工选择与LLM选择的PTKB效果，分析不同数据集和指标的方差。

Result: 人工选择的PTKB能持续提升检索性能；LLM选择方法不可靠；iKAT数据集比CAsT方差更高；召回指标比精度指标方差更低。

Conclusion: 需要多轮评估和方差报告来评估基于LLM的CIR系统，为个性化CIR提供更稳健和可泛化的实践指导。

Abstract: Personalized Conversational Information Retrieval (CIR) has seen rapid
progress in recent years, driven by the development of Large Language Models
(LLMs). Personalized CIR aims to enhance document retrieval by leveraging
user-specific information, such as preferences, knowledge, or constraints, to
tailor responses to individual needs. A key resource for this task is the TREC
iKAT 2023 dataset, designed to evaluate personalization in CIR pipelines.
Building on this resource, Mo et al. explored several strategies for
incorporating Personal Textual Knowledge Bases (PTKB) into LLM-based query
reformulation. Their findings suggested that personalization from PTKBs could
be detrimental and that human annotations were often noisy. However, these
conclusions were based on single-run experiments using the GPT-3.5 Turbo model,
raising concerns about output variability and repeatability. In this
reproducibility study, we rigorously reproduce and extend their work, focusing
on LLM output variability and model generalization. We apply the original
methods to the new TREC iKAT 2024 dataset and evaluate a diverse range of
models, including Llama (1B-70B), Qwen-7B, GPT-4o-mini. Our results show that
human-selected PTKBs consistently enhance retrieval performance, while
LLM-based selection methods do not reliably outperform manual choices. We
further compare variance across datasets and observe higher variability on iKAT
than on CAsT, highlighting the challenges of evaluating personalized CIR.
Notably, recall-oriented metrics exhibit lower variance than precision-oriented
ones, a critical insight for first-stage retrievers. Finally, we underscore the
need for multi-run evaluations and variance reporting when assessing LLM-based
CIR systems. By broadening evaluation across models, datasets, and metrics, our
study contributes to more robust and generalizable practices for personalized
CIR.

</details>


### [3] [Beyond Static Evaluation: Rethinking the Assessment of Personalized Agent Adaptability in Information Retrieval](https://arxiv.org/abs/2510.03984)
*Kirandeep Kaur,Preetam Prabhu Srikar Dammu,Hideo Joho,Chirag Shah*

Main category: cs.IR

TL;DR: 本文提出了一种评估个性化AI代理的新概念框架，从静态性能快照转向交互感知的演化评估，包含基于角色的用户模拟、结构化偏好提取协议和适应感知评估机制。


<details>
  <summary>Details</summary>
Motivation: 当前个性化AI代理评估方法多为静态基准测试，无法反映用户需求随时间演变的动态特性，限制了评估代理在长期交互中适应能力的有效性。

Method: 提出了包含三个核心组件的概念框架：1) 基于角色的用户模拟与时序演化偏好模型；2) 受参考访谈启发的结构化偏好提取协议；3) 测量代理跨会话和任务行为改进的适应感知评估机制。

Result: 通过电子商务搜索案例研究（使用PersonalWAB数据集）验证了该框架的可行性，展示了评估个性化代理随时间演化的有效性。

Conclusion: 该工作为理解和评估个性化作为连续、以用户为中心的过程奠定了概念基础，推动了评估方法从静态快照向动态交互感知的转变。

Abstract: Personalized AI agents are becoming central to modern information retrieval,
yet most evaluation methodologies remain static, relying on fixed benchmarks
and one-off metrics that fail to reflect how users' needs evolve over time.
These limitations hinder our ability to assess whether agents can meaningfully
adapt to individuals across dynamic, longitudinal interactions. In this
perspective paper, we propose a conceptual lens for rethinking evaluation in
adaptive personalization, shifting the focus from static performance snapshots
to interaction-aware, evolving assessments. We organize this lens around three
core components: (1) persona-based user simulation with temporally evolving
preference models; (2) structured elicitation protocols inspired by reference
interviews to extract preferences in context; and (3) adaptation-aware
evaluation mechanisms that measure how agent behavior improves across sessions
and tasks. While recent works have embraced LLM-driven user simulation, we
situate this practice within a broader paradigm for evaluating agents over
time. To illustrate our ideas, we conduct a case study in e-commerce search
using the PersonalWAB dataset. Beyond presenting a framework, our work lays a
conceptual foundation for understanding and evaluating personalization as a
continuous, user-centric endeavor.

</details>


### [4] [Visual Lifelog Retrieval through Captioning-Enhanced Interpretation](https://arxiv.org/abs/2510.04010)
*Yu-Fei Shih,An-Zi Yen,Hen-Hsen Huang,Hsin-Hsi Chen*

Main category: cs.IR

TL;DR: 提出了一个集成字幕的视觉生活日志检索系统(CIVIL)，通过生成字幕和使用文本嵌入模型来改进基于文本查询的生活日志图像检索效果。


<details>
  <summary>Details</summary>
Motivation: 人们经常难以记住过去经历的具体细节，需要重新访问这些记忆。生活日志检索已成为重要应用，需要帮助用户快速访问个人生活日志以辅助记忆回忆。

Method: 系统首先生成视觉生活日志的字幕，然后使用文本嵌入模型将字幕和用户查询投影到共享向量空间。提出了三种字幕方法：单字幕法、集体字幕法和合并字幕法，专门用于解释第一人称视角的生活体验。

Result: 实验结果表明，该方法能有效描述第一人称视觉图像，提升了生活日志检索的效果。还构建了将视觉生活日志转换为字幕的文本数据集。

Conclusion: 提出的CIVIL系统通过字幕生成和文本嵌入方法，成功改善了基于文本查询的视觉生活日志检索性能，能够更好地重构个人生活体验。

Abstract: People often struggle to remember specific details of past experiences, which
can lead to the need to revisit these memories. Consequently, lifelog retrieval
has emerged as a crucial application. Various studies have explored methods to
facilitate rapid access to personal lifelogs for memory recall assistance. In
this paper, we propose a Captioning-Integrated Visual Lifelog (CIVIL) Retrieval
System for extracting specific images from a user's visual lifelog based on
textual queries. Unlike traditional embedding-based methods, our system first
generates captions for visual lifelogs and then utilizes a text embedding model
to project both the captions and user queries into a shared vector space.
Visual lifelogs, captured through wearable cameras, provide a first-person
viewpoint, necessitating the interpretation of the activities of the individual
behind the camera rather than merely describing the scene. To address this, we
introduce three distinct approaches: the single caption method, the collective
caption method, and the merged caption method, each designed to interpret the
life experiences of lifeloggers. Experimental results show that our method
effectively describes first-person visual images, enhancing the outcomes of
lifelog retrieval. Furthermore, we construct a textual dataset that converts
visual lifelogs into captions, thereby reconstructing personal life
experiences.

</details>


### [5] [The LCLStream Ecosystem for Multi-Institutional Dataset Exploration](https://arxiv.org/abs/2510.04012)
*David Rogers,Valerio Mariani,Cong Wang,Ryan Coffee,Wilko Kroeger,Murali Shankar,Hans Thorsten Schwander,Tom Beck,Frédéric Poitevin,Jana Thayer*

Main category: cs.IR

TL;DR: 开发了一个新的端到端实验数据流框架，支持AI训练、高速率X射线飞行时间分析、分布式晶体结构测定等新型应用，融合云微服务和传统HPC批处理模型。


<details>
  <summary>Details</summary>
Motivation: 为X射线科学数据分析社区解决对高速数据流源的需求，通过创建灵活的API驱动数据请求服务，支持新型应用开发。

Method: 采用融合云微服务和传统HPC批处理模型的设计选择，包含数据请求API、相互认证Web安全框架、作业队列系统、高速数据缓冲区等组件。

Result: 成功原型化和实现了几个对未来实验至关重要的新范式，为DOE综合研究基础设施做出了独特贡献。

Conclusion: LCLStreamer框架通过其灵活性和互补性，为下一代实验提供了关键的数据流处理能力。

Abstract: We describe a new end-to-end experimental data streaming framework designed
from the ground up to support new types of applications -- AI training,
extremely high-rate X-ray time-of-flight analysis, crystal structure
determination with distributed processing, and custom data science applications
and visualizers yet to be created. Throughout, we use design choices merging
cloud microservices with traditional HPC batch execution models for security
and flexibility. This project makes a unique contribution to the DOE Integrated
Research Infrastructure (IRI) landscape. By creating a flexible, API-driven
data request service, we address a significant need for high-speed data
streaming sources for the X-ray science data analysis community. With the
combination of data request API, mutual authentication web security framework,
job queue system, high-rate data buffer, and complementary nature to facility
infrastructure, the LCLStreamer framework has prototyped and implemented
several new paradigms critical for future generation experiments.

</details>


### [6] [RLRF: Competitive Search Agent Design via Reinforcement Learning from Ranker Feedback](https://arxiv.org/abs/2510.04096)
*Tommy Mordo,Sagie Dekel,Omer Madmon,Moshe Tennenholtz,Oren Kurland*

Main category: cs.IR

TL;DR: 提出了RLRF框架，使用从排名竞赛中获得的偏好数据集来训练LLM，以优化竞争性搜索中的文档排名性能。


<details>
  <summary>Details</summary>
Motivation: 随着出版商越来越多地使用LLM生成和修改竞争性内容，需要开发能够优化排名性能并考虑竞争对手策略的方法。

Method: 使用强化学习从排名器反馈中学习，基于排名竞赛生成偏好数据集，不依赖人工编写的数据。

Result: 提出的智能体在LLM竞争性文档修改方面显著优于先前方法，能够适应未见过的排名函数和战略性对手。

Conclusion: 强化学习在竞争性搜索中具有巨大潜力，RLRF框架能够有效提升文档排名性能。

Abstract: Competitive search is a setting where document publishers modify them to
improve their ranking in response to a query. Recently, publishers have
increasingly leveraged LLMs to generate and modify competitive content. We
introduce Reinforcement Learning from Ranker Feedback (RLRF), a framework that
trains LLMs using preference datasets derived from ranking competitions. The
goal of a publisher (LLM-based) agent is to optimize content for improved
ranking while accounting for the strategies of competing agents. We generate
the datasets using approaches that do not rely on human-authored data. We show
that our proposed agents consistently and substantially outperform previously
suggested approaches for LLM-based competitive document modification. We
further show that our agents are effective with ranking functions they were not
trained for (i.e., out of distribution) and they adapt to strategic opponents.
These findings provide support to the significant potential of using
reinforcement learning in competitive search.

</details>


### [7] [Learning-Based Hashing for ANN Search: Foundations and Early Advances](https://arxiv.org/abs/2510.04127)
*Sean Moran*

Main category: cs.IR

TL;DR: 该论文是对早期基于学习的哈希方法的系统性综述，重点介绍近似最近邻搜索中哈希技术的核心概念、历史发展和基础原理。


<details>
  <summary>Details</summary>
Motivation: 近似最近邻搜索是信息检索中的基础问题，哈希方法通过将高维数据映射为紧凑二进制码来支持快速相似性计算。该综述旨在介绍基于学习哈希的概念基础，而非详尽罗列最新方法。

Method: 回顾监督、无监督和半监督的哈希方法，分析投影函数如何生成有意义的嵌入表示，以及量化策略如何将这些嵌入转换为二进制码。还探讨了多比特、多阈值模型和跨模态检索的早期进展。

Result: 提供了基于学习哈希方法的结构化理解框架，涵盖了该领域的基本原则、权衡取舍和开放挑战。

Conclusion: 通过将早期模型置于历史背景中，该综述为读者提供了理解当前该领域研究的原则、权衡和挑战的结构化知识基础。

Abstract: Approximate Nearest Neighbour (ANN) search is a fundamental problem in
information retrieval, underpinning large-scale applications in computer
vision, natural language processing, and cross-modal search. Hashing-based
methods provide an efficient solution by mapping high-dimensional data into
compact binary codes that enable fast similarity computations in Hamming space.
Over the past two decades, a substantial body of work has explored learning to
hash, where projection and quantisation functions are optimised from data
rather than chosen at random.
  This article offers a foundational survey of early learning-based hashing
methods, with an emphasis on the core ideas that shaped the field. We review
supervised, unsupervised, and semi-supervised approaches, highlighting how
projection functions are designed to generate meaningful embeddings and how
quantisation strategies convert these embeddings into binary codes. We also
examine extensions to multi-bit and multi-threshold models, as well as early
advances in cross-modal retrieval.
  Rather than providing an exhaustive account of the most recent methods, our
goal is to introduce the conceptual foundations of learning-based hashing for
ANN search. By situating these early models in their historical context, we aim
to equip readers with a structured understanding of the principles, trade-offs,
and open challenges that continue to inform current research in this area.

</details>


### [8] [Empowering Denoising Sequential Recommendation with Large Language Model Embeddings](https://arxiv.org/abs/2510.04239)
*Tongzhou Wu,Yuhao Wang,Maolin Wang,Chi Zhang,Xiangyu Zhao*

Main category: cs.IR

TL;DR: 提出IADSR框架，通过整合协同信息和语义信息来解决序列推荐中的噪声问题，避免过度去噪，特别针对冷门物品。


<details>
  <summary>Details</summary>
Motivation: 传统序列推荐模型容易受到噪声交互的影响，仅依赖协同信息可能导致过度去噪问题，特别是对冷门物品效果不佳。

Method: 两阶段框架：第一阶段从传统序列推荐模型和LLM分别获取物品的协同嵌入和语义嵌入；第二阶段对齐两种嵌入，基于长期和短期兴趣识别交互序列中的噪声。

Result: 在四个公共数据集上的广泛实验验证了该框架的有效性及其与不同序列推荐系统的兼容性。

Conclusion: IADSR框架通过整合协同和语义信息，有效解决了序列推荐中的噪声问题，特别改善了冷门物品的处理效果。

Abstract: Sequential recommendation aims to capture user preferences by modeling
sequential patterns in user-item interactions. However, these models are often
influenced by noise such as accidental interactions, leading to suboptimal
performance. Therefore, to reduce the effect of noise, some works propose
explicitly identifying and removing noisy items. However, we find that simply
relying on collaborative information may result in an over-denoising problem,
especially for cold items. To overcome these limitations, we propose a novel
framework: Interest Alignment for Denoising Sequential Recommendation (IADSR)
which integrates both collaborative and semantic information. Specifically,
IADSR is comprised of two stages: in the first stage, we obtain the
collaborative and semantic embeddings of each item from a traditional
sequential recommendation model and an LLM, respectively. In the second stage,
we align the collaborative and semantic embeddings and then identify noise in
the interaction sequence based on long-term and short-term interests captured
in the collaborative and semantic modalities. Our extensive experiments on four
public datasets validate the effectiveness of the proposed framework and its
compatibility with different sequential recommendation systems.

</details>


### [9] [Causality-aware Graph Aggregation Weight Estimator for Popularity Debiasing in Top-K Recommendation](https://arxiv.org/abs/2510.04502)
*Yue Que,Yingyi Zhang,Xiangyu Zhao,Chen Ma*

Main category: cs.IR

TL;DR: 提出CAGED方法，通过因果推理建模图聚合过程来缓解推荐系统中的流行度偏差问题


<details>
  <summary>Details</summary>
Motivation: 现有基于图的去偏方法在信息传播过程中存在回声效应，无法充分缓解流行度偏差，缺乏聚合合理性洞察和训练-去偏平衡

Method: 将图聚合建模为因果推理中的后门调整，设计编码器-解码器架构CAGED，通过优化交互似然证据下界来估计无偏聚合权重，并采用动量更新策略

Result: 在三个数据集上的广泛实验表明，CAGED优于现有的基于图去偏方法

Conclusion: 通过因果推理视角建模图聚合过程，CAGED能有效缓解推荐系统中的流行度偏差问题

Abstract: Graph-based recommender systems leverage neighborhood aggregation to generate
node representations, which is highly sensitive to popularity bias, resulting
in an echo effect during information propagation. Existing graph-based
debiasing solutions refine the aggregation process with attempts such as edge
reconstruction or weight adjustment. However, these methods remain inadequate
in fully alleviating popularity bias. Specifically, this is because 1) they
provide no insights into graph aggregation rationality, thus lacking an
optimality guarantee; 2) they fail to well balance the training and debiasing
process, which undermines the effectiveness. In this paper, we propose a novel
approach to mitigate popularity bias through rational modeling of the graph
aggregation process. We reveal that graph aggregation is a special form of
backdoor adjustment in causal inference, where the aggregation weight
corresponds to the historical interaction likelihood distribution. Based on
this insight, we devise an encoder-decoder architecture, namely Causality-aware
Graph Aggregation Weight Estimator for Debiasing (CAGED), to approximate the
unbiased aggregation weight by optimizing the evidence lower bound of the
interaction likelihood. In order to enhance the debiasing effectiveness during
early training stages, we further design a momentum update strategy that
incrementally refines the aggregation weight matrix. Extensive experiments on
three datasets demonstrate that CAGED outperforms existing graph-based
debiasing methods. Our implementation is available at
https://github.com/QueYork/CAGED.

</details>


### [10] [MARCO: A Cooperative Knowledge Transfer Framework for Personalized Cross-domain Recommendations](https://arxiv.org/abs/2510.04508)
*Lili Xie,Yi Zhang,Ruihong Qiu,Jiajun Liu,Sen Wang*

Main category: cs.IR

TL;DR: 提出MARCO框架，使用多智能体强化学习解决跨域推荐中的负迁移问题，通过多个智能体分别处理不同源域的知识迁移，提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统单智能体强化学习在跨域推荐中因域间贡献不一致和分布差异导致的负迁移问题，提升冷启动场景下的推荐效果。

Method: 采用协作式多智能体强化学习框架，每个智能体专注于估计单个源域的贡献，并引入基于熵的动作多样性惩罚来增强策略表达能力。

Result: 在四个基准数据集上的实验表明，MARCO优于现有最先进方法，展现出强大的鲁棒性和泛化能力。

Conclusion: MARCO通过多智能体协作有效缓解了跨域推荐中的负迁移问题，为处理数据稀疏性和冷启动问题提供了有效解决方案。

Abstract: Recommender systems frequently encounter data sparsity issues, particularly
when addressing cold-start scenarios involving new users or items. Multi-source
cross-domain recommendation (CDR) addresses these challenges by transferring
valuable knowledge from multiple source domains to enhance recommendations in a
target domain. However, existing reinforcement learning (RL)-based CDR methods
typically rely on a single-agent framework, leading to negative transfer issues
caused by inconsistent domain contributions and inherent distributional
discrepancies among source domains. To overcome these limitations, MARCO, a
Multi-Agent Reinforcement Learning-based Cross-Domain recommendation framework,
is proposed. It leverages cooperative multi-agent reinforcement learning, where
each agent is dedicated to estimating the contribution from an individual
source domain, effectively managing credit assignment and mitigating negative
transfer. In addition, an entropy-based action diversity penalty is introduced
to enhance policy expressiveness and stabilize training by encouraging diverse
agents' joint actions. Extensive experiments across four benchmark datasets
demonstrate MARCO's superior performance over state-of-the-art methods,
highlighting its robustness and strong generalization capabilities. The code is
at https://github.com/xiewilliams/MARCO.

</details>


### [11] [Topic-Specific Classifiers are Better Relevance Judges than Prompted LLMs](https://arxiv.org/abs/2510.04633)
*Lukas Gienapp,Martin Potthast,Harrisen Scells,Eugene Yang*

Main category: cs.IR

TL;DR: 提出训练主题特定相关性分类器来解决未评判文档问题，通过微调monoT5模型并适配单个评估者的相关性判断，实现与人工评估高度一致的系统排序。


<details>
  <summary>Details</summary>
Motivation: 解决测试集合中未评判文档对检索系统评估的影响，避免现有LLM作为评判者方法的循环性问题，同时保持人工判断作为评估黄金标准。

Method: 使用独立LoRA权重适配在单个主题池的评估者判断上微调monoT5模型，训练主题特定相关性分类器。

Result: 分类器的相关性判断获得的系统排序与真实系统排序的Spearman相关系数>0.95，每个主题仅需128个初始人工判断即可改善模型可比性。

Conclusion: 主题特定相关性分类器是解决未评判文档问题的轻量级直接方法，比将未评判文档视为不相关更可靠，比现有LLM评判方法更可信。

Abstract: The unjudged document problem, where pooled test collections have incomplete
relevance judgments for evaluating new retrieval systems, is a key obstacle to
the reusability of test collections in information retrieval. While the de
facto standard to deal with the problem is to treat unjudged documents as
non-relevant, many alternatives have been proposed, including the use of large
language models (LLMs) as a relevance judge (LLM-as-a-judge). However, this has
been criticized as circular, since the same LLM can be used as a judge and as a
ranker at the same time. We propose to train topic-specific relevance
classifiers instead: By finetuning monoT5 with independent LoRA weight
adaptation on the judgments of a single assessor for a single topic's pool, we
align it to that assessor's notion of relevance for the topic. The system
rankings obtained through our classifier's relevance judgments achieve a
Spearmans' $\rho$ correlation of $>0.95$ with ground truth system rankings. As
little as 128 initial human judgments per topic suffice to improve the
comparability of models, compared to treating unjudged documents as
non-relevant, while achieving more reliability than existing LLM-as-a-judge
approaches. Topic-specific relevance classifiers thus are a lightweight and
straightforward way to tackle the unjudged document problem, while maintaining
human judgments as the gold standard for retrieval evaluation. Code, models,
and data are made openly available.

</details>
