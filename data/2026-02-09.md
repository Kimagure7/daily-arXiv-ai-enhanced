<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 5]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [MuCo: Multi-turn Contrastive Learning for Multimodal Embedding Model](https://arxiv.org/abs/2602.06393)
*Geonmo Gu,Byeongho Heo,Jaemyung Yu,Jaehui Hwang,Taekyung Kim,Sangmin Lee,HeeJae Jun,Yoohoon Kang,Sangdoo Yun,Dongyoon Han*

Main category: cs.IR

TL;DR: MuCo提出多轮对比学习框架，利用MLLM的对话特性在单次前向传播中处理多个相关查询-目标对，提高训练效率和表示一致性。


<details>
  <summary>Details</summary>
Motivation: 传统多模态嵌入模型使用对比学习，但基于"单轮"范式，每个查询-目标对独立处理，导致计算效率低下且忽略多个查询间的上下文关系。

Method: 提出多轮对比学习(MuCo)框架，利用MLLM的对话特性，在单次前向传播中处理与同一图像相关的多个查询-目标对，提取共享上下文条件下的多组嵌入。

Result: 在新建的500万规模多模态多轮数据集(M3T)上实验，在MMEB和M-BEIR基准上达到最先进的检索性能，显著提升训练效率和跨模态表示一致性。

Conclusion: MuCo通过多轮对比学习框架有效解决了传统单轮对比学习的计算效率问题，同时提升了多模态表示的质量和一致性。

Abstract: Universal Multimodal embedding models built on Multimodal Large Language Models (MLLMs) have traditionally employed contrastive learning, which aligns representations of query-target pairs across different modalities. Yet, despite its empirical success, they are primarily built on a "single-turn" formulation where each query-target pair is treated as an independent data point. This paradigm leads to computational inefficiency when scaling, as it requires a separate forward pass for each pair and overlooks potential contextual relationships between multiple queries that can relate to the same context. In this work, we introduce Multi-Turn Contrastive Learning (MuCo), a dialogue-inspired framework that revisits this process. MuCo leverages the conversational nature of MLLMs to process multiple, related query-target pairs associated with a single image within a single forward pass. This allows us to extract a set of multiple query and target embeddings simultaneously, conditioned on a shared context representation, amplifying the effective batch size and overall training efficiency. Experiments exhibit MuCo with a newly curated 5M multimodal multi-turn dataset (M3T), which yields state-of-the-art retrieval performance on MMEB and M-BEIR benchmarks, while markedly enhancing both training efficiency and representation coherence across modalities. Code and M3T are available at https://github.com/naver-ai/muco

</details>


### [2] [TokenMixer-Large: Scaling Up Large Ranking Models in Industrial Recommenders](https://arxiv.org/abs/2602.06563)
*Yuchen Jiang,Jie Zhu,Xintian Han,Hui Lu,Kunmin Bai,Mingyu Yang,Shikang Wu,Ruihao Zhang,Wenlin Zhao,Shipeng Bai,Sijin Zhou,Huizhi Yang,Tianyi Liu,Wenda Liu,Ziyan Gong,Haoran Ding,Zheng Chai,Deping Xie,Zhe Chen,Yuchao Zheng,Peng Xu*

Main category: cs.IR

TL;DR: 本文提出TokenMixer-Large，通过改进残差设计、梯度更新、MoE稀疏化和可扩展性，将推荐模型扩展到70亿和150亿参数，在字节跳动多个场景中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有推荐模型缩放研究（如Wukong、HiFormer、DHEN）的实验规模有限，且基础TokenMixer架构存在残差设计不优、深度模型梯度更新不足、MoE稀疏化不完全和可扩展性探索有限等核心问题。

Method: 提出TokenMixer-Large，采用混合-还原操作、层间残差、辅助损失和新型稀疏-每令牌MoE架构，系统解决基础架构的核心限制。

Result: 成功将参数扩展到70亿（在线流量）和150亿（离线实验），在字节跳动多个场景部署，取得显著的离线和在线性能提升。

Conclusion: TokenMixer-Large通过系统性架构改进，成功实现了大规模推荐模型的扩展，在实际应用中验证了其有效性。

Abstract: In recent years, the study of scaling laws for large recommendation models has gradually gained attention. Works such as Wukong, HiFormer, and DHEN have attempted to increase the complexity of interaction structures in ranking models and validate scaling laws between performance and parameters/FLOPs by stacking multiple layers. However, their experimental scale remains relatively limited. Our previous work introduced the TokenMixer architecture, an efficient variant of the standard Transformer where the self-attention mechanism is replaced by a simple reshape operation, and the feed-forward network is adapted to a pertoken FFN. The effectiveness of this architecture was demonstrated in the ranking stage by the model presented in the RankMixer paper. However, this foundational TokenMixer architecture itself has several design limitations. In this paper, we propose TokenMixer-Large, which systematically addresses these core issues: sub-optimal residual design, insufficient gradient updates in deep models, incomplete MoE sparsification, and limited exploration of scalability. By leveraging a mixing-and-reverting operation, inter-layer residuals, the auxiliary loss and a novel Sparse-Pertoken MoE architecture, TokenMixer-Large successfully scales its parameters to 7-billion and 15-billion on online traffic and offline experiments, respectively. Currently deployed in multiple scenarios at ByteDance, TokenMixer -Large has achieved significant offline and online performance gains.

</details>


### [3] [R2LED: Equipping Retrieval and Refinement in Lifelong User Modeling with Semantic IDs for CTR Prediction](https://arxiv.org/abs/2602.06622)
*Qidong Liu,Gengnan Wang,Zhichen Liu,Moranxin Wang,Zijian Zhang,Xiao Han,Ni Zhang,Tao Qin,Chen Li*

Main category: cs.IR

TL;DR: R2LED：一种利用语义ID增强终身用户建模的新范式，通过多路由混合检索和双层融合精炼，解决噪声检索和语义理解不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有终身用户建模方法采用"检索-精炼"两阶段策略，但仍面临两个问题：(1) 数据分布倾斜导致的噪声检索；(2) 精炼阶段缺乏语义理解。虽然语义增强（如LLM建模或语义嵌入）提供了潜在解决方案，但这些方法存在推理成本过高或表示粒度不足的问题。

Method: 提出R2LED范式，利用语义ID的多粒度性和轻量性优势：1) 多路由混合检索：通过多个并行召回路由从不同粒度捕获用户兴趣，同时提出混合检索机制从协同和语义视角高效检索候选项；2) 双层融合精炼：包括用于路由级融合的目标感知交叉注意力，以及用于SID级融合的门控机制，桥接语义和协同空间。

Result: 在两个公共数据集上的综合实验结果表明，该方法在性能和效率方面均表现出优越性。

Conclusion: R2LED通过语义ID增强终身用户建模，有效解决了噪声检索和语义理解不足的问题，在保持效率的同时提升了CTR预测性能。

Abstract: Lifelong user modeling, which leverages users' long-term behavior sequences for CTR prediction, has been widely applied in personalized services. Existing methods generally adopted a two-stage "retrieval-refinement" strategy to balance effectiveness and efficiency. However, they still suffer from (i) noisy retrieval due to skewed data distribution and (ii) lack of semantic understanding in refinement. While semantic enhancement, e.g., LLMs modeling or semantic embeddings, offers potential solutions to these two challenges, these approaches face impractical inference costs or insufficient representation granularity. Obsorbing multi-granularity and lightness merits of semantic identity (SID), we propose a novel paradigm that equips retrieval and refinement in Lifelong User Modeling with SEmantic IDs (R2LED) to address these issues. First, we introduce a Multi-route Mixed Retrieval for the retrieval stage. On the one hand, it captures users' interests from various granularities by several parallel recall routes. On the other hand, a mixed retrieval mechanism is proposed to efficiently retrieve candidates from both collaborative and semantic views, reducing noise. Then, for refinement, we design a Bi-level Fusion Refinement, including a target-aware cross-attention for route-level fusion and a gate mechanism for SID-level fusion. It can bridge the gap between semantic and collaborative spaces, exerting the merits of SID. The comprehensive experimental results on two public datasets demonstrate the superiority of our method in both performance and efficiency. To facilitate the reproduction, we have released the code online https://github.com/abananbao/R2LED.

</details>


### [4] [Multimodal Generative Retrieval Model with Staged Pretraining for Food Delivery on Meituan](https://arxiv.org/abs/2602.06654)
*Boyu Chen,Tai Guo,Weiyu Cui,Yuqing Li,Xingxing Wang,Chuan Shi,Cheng Yang*

Main category: cs.IR

TL;DR: 提出分阶段预训练策略解决多模态检索中模态主导和训练不一致问题，通过生成式和判别式任务利用语义ID提升性能，在美团数据上取得显著效果提升。


<details>
  <summary>Details</summary>
Motivation: 多模态检索模型在美团等场景中日益重要，但现有双塔架构的联合优化常导致某些模态主导训练而其他模态被忽视，且不同模态训练速度不一致容易产生"单轮次问题"。

Method: 提出分阶段预训练策略，每阶段专注于特定任务，使模型能有效关注和利用多模态特征；设计生成式和判别式任务帮助模型理解语义ID、查询和物品特征之间的关联。

Result: 在美团大规模真实数据上，R@5、R@10、R@20分别提升3.80%、2.64%、2.17%，N@5、N@10、N@20分别提升5.10%、4.22%、2.09%；线上A/B测试显示收入提升1.12%，点击率提升1.02%。

Conclusion: 分阶段预训练策略能有效解决多模态检索中的模态不平衡问题，通过语义ID的充分利用显著提升检索性能，在实际应用中验证了方法的有效性和优越性。

Abstract: Multimodal retrieval models are becoming increasingly important in scenarios such as food delivery, where rich multimodal features can meet diverse user needs and enable precise retrieval. Mainstream approaches typically employ a dual-tower architecture between queries and items, and perform joint optimization of intra-tower and inter-tower tasks. However, we observe that joint optimization often leads to certain modalities dominating the training process, while other modalities are neglected. In addition, inconsistent training speeds across modalities can easily result in the one-epoch problem. To address these challenges, we propose a staged pretraining strategy, which guides the model to focus on specialized tasks at each stage, enabling it to effectively attend to and utilize multimodal features, and allowing flexible control over the training process at each stage to avoid the one-epoch problem. Furthermore, to better utilize the semantic IDs that compress high-dimensional multimodal embeddings, we design both generative and discriminative tasks to help the model understand the associations between SIDs, queries, and item features, thereby improving overall performance. Extensive experiments on large-scale real-world Meituan data demonstrate that our method achieves improvements of 3.80%, 2.64%, and 2.17% on R@5, R@10, and R@20, and 5.10%, 4.22%, and 2.09% on N@5, N@10, and N@20 compared to mainstream baselines. Online A/B testing on the Meituan platform shows that our approach achieves a 1.12% increase in revenue and a 1.02% increase in click-through rate, validating the effectiveness and superiority of our method in practical applications.

</details>


### [5] [On the Efficiency of Sequentially Aware Recommender Systems: Cotten4Rec](https://arxiv.org/abs/2602.06935)
*Shankar Veludandi,Gulrukh Kurdistan,Uzma Mushtaque*

Main category: cs.IR

TL;DR: Cotten4Rec是一种新型序列推荐模型，使用线性时间余弦相似度注意力机制，通过单一优化CUDA内核实现，显著降低BERT4Rec和LinRec的计算开销，在保持推荐准确性的同时大幅减少内存和运行时间。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的序列推荐方法（如BERT4Rec）能有效捕捉用户行为模式，但由于Softmax注意力机制产生大量中间计算，导致显著的计算开销。需要一种更高效的替代方案，特别是在计算资源受限的大规模实际应用场景中。

Method: 提出Cotten4Rec模型，采用线性时间余弦相似度注意力机制，通过单一优化的CUDA内核实现，最小化中间缓冲区和内核启动开销。该方法特别适用于中等序列长度和词汇量的数据集。

Result: 在三个基准数据集上的评估显示，Cotten4Rec相比BERT4Rec和线性注意力基线LinRec，在内存使用和运行时间上实现了显著减少，同时推荐准确性损失最小。

Conclusion: Cotten4Rec证明了作为高效替代方案的可行性，特别适用于计算资源关键的大规模实际序列推荐场景，在保持良好推荐性能的同时大幅提升计算效率。

Abstract: Sequential recommendation (SR) models predict a user's next interaction by modeling their historical behaviors. Transformer-based SR methods, notably BERT4Rec, effectively capture these patterns but incur significant computational overhead due to extensive intermediate computations associated with Softmax-based attention. We propose Cotten4Rec, a novel SR model utilizing linear-time cosine similarity attention, implemented through a single optimized compute unified device architecture (CUDA) kernel. By minimizing intermediate buffers and kernel-launch overhead, Cotten4Rec substantially reduces resource usage compared to BERT4Rec and the linear-attention baseline, LinRec, especially for datasets with moderate sequence lengths and vocabulary sizes. Evaluations across three benchmark datasets confirm that Cotten4Rec achieves considerable reductions in memory and runtime with minimal compromise in recommendation accuracy, demonstrating Cotten4Rec's viability as an efficient alternative for practical, large-scale sequential recommendation scenarios where computational resources are critical.

</details>
