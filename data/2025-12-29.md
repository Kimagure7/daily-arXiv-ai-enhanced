<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 5]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Selective LLM-Guided Regularization for Enhancing Recommendation Models](https://arxiv.org/abs/2512.21526)
*Shanglin Yang,Zhan Shi*

Main category: cs.IR

TL;DR: 提出选择性LLM引导正则化框架，通过可训练门控机制仅在LLM可靠时激活其成对排序监督，提升推荐系统性能


<details>
  <summary>Details</summary>
Motivation: 现有LLM推荐方法存在成本高、有偏见、不可靠等问题，全局知识蒸馏会强制下游模型模仿不准确的LLM预测，而LLM在重排序和挑战性场景中表现更好

Method: 选择性LLM引导正则化框架，使用基于用户历史长度、物品流行度和模型不确定性的可训练门控机制预测LLM可靠性，仅在可靠时激活LLM成对排序监督，所有LLM评分离线进行

Result: 在多个数据集上，该选择性策略持续提升整体准确性，在冷启动和长尾场景中获得显著增益，优于全局蒸馏基线方法

Conclusion: 选择性激活LLM监督比全局蒸馏更有效，能够在不增加推理成本的情况下提升推荐性能，特别是在冷启动和长尾场景中表现优异

Abstract: Large language models provide rich semantic priors and strong reasoning capabilities, making them promising auxiliary signals for recommendation. However, prevailing approaches either deploy LLMs as standalone recommender or apply global knowledge distillation, both of which suffer from inherent drawbacks. Standalone LLM recommender are costly, biased, and unreliable across large regions of the user item space, while global distillation forces the downstream model to imitate LLM predictions even when such guidance is inaccurate. Meanwhile, recent studies show that LLMs excel particularly in re-ranking and challenging scenarios, rather than uniformly across all contexts.We introduce Selective LLM Guided Regularization, a model-agnostic and computation efficient framework that activates LLM based pairwise ranking supervision only when a trainable gating mechanism informing by user history length, item popularity, and model uncertainty predicts the LLM to be reliable. All LLM scoring is performed offline, transferring knowledge without increasing inference cost. Experiments across multiple datasets show that this selective strategy consistently improves overall accuracy and yields substantial gains in cold start and long tail regimes, outperforming global distillation baselines.

</details>


### [2] [CEMG: Collaborative-Enhanced Multimodal Generative Recommendation](https://arxiv.org/abs/2512.21543)
*Yuzhen Lin,Hongyi Chen,Xuanjing Chen,Shaowen Wang,Ivonne Xu,Dongming Jiang*

Main category: cs.IR

TL;DR: CEMG框架通过协同信号引导的多模态融合、统一模态标记化和端到端生成式推荐，解决了生成式推荐中协同信号浅层集成和多模态特征解耦融合的问题。


<details>
  <summary>Details</summary>
Motivation: 生成式推荐模型面临两个关键挑战：1）协同信号的浅层集成，2）多模态特征的解耦融合。这些限制阻碍了真正全面的物品表示。

Method: 提出CEMG框架：1）多模态融合层在协同信号引导下动态整合视觉和文本特征；2）统一模态标记化阶段使用残差量化VAE将融合表示转换为离散语义代码；3）端到端生成式推荐阶段微调大语言模型自回归生成这些物品代码。

Result: 大量实验表明，CEMG显著优于最先进的基线方法。

Conclusion: CEMG通过协同增强的多模态融合和生成式建模，有效解决了生成式推荐中的关键挑战，实现了更好的物品表示和推荐性能。

Abstract: Generative recommendation models often struggle with two key challenges: (1) the superficial integration of collaborative signals, and (2) the decoupled fusion of multimodal features. These limitations hinder the creation of a truly holistic item representation. To overcome this, we propose CEMG, a novel Collaborative-Enhaned Multimodal Generative Recommendation framework. Our approach features a Multimodal Fusion Layer that dynamically integrates visual and textual features under the guidance of collaborative signals. Subsequently, a Unified Modality Tokenization stage employs a Residual Quantization VAE (RQ-VAE) to convert this fused representation into discrete semantic codes. Finally, in the End-to-End Generative Recommendation stage, a large language model is fine-tuned to autoregressively generate these item codes. Extensive experiments demonstrate that CEMG significantly outperforms state-of-the-art baselines.

</details>


### [3] [LLM-I2I: Boost Your Small Item2Item Recommendation Model with Large Language Model](https://arxiv.org/abs/2512.21595)
*Yinfu Feng,Yanjing Wu,Rong Xiao,Xiaoyi Zen*

Main category: cs.IR

TL;DR: LLM-I2I：利用大语言模型解决I2I推荐中的数据稀疏和噪声问题，通过生成合成交互和过滤噪声数据来提升推荐性能，特别是在长尾物品上效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有I2I推荐系统面临两个主要挑战：模型中心方法计算成本高且部署复杂，数据中心方法虽然成本效益高但受限于数据稀疏和噪声问题。需要一种既能保持成本效益又能解决数据质量问题的方案。

Method: 提出LLM-I2I框架，包含两个核心组件：1) LLM生成器，为长尾物品合成用户-物品交互数据以缓解数据稀疏；2) LLM判别器，从真实和合成数据中过滤噪声交互。然后将精炼后的数据融合用于训练I2I模型。

Result: 在工业(AEDS)和学术(ARD)数据集上评估，LLM-I2I持续提升推荐准确性，特别是对长尾物品。在大型跨境电商平台部署后，召回数(RN)提升6.02%，商品交易总额(GMV)提升1.22%。

Conclusion: 这项工作展示了LLMs在不修改模型架构的情况下增强数据中心推荐系统的潜力，为解决推荐系统中的数据质量问题提供了有效途径。

Abstract: Item-to-Item (I2I) recommendation models are widely used in real-world systems due to their scalability, real-time capabilities, and high recommendation quality. Research to enhance I2I performance focuses on two directions: 1) model-centric approaches, which adopt deeper architectures but risk increased computational costs and deployment complexity, and 2) data-centric methods, which refine training data without altering models, offering cost-effectiveness but struggling with data sparsity and noise. To address these challenges, we propose LLM-I2I, a data-centric framework leveraging Large Language Models (LLMs) to mitigate data quality issues. LLM-I2I includes (1) an LLM-based generator that synthesizes user-item interactions for long-tail items, alleviating data sparsity, and (2) an LLM-based discriminator that filters noisy interactions from real and synthetic data. The refined data is then fused to train I2I models. Evaluated on industry (AEDS) and academic (ARD) datasets, LLM-I2I consistently improves recommendation accuracy, particularly for long-tail items. Deployed on a large-scale cross-border e-commerce platform, it boosts recall number (RN) by 6.02% and gross merchandise value (GMV) by 1.22% over existing I2I models. This work highlights the potential of LLMs in enhancing data-centric recommendation systems without modifying model architectures.

</details>


### [4] [KG20C & KG20C-QA: Scholarly Knowledge Graph Benchmarks for Link Prediction and Question Answering](https://arxiv.org/abs/2512.21799)
*Hung-Nghiep Tran,Atsuhiro Takasu*

Main category: cs.IR

TL;DR: 本文介绍了KG20C和KG20C-QA两个用于学术数据问答研究的精选数据集，包括知识图谱构建和问答基准创建。


<details>
  <summary>Details</summary>
Motivation: 为学术领域的问答研究提供高质量、可复用的数据集资源，支持知识图谱嵌入和大型语言模型等不同方法的评估。

Method: 从微软学术图谱中通过场地选择、质量过滤和模式定义构建KG20C知识图谱；基于KG20C定义问答模板，将图三元组转换为自然语言问答对创建KG20C-QA基准。

Result: 创建了高质量的学术知识图谱KG20C和问答基准KG20C-QA，对标准知识图谱嵌入方法进行了基准测试，分析了不同关系类型的性能表现。

Conclusion: 通过正式发布这些经过充分文档化的数据集，为研究社区提供了可重用、可扩展的资源，支持学术领域问答、推理和知识驱动应用的未来研究。

Abstract: In this paper, we present KG20C and KG20C-QA, two curated datasets for advancing question answering (QA) research on scholarly data. KG20C is a high-quality scholarly knowledge graph constructed from the Microsoft Academic Graph through targeted selection of venues, quality-based filtering, and schema definition. Although KG20C has been available online in non-peer-reviewed sources such as GitHub repository, this paper provides the first formal, peer-reviewed description of the dataset, including clear documentation of its construction and specifications. KG20C-QA is built upon KG20C to support QA tasks on scholarly data. We define a set of QA templates that convert graph triples into natural language question--answer pairs, producing a benchmark that can be used both with graph-based models such as knowledge graph embeddings and with text-based models such as large language models. We benchmark standard knowledge graph embedding methods on KG20C-QA, analyze performance across relation types, and provide reproducible evaluation protocols. By officially releasing these datasets with thorough documentation, we aim to contribute a reusable, extensible resource for the research community, enabling future work in QA, reasoning, and knowledge-driven applications in the scholarly domain. The full datasets will be released at https://github.com/tranhungnghiep/KG20C/ upon paper publication.

</details>


### [5] [Frozen LVLMs for Micro-Video Recommendation: A Systematic Study of Feature Extraction and Fusion](https://arxiv.org/abs/2512.21863)
*Huatuan Sun,Yunshan Ma,Changguang Wu,Yanxin Zhang,Pengfei Wang,Xiaoyu Du*

Main category: cs.IR

TL;DR: 本文首次系统评估了冻结大型视频语言模型在微视频推荐中的集成策略，提出了双特征融合框架，在多个基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前冻结大型视频语言模型在微视频推荐中缺乏系统评估，通常被用作黑盒特征提取器，没有比较不同的表示策略。需要填补这一研究空白。

Method: 提出双特征融合框架，系统研究两个关键设计维度：1) 与ID嵌入的集成策略（替换vs融合）；2) 特征提取范式（LVLM生成字幕vs中间解码器隐藏状态）。

Result: 实验发现三个关键原则：1) 中间隐藏状态优于基于字幕的表示；2) ID嵌入捕获不可替代的协同信号，融合优于替换；3) 中间解码器特征在不同层间效果差异显著。DFF框架在两个真实微视频推荐基准上达到SOTA性能。

Conclusion: DFF框架为将现成的大型视觉语言模型集成到微视频推荐系统提供了原则性方法，通过自适应融合多层表示与ID嵌入，实现了轻量级即插即用的解决方案。

Abstract: Frozen Large Video Language Models (LVLMs) are increasingly employed in micro-video recommendation due to their strong multimodal understanding. However, their integration lacks systematic empirical evaluation: practitioners typically deploy LVLMs as fixed black-box feature extractors without systematically comparing alternative representation strategies. To address this gap, we present the first systematic empirical study along two key design dimensions: (i) integration strategies with ID embeddings, specifically replacement versus fusion, and (ii) feature extraction paradigms, comparing LVLM-generated captions with intermediate decoder hidden states. Extensive experiments on representative LVLMs reveal three key principles: (1) intermediate hidden states consistently outperform caption-based representations, as natural-language summarization inevitably discards fine-grained visual semantics crucial for recommendation; (2) ID embeddings capture irreplaceable collaborative signals, rendering fusion strictly superior to replacement; and (3) the effectiveness of intermediate decoder features varies significantly across layers. Guided by these insights, we propose the Dual Feature Fusion (DFF) Framework, a lightweight and plug-and-play approach that adaptively fuses multi-layer representations from frozen LVLMs with item ID embeddings. DFF achieves state-of-the-art performance on two real-world micro-video recommendation benchmarks, consistently outperforming strong baselines and providing a principled approach to integrating off-the-shelf large vision-language models into micro-video recommender systems.

</details>
