<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 9]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Selecting User Histories to Generate LLM Users for Cold-Start Item Recommendation](https://arxiv.org/abs/2511.21989)
*Nachiket Subbaraman,Jaskinder Sarai,Aniruddh Nath,Lichan Hong,Lukasz Heldt,Li Wei,Zhe Zhao*

Main category: cs.IR

TL;DR: 本文提出一种基于强化学习的框架，利用大语言模型作为用户模拟器，优化冷启动物品的数据增强策略，显著提升推荐系统在冷启动场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用大语言模型进行冷启动物品的数据增强存在两个主要问题：1) 仅使用部分用户历史，无法让LLM充分模拟用户行为；2) 随机选择用户进行增强不是最优策略。需要更智能的用户选择方法来优化冷启动物品的性能。

Method: 提出强化学习框架，将LLM作为用户模拟器，训练策略模型基于用户行为特征和历史来选择最适合进行数据增强的用户。采用策略梯度方法更新策略，使选择行为导向高奖励（即冷启动物品性能提升）。

Result: 在Amazon产品评论数据集上的实验表明，该方法在冷启动物品召回率方面取得显著提升，证明了其作为可扩展、服务高效的数据增强策略的有效性。

Conclusion: 通过强化学习优化用户选择策略，结合LLM作为用户模拟器，能够有效解决传统推荐系统在冷启动物品场景下的数据稀疏问题，为现代推荐系统提供了一种高效的数据增强方法。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in reasoning, generalization, and simulating human-like behavior across a wide range of tasks. These strengths present new opportunities to enhance traditional recommendation systems (RS), especially in the cold-start item scenario where newly introduced items lack interactions. Existing works have used LLMs to address cold-start issues in traditional RS through data augmentation, but they have limitations. One recent work directly addresses this issue by prompting LLMs to generate augmented interaction data between randomly sampled users and cold-start items. Then, they train the traditional RS with augmented data, incorporating collaborative signals for cold-start items. Although they use LLMs to provide cold-start items with feedback, they use partial user histories, which does not allow the LLM to fully emulate the user. Furthermore, randomly selecting users is not optimal for augmentation. To address these challenges, we leverage the LLM as a user and develop a reinforcement learning (RL) framework that trains a policy to select users for augmentation, optimizing for cold-start item performance after augmented training. The policy model learns to select users for cold-start item data augmentation based on their behavioral features and histories. To optimize user selection for cold-start item performance, we employ a policy gradient method that updates the policy in the direction of actions that lead to high rewards. Experiments on Amazon Product Review datasets show substantial gains in cold-start item recall, demonstrating the effectiveness of our method as a scalable, serving-efficient augmentation strategy for modern RS.

</details>


### [2] [Evaluating Embedding Models and Pipeline Optimization for AI Search Quality](https://arxiv.org/abs/2511.22240)
*Philip Zhong,Kent Chen,Don Wang*

Main category: cs.IR

TL;DR: 评估不同文本嵌入模型和配置对AI搜索系统性能的影响，发现高维嵌入、神经重排器和细粒度分块能显著提升检索准确率。


<details>
  <summary>Details</summary>
Motivation: 研究不同文本嵌入模型、索引方法和分块策略对AI驱动搜索系统性能的影响，为构建高效检索系统提供实证依据。

Method: 使用本地LLM从美国市议会会议记录合成11,975个查询-分块对评估数据集，比较不同嵌入模型（All-MPNet、BGE、GTE、Qwen）、维度、索引方法（Milvus HNSW/IVF）和分块策略，采用Top-K准确率和NDCG指标评估检索性能。

Result: 高维嵌入显著提升搜索质量（如Qwen3-Embedding-8B/4096的Top-3准确率约0.571 vs GTE-large/1024的0.412），神经重排器（如BGE交叉编码器）进一步提升排名准确率（Top-3达0.527），细粒度分块（512字符vs2000字符）也提高准确率。

Conclusion: 嵌入维度、重排器和分块粒度是影响检索系统性能的关键因素，未来需进一步探索管道自动化和评估方法的改进。

Abstract: We evaluate the performance of various text embedding models and pipeline configurations for AI-driven search systems. We compare sentence-transformer and generative embedding models (e.g., All-MPNet, BGE, GTE, and Qwen) at different dimensions, indexing methods (Milvus HNSW/IVF), and chunking strategies. A custom evaluation dataset of 11,975 query-chunk pairs was synthesized from US City Council meeting transcripts using a local large language model (LLM). The data pipeline includes preprocessing, automated question generation per chunk, manual validation, and continuous integration/continuous deployment (CI/CD) integration. We measure retrieval accuracy using reference-based metrics: Top-K Accuracy and Normalized Discounted Cumulative Gain (NDCG). Our results demonstrate that higher-dimensional embeddings significantly boost search quality (e.g., Qwen3-Embedding-8B/4096 achieves Top-3 accuracy about 0.571 versus 0.412 for GTE-large/1024), and that neural re-rankers (e.g., a BGE cross-encoder) further improve ranking accuracy (Top-3 up to 0.527). Finer-grained chunking (512 characters versus 2000 characters) also improves accuracy. We discuss the impact of these factors and outline future directions for pipeline automation and evaluation.

</details>


### [3] [FIGROTD: A Friendly-to-Handle Dataset for Image Guided Retrieval with Optional Text](https://arxiv.org/abs/2511.22247)
*Hoang-Bao Le,Allie Tran,Binh T. Nguyen,Liting Zhou,Cathal Gurrin*

Main category: cs.IR

TL;DR: IGROT统一视觉检索和组合检索，但缺乏可访问基准。作者提出轻量级数据集FIGROTD和方差引导特征掩码方法VaGFeM，在多个基准上取得竞争性结果。


<details>
  <summary>Details</summary>
Motivation: IGROT（图像引导检索与可选文本）统一了视觉检索（无文本）和组合检索（有文本），在Google Image和Bing等应用中很重要。但进展受限，因为缺乏可访问的基准，且现有方法往往偏向视觉或组合查询之一。大规模数据集如MagicLens计算成本高，需要轻量级解决方案。

Method: 1. 引入FIGROTD数据集：轻量级高质量的IGROT数据集，包含16,474个训练三元组和1,262个测试三元组，涵盖CIR、SBIR和CSTBIR任务。2. 提出方差引导特征掩码（VaGFeM）：基于方差统计选择性增强判别性维度，减少冗余。3. 采用双损失设计：结合InfoNCE和Triplet损失，改进组合推理能力。

Result: 在FIGROTD上训练的VaGFeM在九个基准测试中取得竞争性结果：在CIRCO上达到34.8 mAP@10，在Sketchy上达到75.7 mAP@200。尽管使用较少的三元组，但性能优于更强的基线方法。

Conclusion: 提出的FIGROTD数据集和VaGFeM方法有效解决了IGROT任务的数据集可访问性和方法平衡问题。轻量级数据集配合选择性特征增强和双损失设计，在多个检索任务上实现了竞争性性能。

Abstract: Image-Guided Retrieval with Optional Text (IGROT) unifies visual retrieval (without text) and composed retrieval (with text). Despite its relevance in applications like Google Image and Bing, progress has been limited by the lack of an accessible benchmark and methods that balance performance across subtasks. Large-scale datasets such as MagicLens are comprehensive but computationally prohibitive, while existing models often favor either visual or compositional queries. We introduce FIGROTD, a lightweight yet high-quality IGROT dataset with 16,474 training triplets and 1,262 test triplets across CIR, SBIR, and CSTBIR. To reduce redundancy, we propose the Variance Guided Feature Mask (VaGFeM), which selectively enhances discriminative dimensions based on variance statistics. We further adopt a dual-loss design (InfoNCE + Triplet) to improve compositional reasoning. Trained on FIGROTD, VaGFeM achieves competitive results on nine benchmarks, reaching 34.8 mAP@10 on CIRCO and 75.7 mAP@200 on Sketchy, outperforming stronger baselines despite fewer triplets.

</details>


### [4] [UNION: A Lightweight Target Representation for Efficient Zero-Shot Image-Guided Retrieval with Optional Textual Queries](https://arxiv.org/abs/2511.22253)
*Hoang-Bao Le,Allie Tran,Binh T. Nguyen,Liting Zhou,Cathal Gurrin*

Main category: cs.IR

TL;DR: IGROT统一了CIR和SBIR任务，UNION方法通过融合图像嵌入和空文本提示，仅需少量训练数据就能实现跨模态检索的竞争性表现。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要大量监督数据，而IGROT（图像引导检索）作为统一框架，需要在低数据监督下解决跨模态图像检索问题，特别是结合图像和可选文本的查询场景。

Method: 提出UNION方法：轻量级、可泛化的目标表示，融合图像嵌入和空文本提示，无需修改预训练视觉语言模型架构，仅需少量训练样本（5000个）。

Result: 在CIRCO数据集上达到mAP@50 38.5，Sketchy数据集上达到mAP@200 82.7，超越了许多需要大量监督的基线方法。

Conclusion: UNION方法在低数据监督下展现了强大的鲁棒性和效率，能够有效桥接视觉和语言模态，适用于多样化的查询类型。

Abstract: Image-Guided Retrieval with Optional Text (IGROT) is a general retrieval setting where a query consists of an anchor image, with or without accompanying text, aiming to retrieve semantically relevant target images. This formulation unifies two major tasks: Composed Image Retrieval (CIR) and Sketch-Based Image Retrieval (SBIR). In this work, we address IGROT under low-data supervision by introducing UNION, a lightweight and generalisable target representation that fuses the image embedding with a null-text prompt. Unlike traditional approaches that rely on fixed target features, UNION enhances semantic alignment with multimodal queries while requiring no architectural modifications to pretrained vision-language models. With only 5,000 training samples - from LlavaSCo for CIR and Training-Sketchy for SBIR - our method achieves competitive results across benchmarks, including CIRCO mAP@50 of 38.5 and Sketchy mAP@200 of 82.7, surpassing many heavily supervised baselines. This demonstrates the robustness and efficiency of UNION in bridging vision and language across diverse query types.

</details>


### [5] [Efficiency and Effectiveness of SPLADE Models on Billion-Scale Web Document Title](https://arxiv.org/abs/2511.22263)
*Taeryun Won,Tae Kwan Lee,Hiun Kim,Hyemin Lee*

Main category: cs.IR

TL;DR: 本文全面比较了BM25、SPLADE和Expanded-SPLADE在大规模网页文档检索中的表现，发现稀疏表示模型（SPLADE系列）在复杂查询上优于BM25但计算成本更高，通过剪枝策略可显著提升效率，其中Expanded-SPLADE在效果与效率间达到最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 在大规模网页文档检索场景中，需要评估不同检索模型（传统BM25与基于稀疏表示的SPLADE系列）在效果与效率之间的权衡，为实际搜索引擎部署提供指导。

Method: 在千万到数十亿规模的网页文档标题数据集上对比BM25、SPLADE和Expanded-SPLADE模型；提出文档中心剪枝、top-k查询词选择、布尔查询加词项阈值等剪枝策略来优化稀疏模型的计算效率。

Result: SPLADE和Expanded-SPLADE在检索效果上显著优于BM25，尤其在复杂查询上；但计算成本更高；通过剪枝策略可在不明显牺牲效果的情况下大幅提升效率；Expanded-SPLADE在效果与效率间达到最佳平衡。

Conclusion: 稀疏检索模型（特别是Expanded-SPLADE）在大规模搜索引擎部署中具有实用价值，通过适当的剪枝策略可以在保持优异检索效果的同时控制计算成本，为实际应用提供了重要参考。

Abstract: This paper presents a comprehensive comparison of BM25, SPLADE, and Expanded-SPLADE models in the context of large-scale web document retrieval. We evaluate the effectiveness and efficiency of these models on datasets spanning from tens of millions to billions of web document titles. SPLADE and Expanded-SPLADE, which utilize sparse lexical representations, demonstrate superior retrieval performance compared to BM25, especially for complex queries. However, these models incur higher computational costs. We introduce pruning strategies, including document-centric pruning and top-k query term selection, boolean query with term threshold to mitigate these costs and improve the models' efficiency without significantly sacrificing retrieval performance. The results show that Expanded-SPLADE strikes the best balance between effectiveness and efficiency, particularly when handling large datasets. Our findings offer valuable insights for deploying sparse retrieval models in large-scale search engines.

</details>


### [6] [CoFiRec: Coarse-to-Fine Tokenization for Generative Recommendation](https://arxiv.org/abs/2511.22707)
*Tianxin Wei,Xuying Ning,Xuxing Chen,Ruizhong Qiu,Yupeng Hou,Yan Xie,Shuang Yang,Zhigang Hua,Jingrui He*

Main category: cs.IR

TL;DR: CoFiRec是一个生成式推荐框架，通过粗粒度到细粒度的语义层次化tokenization来建模用户意图的渐进演化过程，提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐方法将物品的异构属性（ID、类别、标题、描述等）融合为单一嵌入再进行量化，这种扁平化处理忽略了物品固有的语义层次结构，无法捕捉用户在网页交互中意图的渐进演化过程。

Method: 提出CoFiRec框架，将物品信息分解为多个语义层次（从高层类别到详细描述和协同过滤信号），独立tokenize每个层次并保持结构顺序。在自回归解码时，语言模型被指导从粗粒度到细粒度生成物品token，渐进建模用户意图。

Result: 在多个公开基准测试和不同骨干网络上，CoFiRec均优于现有方法。理论上证明了结构化tokenization能降低生成物品与真实物品之间的差异度。

Conclusion: CoFiRec通过显式融入物品语义的粗粒度到细粒度特性，为生成式推荐提供了新视角，能更好地建模用户意图的渐进演化过程。

Abstract: In web environments, user preferences are often refined progressively as users move from browsing broad categories to exploring specific items. However, existing generative recommenders overlook this natural refinement process. Generative recommendation formulates next-item prediction as autoregressive generation over tokenized user histories, where each item is represented as a sequence of discrete tokens. Prior models typically fuse heterogeneous attributes such as ID, category, title, and description into a single embedding before quantization, which flattens the inherent semantic hierarchy of items and fails to capture the gradual evolution of user intent during web interactions. To address this limitation, we propose CoFiRec, a novel generative recommendation framework that explicitly incorporates the Coarse-to-Fine nature of item semantics into the tokenization process. Instead of compressing all attributes into a single latent space, CoFiRec decomposes item information into multiple semantic levels, ranging from high-level categories to detailed descriptions and collaborative filtering signals. Based on this design, we introduce the CoFiRec Tokenizer, which tokenizes each level independently while preserving structural order. During autoregressive decoding, the language model is instructed to generate item tokens from coarse to fine, progressively modeling user intent from general interests to specific item-level interests. Experiments across multiple public benchmarks and backbones demonstrate that CoFiRec outperforms existing methods, offering a new perspective for generative recommendation. Theoretically, we prove that structured tokenization leads to lower dissimilarity between generated and ground truth items, supporting its effectiveness in generative recommendation. Our code is available at https://github.com/YennNing/CoFiRec.

</details>


### [7] [Two-Stage Distributionally Robust Optimization Framework for Secure Communications in Aerial-RIS Systems](https://arxiv.org/abs/2511.22855)
*Zhongming Feng,Qiling Gao,Zeping Sui,Yun Lin,Michail Matthaiou*

Main category: cs.IR

TL;DR: 提出基于条件风险价值的两阶段分布鲁棒优化框架，用于空中可重构智能表面辅助毫米波系统的安全部署和波束成形，以应对多时间尺度不确定性。


<details>
  <summary>Details</summary>
Motivation: 空中可重构智能表面辅助毫米波系统面临用户移动性、不完美信道状态信息和硬件损伤等多时间尺度不确定性，传统方法难以保证系统安全性能。

Method: 采用两阶段分布鲁棒优化框架：第一阶段进行长期无人机部署，第二阶段进行时隙级波束成形设计。使用条件风险价值作为分布无关的风险度量，结合代理模型和交替优化算法实现高效部署和鲁棒波束成形。

Result: 仿真结果表明，所提DRO-CVaR框架显著提升了尾部保密频谱效率，在严重不确定性条件下相比基准方案保持了更低的中断概率。

Conclusion: 该两阶段分布鲁棒优化框架能有效应对空中可重构智能表面辅助毫米波系统中的多时间尺度不确定性，为系统安全部署和波束成形提供了鲁棒解决方案。

Abstract: This letter proposes a two-stage distributionally robust optimization (DRO) framework for secure deployment and beamforming in an aerial reconfigurable intelligent surface (A-RIS) assisted millimeter-wave system. To account for multi-timescale uncertainties arising from user mobility, imperfect channel state information (CSI), and hardware impairments, our approach decouples the long-term unmanned aerial vehicle (UAV) placement from the per-slot beamforming design. By employing the conditional value-at-risk (CVaR) as a distribution-free risk metric, a low-complexity algorithm is developed, which combines a surrogate model for efficient deployment with an alternating optimization (AO) scheme for robust real-time beamforming. Simulation results validate that the proposed DRO-CVaR framework significantly enhances the tail-end secrecy spectral efficiency and maintains a lower outage probability compared to benchmark schemes, especially under severe uncertainty conditions.

</details>


### [8] [FedAU2: Attribute Unlearning for User-Level Federated Recommender Systems with Adaptive and Robust Adversarial Training](https://arxiv.org/abs/2511.22872)
*Yuyuan Li,Junjie Fang,Fengyuan Yu,Xichun Sheng,Tianyu Du,Xuyang Teng,Shaowei Jiang,Linbo Jiang,Jianan Lin,Chaochao Chen*

Main category: cs.IR

TL;DR: FedAU2：一种针对用户级联邦推荐系统的属性遗忘方法，通过自适应对抗训练和双重随机变分自编码器解决训练不稳定性和梯度信息泄露问题


<details>
  <summary>Details</summary>
Motivation: 联邦推荐系统虽然保护用户隐私，但用户嵌入中仍包含敏感属性信息，易受属性推断攻击。属性遗忘是缓解此问题的有前景方法，但在用户级联邦推荐系统中面临训练不稳定和梯度信息泄露两大挑战

Method: 提出FedAU2方法：1) 自适应对抗训练策略，根据本地优化行为动态调整训练过程；2) 双重随机变分自编码器扰动对抗模型，防止基于梯度的信息泄露

Result: 在三个真实世界数据集上的实验表明，FedAU2在遗忘效果和推荐性能方面均优于现有基线方法

Conclusion: FedAU2有效解决了用户级联邦推荐系统中属性遗忘的关键挑战，实现了更好的隐私保护与推荐性能平衡

Abstract: Federated Recommender Systems (FedRecs) leverage federated learning to protect user privacy by retaining data locally. However, user embeddings in FedRecs often encode sensitive attribute information, rendering them vulnerable to attribute inference attacks. Attribute unlearning has emerged as a promising approach to mitigate this issue. In this paper, we focus on user-level FedRecs, which is a more practical yet challenging setting compared to group-level FedRecs. Adversarial training emerges as the most feasible approach within this context. We identify two key challenges in implementing adversarial training-based attribute unlearning for user-level FedRecs: i) mitigating training instability caused by user data heterogeneity, and ii) preventing attribute information leakage through gradients. To address these challenges, we propose FedAU2, an attribute unlearning method for user-level FedRecs. For CH1, we propose an adaptive adversarial training strategy, where the training dynamics are adjusted in response to local optimization behavior. For CH2, we propose a dual-stochastic variational autoencoder to perturb the adversarial model, effectively preventing gradient-based information leakage. Extensive experiments on three real-world datasets demonstrate that our proposed FedAU2 achieves superior performance in unlearning effectiveness and recommendation performance compared to existing baselines.

</details>


### [9] [Do LLM-judges Align with Human Relevance in Cranfield-style Recommender Evaluation?](https://arxiv.org/abs/2511.23312)
*Gustavo Penha,Aleksandr V. Petrov,Claudia Hauff,Enrico Palumbo,Ali Vardasbi,Edoardo D'Amico,Francesco Fabbri,Alice Wang,Praveen Chandar,Henrik Lindstrom,Hugues Bouchard,Mounia Lalmas*

Main category: cs.IR

TL;DR: LLM-judge可以作为推荐系统评估的可扩展自动评判方法，与人类评判结果高度一致


<details>
  <summary>Details</summary>
Motivation: 推荐系统评估面临历史交互数据偏差、流行度偏差等问题，传统Cranfield风格测试集构建成本高，需要探索可扩展的自动评估方法

Method: 使用ML-32M-ext电影推荐数据集，研究LLM作为自动评判者与人类相关性标注的对齐程度，并在播客推荐领域进行工业案例研究

Result: 丰富的项目元数据和更长的用户历史记录能提高对齐度，LLM评判者与人类排名高度一致（Kendall's tau = 0.87），在模型选择中具有实用价值

Conclusion: LLM-judge是推荐系统评估的可行且可扩展的方法，能够解决传统评估方法的成本限制问题

Abstract: Evaluating recommender systems remains a long-standing challenge, as offline methods based on historical user interactions and train-test splits often yield unstable and inconsistent results due to exposure bias, popularity bias, sampled evaluations, and missing-not-at-random patterns. In contrast, textual document retrieval benefits from robust, standardized evaluation via Cranfield-style test collections, which combine pooled relevance judgments with controlled setups. While recent work shows that adapting this methodology to recommender systems is feasible, constructing such collections remains costly due to the need for manual relevance judgments, thus limiting scalability. This paper investigates whether Large Language Models (LLMs) can serve as reliable automatic judges to address these scalability challenges. Using the ML-32M-ext Cranfield-style movie recommendation collection, we first examine the limitations of existing evaluation methodologies. Then we explore the alignment and the recommender systems ranking agreement between the LLM-judge and human provided relevance labels. We find that incorporating richer item metadata and longer user histories improves alignment, and that LLM-judge yields high agreement with human-based rankings (Kendall's tau = 0.87). Finally, an industrial case study in the podcast recommendation domain demonstrates the practical value of LLM-judge for model selection. Overall, our results show that LLM-judge is a viable and scalable approach for evaluating recommender systems.

</details>
