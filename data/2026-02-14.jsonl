{"id": "2602.11235", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.11235", "abs": "https://arxiv.org/abs/2602.11235", "authors": ["Xin Song", "Zhilin Guan", "Ruidong Han", "Binghao Tang", "Tianwen Chen", "Bing Li", "Zihao Li", "Han Zhang", "Fei Jiang", "Chaolin Xie", "Chi Ma", "Chunyang Jiang", "Chunzhen Jing", "Dengxuan Li", "Fengyi Li", "Lei Yu", "Mengyao Sun", "Pu Wang", "Qing Wang", "Rui Fan", "Shangyu Chen", "Shifeng Du", "Siyuan Bai", "Wei Lin", "Wentao Zhu", "Zhou Han", "Zhuo Chen", "Zikang Xu"], "title": "MTFM: A Scalable and Alignment-free Foundation Model for Industrial Recommendation in Meituan", "comment": null, "summary": "Industrial recommendation systems typically involve multiple scenarios, yet existing cross-domain (CDR) and multi-scenario (MSR) methods often require prohibitive resources and strict input alignment, limiting their extensibility. We propose MTFM (Meituan Foundation Model for Recommendation), a transformer-based framework that addresses these challenges. Instead of pre-aligning inputs, MTFM transforms cross-domain data into heterogeneous tokens, capturing multi-scenario knowledge in an alignment-free manner. To enhance efficiency, we first introduce a multi-scenario user-level sample aggregation that significantly enhances training throughput by reducing the total number of instances. We further integrate Grouped-Query Attention and a customized Hybrid Target Attention to minimize memory usage and computational complexity. Furthermore, we implement various system-level optimizations, such as kernel fusion and the elimination of CPU-GPU blocking, to further enhance both training and inference throughput. Offline and online experiments validate the effectiveness of MTFM, demonstrating that significant performance gains are achieved by scaling both model capacity and multi-scenario training data."}
