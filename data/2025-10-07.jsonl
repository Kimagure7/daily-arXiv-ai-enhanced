{"id": "2510.03750", "categories": ["cs.IR", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.03750", "abs": "https://arxiv.org/abs/2510.03750", "authors": ["Hanwen Zhang", "Kun Fang", "Ziyu Wang", "Ichiro Fujinaga"], "title": "Evaluating High-Resolution Piano Sustain Pedal Depth Estimation with Musically Informed Metrics", "comment": null, "summary": "Evaluation for continuous piano pedal depth estimation tasks remains\nincomplete when relying only on conventional frame-level metrics, which\noverlook musically important features such as direction-change boundaries and\npedal curve contours. To provide more interpretable and musically meaningful\ninsights, we propose an evaluation framework that augments standard frame-level\nmetrics with an action-level assessment measuring direction and timing using\nsegments of press/hold/release states and a gesture-level analysis that\nevaluates contour similarity of each press-release cycle. We apply this\nframework to compare an audio-only baseline with two variants: one\nincorporating symbolic information from MIDI, and another trained in a\nbinary-valued setting, all within a unified architecture. Results show that the\nMIDI-informed model significantly outperforms the others at action and gesture\nlevels, despite modest frame-level gains. These findings demonstrate that our\nframework captures musically relevant improvements indiscernible by traditional\nmetrics, offering a more practical and effective approach to evaluating pedal\ndepth estimation models."}
{"id": "2510.03795", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03795", "abs": "https://arxiv.org/abs/2510.03795", "authors": ["Simon Lupart", "Daniël van Dijk", "Eric Langezaal", "Ian van Dort", "Mohammad Aliannejadi"], "title": "Investigating LLM Variability in Personalized Conversational Information Retrieval", "comment": "11 pages, 5 figures, SIGIR-AP'25 Proceedings of the 2025 Annual\n  International ACM SIGIR Conference on Research and Development in Information\n  Retrieval in the Asia Pacific Region (SIGIR-AP 2025), December 7--10, 2025,\n  Xi'an, China", "summary": "Personalized Conversational Information Retrieval (CIR) has seen rapid\nprogress in recent years, driven by the development of Large Language Models\n(LLMs). Personalized CIR aims to enhance document retrieval by leveraging\nuser-specific information, such as preferences, knowledge, or constraints, to\ntailor responses to individual needs. A key resource for this task is the TREC\niKAT 2023 dataset, designed to evaluate personalization in CIR pipelines.\nBuilding on this resource, Mo et al. explored several strategies for\nincorporating Personal Textual Knowledge Bases (PTKB) into LLM-based query\nreformulation. Their findings suggested that personalization from PTKBs could\nbe detrimental and that human annotations were often noisy. However, these\nconclusions were based on single-run experiments using the GPT-3.5 Turbo model,\nraising concerns about output variability and repeatability. In this\nreproducibility study, we rigorously reproduce and extend their work, focusing\non LLM output variability and model generalization. We apply the original\nmethods to the new TREC iKAT 2024 dataset and evaluate a diverse range of\nmodels, including Llama (1B-70B), Qwen-7B, GPT-4o-mini. Our results show that\nhuman-selected PTKBs consistently enhance retrieval performance, while\nLLM-based selection methods do not reliably outperform manual choices. We\nfurther compare variance across datasets and observe higher variability on iKAT\nthan on CAsT, highlighting the challenges of evaluating personalized CIR.\nNotably, recall-oriented metrics exhibit lower variance than precision-oriented\nones, a critical insight for first-stage retrievers. Finally, we underscore the\nneed for multi-run evaluations and variance reporting when assessing LLM-based\nCIR systems. By broadening evaluation across models, datasets, and metrics, our\nstudy contributes to more robust and generalizable practices for personalized\nCIR."}
{"id": "2510.03984", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03984", "abs": "https://arxiv.org/abs/2510.03984", "authors": ["Kirandeep Kaur", "Preetam Prabhu Srikar Dammu", "Hideo Joho", "Chirag Shah"], "title": "Beyond Static Evaluation: Rethinking the Assessment of Personalized Agent Adaptability in Information Retrieval", "comment": null, "summary": "Personalized AI agents are becoming central to modern information retrieval,\nyet most evaluation methodologies remain static, relying on fixed benchmarks\nand one-off metrics that fail to reflect how users' needs evolve over time.\nThese limitations hinder our ability to assess whether agents can meaningfully\nadapt to individuals across dynamic, longitudinal interactions. In this\nperspective paper, we propose a conceptual lens for rethinking evaluation in\nadaptive personalization, shifting the focus from static performance snapshots\nto interaction-aware, evolving assessments. We organize this lens around three\ncore components: (1) persona-based user simulation with temporally evolving\npreference models; (2) structured elicitation protocols inspired by reference\ninterviews to extract preferences in context; and (3) adaptation-aware\nevaluation mechanisms that measure how agent behavior improves across sessions\nand tasks. While recent works have embraced LLM-driven user simulation, we\nsituate this practice within a broader paradigm for evaluating agents over\ntime. To illustrate our ideas, we conduct a case study in e-commerce search\nusing the PersonalWAB dataset. Beyond presenting a framework, our work lays a\nconceptual foundation for understanding and evaluating personalization as a\ncontinuous, user-centric endeavor."}
{"id": "2510.04010", "categories": ["cs.IR", "cs.CL", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.04010", "abs": "https://arxiv.org/abs/2510.04010", "authors": ["Yu-Fei Shih", "An-Zi Yen", "Hen-Hsen Huang", "Hsin-Hsi Chen"], "title": "Visual Lifelog Retrieval through Captioning-Enhanced Interpretation", "comment": null, "summary": "People often struggle to remember specific details of past experiences, which\ncan lead to the need to revisit these memories. Consequently, lifelog retrieval\nhas emerged as a crucial application. Various studies have explored methods to\nfacilitate rapid access to personal lifelogs for memory recall assistance. In\nthis paper, we propose a Captioning-Integrated Visual Lifelog (CIVIL) Retrieval\nSystem for extracting specific images from a user's visual lifelog based on\ntextual queries. Unlike traditional embedding-based methods, our system first\ngenerates captions for visual lifelogs and then utilizes a text embedding model\nto project both the captions and user queries into a shared vector space.\nVisual lifelogs, captured through wearable cameras, provide a first-person\nviewpoint, necessitating the interpretation of the activities of the individual\nbehind the camera rather than merely describing the scene. To address this, we\nintroduce three distinct approaches: the single caption method, the collective\ncaption method, and the merged caption method, each designed to interpret the\nlife experiences of lifeloggers. Experimental results show that our method\neffectively describes first-person visual images, enhancing the outcomes of\nlifelog retrieval. Furthermore, we construct a textual dataset that converts\nvisual lifelogs into captions, thereby reconstructing personal life\nexperiences."}
{"id": "2510.04012", "categories": ["cs.IR", "physics.ins-det"], "pdf": "https://arxiv.org/pdf/2510.04012", "abs": "https://arxiv.org/abs/2510.04012", "authors": ["David Rogers", "Valerio Mariani", "Cong Wang", "Ryan Coffee", "Wilko Kroeger", "Murali Shankar", "Hans Thorsten Schwander", "Tom Beck", "Frédéric Poitevin", "Jana Thayer"], "title": "The LCLStream Ecosystem for Multi-Institutional Dataset Exploration", "comment": "3 figures", "summary": "We describe a new end-to-end experimental data streaming framework designed\nfrom the ground up to support new types of applications -- AI training,\nextremely high-rate X-ray time-of-flight analysis, crystal structure\ndetermination with distributed processing, and custom data science applications\nand visualizers yet to be created. Throughout, we use design choices merging\ncloud microservices with traditional HPC batch execution models for security\nand flexibility. This project makes a unique contribution to the DOE Integrated\nResearch Infrastructure (IRI) landscape. By creating a flexible, API-driven\ndata request service, we address a significant need for high-speed data\nstreaming sources for the X-ray science data analysis community. With the\ncombination of data request API, mutual authentication web security framework,\njob queue system, high-rate data buffer, and complementary nature to facility\ninfrastructure, the LCLStreamer framework has prototyped and implemented\nseveral new paradigms critical for future generation experiments."}
{"id": "2510.04096", "categories": ["cs.IR", "cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04096", "abs": "https://arxiv.org/abs/2510.04096", "authors": ["Tommy Mordo", "Sagie Dekel", "Omer Madmon", "Moshe Tennenholtz", "Oren Kurland"], "title": "RLRF: Competitive Search Agent Design via Reinforcement Learning from Ranker Feedback", "comment": null, "summary": "Competitive search is a setting where document publishers modify them to\nimprove their ranking in response to a query. Recently, publishers have\nincreasingly leveraged LLMs to generate and modify competitive content. We\nintroduce Reinforcement Learning from Ranker Feedback (RLRF), a framework that\ntrains LLMs using preference datasets derived from ranking competitions. The\ngoal of a publisher (LLM-based) agent is to optimize content for improved\nranking while accounting for the strategies of competing agents. We generate\nthe datasets using approaches that do not rely on human-authored data. We show\nthat our proposed agents consistently and substantially outperform previously\nsuggested approaches for LLM-based competitive document modification. We\nfurther show that our agents are effective with ranking functions they were not\ntrained for (i.e., out of distribution) and they adapt to strategic opponents.\nThese findings provide support to the significant potential of using\nreinforcement learning in competitive search."}
{"id": "2510.04127", "categories": ["cs.IR", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04127", "abs": "https://arxiv.org/abs/2510.04127", "authors": ["Sean Moran"], "title": "Learning-Based Hashing for ANN Search: Foundations and Early Advances", "comment": null, "summary": "Approximate Nearest Neighbour (ANN) search is a fundamental problem in\ninformation retrieval, underpinning large-scale applications in computer\nvision, natural language processing, and cross-modal search. Hashing-based\nmethods provide an efficient solution by mapping high-dimensional data into\ncompact binary codes that enable fast similarity computations in Hamming space.\nOver the past two decades, a substantial body of work has explored learning to\nhash, where projection and quantisation functions are optimised from data\nrather than chosen at random.\n  This article offers a foundational survey of early learning-based hashing\nmethods, with an emphasis on the core ideas that shaped the field. We review\nsupervised, unsupervised, and semi-supervised approaches, highlighting how\nprojection functions are designed to generate meaningful embeddings and how\nquantisation strategies convert these embeddings into binary codes. We also\nexamine extensions to multi-bit and multi-threshold models, as well as early\nadvances in cross-modal retrieval.\n  Rather than providing an exhaustive account of the most recent methods, our\ngoal is to introduce the conceptual foundations of learning-based hashing for\nANN search. By situating these early models in their historical context, we aim\nto equip readers with a structured understanding of the principles, trade-offs,\nand open challenges that continue to inform current research in this area."}
{"id": "2510.04239", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04239", "abs": "https://arxiv.org/abs/2510.04239", "authors": ["Tongzhou Wu", "Yuhao Wang", "Maolin Wang", "Chi Zhang", "Xiangyu Zhao"], "title": "Empowering Denoising Sequential Recommendation with Large Language Model Embeddings", "comment": "Accepted by CIKM2025", "summary": "Sequential recommendation aims to capture user preferences by modeling\nsequential patterns in user-item interactions. However, these models are often\ninfluenced by noise such as accidental interactions, leading to suboptimal\nperformance. Therefore, to reduce the effect of noise, some works propose\nexplicitly identifying and removing noisy items. However, we find that simply\nrelying on collaborative information may result in an over-denoising problem,\nespecially for cold items. To overcome these limitations, we propose a novel\nframework: Interest Alignment for Denoising Sequential Recommendation (IADSR)\nwhich integrates both collaborative and semantic information. Specifically,\nIADSR is comprised of two stages: in the first stage, we obtain the\ncollaborative and semantic embeddings of each item from a traditional\nsequential recommendation model and an LLM, respectively. In the second stage,\nwe align the collaborative and semantic embeddings and then identify noise in\nthe interaction sequence based on long-term and short-term interests captured\nin the collaborative and semantic modalities. Our extensive experiments on four\npublic datasets validate the effectiveness of the proposed framework and its\ncompatibility with different sequential recommendation systems."}
{"id": "2510.04502", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04502", "abs": "https://arxiv.org/abs/2510.04502", "authors": ["Yue Que", "Yingyi Zhang", "Xiangyu Zhao", "Chen Ma"], "title": "Causality-aware Graph Aggregation Weight Estimator for Popularity Debiasing in Top-K Recommendation", "comment": "Accepted by CIKM 2025", "summary": "Graph-based recommender systems leverage neighborhood aggregation to generate\nnode representations, which is highly sensitive to popularity bias, resulting\nin an echo effect during information propagation. Existing graph-based\ndebiasing solutions refine the aggregation process with attempts such as edge\nreconstruction or weight adjustment. However, these methods remain inadequate\nin fully alleviating popularity bias. Specifically, this is because 1) they\nprovide no insights into graph aggregation rationality, thus lacking an\noptimality guarantee; 2) they fail to well balance the training and debiasing\nprocess, which undermines the effectiveness. In this paper, we propose a novel\napproach to mitigate popularity bias through rational modeling of the graph\naggregation process. We reveal that graph aggregation is a special form of\nbackdoor adjustment in causal inference, where the aggregation weight\ncorresponds to the historical interaction likelihood distribution. Based on\nthis insight, we devise an encoder-decoder architecture, namely Causality-aware\nGraph Aggregation Weight Estimator for Debiasing (CAGED), to approximate the\nunbiased aggregation weight by optimizing the evidence lower bound of the\ninteraction likelihood. In order to enhance the debiasing effectiveness during\nearly training stages, we further design a momentum update strategy that\nincrementally refines the aggregation weight matrix. Extensive experiments on\nthree datasets demonstrate that CAGED outperforms existing graph-based\ndebiasing methods. Our implementation is available at\nhttps://github.com/QueYork/CAGED."}
{"id": "2510.04508", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.04508", "abs": "https://arxiv.org/abs/2510.04508", "authors": ["Lili Xie", "Yi Zhang", "Ruihong Qiu", "Jiajun Liu", "Sen Wang"], "title": "MARCO: A Cooperative Knowledge Transfer Framework for Personalized Cross-domain Recommendations", "comment": "SIGIR-AP 2025", "summary": "Recommender systems frequently encounter data sparsity issues, particularly\nwhen addressing cold-start scenarios involving new users or items. Multi-source\ncross-domain recommendation (CDR) addresses these challenges by transferring\nvaluable knowledge from multiple source domains to enhance recommendations in a\ntarget domain. However, existing reinforcement learning (RL)-based CDR methods\ntypically rely on a single-agent framework, leading to negative transfer issues\ncaused by inconsistent domain contributions and inherent distributional\ndiscrepancies among source domains. To overcome these limitations, MARCO, a\nMulti-Agent Reinforcement Learning-based Cross-Domain recommendation framework,\nis proposed. It leverages cooperative multi-agent reinforcement learning, where\neach agent is dedicated to estimating the contribution from an individual\nsource domain, effectively managing credit assignment and mitigating negative\ntransfer. In addition, an entropy-based action diversity penalty is introduced\nto enhance policy expressiveness and stabilize training by encouraging diverse\nagents' joint actions. Extensive experiments across four benchmark datasets\ndemonstrate MARCO's superior performance over state-of-the-art methods,\nhighlighting its robustness and strong generalization capabilities. The code is\nat https://github.com/xiewilliams/MARCO."}
{"id": "2510.04633", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.04633", "abs": "https://arxiv.org/abs/2510.04633", "authors": ["Lukas Gienapp", "Martin Potthast", "Harrisen Scells", "Eugene Yang"], "title": "Topic-Specific Classifiers are Better Relevance Judges than Prompted LLMs", "comment": "15 pages, 3 figures, 2 tables", "summary": "The unjudged document problem, where pooled test collections have incomplete\nrelevance judgments for evaluating new retrieval systems, is a key obstacle to\nthe reusability of test collections in information retrieval. While the de\nfacto standard to deal with the problem is to treat unjudged documents as\nnon-relevant, many alternatives have been proposed, including the use of large\nlanguage models (LLMs) as a relevance judge (LLM-as-a-judge). However, this has\nbeen criticized as circular, since the same LLM can be used as a judge and as a\nranker at the same time. We propose to train topic-specific relevance\nclassifiers instead: By finetuning monoT5 with independent LoRA weight\nadaptation on the judgments of a single assessor for a single topic's pool, we\nalign it to that assessor's notion of relevance for the topic. The system\nrankings obtained through our classifier's relevance judgments achieve a\nSpearmans' $\\rho$ correlation of $>0.95$ with ground truth system rankings. As\nlittle as 128 initial human judgments per topic suffice to improve the\ncomparability of models, compared to treating unjudged documents as\nnon-relevant, while achieving more reliability than existing LLM-as-a-judge\napproaches. Topic-specific relevance classifiers thus are a lightweight and\nstraightforward way to tackle the unjudged document problem, while maintaining\nhuman judgments as the gold standard for retrieval evaluation. Code, models,\nand data are made openly available."}
