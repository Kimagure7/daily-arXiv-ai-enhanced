<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 16]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [XProvence: Zero-Cost Multilingual Context Pruning for Retrieval-Augmented Generation](https://arxiv.org/abs/2601.18886)
*Youssef Mohamed,Mohamed Elhoseiny,Thibault Formal,Nadezhda Chirkova*

Main category: cs.IR

TL;DR: XProvence是一个多语言零成本上下文剪枝模型，用于检索增强生成(RAG)，支持100+语言，在16种语言上训练，通过跨语言迁移实现多语言扩展。


<details>
  <summary>Details</summary>
Motivation: 随着RAG系统在不同语言中的广泛应用，需要将原本仅支持英语的Provence框架扩展到多语言环境，以支持全球范围内的多语言RAG应用。

Method: 基于Provence框架，探索了多种策略将其扩展到多语言，在16种语言上训练模型，并通过有效的跨语言迁移支持100+种语言，将高效的零成本上下文剪枝直接集成到重排序模型中。

Result: 在四个多语言问答基准测试中，XProvence能够以最小到无性能损失的方式剪枝RAG上下文，并优于强基线模型。

Conclusion: XProvence成功将零成本上下文剪枝技术扩展到多语言环境，为多语言RAG系统提供了有效的上下文管理解决方案，模型已公开发布。

Abstract: This paper introduces XProvence, a multilingual zero-cost context pruning model for retrieval-augmented generation (RAG), trained on 16 languages and supporting 100+ languages through effective cross-lingual transfer. Motivated by the growing use of RAG systems across diverse languages, we explore several strategies to generalize the Provence framework-which first integrated efficient zero-cost context pruning directly into the re-ranking model-beyond English. Across four multilingual question answering benchmarks, we show how XProvence can prune RAG contexts with minimal-to-no performance degradation and outperforms strong baselines. Our model is available at https://huggingface.co/naver/xprovence-reranker-bgem3-v2.

</details>


### [2] [Recommending Composite Items Using Multi-Level Preference Information: A Joint Interaction Modeling Approach](https://arxiv.org/abs/2601.19005)
*Xuan Bi,Yaqiong Wang,Gediminas Adomavicius,Shawn Curley*

Main category: cs.IR

TL;DR: JIMA是一个联合交互建模方法，使用单一模型整合不同粒度级别的数据，学习原子物品和组合物品的用户偏好以及领域专业知识之间的复杂关系。


<details>
  <summary>Details</summary>
Motivation: 随着推荐系统应用场景变得更加多样和复杂，需要更复杂的推荐技术。特别是在组合物品推荐（如时尚搭配）中，用户偏好信息可能在不同粒度级别上可用且相关。

Method: 提出JIMA方法，使用单一模型整合所有不同粒度的数据，通过交互建模学习低阶（原子物品）和高阶（组合物品）用户偏好以及领域专业知识（如风格匹配）之间的复杂关系。

Result: 通过多个模拟研究和真实数据的离线与在线评估，与先进基线方法比较，结果一致表明所提方法具有优越性能。

Conclusion: JIMA方法能够有效利用多粒度用户偏好信息，在组合物品推荐任务中表现出色，为复杂推荐场景提供了有效的解决方案。

Abstract: With the advancement of machine learning and artificial intelligence technologies, recommender systems have been increasingly used across a vast variety of platforms to efficiently and effectively match users with items. As application contexts become more diverse and complex, there is a growing need for more sophisticated recommendation techniques. One example is the composite item (for example, fashion outfit) recommendation where multiple levels of user preference information might be available and relevant. In this study, we propose JIMA, a joint interaction modeling approach that uses a single model to take advantage of all data from different levels of granularity and incorporate interactions to learn the complex relationships among lower-order (atomic item) and higher-order (composite item) user preferences as well as domain expertise (e.g., on the stylistic fit). We comprehensively evaluate the proposed method and compare it with advanced baselines through multiple simulation studies as well as with real data in both offline and online settings. The results consistently demonstrate the superior performance of the proposed approach.

</details>


### [3] [RobustExplain: Evaluating Robustness of LLM-Based Explanation Agents for Recommendation](https://arxiv.org/abs/2601.19120)
*Guilin Zhang,Kai Zhao,Jeffrey Friedman,Xu Chu*

Main category: cs.IR

TL;DR: 论文提出了RobustExplain框架，首次系统评估LLM生成推荐解释在真实用户行为噪声下的鲁棒性，发现当前模型仅具中等鲁棒性，大模型稳定性提升有限。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LLM生成推荐解释的流畅性和相关性，但忽略了真实用户行为噪声（如误点击、时间不一致、缺失值、偏好变化）对解释稳定性的影响，这关系到用户信任和推荐系统的可靠性。

Method: 提出RobustExplain评估框架，包含五种真实用户行为扰动（在不同严重级别下评估）和多维度鲁棒性指标（语义一致性、关键词一致性、结构一致性、长度一致性）。在四个代表性LLM（7B-70B）上进行实验。

Result: 当前模型仅表现出中等鲁棒性，大模型（70B）比小模型（7B）稳定性提升最多8%。建立了首个解释代理的鲁棒性基准，揭示了鲁棒性作为可信推荐系统关键维度的现状。

Conclusion: 鲁棒性是可信、代理驱动的推荐系统在Web规模应用中的关键维度，RobustExplain为系统评估LLM生成解释的稳定性提供了原则性框架和基准，未来需要进一步改进模型对用户行为噪声的鲁棒性。

Abstract: Large Language Models (LLMs) are increasingly used to generate natural-language explanations in recommender systems, acting as explanation agents that reason over user behavior histories. While prior work has focused on explanation fluency and relevance under fixed inputs, the robustness of LLM-generated explanations to realistic user behavior noise remains largely unexplored. In real-world web platforms, interaction histories are inherently noisy due to accidental clicks, temporal inconsistencies, missing values, and evolving preferences, raising concerns about explanation stability and user trust. We present RobustExplain, the first systematic evaluation framework for measuring the robustness of LLM-generated recommendation explanations. RobustExplain introduces five realistic user behavior perturbations evaluated across multiple severity levels and a multi-dimensional robustness metric capturing semantic, keyword, structural, and length consistency. Our goal is to establish a principled, task-level evaluation framework and initial robustness baselines, rather than to provide a comprehensive leaderboard across all available LLMs. Experiments on four representative LLMs (7B--70B) show that current models exhibit only moderate robustness, with larger models achieving up to 8% higher stability. Our results establish the first robustness benchmarks for explanation agents and highlight robustness as a critical dimension for trustworthy, agent-driven recommender systems at web scale.

</details>


### [4] [LLMs as Orchestrators: Constraint-Compliant Multi-Agent Optimization for Recommendation Systems](https://arxiv.org/abs/2601.19121)
*Guilin Zhang,Kai Zhao,Jeffrey Friedman,Xu Chu*

Main category: cs.IR

TL;DR: DualAgent-Rec：基于LLM协调的双智能体框架，用于约束多目标电商推荐，实现100%约束满足和4-6%的帕累托超体积提升


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统在处理硬业务约束（如公平性、覆盖率）时存在不足，传统方法将约束视为软惩罚，导致实际部署中频繁违反约束。LLM在推荐系统约束协调优化中的应用尚未充分探索。

Method: 提出DualAgent-Rec框架：1）利用智能体负责在硬约束下优先考虑准确性；2）探索智能体通过无约束帕累托搜索促进多样性；3）基于LLM的协调器根据优化进度和约束满足情况自适应分配资源；4）自适应epsilon松弛机制保证最终解的可行性。

Result: 在Amazon Reviews 2023数据集上的实验表明：DualAgent-Rec实现了100%的约束满足率，帕累托超体积比强基线提升4-6%，同时保持了竞争力的准确性-多样性权衡。

Conclusion: LLM可以作为有效的编排智能体，用于可部署且符合约束的推荐系统，为解决实际生产中的硬约束优化问题提供了新思路。

Abstract: Recommendation systems must optimize multiple objectives while satisfying hard business constraints such as fairness and coverage. For example, an e-commerce platform may require every recommendation list to include items from multiple sellers and at least one newly listed product; violating such constraints--even once--is unacceptable in production. Prior work on multi-objective recommendation and recent LLM-based recommender agents largely treat constraints as soft penalties or focus on item scoring and interaction, leading to frequent violations in real-world deployments. How to leverage LLMs for coordinating constrained optimization in recommendation systems remains underexplored. We propose DualAgent-Rec, an LLM-coordinated dual-agent framework for constrained multi-objective e-commerce recommendation. The framework separates optimization into an Exploitation Agent that prioritizes accuracy under hard constraints and an Exploration Agent that promotes diversity through unconstrained Pareto search. An LLM-based coordinator adaptively allocates resources between agents based on optimization progress and constraint satisfaction, while an adaptive epsilon-relaxation mechanism guarantees feasibility of final solutions. Experiments on the Amazon Reviews 2023 dataset demonstrate that DualAgent-Rec achieves 100% constraint satisfaction and improves Pareto hypervolume by 4-6% over strong baselines, while maintaining competitive accuracy-diversity trade-offs. These results indicate that LLMs can act as effective orchestration agents for deployable and constraint-compliant recommendation systems.

</details>


### [5] [Accelerating Generative Recommendation via Simple Categorical User Sequence Compression](https://arxiv.org/abs/2601.19158)
*Qijiong Liu,Lu Fan,Zhongzhou Liu,Xiaoyu Dong,Yuankai Luo,Guoyuan An,Nuo Chen,Wei Guo,Yong Liu,Xiao-Ming Wu*

Main category: cs.IR

TL;DR: 提出基于物品类别特征压缩长序列用户历史的方法，在降低计算成本的同时提升推荐性能


<details>
  <summary>Details</summary>
Motivation: 生成式推荐系统虽然长序列能提升性能，但实际部署面临高计算成本问题，需要高效的序列压缩方法

Method: 利用物品固有的类别特征来压缩长期用户历史，在保持用户兴趣的同时提高效率

Result: 在两个大规模数据集上，相比HSTU模型，计算成本降低6倍，在相似序列长度下准确率提升39%

Conclusion: 提出的简单有效方法能显著降低计算成本并提升推荐准确性，解决了生成式推荐系统实时部署的瓶颈

Abstract: Although generative recommenders demonstrate improved performance with longer sequences, their real-time deployment is hindered by substantial computational costs. To address this challenge, we propose a simple yet effective method for compressing long-term user histories by leveraging inherent item categorical features, thereby preserving user interests while enhancing efficiency. Experiments on two large-scale datasets demonstrate that, compared to the influential HSTU model, our approach achieves up to a 6x reduction in computational cost and up to 39% higher accuracy at comparable cost (i.e., similar sequence length).

</details>


### [6] [HELM: A Human-Centered Evaluation Framework for LLM-Powered Recommender Systems](https://arxiv.org/abs/2601.19197)
*Sushant Mehta*

Main category: cs.IR

TL;DR: 提出了HELM框架，用于系统评估LLM推荐系统在五个以人为中心的维度上的表现，弥补传统准确性指标的不足。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法主要关注传统准确性指标，无法捕捉决定真实用户体验的多方面以人为中心的品质。LLM在推荐系统中的集成带来了前所未有的能力，但需要更全面的评估框架。

Method: 提出HELM框架，系统评估LLM推荐系统在五个维度：意图对齐、解释质量、交互自然性、信任与透明度、公平性与多样性。使用三个最先进的LLM推荐系统（GPT-4、LLaMA-3.1、P5）在三个领域（电影、书籍、餐厅）进行实验，由12位领域专家对847个推荐场景进行严格评估。

Result: HELM揭示了传统指标无法看到的批判性质量维度。GPT-4在解释质量（4.21/5.0）和交互自然性（4.35/5.0）方面表现优异，但表现出显著的热门偏见（基尼系数0.73），而传统协同过滤为0.58。

Conclusion: HELM框架为推荐系统社区提供了全面的以人为中心的评估工具，揭示了LLM推荐系统的优势和局限性，并作为开源工具包发布以推进该领域的评估实践。

Abstract: The integration of Large Language Models (LLMs) into recommendation systems has introduced unprecedented capabilities for natural language understanding, explanation generation, and conversational interactions. However, existing evaluation methodologies focus predominantly on traditional accuracy metrics, failing to capture the multifaceted human-centered qualities that determine the real-world user experience. We introduce \framework{} (\textbf{H}uman-centered \textbf{E}valuation for \textbf{L}LM-powered reco\textbf{M}menders), a comprehensive evaluation framework that systematically assesses LLM-powered recommender systems across five human-centered dimensions: \textit{Intent Alignment}, \textit{Explanation Quality}, \textit{Interaction Naturalness}, \textit{Trust \& Transparency}, and \textit{Fairness \& Diversity}. Through extensive experiments involving three state-of-the-art LLM-based recommenders (GPT-4, LLaMA-3.1, and P5) across three domains (movies, books, and restaurants), and rigorous evaluation by 12 domain experts using 847 recommendation scenarios, we demonstrate that \framework{} reveals critical quality dimensions invisible to traditional metrics. Our results show that while GPT-4 achieves superior explanation quality (4.21/5.0) and interaction naturalness (4.35/5.0), it exhibits a significant popularity bias (Gini coefficient 0.73) compared to traditional collaborative filtering (0.58). We release \framework{} as an open-source toolkit to advance human-centered evaluation practices in the recommender systems community.

</details>


### [7] [Propagating Similarity, Mitigating Uncertainty: Similarity Propagation-enhanced Uncertainty for Multimodal Recommendation](https://arxiv.org/abs/2601.19198)
*Xinzhuo Wu,Hongbo Wang,Yuan Lin,Kan Xu,Liang Yang,Hongfei Lin*

Main category: cs.IR

TL;DR: SPUMR是一个新的多模态推荐框架，通过相似性传播增强的不确定性建模来改进推荐效果，解决了模态特征中的噪声和不确定性问题。


<details>
  <summary>Details</summary>
Motivation: 多模态推荐系统面临模态特征固有的噪声和不确定性（如模糊图像、多样视觉外观、模糊文本），现有方法往往忽视模态特定的不确定性，导致特征融合效果不佳，且未能利用用户和物品间的丰富相似性模式来优化表示和不确定性估计。

Method: SPUMR首先构建模态相似性图和协同相似性图，从内容和行为角度优化表示；然后通过不确定性感知偏好聚合模块自适应融合多模态特征，为更可靠的模态分配更大权重。

Result: 在三个基准数据集上的广泛实验表明，SPUMR相比现有领先方法取得了显著改进。

Conclusion: SPUMR通过显式建模和缓解不确定性，并利用相似性传播来优化表示，有效提升了多模态推荐系统的性能。

Abstract: Multimodal Recommendation (MMR) systems are crucial for modern platforms but are often hampered by inherent noise and uncertainty in modal features, such as blurry images, diverse visual appearances, or ambiguous text. Existing methods often overlook this modality-specific uncertainty, leading to ineffective feature fusion. Furthermore, they fail to leverage rich similarity patterns among users and items to refine representations and their corresponding uncertainty estimates. To address these challenges, we propose a novel framework, Similarity Propagation-enhanced Uncertainty for Multimodal Recommendation (SPUMR). SPUMR explicitly models and mitigates uncertainty by first constructing the Modality Similarity Graph and the Collaborative Similarity Graph to refine representations from both content and behavioral perspectives. The Uncertainty-aware Preference Aggregation module then adaptively fuses the refined multimodal features, assigning greater weight to more reliable modalities. Extensive experiments on three benchmark datasets demonstrate that SPUMR achieves significant improvements over existing leading methods.

</details>


### [8] [Physics-Informed Neuro-Symbolic Recommender System: A Dual-Physics Approach for Personalized Nutrition](https://arxiv.org/abs/2601.19244)
*Chayan Banerjee*

Main category: cs.IR

TL;DR: 提出物理信息神经符号推荐系统，将营养科学融入推荐流程，确保食品推荐符合用户能量和营养需求


<details>
  <summary>Details</summary>
Motivation: 传统电商推荐系统主要优化用户参与度和购买可能性，忽视人类健康所需的刚性生理约束。标准协同过滤算法在结构上无视这些硬性限制，经常推荐不符合每日总能量消耗和宏量营养素平衡要求的食品组合。

Method: 采用双层次架构：1) 使用句子级编码器构建语义知识图谱，将商业产品与权威营养数据严格对齐；2) 训练阶段使用隐式物理正则化器，应用可微分热力学损失函数，确保学习的潜在嵌入反映营养合理性而非简单流行度；3) 推理阶段使用显式物理优化器，采用模拟退火和弹性数量优化生成严格符合用户蛋白质和热量目标的离散食品组合。

Result: 论文提出了一个能够生成符合营养约束的食品推荐框架，但摘要中未提供具体的实验结果数据。

Conclusion: 该物理信息神经符号推荐系统成功将营养科学直接整合到推荐流程中，解决了传统推荐系统忽视健康约束的问题，能够生成严格符合用户生理需求的食品组合。

Abstract: Traditional e-commerce recommender systems primarily optimize for user engagement and purchase likelihood, often neglecting the rigid physiological constraints required for human health. Standard collaborative filtering algorithms are structurally blind to these hard limits, frequently suggesting bundles that fail to meet specific total daily energy expenditure and macronutrient balance requirements. To address this disconnect, this paper introduces a Physics-Informed Neuro-Symbolic Recommender System that integrates nutritional science directly into the recommendation pipeline via a dual-layer architecture. The framework begins by constructing a semantic knowledge graph using sentence-level encoders to strictly align commercial products with authoritative nutritional data. During the training phase, an implicit physics regularizer applies a differentiable thermodynamic loss function, ensuring that learned latent embeddings reflect nutritional plausibility rather than simple popularity. Subsequently, during the inference phase, an explicit physics optimizer employs simulated annealing and elastic quantity optimization to generate discrete grocery bundles that strictly adhere to the user's protein and caloric targets.

</details>


### [9] [Talos: Optimizing Top-$K$ Accuracy in Recommender Systems](https://arxiv.org/abs/2601.19276)
*Shengjia Zhang,Weiqin Yang,Jiawei Chen,Peng Wu,Yuegang Sun,Gang Wang,Qihao Shi,Can Wang*

Main category: cs.IR

TL;DR: Talos是一种专门优化Top-K推荐准确性的损失函数，使用分位数技术将复杂的排名相关操作简化为预测分数与学习阈值之间的比较，并通过采样回归算法、约束项和代理函数提高效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 推荐系统主要关注Top-K结果质量，但评估Top-K准确性需要确定物品的排名位置，这带来了巨大的计算开销和优化挑战。此外，推荐系统常面临用户偏好演变或数据偏差导致的分布偏移问题。

Method: 提出Talos损失函数，使用分位数技术将排名相关操作转换为预测分数与学习阈值之间的比较；开发采样回归算法进行高效准确的阈值估计；引入约束项防止分数膨胀；设计定制代理函数处理不连续性和增强对分布偏移的鲁棒性。

Result: 通过全面的理论分析和实证实验，证明了Talos在有效性、效率、收敛性和分布鲁棒性方面的优越性。

Conclusion: Talos能够有效优化Top-K推荐准确性，解决了传统方法中的计算复杂性和分布偏移问题，为推荐系统提供了一种高效且鲁棒的优化方案。

Abstract: Recommender systems (RS) aim to retrieve a small set of items that best match individual user preferences. Naturally, RS place primary emphasis on the quality of the Top-$K$ results rather than performance across the entire item set. However, estimating Top-$K$ accuracy (e.g., Precision@$K$, Recall@$K$) requires determining the ranking positions of items, which imposes substantial computational overhead and poses significant challenges for optimization. In addition, RS often suffer from distribution shifts due to evolving user preferences or data biases, further complicating the task.
  To address these issues, we propose Talos, a loss function that is specifically designed to optimize the Talos recommendation accuracy. Talos leverages a quantile technique that replaces the complex ranking-dependent operations into simpler comparisons between predicted scores and learned score thresholds. We further develop a sampling-based regression algorithm for efficient and accurate threshold estimation, and introduce a constraint term to maintain optimization stability by preventing score inflation. Additionally, we incorporate a tailored surrogate function to address discontinuity and enhance robustness against distribution shifts. Comprehensive theoretical analyzes and empirical experiments are conducted to demonstrate the effectiveness, efficiency, convergence, and distributional robustness of Talos. The code is available at https://github.com/cynthia-shengjia/WWW-2026-Talos.

</details>


### [10] [UniRec: Unified Multimodal Encoding for LLM-Based Recommendations](https://arxiv.org/abs/2601.19423)
*Zijie Lei,Tao Feng,Zhigang Hua,Yan Xie,Guanyu Lin,Shuang Yang,Ge Liu,Jiaxuan You*

Main category: cs.IR

TL;DR: UniRec是一个统一的LLM多模态推荐编码器，通过模态特定编码器、三元组表示和分层Q-Former处理文本、图像、分类和数值四种模态，解决多模态异质性和嵌套结构挑战，在多个基准上优于现有方法15%。


<details>
  <summary>Details</summary>
Motivation: 现有LLM多模态推荐主要关注文本和图像，但真实推荐信号包含更多模态（分类特征和数值属性）。多模态异质性带来跨模态和模态内挑战，特别是数值属性（如价格、评分、时间）具有不同语义含义。此外，推荐信号具有嵌套结构（用户历史是项目序列，每个项目有多个属性），这给LLM理解多模态信息带来独特挑战。

Method: UniRec采用三阶段方法：1) 使用模态特定编码器为异质信号生成一致嵌入；2) 采用三元组表示（属性名、类型、值）分离模式与原始输入，保持语义区分；3) 使用分层Q-Former建模用户交互的嵌套结构，同时保持分层组织。

Result: 在多个真实世界基准测试中，UniRec比最先进的多模态和LLM推荐器性能提升高达15%。广泛的消融研究进一步验证了每个组件的贡献。

Conclusion: UniRec通过统一的编码框架有效解决了多模态推荐中的异质性和嵌套结构挑战，证明了在LLM推荐中全面考虑多种模态和结构信息的重要性。

Abstract: Large language models have recently shown promise for multimodal recommendation, particularly with text and image inputs. Yet real-world recommendation signals extend far beyond these modalities. To reflect this, we formalize recommendation features into four modalities: text, images, categorical features, and numerical attributes, and highlight the unique challenges this heterogeneity poses for LLMs in understanding multimodal information. In particular, these challenges arise not only across modalities but also within them, as attributes such as price, rating, and time may all be numeric yet carry distinct semantic meanings. Beyond this intra-modality ambiguity, another major challenge is the nested structure of recommendation signals, where user histories are sequences of items, each associated with multiple attributes. To address these challenges, we propose UniRec, a unified multimodal encoder for LLM-based recommendation. UniRec first employs modality-specific encoders to produce consistent embeddings across heterogeneous signals. It then adopts a triplet representation, comprising attribute name, type, and value, to separate schema from raw inputs and preserve semantic distinctions. Finally, a hierarchical Q-Former models the nested structure of user interactions while maintaining their layered organization. Across multiple real-world benchmarks, UniRec outperforms state-of-the-art multimodal and LLM-based recommenders by up to 15%, and extensive ablation studies further validate the contributions of each component.

</details>


### [11] [Masked Diffusion Generative Recommendation](https://arxiv.org/abs/2601.19501)
*Lingyu Mu,Hao Deng,Haibo Xing,Jinxin Hu,Yu Zhang,Xiaoyi Zeng,Jing Zhang*

Main category: cs.IR

TL;DR: 提出MDGR框架，使用掩码扩散生成推荐，解决传统自回归解码的三个限制，在多个数据集上表现优异并提升在线广告收入1.20%


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐的自回归解码存在三个关键限制：1) 难以联合捕捉SID多维特征的全局依赖；2) 固定解码路径假设所有用户以相同顺序关注项目属性；3) 推理效率低难以满足实时需求

Method: 提出MDGR掩码扩散生成推荐框架，从三个角度重塑GR流程：1) 采用并行码本为扩散基础；2) 训练时沿时间和样本维度自适应构建掩码监督信号；3) 推理时使用基于预热的两阶段并行解码策略

Result: 在多个公共和工业级数据集上超越10个SOTA基线达10.78%，在大型在线广告平台部署实现收入提升1.20%

Conclusion: MDGR通过掩码扩散方法有效解决了自回归解码的限制，在推荐性能和推理效率上均有显著提升，具有实际应用价值

Abstract: Generative recommendation (GR) typically first quantizes continuous item embeddings into multi-level semantic IDs (SIDs), and then generates the next item via autoregressive decoding. Although existing methods are already competitive in terms of recommendation performance, directly inheriting the autoregressive decoding paradigm from language models still suffers from three key limitations: (1) autoregressive decoding struggles to jointly capture global dependencies among the multi-dimensional features associated with different positions of SID; (2) using a unified, fixed decoding path for the same item implicitly assumes that all users attend to item attributes in the same order; (3) autoregressive decoding is inefficient at inference time and struggles to meet real-time requirements. To tackle these challenges, we propose MDGR, a Masked Diffusion Generative Recommendation framework that reshapes the GR pipeline from three perspectives: codebook, training, and inference. (1) We adopt a parallel codebook to provide a structural foundation for diffusion-based GR. (2) During training, we adaptively construct masking supervision signals along both the temporal and sample dimensions. (3) During inference, we develop a warm-up-based two-stage parallel decoding strategy for efficient generation of SIDs. Extensive experiments on multiple public and industrial-scale datasets show that MDGR outperforms ten state-of-the-art baselines by up to 10.78%. Furthermore, by deploying MDGR on a large-scale online advertising platform, we achieve a 1.20% increase in revenue, demonstrating its practical value. The code will be released upon acceptance.

</details>


### [12] [Enhancing Academic Paper Recommendations Using Fine-Grained Knowledge Entities and Multifaceted Document Embeddings](https://arxiv.org/abs/2601.19513)
*Haixu Xi,Heng Zhang,Chengzhi Zhang*

Main category: cs.IR

TL;DR: 提出一种新颖的学术论文推荐方法，通过整合细粒度知识实体、文档标题摘要和引用数据来嵌入多维信息，相比现有基于主题相似性的粗粒度推荐系统，能更好地满足学者对特定研究方法或任务的文献需求。


<details>
  <summary>Details</summary>
Motivation: 学术文献爆炸式增长增加了学者的文献综述负担。现有推荐系统主要基于宽泛的主题相似性进行粗粒度推荐，无法满足学者在研究过程中对特定研究方法、任务等细粒度文献需求。

Method: 提出一种新颖的学术论文推荐方法，通过整合细粒度知识实体、文档标题摘要和引用数据来嵌入多维信息，然后通过计算组合论文向量之间的相似性来生成推荐。

Result: 在STM-KG数据集（包含十个不同领域科学概念的知识图谱）上的实验结果表明，该方法优于基线模型，在前50个推荐中平均精度达到27.3%，比现有方法提高了6.7%。

Conclusion: 该方法能够更好地满足学者在研究过程中的多样化和具体化文献需求，通过细粒度知识实体整合和多维信息嵌入，显著提升了学术论文推荐的精度和实用性。

Abstract: In the era of explosive growth in academic literature, the burden of literature review on scholars are increasing. Proactively recommending academic papers that align with scholars' literature needs in the research process has become one of the crucial pathways to enhance research efficiency and stimulate innovative thinking. Current academic paper recommendation systems primarily focus on broad and coarse-grained suggestions based on general topic or field similarities. While these systems effectively identify related literature, they fall short in addressing scholars' more specific and fine-grained needs, such as locating papers that utilize particular research methods, or tackle distinct research tasks within the same topic. To meet the diverse and specific literature needs of scholars in the research process, this paper proposes a novel academic paper recommendation method. This approach embeds multidimensional information by integrating new types of fine-grained knowledge entities, title and abstract of document, and citation data. Recommendations are then generated by calculating the similarity between combined paper vectors. The proposed recommendation method was evaluated using the STM-KG dataset, a knowledge graph that incorporates scientific concepts derived from papers across ten distinct domains. The experimental results indicate that our method outperforms baseline models, achieving an average precision of 27.3% among the top 50 recommendations. This represents an improvement of 6.7% over existing approaches.

</details>


### [13] [LURE-RAG: Lightweight Utility-driven Reranking for Efficient RAG](https://arxiv.org/abs/2601.19535)
*Manish Chandra,Debasis Ganguly,Iadh Ounis*

Main category: cs.IR

TL;DR: LURE-RAG提出了一种轻量级效用驱动的重排序框架，通过LambdaMART重排序器和列表排序损失优化检索文档的排序，提升RAG生成质量，在保持高效的同时达到接近SOTA的性能。


<details>
  <summary>Details</summary>
Motivation: 传统RAG管道依赖相关性检索，但相关性常与效用（即检索到的段落是否真正提升下游任务生成质量）不匹配。现有效用驱动检索方法存在两个局限：1) 需要查询编码，资源密集；2) 训练中未使用列表排序损失，而文档相对顺序直接影响RAG生成效果。

Method: 提出LURE-RAG框架，为任何黑盒检索器添加高效的LambdaMART重排序器。与先前方法不同，LURE-RAG使用LLM效用指导的列表排序损失训练重排序器，直接优化检索文档的排序。还提出了其密集变体UR-RAG。

Result: 在两个标准数据集上的实验表明，LURE-RAG达到竞争性性能，达到最先进密集神经基线的97-98%，同时在训练和推理中保持高效。其密集变体UR-RAG显著优于现有最佳基线，提升高达3%。

Conclusion: LURE-RAG通过轻量级效用驱动重排序有效解决了传统RAG中检索与效用不匹配的问题，在保持效率的同时显著提升生成质量，为RAG系统提供了实用的优化方案。

Abstract: Most conventional Retrieval-Augmented Generation (RAG) pipelines rely on relevance-based retrieval, which often misaligns with utility -- that is, whether the retrieved passages actually improve the quality of the generated text specific to a downstream task such as question answering or query-based summarization. The limitations of existing utility-driven retrieval approaches for RAG are that, firstly, they are resource-intensive typically requiring query encoding, and that secondly, they do not involve listwise ranking loss during training. The latter limitation is particularly critical, as the relative order between documents directly affects generation in RAG. To address this gap, we propose Lightweight Utility-driven Reranking for Efficient RAG (LURE-RAG), a framework that augments any black-box retriever with an efficient LambdaMART-based reranker. Unlike prior methods, LURE-RAG trains the reranker with a listwise ranking loss guided by LLM utility, thereby directly optimizing the ordering of retrieved documents. Experiments on two standard datasets demonstrate that LURE-RAG achieves competitive performance, reaching 97-98% of the state-of-the-art dense neural baseline, while remaining efficient in both training and inference. Moreover, its dense variant, UR-RAG, significantly outperforms the best existing baseline by up to 3%.

</details>


### [14] [Comparing how Large Language Models perform against keyword-based searches for social science research data discovery](https://arxiv.org/abs/2601.19559)
*Mark Green,Maura Halstead,Caroline Jay,Richard Kingston,Alex Singleton,David Topping*

Main category: cs.IR

TL;DR: 比较基于大语言模型的语义搜索与传统关键词搜索在数据发现中的性能，发现语义搜索在结果数量、处理复杂查询和容错方面表现更优，但两者排名策略不同，建议互补使用。


<details>
  <summary>Details</summary>
Motivation: 评估基于大语言模型（LLM）的语义搜索工具相对于传统关键词搜索在数据发现中的性能表现，探索语义搜索是否能提供更好的搜索体验和结果质量。

Method: 使用英国研究创新署（UKRI）数据服务的真实搜索行为，比较定制语义搜索系统与消费者数据研究中心（CDRC）关键词搜索。基于2023年12月至2024年10月CDRC搜索日志中提取的131个最常用搜索词，通过描述性统计、定性检查和定量相似性度量（包括精确数据集重叠、Jaccard相似性和BERT嵌入的余弦相似性）评估返回数据集的差异。

Result: 语义搜索始终返回比关键词搜索更多的结果，在处理基于地点、拼写错误、模糊或复杂查询方面表现尤为出色。虽然语义搜索未能捕获所有基于关键词的结果，但返回的数据集在语义上高度相似，余弦相似性得分高而精确重叠率较低。两种工具的排名策略差异显著。案例研究表明，基于LLM的工具对拼写错误具有鲁棒性，能有效解释地理和上下文相关性，并支持关键词搜索无法解决的自然语言查询。

Conclusion: 基于大语言模型的语义搜索为数据发现提供了实质性改进，但并非完全取代传统关键词搜索方法，而是作为互补工具使用。

Abstract: This paper evaluates the performance of a large language model (LLM) based semantic search tool relative to a traditional keyword-based search for data discovery. Using real-world search behaviour, we compare outputs from a bespoke semantic search system applied to UKRI data services with the Consumer Data Research Centre (CDRC) keyword search. Analysis is based on 131 of the most frequently used search terms extracted from CDRC search logs between December 2023 and October 2024. We assess differences in the volume, overlap, ranking, and relevance of returned datasets using descriptive statistics, qualitative inspection, and quantitative similarity measures, including exact dataset overlap, Jaccard similarity, and cosine similarity derived from BERT embeddings. Results show that the semantic search consistently returns a larger number of results than the keyword search and performs particularly well for place based, misspelled, obscure, or complex queries. While the semantic search does not capture all keyword based results, the datasets returned are overwhelmingly semantically similar, with high cosine similarity scores despite lower exact overlap. Rankings of the most relevant results differ substantially between tools, reflecting contrasting prioritisation strategies. Case studies demonstrate that the LLM based tool is robust to spelling errors, interprets geographic and contextual relevance effectively, and supports natural-language queries that keyword search fails to resolve. Overall, the findings suggest that LLM driven semantic search offers a substantial improvement for data discovery, complementing rather than fully replacing traditional keyword-based approaches.

</details>


### [15] [LLM-Enhanced Reinforcement Learning for Long-Term User Satisfaction in Interactive Recommendation](https://arxiv.org/abs/2601.19585)
*Chongjun Xia,Yanchun Peng,Xianzhi Wang*

Main category: cs.IR

TL;DR: 提出LLM增强强化学习框架LERL，通过LLM进行高层语义规划选择多样内容类别，RL进行低层个性化推荐，解决推荐系统中的内容同质化和长期用户满意度问题。


<details>
  <summary>Details</summary>
Motivation: 交互式推荐系统容易因过度拟合短期用户偏好而导致内容同质化和过滤气泡效应。现有方法多在静态或一次性设置中运作，忽视了用户兴趣的长期演化。强化学习虽能优化长期满意度，但受限于稀疏的用户-物品交互和有限的语义规划能力。

Method: 提出分层推荐框架LERL：高层使用LLM作为规划器选择语义多样的内容类别，低层使用RL策略在选定语义空间内推荐个性化物品。这种分层设计缩小了动作空间，提高了规划效率，并减少了冗余内容的过度曝光。

Result: 在真实世界数据集上的大量实验表明，与最先进的基线方法相比，LERL显著提高了长期用户满意度。

Conclusion: LERL成功地将LLM的语义规划能力与RL的细粒度适应性相结合，有效解决了推荐系统中的内容多样性问题和长期用户满意度优化挑战。

Abstract: Interactive recommender systems can dynamically adapt to user feedback, but often suffer from content homogeneity and filter bubble effects due to overfitting short-term user preferences. While recent efforts aim to improve content diversity, they predominantly operate in static or one-shot settings, neglecting the long-term evolution of user interests. Reinforcement learning provides a principled framework for optimizing long-term user satisfaction by modeling sequential decision-making processes. However, its application in recommendation is hindered by sparse, long-tailed user-item interactions and limited semantic planning capabilities. In this work, we propose LLM-Enhanced Reinforcement Learning (LERL), a novel hierarchical recommendation framework that integrates the semantic planning power of LLM with the fine-grained adaptability of RL. LERL consists of a high-level LLM-based planner that selects semantically diverse content categories, and a low-level RL policy that recommends personalized items within the selected semantic space. This hierarchical design narrows the action space, enhances planning efficiency, and mitigates overexposure to redundant content. Extensive experiments on real-world datasets demonstrate that LERL significantly improves long-term user satisfaction when compared with state-of-the-art baselines. The implementation of LERL is available at https://anonymous.4open.science/r/code3-18D3/.

</details>


### [16] [Differentiable Semantic ID for Generative Recommendation](https://arxiv.org/abs/2601.19711)
*Junchen Fu,Xuri Ge,Alexandros Karatzoglou,Ioannis Arapakis,Suzan Verberne,Joemon M. Jose,Zhaochun Ren*

Main category: cs.IR

TL;DR: 提出DIGER方法，通过可微分语义ID解决生成式推荐中索引与推荐目标不匹配问题，使用Gumbel噪声促进早期代码探索，防止码本崩溃


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐方法将语义ID视为预定义，独立训练分词器用于内容重建而非推荐准确性，导致索引损失与推荐损失目标不匹配，且推荐梯度无法更新分词器

Method: 提出DIGER方法：1) 引入Gumbel噪声促进早期代码探索，防止码本崩溃；2) 设计两种不确定性衰减策略，逐步减少Gumbel噪声，实现从探索到利用的平滑过渡

Result: 在多个公共数据集上的实验显示，可微分语义ID带来一致改进，验证了通过可微分语义ID对齐索引与推荐目标的有效性

Conclusion: 可微分语义索引是生成式推荐中有前景的研究方向，DIGER通过促进早期探索和平衡探索-利用，有效解决了码本崩溃问题，提高了代码利用率

Abstract: Generative recommendation provides a novel paradigm in which each item is represented by a discrete semantic ID (SID) learned from rich content. Most existing methods treat SIDs as predefined and train recommenders under static indexing. In practice, SIDs are typically optimized only for content reconstruction rather than recommendation accuracy. This leads to an objective mismatch: the system optimizes an indexing loss to learn the SID and a recommendation loss for interaction prediction, but because the tokenizer is trained independently, the recommendation loss cannot update it. A natural approach is to make semantic indexing differentiable so that recommendation gradients can directly influence SID learning, but this often causes codebook collapse, where only a few codes are used. We attribute this issue to early deterministic assignments that limit codebook exploration, resulting in imbalance and unstable optimization.
  In this paper, we propose DIGER (Differentiable Semantic ID for Generative Recommendation), a first step toward effective differentiable semantic IDs for generative recommendation. DIGER introduces Gumbel noise to explicitly encourage early-stage exploration over codes, mitigating codebook collapse and improving code utilization. To balance exploration and convergence, we further design two uncertainty decay strategies that gradually reduce the Gumbel noise, enabling a smooth transition from early exploration to exploitation of learned SIDs. Extensive experiments on multiple public datasets demonstrate consistent improvements from differentiable semantic IDs. These results confirm the effectiveness of aligning indexing and recommendation objectives through differentiable SIDs and highlight differentiable semantic indexing as a promising research direction.

</details>
