{"id": "2510.02512", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.02512", "abs": "https://arxiv.org/abs/2510.02512", "authors": ["Fangzheng Tian", "Debasis Ganguly", "Craig Macdonald"], "title": "Revisiting Query Variants: The Advantage of Retrieval Over Generation of Query Variants for Effective QPP", "comment": "11 pages, 4 figures", "summary": "Leveraging query variants (QVs), i.e., queries with potentially similar\ninformation needs to the target query, has been shown to improve the\neffectiveness of query performance prediction (QPP) approaches. Existing\nQV-based QPP methods generate QVs facilitated by either query expansion or\nnon-contextual embeddings, which may introduce topical drifts and\nhallucinations. In this paper, we propose a method that retrieves QVs from a\ntraining set (e.g., MS MARCO) for a given target query of QPP. To achieve a\nhigh recall in retrieving queries with the most similar information needs as\nthe target query from a training set, we extend the directly retrieved QVs\n(1-hop QVs) by a second retrieval using their denoted relevant documents (which\nyields 2-hop QVs). Our experiments, conducted on TREC DL'19 and DL'20, show\nthat the QPP methods with QVs retrieved by our method outperform the\nbest-performing existing generated-QV-based QPP approaches by as much as around\n20\\%, on neural ranking models like MonoT5."}
{"id": "2510.02656", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.02656", "abs": "https://arxiv.org/abs/2510.02656", "authors": ["Qianfeng Wen", "Yifan Liu", "Justin Cui", "Joshua Zhang", "Anton Korikov", "George-Kirollos Saad", "Scott Sanner"], "title": "A Simple but Effective Elaborative Query Reformulation Approach for Natural Language Recommendation", "comment": "11 pages, 5 figures", "summary": "Natural Language (NL) recommender systems aim to retrieve relevant items from\nfree-form user queries and item descriptions. Existing systems often rely on\ndense retrieval (DR), which struggles to interpret challenging queries that\nexpress broad (e.g., \"cities for youth friendly activities\") or indirect (e.g.,\n\"cities for a high school graduation trip\") user intents. While query\nreformulation (QR) has been widely adopted to improve such systems, existing QR\nmethods tend to focus only on expanding the range of query subtopics (breadth)\nor elaborating on the potential meaning of a query (depth), but not both. In\nthis paper, we propose EQR (Elaborative Subtopic Query Reformulation), a large\nlanguage model-based QR method that combines both breadth and depth by\ngenerating potential query subtopics with information-rich elaborations. We\nalso introduce three new natural language recommendation benchmarks in travel,\nhotel, and restaurant domains to establish evaluation of NL recommendation with\nchallenging queries. Experiments show EQR substantially outperforms\nstate-of-the-art QR methods in various evaluation metrics, highlighting that a\nsimple yet effective QR approach can significantly improve NL recommender\nsystems for queries with broad and indirect user intents."}
{"id": "2510.02657", "categories": ["cs.IR", "cs.CL", "H.3.3; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.02657", "abs": "https://arxiv.org/abs/2510.02657", "authors": ["Jingjie Ning", "Yibo Kong", "Yunfan Long", "Jamie Callan"], "title": "Less LLM, More Documents: Searching for Improved RAG", "comment": "16 pages. Submitted to ECIR 2026", "summary": "Retrieval-Augmented Generation (RAG) couples document retrieval with large\nlanguage models (LLMs). While scaling generators improves accuracy, it also\nraises cost and limits deployability. We explore an orthogonal axis: enlarging\nthe retriever's corpus to reduce reliance on large LLMs. Experimental results\nshow that corpus scaling consistently strengthens RAG and can often serve as a\nsubstitute for increasing model size, though with diminishing returns at larger\nscales. Small- and mid-sized generators paired with larger corpora often rival\nmuch larger models with smaller corpora; mid-sized models tend to gain the\nmost, while tiny and large models benefit less. Our analysis shows that\nimprovements arise primarily from increased coverage of answer-bearing\npassages, while utilization efficiency remains largely unchanged. These\nfindings establish a principled corpus-generator trade-off: investing in larger\ncorpora offers an effective path to stronger RAG, often comparable to enlarging\nthe LLM itself."}
{"id": "2510.02668", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02668", "abs": "https://arxiv.org/abs/2510.02668", "authors": ["Bo Ma", "Hang Li", "ZeHua Hu", "XiaoFan Gui", "LuYao Liu", "Simon Liu"], "title": "AgenticRAG: Tool-Augmented Foundation Models for Zero-Shot Explainable Recommender Systems", "comment": null, "summary": "Foundation models have revolutionized artificial intelligence, yet their\napplication in recommender systems remains limited by reasoning opacity and\nknowledge constraints. This paper introduces AgenticRAG, a novel framework that\ncombines tool-augmented foundation models with retrieval-augmented generation\nfor zero-shot explainable recommendations. Our approach integrates external\ntool invocation, knowledge retrieval, and chain-of-thought reasoning to create\nautonomous recommendation agents capable of transparent decision-making without\ntask-specific training. Experimental results on three real-world datasets\ndemonstrate that AgenticRAG achieves consistent improvements over\nstate-of-the-art baselines, with NDCG@10 improvements of 0.4\\% on Amazon\nElectronics, 0.8\\% on MovieLens-1M, and 1.6\\% on Yelp datasets. The framework\nexhibits superior explainability while maintaining computational efficiency\ncomparable to traditional methods."}
{"id": "2510.03203", "categories": ["cs.IR", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.03203", "abs": "https://arxiv.org/abs/2510.03203", "authors": ["Yann Collet", "Nick Terrell", "W. Felix Handte", "Danielle Rozenblit", "Victor Zhang", "Kevin Zhang", "Yaelle Goldschlag", "Jennifer Lee", "Daniel Riegel", "Stan Angelov", "Nadav Rotem"], "title": "OpenZL: A Graph-Based Model for Compression", "comment": null, "summary": "Research in general-purpose lossless compression over the last decade has\nlargely found improvements in compression ratio that come at great cost to\nresource utilization and processing throughput. However, most production\nworkloads require high throughput and low resource utilization, so most\nresearch systems have seen little adoption. Instead, real world improvements in\ncompression are increasingly often realized by building application-specific\ncompressors which can exploit knowledge about the structure and semantics of\nthe data being compressed. These systems easily outperform even the best\ngeneric compressors, but application-specific compression schemes are not\nwithout drawbacks. They are inherently limited in applicability and are\ndifficult to maintain and deploy.\n  We show that these challenges can be overcome with a new way of thinking\nabout compression. We propose the ``graph model'' of compression, a new\ntheoretical framework for representing compression as a directed acyclic graph\nof modular codecs. This motivates OpenZL, an implementation of this model that\ncompresses data into a self-describing wire format, any configuration of which\ncan be decompressed by a universal decoder. OpenZL's design enables rapid\ndevelopment of tailored compressors with minimal code, its universal decoder\neliminates deployment lag, and its investment in a well-vetted standard\ncomponent library minimizes security risks. Experimental results demonstrate\nthat OpenZL achieves superior compression ratios and speeds compared to\nstate-of-the-art general-purpose compressors on a variety of real-world\ndatasets. Internal deployments at Meta have also shown consistent improvements\nin size and/or speed, with development timelines reduced from months to days.\nOpenZL thus represents an advance in practical, scalable, and maintainable data\ncompression for modern data-intensive applications."}
