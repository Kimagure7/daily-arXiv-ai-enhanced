{"id": "2601.06389", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.06389", "abs": "https://arxiv.org/abs/2601.06389", "authors": ["Ramnath Kumar", "Prateek Jain", "Cho-Jui Hsieh"], "title": "Towards Building efficient Routed systems for Retrieval", "comment": null, "summary": "Late-interaction retrieval models like ColBERT achieve superior accuracy by enabling token-level interactions, but their computational cost hinders scalability and integration with Approximate Nearest Neighbor Search (ANNS). We introduce FastLane, a novel retrieval framework that dynamically routes queries to their most informative representations, eliminating redundant token comparisons. FastLane employs a learnable routing mechanism optimized alongside the embedding model, leveraging self-attention and differentiable selection to maximize efficiency. Our approach reduces computational complexity by up to 30x while maintaining competitive retrieval performance. By bridging late-interaction models with ANNS, FastLane enables scalable, low-latency retrieval, making it feasible for large-scale applications such as search engines, recommendation systems, and question-answering platforms. This work opens pathways for multi-lingual, multi-modal, and long-context retrieval, pushing the frontier of efficient and adaptive information retrieval."}
{"id": "2601.06458", "categories": ["cs.IR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.06458", "abs": "https://arxiv.org/abs/2601.06458", "authors": ["Sayak Chakrabarty", "Souradip Pal"], "title": "PixRec: Leveraging Visual Context for Next-Item Prediction in Sequential Recommendation", "comment": "9 pages, 2 figures", "summary": "Large Language Models (LLMs) have recently shown strong potential for usage in sequential recommendation tasks through text-only models, which combine advanced prompt design, contrastive alignment, and fine-tuning on downstream domain-specific data. While effective, these approaches overlook the rich visual information present in many real-world recommendation scenarios, particularly in e-commerce. This paper proposes PixRec - a vision-language framework that incorporates both textual attributes and product images into the recommendation pipeline. Our architecture leverages a vision-language model backbone capable of jointly processing image-text sequences, maintaining a dual-tower structure and mixed training objective while aligning multi-modal feature projections for both item-item and user-item interactions. Using the Amazon Reviews dataset augmented with product images, our experiments demonstrate $3\\times$ and 40% improvements in top-rank and top-10 rank accuracy over text-only recommenders respectively, indicating that visual features can help distinguish items with similar textual descriptions. Our work outlines future directions for scaling multi-modal recommenders training, enhancing visual-text feature fusion, and evaluating inference-time performance. This work takes a step toward building software systems utilizing visual information in sequential recommendation for real-world applications like e-commerce."}
{"id": "2601.06551", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.06551", "abs": "https://arxiv.org/abs/2601.06551", "authors": ["Sergii Voloshyn"], "title": "L-RAG: Balancing Context and Retrieval with Entropy-Based Lazy Loading", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as the predominant paradigm for grounding Large Language Model outputs in factual knowledge, effectively mitigating hallucinations. However, conventional RAG systems operate under a \"retrieve-always\" assumption, querying vector databases for every input regardless of query complexity. This static approach incurs substantial computational overhead and inference latency, particularly problematic for high-throughput production deployments. We introduce L-RAG (Lazy Retrieval-Augmented Generation), an adaptive framework that implements hierarchical context management through entropy-based gating. L-RAG employs a two-tier architecture: queries are first processed with a compact document summary, and expensive chunk retrieval is triggered only when the model's predictive entropy exceeds a calibrated threshold, signaling genuine uncertainty. Through experiments on SQuAD 2.0 (N=500) using the Phi-2 model, we demonstrate that L-RAG provides a tunable accuracy-efficiency trade-off: at a conservative threshold (tau=0.5), L-RAG achieves 78.2% accuracy, matching Standard RAG (77.8%), with 8% retrieval reduction; at a balanced threshold (tau=1.0), retrieval reduction increases to 26% with modest accuracy trade-off (76.0%). Latency analysis shows that L-RAG saves 80-210ms per query when retrieval latency exceeds 500ms. Analysis of entropy distributions reveals statistically significant separation (p < 0.001) between correct predictions (H=1.72) and errors (H=2.20), validating entropy as a reliable uncertainty signal. L-RAG offers a practical, training-free approach toward more efficient RAG deployment, providing system architects with a configurable knob to balance accuracy and throughput requirements."}
{"id": "2601.06613", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.06613", "abs": "https://arxiv.org/abs/2601.06613", "authors": ["Ariana Metović", "Nicolai Maisch", "Samed Ajdinović", "Armin Lechler", "Andreas Wortmann", "Oliver Riedel"], "title": "Industrial Semantics-Aware Digital Twins: A Hybrid Graph Matching Approach for Asset Administration Shells", "comment": null, "summary": "Although the Asset Administration Shell (AAS) standard provides a structured and machine-readable representation of industrial assets, their semantic comparability remains a major challenge, particularly when different vocabularies and modeling practices are used. Engineering would benefit from retrieving existing AAS models that are similar to the target in order to reuse submodels, parameters, and metadata. In practice, however, heterogeneous vocabularies and divergent modeling conventions hinder automated, content-level comparison across AAS. This paper proposes a hybrid graph matching approach to enable semantics-aware comparison of Digital Twin representations. The method combines rule-based pre-filtering using SPARQL with embedding-based similarity calculation leveraging RDF2vec to capture both structural and semantic relationships between AAS models. This contribution provides a foundation for enhanced discovery, reuse, and automated configuration in Digital Twin networks."}
{"id": "2601.06798", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.06798", "abs": "https://arxiv.org/abs/2601.06798", "authors": ["Zhiyang Zhang", "Junda She", "Kuo Cai", "Bo Chen", "Shiyao Wang", "Xinchen Luo", "Qiang Luo", "Ruiming Tang", "Han Li", "Kun Gai", "Guorui Zhou"], "title": "Unleashing the Native Recommendation Potential: LLM-Based Generative Recommendation via Structured Term Identifiers", "comment": null, "summary": "Leveraging the vast open-world knowledge and understanding capabilities of Large Language Models (LLMs) to develop general-purpose, semantically-aware recommender systems has emerged as a pivotal research direction in generative recommendation. However, existing methods face bottlenecks in constructing item identifiers. Text-based methods introduce LLMs' vast output space, leading to hallucination, while methods based on Semantic IDs (SIDs) encounter a semantic gap between SIDs and LLMs' native vocabulary, requiring costly vocabulary expansion and alignment training. To address this, this paper introduces Term IDs (TIDs), defined as a set of semantically rich and standardized textual keywords, to serve as robust item identifiers. We propose GRLM, a novel framework centered on TIDs, employs Context-aware Term Generation to convert item's metadata into standardized TIDs and utilizes Integrative Instruction Fine-tuning to collaboratively optimize term internalization and sequential recommendation. Additionally, Elastic Identifier Grounding is designed for robust item mapping. Extensive experiments on real-world datasets demonstrate that GRLM significantly outperforms baselines across multiple scenarios, pointing a promising direction for generalizable and high-performance generative recommendation systems."}
{"id": "2601.06873", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.06873", "abs": "https://arxiv.org/abs/2601.06873", "authors": ["Mustafa Abdool", "Soumyadip Banerjee", "Moutupsi Paul", "Do-kyum Kim", "Xioawei Liu", "Bin Xu", "Tracy Yu", "Hui Gao", "Karen Ouyang", "Huiji Gao", "Liwei He", "Stephanie Moyerman", "Sanjeev Katariya"], "title": "Applying Embedding-Based Retrieval to Airbnb Search", "comment": "14 pages, 9 figures", "summary": "The goal of Airbnb search is to match guests with the ideal accommodation that fits their travel needs. This is a challenging problem, as popular search locations can have around a hundred thousand available homes, and guests themselves have a wide variety of preferences. Furthermore, the launch of new product features, such as \\textit{flexible date search,} significantly increased the number of eligible homes per search query. As such, there is a need for a sophisticated retrieval system which can provide high-quality candidates with low latency in a way that integrates with the overall ranking stack.\n  This paper details our journey to build an efficient and high-quality retrieval system for Airbnb search. We describe the key unique challenges we encountered when implementing an Embedding-Based Retrieval (EBR) system for a two sided marketplace like Airbnb -- such as the dynamic nature of the inventory, a lengthy user funnel with multiple stages, and a variety of product surfaces. We cover unique insights when modeling the retrieval problem, how to build robust evaluation systems, and design choices for online serving. The EBR system was launched to production and powers several use-cases such as regular search, flexible date and promotional emails for marketing campaigns. The system demonstrated statistically-significant improvements in key metrics, such as booking conversion, via A/B testing."}
{"id": "2601.06992", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.06992", "abs": "https://arxiv.org/abs/2601.06992", "authors": ["Yixi Zhou", "Fan Zhang", "Yu Chen", "Haipeng Zhang", "Preslav Nakov", "Zhuohan Xie"], "title": "FinCARDS: Card-Based Analyst Reranking for Financial Document Question Answering", "comment": "15 pages, including figures and tables", "summary": "Financial question answering (QA) over long corporate filings requires evidence to satisfy strict constraints on entities, financial metrics, fiscal periods, and numeric values. However, existing LLM-based rerankers primarily optimize semantic relevance, leading to unstable rankings and opaque decisions on long documents. We propose FinCards, a structured reranking framework that reframes financial evidence selection as constraint satisfaction under a finance-aware schema. FinCards represents filing chunks and questions using aligned schema fields (entities, metrics, periods, and numeric spans), enabling deterministic field-level matching. Evidence is selected via a multi-stage tournament reranking with stability-aware aggregation, producing auditable decision traces. Across two corporate filing QA benchmarks, FinCards substantially improves early-rank retrieval over both lexical and LLM-based reranking baselines, while reducing ranking variance, without requiring model fine-tuning or unpredictable inference budgets. Our code is available at https://github.com/XanderZhou2022/FINCARDS."}
{"id": "2601.07125", "categories": ["cs.IR", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.07125", "abs": "https://arxiv.org/abs/2601.07125", "authors": ["Sungguk Cha", "DongWook Kim", "Mintae Kim", "Youngsub Han", "Byoung-Ki Jeon", "Sangyeob Lee"], "title": "ReinPool: Reinforcement Learning Pooling Multi-Vector Embeddings for Retrieval System", "comment": "5 pages", "summary": "Multi-vector embedding models have emerged as a powerful paradigm for document retrieval, preserving fine-grained visual and textual details through token-level representations. However, this expressiveness comes at a staggering cost: storing embeddings for every token inflates index sizes by over $1000\\times$ compared to single-vector approaches, severely limiting scalability. We introduce \\textbf{ReinPool}, a reinforcement learning framework that learns to dynamically filter and pool multi-vector embeddings into compact, retrieval-optimized representations. By training with an inverse retrieval objective and NDCG-based rewards, ReinPool identifies and retains only the most discriminative vectors without requiring manual importance annotations. On the Vidore V2 benchmark across three vision-language embedding models, ReinPool compresses multi-vector representations by $746$--$1249\\times$ into single vectors while recovering 76--81\\% of full multi-vector retrieval performance. Compared to static mean pooling baselines, ReinPool achieves 22--33\\% absolute NDCG@3 improvement, demonstrating that learned selection significantly outperforms heuristic aggregation."}
{"id": "2601.07294", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.07294", "abs": "https://arxiv.org/abs/2601.07294", "authors": ["Wenhao Lai", "Weike Pan", "Zhong Ming"], "title": "Towards Multi-Behavior Multi-Task Recommendation via Behavior-informed Graph Embedding Learning", "comment": null, "summary": "Multi-behavior recommendation (MBR) aims to improve the performance w.r.t. the target behavior (i.e., purchase) by leveraging auxiliary behaviors (e.g., click, favourite). However, in real-world scenarios, a recommendation method often needs to process different types of behaviors and generate personalized lists for each task (i.e., each behavior type). Such a new recommendation problem is referred to as multi-behavior multi-task recommendation (MMR). So far, the most powerful MBR methods usually model multi-behavior interactions using a cascading graph paradigm. Although significant progress has been made in optimizing the performance of the target behavior, it often neglects the performance of auxiliary behaviors. To compensate for the deficiencies of the cascading paradigm, we propose a novel solution for MMR, i.e., behavior-informed graph embedding learning (BiGEL). Specifically, we first obtain a set of behavior-aware embeddings by using a cascading graph paradigm. Subsequently, we introduce three key modules to improve the performance of the model. The cascading gated feedback (CGF) module enables a feedback-driven optimization process by integrating feedback from the target behavior to refine the auxiliary behaviors preferences. The global context enhancement (GCE) module integrates the global context to maintain the user's overall preferences, preventing the loss of key preferences due to individual behavior graph modeling. Finally, the contrastive preference alignment (CPA) module addresses the potential changes in user preferences during the cascading process by aligning the preferences of the target behaviors with the global preferences through contrastive learning. Extensive experiments on two real-world datasets demonstrate the effectiveness of our BiGEL compared with ten very competitive methods."}
{"id": "2601.07449", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07449", "abs": "https://arxiv.org/abs/2601.07449", "authors": ["Hao Jiang", "Zhi Yang", "Annan Wang", "Yichi Zhang", "Weisi Lin"], "title": "RLPO: Residual Listwise Preference Optimization for Long-Context Review Ranking", "comment": null, "summary": "Review ranking is pivotal in e-commerce for prioritizing diagnostic and authentic feedback from the deluge of user-generated content. While large language models have improved semantic assessment, existing ranking paradigms face a persistent trade-off in long-context settings. Pointwise scoring is efficient but often fails to account for list-level interactions, leading to miscalibrated top-$k$ rankings. Listwise approaches can leverage global context, yet they are computationally expensive and become unstable as candidate lists grow. To address this, we propose Residual Listwise Preference Optimization (RLPO), which formulates ranking as listwise representation-level residual correction over a strong pointwise LLM scorer. RLPO first produces calibrated pointwise scores and item representations, then applies a lightweight encoder over the representations to predict listwise score residuals, avoiding full token-level listwise processing. We also introduce a large-scale benchmark for long-context review ranking with human verification. Experiments show RLPO improves NDCG@k over strong pointwise and listwise baselines and remains robust as list length increases."}
{"id": "2601.07533", "categories": ["cs.IR", "cs.DL"], "pdf": "https://arxiv.org/pdf/2601.07533", "abs": "https://arxiv.org/abs/2601.07533", "authors": ["Julian Schelb", "Michael Wittweiler", "Marie Revellio", "Barbara Feichtinger", "Andreas Spitz"], "title": "Loci Similes: A Benchmark for Extracting Intertextualities in Latin Literature", "comment": null, "summary": "Tracing connections between historical texts is an important part of intertextual research, enabling scholars to reconstruct the virtual library of a writer and identify the sources influencing their creative process. These intertextual links manifest in diverse forms, ranging from direct verbatim quotations to subtle allusions and paraphrases disguised by morphological variation. Language models offer a promising path forward due to their capability of capturing semantic similarity beyond lexical overlap. However, the development of new methods for this task is held back by the scarcity of standardized benchmarks and easy-to-use datasets. We address this gap by introducing Loci Similes, a benchmark for Latin intertextuality detection comprising of a curated dataset of ~172k text segments containing 545 expert-verified parallels linking Late Antique authors to a corpus of classical authors. Using this data, we establish baselines for retrieval and classification of intertextualities with state-of-the-art LLMs."}
{"id": "2601.07613", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.07613", "abs": "https://arxiv.org/abs/2601.07613", "authors": ["Ke Shenqiang", "Wei Jianxiong", "Hua Qingsong"], "title": "GAP-Net: Calibrating User Intent via Gated Adaptive Progressive Learning for CTR Prediction", "comment": "9 pages, 3 figures", "summary": "Sequential user behavior modeling is pivotal for Click-Through Rate (CTR) prediction yet is hindered by three intrinsic bottlenecks: (1) the \"Attention Sink\" phenomenon, where standard Softmax compels the model to allocate probability mass to noisy behaviors; (2) the Static Query Assumption, which overlooks dynamic shifts in user intent driven by real-time contexts; and (3) Rigid View Aggregation, which fails to adaptively weight heterogeneous temporal signals according to the decision context. To bridge these gaps, we propose GAP-Net (Gated Adaptive Progressive Network), a unified framework establishing a \"Triple Gating\" architecture to progressively refine information from micro-level features to macro-level views. GAP-Net operates through three integrated mechanisms: (1) Adaptive Sparse-Gated Attention (ASGA) employs micro-level gating to enforce sparsity, effectively suppressing massive noise activations; (2) Gated Cascading Query Calibration (GCQC) dynamically aligns user intent by bridging real-time triggers and long-term memories via a meso-level cascading channel; and (3) Context-Gated Denoising Fusion (CGDF) performs macro-level modulation to orchestrate the aggregation of multi-view sequences. Extensive experiments on industrial datasets demonstrate that GAP-Net achieves substantial improvements over state-of-the-art baselines, exhibiting superior robustness against interaction noise and intent drift."}
{"id": "2601.07684", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.07684", "abs": "https://arxiv.org/abs/2601.07684", "authors": ["Geoffrey Taghon"], "title": "AptaFind: A lightweight local interface for automated aptamer curation from scientific literature", "comment": "for associated source code, see https://github.com/usnistgov/aptafind", "summary": "Aptamer researchers face a literature landscape scattered across publications, supplements, and databases, with each search consuming hours that could be spent at the bench. AptaFind transforms this navigation problem through a three-tier intelligence architecture that recognizes research mining is a spectrum, not a binary success or failure. The system delivers direct sequence extraction when possible, curated research leads when extraction fails, and exhaustive literature discovery for additional confidence. By combining local language models for semantic understanding with deterministic algorithms for reliability, AptaFind operates without cloud dependencies or subscription barriers. Validation across 300 University of Texas Aptamer Database targets demonstrates 84 % with some literature found, 84 % with curated research leads, and 79 % with a direct sequence extraction, at a laptop-compute rate of over 900 targets an hour. The platform proves that even when direct sequence extraction fails, automation can still deliver the actionable intelligence researchers need by rapidly narrowing the search to high quality references."}
