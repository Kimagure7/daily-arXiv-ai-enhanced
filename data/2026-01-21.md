<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 20]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [DeepEvidence: Empowering Biomedical Discovery with Deep Knowledge Graph Research](https://arxiv.org/abs/2601.11560)
*Zifeng Wang,Zheng Chen,Ziwei Yang,Xuan Wang,Qiao Jin,Yifan Peng,Zhiyong Lu,Jimeng Sun*

Main category: cs.IR

TL;DR: DeepEvidence是一个AI代理框架，通过协调广度优先和深度优先搜索策略，在异构生物医学知识图谱上进行深度研究，加速生物医学发现。


<details>
  <summary>Details</summary>
Motivation: 生物医学知识图谱包含大量异构信息，但由于结构差异、持续演化和有限的跨资源对齐，需要大量人工整合，限制了知识探索的深度和规模。

Method: 采用AI代理框架，包含协调器、广度优先搜索代理（BFRS）和深度优先搜索代理（DFRS），构建增量证据图，提供统一接口查询多样生物医学API和执行沙箱。

Result: 在深度推理基准测试和生物医学发现生命周期的四个关键阶段（药物发现、临床前实验、临床试验开发、循证医学）中，DeepEvidence在系统探索和证据合成方面表现出显著优势。

Conclusion: 知识图谱驱动的深度研究有潜力加速生物医学发现，DeepEvidence框架展示了在异构生物医学知识图谱上进行系统探索和证据合成的能力。

Abstract: Biomedical knowledge graphs (KGs) encode vast, heterogeneous information spanning literature, genes, pathways, drugs, diseases, and clinical trials, but leveraging them collectively for scientific discovery remains difficult. Their structural differences, continual evolution, and limited cross-resource alignment require substantial manual integration, limiting the depth and scale of knowledge exploration. We introduce DeepEvidence, an AI-agent framework designed to perform Deep Research across various heterogeneous biomedical KGs. Unlike generic Deep Research systems that rely primarily on internet-scale text, DeepEvidence incorporates specialized knowledge-graph tooling and coordinated exploration strategies to systematically bridge heterogeneous resources. At its core is an orchestrator that directs two complementary agents: Breadth-First ReSearch (BFRS) for broad, multi-graph entity search, and Depth-First ReSearch (DFRS) for multi-hop, evidence-focused reasoning. An internal, incrementally built evidence graph provides a structured record of retrieved entities, relations, and supporting evidence. To operate at scale, DeepEvidence includes unified interfaces for querying diverse biomedical APIs and an execution sandbox that enables programmatic data retrieval, extraction, and analysis. Across established deep-reasoning benchmarks and four key stages of the biomedical discovery lifecycle: drug discovery, pre-clinical experimentation, clinical trial development, and evidence-based medicine, DeepEvidence demonstrates substantial gains in systematic exploration and evidence synthesis. These results highlight the potential of knowledge-graph-driven Deep Research to accelerate biomedical discovery.

</details>


### [2] [Utilizing Metadata for Better Retrieval-Augmented Generation](https://arxiv.org/abs/2601.11863)
*Raquib Bin Yousuf,Shengzhe Xu,Mandar Sharma,Andrew Neeser,Chris Latimer,Naren Ramakrishnan*

Main category: cs.IR

TL;DR: 本文系统研究了元数据感知检索策略，发现在结构化重复语料中，元数据集成能显著提升检索效果，前缀法和统一嵌入表现最佳。


<details>
  <summary>Details</summary>
Motivation: 在结构化重复语料（如监管文件）中，仅依赖文本相似性的检索方法难以区分语言重叠的文档。实践中常将元数据作为启发式方法，但其影响和权衡尚不明确。

Method: 比较了多种元数据感知检索策略：元数据作为文本（前缀和后缀）、融合元数据和内容的统一嵌入、双编码器后期融合检索、以及元数据感知查询重构。

Result: 前缀法和统一嵌入一致优于纯文本基线，统一嵌入有时超越前缀法且更易维护。元数据集成通过增强文档内聚性、减少文档间混淆、扩大相关与不相关块间距离来提升效果。

Conclusion: 元数据集成能显著改善结构化语料中的检索效果，结构线索提供强消歧信号。研究提供了代码、评估框架和RAGMATE-10K数据集。

Abstract: Retrieval-Augmented Generation systems depend on retrieving semantically relevant document chunks to support accurate, grounded outputs from large language models. In structured and repetitive corpora such as regulatory filings, chunk similarity alone often fails to distinguish between documents with overlapping language. Practitioners often flatten metadata into input text as a heuristic, but the impact and trade-offs of this practice remain poorly understood. We present a systematic study of metadata-aware retrieval strategies, comparing plain-text baselines with approaches that embed metadata directly. Our evaluation spans metadata-as-text (prefix and suffix), a dual-encoder unified embedding that fuses metadata and content in a single index, dual-encoder late-fusion retrieval, and metadata-aware query reformulation. Across multiple retrieval metrics and question types, we find that prefixing and unified embeddings consistently outperform plain-text baselines, with the unified at times exceeding prefixing while being easier to maintain. Beyond empirical comparisons, we analyze embedding space, showing that metadata integration improves effectiveness by increasing intra-document cohesion, reducing inter-document confusion, and widening the separation between relevant and irrelevant chunks. Field-level ablations show that structural cues provide strong disambiguating signals. Our code, evaluation framework, and the RAGMATE-10K dataset are publicly hosted.

</details>


### [3] [Cultural Analytics for Good: Building Inclusive Evaluation Frameworks for Historical IR](https://arxiv.org/abs/2601.11874)
*Suchana Datta,Dwaipayan Roy,Derek Greene,Gerardine Meaney,Karen Wade,Philipp Mayr*

Main category: cs.IR

TL;DR: 构建跨信息检索与文化分析的基准框架，利用英国图书馆BL19数字馆藏（1700-1899年3.5万+作品），研究19世纪小说与非小说中的语言变化与检索，结合专家查询设计、段落级相关性标注和LLM辅助，探索小说到非小说的知识迁移，提升数字档案的检索准确性、可解释性和文化包容性。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在弥合信息检索与文化分析之间的鸿沟，支持历史知识的公平获取。通过构建19世纪文本的评估基准，解决数字档案中语言演变、术语变化对检索系统的影响，促进更具包容性和解释性的知识基础设施。

Method: 使用英国图书馆BL19数字馆藏（1700-1899年超过35,000部作品）构建基准；结合专家驱动的查询设计、段落级相关性标注和大型语言模型（LLM）辅助，创建基于人类专业知识的可扩展评估框架；重点研究从小说到非小说的知识迁移。

Result: 开发了一个实用的评估资源和方法论范式，不仅提高了检索准确性，还促进了数字档案的可解释性、透明度和文化包容性，支持更丰富、更具历史意识的对数字档案的参与。

Conclusion: 该研究为开发支持更丰富、更具历史意识参与数字档案的检索系统提供了实践资源和方法论框架，最终朝着更解放的知识基础设施迈进，促进了跨学科合作和文化包容性。

Abstract: This work bridges the fields of information retrieval and cultural analytics to support equitable access to historical knowledge. Using the British Library BL19 digital collection (more than 35,000 works from 1700-1899), we construct a benchmark for studying changes in language, terminology and retrieval in the 19th-century fiction and non-fiction. Our approach combines expert-driven query design, paragraph-level relevance annotation, and Large Language Model (LLM) assistance to create a scalable evaluation framework grounded in human expertise. We focus on knowledge transfer from fiction to non-fiction, investigating how narrative understanding and semantic richness in fiction can improve retrieval for scholarly and factual materials. This interdisciplinary framework not only improves retrieval accuracy but also fosters interpretability, transparency, and cultural inclusivity in digital archives. Our work provides both practical evaluation resources and a methodological paradigm for developing retrieval systems that support richer, historically aware engagement with digital archives, ultimately working towards more emancipatory knowledge infrastructures.

</details>


### [4] [Agentic-R: Learning to Retrieve for Agentic Search](https://arxiv.org/abs/2601.11888)
*Wenhan Liu,Xinyu Ma,Yutao Zhu,Yuchen Li,Daiting Shi,Dawei Yin,Zhicheng Dou*

Main category: cs.IR

TL;DR: 提出了一种专门为智能体搜索设计的检索器训练框架，使用局部查询-段落相关性和全局答案正确性来衡量段落效用，并通过迭代训练策略优化检索器和搜索智能体。


<details>
  <summary>Details</summary>
Motivation: 现有搜索智能体通常依赖基于相似性的检索器，但相似段落并不总是对最终答案生成有用。如何为智能体搜索设计检索器仍然是一个未充分探索的问题。

Method: 提出了一种新颖的检索器训练框架：1) 使用局部查询-段落相关性和全局答案正确性来衡量多轮智能体搜索中的段落效用；2) 引入迭代训练策略，搜索智能体和检索器双向迭代优化；3) 检索器使用智能体生成的演化高质量查询持续改进。

Result: 在七个单跳和多跳问答基准测试上的广泛实验表明，提出的检索器（称为Agentic-R）在不同搜索智能体上始终优于强基线方法。

Conclusion: 该工作为智能体搜索设计了专门的检索器训练框架，通过结合局部和全局效用评估以及迭代优化策略，显著提升了智能体搜索的性能。

Abstract: Agentic search has recently emerged as a powerful paradigm, where an agent interleaves multi-step reasoning with on-demand retrieval to solve complex questions. Despite its success, how to design a retriever for agentic search remains largely underexplored. Existing search agents typically rely on similarity-based retrievers, while similar passages are not always useful for final answer generation. In this paper, we propose a novel retriever training framework tailored for agentic search. Unlike retrievers designed for single-turn retrieval-augmented generation (RAG) that only rely on local passage utility, we propose to use both local query-passage relevance and global answer correctness to measure passage utility in a multi-turn agentic search. We further introduce an iterative training strategy, where the search agent and the retriever are optimized bidirectionally and iteratively. Different from RAG retrievers that are only trained once with fixed questions, our retriever is continuously improved using evolving and higher-quality queries from the agent. Extensive experiments on seven single-hop and multi-hop QA benchmarks demonstrate that our retriever, termed \ours{}, consistently outperforms strong baselines across different search agents. Our codes are available at: https://github.com/8421BCD/Agentic-R.

</details>


### [5] [Facet-Aware Multi-Head Mixture-of-Experts Model with Text-Enhanced Pre-training for Sequential Recommendation](https://arxiv.org/abs/2601.12301)
*Mingrui Liu,Sixiao Zhang,Cheng Long*

Main category: cs.IR

TL;DR: 提出FAME模型，通过多面感知的多头混合专家架构解决序列推荐中物品多面性和用户偏好多样性的问题，并结合文本增强的预训练模块提升语义表示。


<details>
  <summary>Details</summary>
Motivation: 现有序列推荐系统通常为每个物品分配单一嵌入向量，无法充分捕捉物品的多面性（如电影类型、演员等）和用户在各个面上的复杂偏好变化。

Method: 1. FAME架构：利用多头注意力最后一层的子嵌入分别预测下一物品，捕捉不同物品面；通过门控机制动态整合预测结果。2. 每个注意力头内引入混合专家网络，使用可学习路由器网络基于上下文聚合专家输出，解耦用户在各面上的多样偏好。3. 文本增强的多面感知预训练模块：使用预训练文本编码器，通过交替监督对比学习目标从文本元数据中显式解耦面特定特征。

Result: 该方法能够更有效地捕捉物品的多面性和用户的复杂偏好，通过预训练确保物品嵌入具有语义鲁棒性并与下游多面框架对齐。

Conclusion: FAME模型通过解耦物品多面性和用户偏好多样性，结合文本增强预训练，显著提升了序列推荐的性能，为处理复杂推荐场景提供了有效解决方案。

Abstract: Sequential recommendation (SR) systems excel at capturing users' dynamic preferences by leveraging their interaction histories. Most existing SR systems assign a single embedding vector to each item to represent its features, adopting various models to combine these embeddings into a sequence representation that captures user intent. However, we argue that this representation alone is insufficient to capture an item's multi-faceted nature (e.g., movie genres, starring actors). Furthermore, users often exhibit complex and varied preferences within these facets (e.g., liking both action and musical films within the genre facet), which are challenging to fully represent with static identifiers. To address these issues, we propose a novel architecture titled Facet-Aware Multi-Head Mixture-of-Experts Model for Sequential Recommendation (FAME). We leverage sub-embeddings from each head in the final multi-head attention layer to predict the next item separately, effectively capturing distinct item facets. A gating mechanism then integrates these predictions by dynamically determining their importance. Additionally, we introduce a Mixture-of-Experts (MoE) network within each attention head to disentangle varied user preferences within each facet, utilizing a learnable router network to aggregate expert outputs based on context. Complementing this architecture, we design a Text-Enhanced Facet-Aware Pre-training module to overcome the limitations of randomly initialized embeddings. By utilizing a pre-trained text encoder and employing an alternating supervised contrastive learning objective, we explicitly disentangle facet-specific features from textual metadata (e.g., descriptions) before sequential training begins. This ensures that the item embeddings are semantically robust and aligned with the downstream multi-facet framework.

</details>


### [6] [Information Farming: From Berry Picking to Berry Growing](https://arxiv.org/abs/2601.12544)
*Leif Azzopardi,Adam Roegiest*

Main category: cs.IR

TL;DR: 论文提出"信息耕作"新框架，类比新石器革命，认为生成式AI正推动人类从信息"采集"转向"耕作"模式


<details>
  <summary>Details</summary>
Motivation: 传统的信息采集理论和信息觅食理论将用户视为信息收集者，但生成式AI的出现正在从根本上改变人们生产、组织和重用信息的方式，这些传统范式已无法完全捕捉这一转变

Method: 采用历史类比和实证证据，引入"信息耕作"作为概念框架，分析其益处、机会、设计影响和伴随风险

Result: 提出信息耕作代表人类与信息互动方式的自然演进，随着生成式AI技术普及，信息耕作将逐渐取代临时性的、基于碎片的信息觅食成为主导模式

Conclusion: 信息耕作框架标志着人机信息交互及其研究的更广泛转变，需要新的设计原则和评估方法来应对这一转型

Abstract: The classic paradigms of Berry Picking and Information Foraging Theory have framed users as gatherers, opportunistically searching across distributed sources to satisfy evolving information needs. However, the rise of GenAI is driving a fundamental transformation in how people produce, structure, and reuse information - one that these paradigms no longer fully capture. This transformation is analogous to the Neolithic Revolution, when societies shifted from hunting and gathering to cultivation. Generative technologies empower users to "farm" information by planting seeds in the form of prompts, cultivating workflows over time, and harvesting richly structured, relevant yields within their own plots, rather than foraging across others people's patches. In this perspectives paper, we introduce the notion of Information Farming as a conceptual framework and argue that it represents a natural evolution in how people engage with information. Drawing on historical analogy and empirical evidence, we examine the benefits and opportunities of information farming, its implications for design and evaluation, and the accompanying risks posed by this transition. We hypothesize that as GenAI technologies proliferate, cultivating information will increasingly supplant transient, patch-based foraging as a dominant mode of engagement, marking a broader shift in human-information interaction and its study.

</details>


### [7] [HyFormer: Revisiting the Roles of Sequence Modeling and Feature Interaction in CTR Prediction](https://arxiv.org/abs/2601.12681)
*Yunwen Huang,Shiyong Hong,Xijun Xiao,Jinqiu Jin,Xuanyuan Luo,Zhe Wang,Zheng Chai,Shikang Wu,Yuchao Zheng,Jingjian Lin*

Main category: cs.IR

TL;DR: HyFormer提出统一混合Transformer架构，将长序列建模和特征交互集成到单一骨干网络中，通过交替优化查询解码和查询增强机制，在工业级推荐系统中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 工业大规模推荐模型面临联合建模长用户行为序列和异构非序列特征的挑战，现有架构采用解耦流水线（先压缩长序列再融合特征），限制了表示能力和交互灵活性。

Method: 设计统一混合Transformer架构，包含两个交替优化的核心组件：1) 查询解码：将非序列特征扩展为全局令牌，在长行为序列的层间键值表示上进行长序列解码；2) 查询增强：通过高效令牌混合增强跨查询和跨序列的异构交互。

Result: 在十亿级工业数据集上，HyFormer在可比参数和FLOPs预算下持续优于LONGER和RankMixer基线，并展现出随着参数和FLOPs增加而更优的扩展行为。大规模在线A/B测试在高流量生产系统中进一步验证了其有效性。

Conclusion: HyFormer作为工业大规模推荐模型的统一建模框架，具有实用性和可扩展性，能够有效整合长序列建模和特征交互，显著提升推荐系统性能。

Abstract: Industrial large-scale recommendation models (LRMs) face the challenge of jointly modeling long-range user behavior sequences and heterogeneous non-sequential features under strict efficiency constraints. However, most existing architectures employ a decoupled pipeline: long sequences are first compressed with a query-token based sequence compressor like LONGER, followed by fusion with dense features through token-mixing modules like RankMixer, which thereby limits both the representation capacity and the interaction flexibility. This paper presents HyFormer, a unified hybrid transformer architecture that tightly integrates long-sequence modeling and feature interaction into a single backbone. From the perspective of sequence modeling, we revisit and redesign query tokens in LRMs, and frame the LRM modeling task as an alternating optimization process that integrates two core components: Query Decoding which expands non-sequential features into Global Tokens and performs long sequence decoding over layer-wise key-value representations of long behavioral sequences; and Query Boosting which enhances cross-query and cross-sequence heterogeneous interactions via efficient token mixing. The two complementary mechanisms are performed iteratively to refine semantic representations across layers. Extensive experiments on billion-scale industrial datasets demonstrate that HyFormer consistently outperforms strong LONGER and RankMixer baselines under comparable parameter and FLOPs budgets, while exhibiting superior scaling behavior with increasing parameters and FLOPs. Large-scale online A/B tests in high-traffic production systems further validate its effectiveness, showing significant gains over deployed state-of-the-art models. These results highlight the practicality and scalability of HyFormer as a unified modeling framework for industrial LRMs.

</details>


### [8] [The Unfairness of Multifactorial Bias in Recommendation](https://arxiv.org/abs/2601.12828)
*Masoud Mansoury,Jin Huang,Mykola Pechenizkiy,Herke van Hoof,Maarten de Rijke*

Main category: cs.IR

TL;DR: 该论文研究了推荐系统中的多因素偏见（流行度偏见和积极性偏见的组合效应），提出了一种基于百分位数评分的预处理方法来缓解这种偏见，实验表明该方法能有效改善曝光公平性且几乎不影响推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中存在流行度偏见和积极性偏见，这两种偏见都源于输入数据并通过推荐模型传播，导致不公平或次优结果。虽然每种偏见都被独立研究过，但它们的组合效应（多因素偏见）尚未得到充分探索。作者发现积极性偏见不成比例地集中在流行物品上，进一步放大了它们的过度曝光。

Method: 作者通过模拟研究分析多因素偏见对物品侧公平性的影响，重点关注曝光偏见。然后采用基于百分位数的评分转换作为预处理策略来缓解多因素偏见。该方法在四个公共数据集上使用六种推荐算法进行实验验证，并将其集成到后处理公平性流程中以评估效果。

Result: 实验结果表明，基于百分位数的预处理方法能够改善曝光公平性，同时准确率损失可忽略不计。将该预处理步骤集成到后处理公平性流程中，可以增强其效果和效率，以更低的计算成本实现相当或更好的公平性。

Conclusion: 该研究强调了解决多因素偏见的重要性，并展示了简单、数据驱动的预处理方法在改善推荐系统公平性方面的实用价值。这些发现为推荐系统公平性研究提供了新的视角和方法。

Abstract: Popularity bias and positivity bias are two prominent sources of bias in recommender systems. Both arise from input data, propagate through recommendation models, and lead to unfair or suboptimal outcomes. Popularity bias occurs when a small subset of items receives most interactions, while positivity bias stems from the over-representation of high rating values. Although each bias has been studied independently, their combined effect, to which we refer to as multifactorial bias, remains underexplored. In this work, we examine how multifactorial bias influences item-side fairness, focusing on exposure bias, which reflects the unequal visibility of items in recommendation outputs. Through simulation studies, we find that positivity bias is disproportionately concentrated on popular items, further amplifying their over-exposure. Motivated by this insight, we adapt a percentile-based rating transformation as a pre-processing strategy to mitigate multifactorial bias. Experiments using six recommendation algorithms across four public datasets show that this approach improves exposure fairness with negligible accuracy loss. We also demonstrate that integrating this pre-processing step into post-processing fairness pipelines enhances their effectiveness and efficiency, enabling comparable or better fairness with reduced computational cost. These findings highlight the importance of addressing multifactorial bias and demonstrate the practical value of simple, data-driven pre-processing methods for improving fairness in recommender systems.

</details>


### [9] [Rules, Resources, and Restrictions: A Taxonomy of Task-Based Information Request Intents](https://arxiv.org/abs/2601.12985)
*Melanie A. Kilian,David Elsweiler*

Main category: cs.IR

TL;DR: 提出基于任务的信息请求意图分类法，弥补传统查询意图分类与AI驱动任务导向搜索之间的差距


<details>
  <summary>Details</summary>
Motivation: 现有意图分类主要基于系统日志数据，关注孤立信息需求，而忽略了更广泛的任务上下文。随着用户对LLMs的期望从简单问答扩展到全面任务支持，需要更强的任务导向视角来理解查询意图。

Method: 采用基于扎根理论的访谈研究，采访机场信息台工作人员，分析真实任务场景中的信息请求模式。

Result: 提出了一个任务型信息请求意图分类法，能够更好地连接传统查询导向方法和AI驱动任务导向搜索的新需求。

Conclusion: 需要从任务角度重新审视查询意图分类，提出的分类法有助于提升检索效果，更好地支持复杂多方面的任务需求。

Abstract: Understanding and classifying query intents can improve retrieval effectiveness by helping align search results with the motivations behind user queries. However, existing intent taxonomies are typically derived from system log data and capture mostly isolated information needs, while the broader task context often remains unaddressed. This limitation becomes increasingly relevant as interactions with Large Language Models (LLMs) expand user expectations from simple query answering toward comprehensive task support, for example, with purchasing decisions or in travel planning. At the same time, current LLMs still struggle to fully interpret complex and multifaceted tasks. To address this gap, we argue for a stronger task-based perspective on query intent. Drawing on a grounded-theory-based interview study with airport information clerks, we present a taxonomy of task-based information request intents that bridges the gap between traditional query-focused approaches and the emerging demands of AI-driven task-oriented search.

</details>


### [10] [Incorporating Q&A Nuggets into Retrieval-Augmented Generation](https://arxiv.org/abs/2601.13222)
*Laura Dietz,Bryan Li,Gabrielle Liu,Jia-Huei Ju,Eugene Yang,Dawn Lawrie,William Walden,James Mayfield*

Main category: cs.IR

TL;DR: RAGE系统将自动评估融入检索增强生成，提出Crucible系统，通过构建问答块库来保持显式引用溯源，在TREC NeuCLIR 2024上表现优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在保持引用溯源和避免信息重复方面存在不足，需要更透明、可解释的语义表示来改进生成质量。

Method: 提出Crucible系统：1）从检索文档构建问答块库；2）使用问答块指导提取、选择和报告生成；3）基于问答语义进行推理，避免信息重复，保持完整引用溯源。

Result: 在TREC NeuCLIR 2024数据集上，Crucible在块召回率、密度和引用基础方面显著优于最近的基于块的RAG系统Ginger。

Conclusion: RAGE系统通过集成自动评估和基于问答块的生成方法，能够有效保持引用溯源，提高生成质量，为RAG系统提供了新的改进方向。

Abstract: RAGE systems integrate ideas from automatic evaluation (E) into Retrieval-augmented Generation (RAG). As one such example, we present Crucible, a Nugget-Augmented Generation System that preserves explicit citation provenance by constructing a bank of Q&A nuggets from retrieved documents and uses them to guide extraction, selection, and report generation. Reasoning on nuggets avoids repeated information through clear and interpretable Q&A semantics - instead of opaque cluster abstractions - while maintaining citation provenance throughout the entire generation process. Evaluated on the TREC NeuCLIR 2024 collection, our Crucible system substantially outperforms Ginger, a recent nugget-based RAG system, in nugget recall, density, and citation grounding.

</details>


### [11] [Insider Knowledge: How Much Can RAG Systems Gain from Evaluation Secrets?](https://arxiv.org/abs/2601.13227)
*Laura Dietz,Bryan Li,Eugene Yang,Dawn Lawrie,William Walden,James Mayfield*

Main category: cs.IR

TL;DR: 论文警告RAG系统中基于nugget的LLM评估存在循环风险，当评估元素泄露时可能导致虚假的高分，强调需要盲评估和方法多样性。


<details>
  <summary>Details</summary>
Motivation: RAG系统越来越多地使用LLM评估进行优化，这种基于nugget的方法已被整合到系统架构中。虽然能带来改进，但也存在循环性导致错误测量的风险，需要研究这种风险的实际影响。

Method: 通过对比实验研究基于nugget的RAG系统（如Ginger和Crucible）与强基线（如GPT-Researcher）。故意修改Crucible以生成针对LLM评估优化的输出，测试当评估元素（如提示模板或黄金nuggets）泄露或可预测时的效果。

Result: 当评估元素泄露时，可以轻松获得接近完美的评估分数。这表明存在严重的度量过拟合风险，可能将系统对评估指标的优化误认为是真正的性能进步。

Conclusion: 必须采用盲评估设置和方法多样性来防止将度量过拟合误认为系统真正进步。RAG系统的评估需要更严谨的方法论来确保评估的有效性。

Abstract: RAG systems are increasingly evaluated and optimized using LLM judges, an approach that is rapidly becoming the dominant paradigm for system assessment. Nugget-based approaches in particular are now embedded not only in evaluation frameworks but also in the architectures of RAG systems themselves. While this integration can lead to genuine improvements, it also creates a risk of faulty measurements due to circularity. In this paper, we investigate this risk through comparative experiments with nugget-based RAG systems, including Ginger and Crucible, against strong baselines such as GPT-Researcher. By deliberately modifying Crucible to generate outputs optimized for an LLM judge, we show that near-perfect evaluation scores can be achieved when elements of the evaluation - such as prompt templates or gold nuggets - are leaked or can be predicted. Our results highlight the importance of blind evaluation settings and methodological diversity to guard against mistaking metric overfitting for genuine system progress.

</details>


### [12] [Guidelines for the Creation of an Annotated Corpus](https://arxiv.org/abs/2601.13353)
*Bahdja Boudoua,Nadia Guiffant,Mathieu Roche,Maguelonne Teisseire,Annelise Tran*

Main category: cs.IR

TL;DR: 提供创建文本标注指南和标注语料库的通用方法论框架，涵盖方法、存储、共享和利用等方面


<details>
  <summary>Details</summary>
Motivation: 为研究人员提供系统化的方法论指导，以支持在各种研究背景下创建和使用高质量的标注语料库

Method: 基于UMR TETIS成员反馈和科学文献，提出包含定义、示例的逐步流程，涵盖方法论、存储、共享和利用等方面

Result: 开发了一个全面的框架，通过清晰的定义和示例说明每个步骤，支持不同研究场景下的语料库创建和使用

Conclusion: 该文档为创建标注指南和标注语料库提供了实用的通用方法论，有助于提高语料库的质量和可用性

Abstract: This document, based on feedback from UMR TETIS members and the scientific literature, provides a generic methodology for creating annotation guidelines and annotated textual datasets (corpora). It covers methodological aspects, as well as storage, sharing, and valorization of the data. It includes definitions and examples to clearly illustrate each step of the process, thus providing a comprehensive framework to support the creation and use of corpora in various research contexts.

</details>


### [13] [Integrating Vision-Centric Text Understanding for Conversational Recommender Systems](https://arxiv.org/abs/2601.13505)
*Wei Yuan,Shutong Qiao,Tong Chen,Quoc Viet Hung Nguyen,Zi Huang,Hongzhi Yin*

Main category: cs.IR

TL;DR: STARCRS是一个结合屏幕阅读和LLM文本理解的对话推荐系统，通过双模态融合提升推荐准确性和响应质量。


<details>
  <summary>Details</summary>
Motivation: 现有对话推荐系统通过扩展对话上下文（如实体信息、相关对话）来提升偏好建模，但这导致输入更长、更异构，引发输入长度限制、文本风格不一致、无关噪声等问题，需要更强的语言理解能力。

Method: 提出STARCRS系统，包含两个互补的文本理解模式：1) 屏幕阅读路径，将辅助文本信息编码为视觉标记，模拟屏幕浏览；2) 基于LLM的文本路径，专注于有限关键内容进行细粒度推理。设计知识锚定融合框架，结合对比对齐、交叉注意力交互和自适应门控来整合两种模式。

Result: 在两个广泛使用的基准测试上进行大量实验，证明STARCRS在推荐准确性和生成响应质量方面都有持续提升。

Conclusion: STARCRS通过整合屏幕阅读和LLM文本理解两种互补模式，有效解决了对话推荐系统中上下文扩展带来的实际问题，提升了偏好建模和响应生成能力。

Abstract: Conversational Recommender Systems (CRSs) have attracted growing attention for their ability to deliver personalized recommendations through natural language interactions. To more accurately infer user preferences from multi-turn conversations, recent works increasingly expand conversational context (e.g., by incorporating diverse entity information or retrieving related dialogues). While such context enrichment can assist preference modeling, it also introduces longer and more heterogeneous inputs, leading to practical issues such as input length constraints, text style inconsistency, and irrelevant textual noise, thereby raising the demand for stronger language understanding ability. In this paper, we propose STARCRS, a Screen-Text-AwaRe Conversational Recommender System that integrates two complementary text understanding modes: (1) a screen-reading pathway that encodes auxiliary textual information as visual tokens, mimicking skim reading on a screen, and (2) an LLM-based textual pathway that focuses on a limited set of critical content for fine-grained reasoning. We design a knowledge-anchored fusion framework that combines contrastive alignment, cross-attention interaction, and adaptive gating to integrate the two modes for improved preference modeling and response generation. Extensive experiments on two widely used benchmarks demonstrate that STARCRS consistently improves both recommendation accuracy and generated response quality.

</details>


### [14] [More Than Efficiency: Embedding Compression Improves Domain Adaptation in Dense Retrieval](https://arxiv.org/abs/2601.13525)
*Chunsheng Zuo,Daniel Khashabi*

Main category: cs.IR

TL;DR: PCA压缩查询嵌入可提升专业领域检索性能，无需昂贵标注和重新训练


<details>
  <summary>Details</summary>
Motivation: 基于预训练嵌入的密集检索器在专业领域表现不佳，因为训练和目标领域分布不匹配。传统领域适应方法需要昂贵的标注和重新训练查询-文档对。

Method: 重新审视被忽视的替代方案：对领域嵌入应用PCA，获得低维表示，保留领域相关特征，丢弃非判别性成分。虽然传统上用于效率，但本文证明这种简单的嵌入压缩能有效提升检索性能。

Result: 在9个检索器和14个MTEB数据集上评估，仅对查询嵌入应用PCA，在75.4%的模型-数据集对上改善了NDCG@10，提供了一种简单轻量的领域适应方法。

Conclusion: PCA嵌入压缩是一种简单有效的领域适应方法，无需昂贵标注和重新训练，能显著提升专业领域检索性能。

Abstract: Dense retrievers powered by pretrained embeddings are widely used for document retrieval but struggle in specialized domains due to the mismatches between the training and target domain distributions. Domain adaptation typically requires costly annotation and retraining of query-document pairs. In this work, we revisit an overlooked alternative: applying PCA to domain embeddings to derive lower-dimensional representations that preserve domain-relevant features while discarding non-discriminative components. Though traditionally used for efficiency, we demonstrate that this simple embedding compression can effectively improve retrieval performance. Evaluated across 9 retrievers and 14 MTEB datasets, PCA applied solely to query embeddings improves NDCG@10 in 75.4% of model-dataset pairs, offering a simple and lightweight method for domain adaptation.

</details>


### [15] [Balancing Fairness and High Match Rates in Reciprocal Recommender Systems: A Nash Social Welfare Approach](https://arxiv.org/abs/2601.13609)
*Yoji Tomita,Tomohiko Yokoyama*

Main category: cs.IR

TL;DR: 该论文研究匹配平台中互惠推荐系统的公平性问题，提出基于社会福利和纳什社会福利的方法来平衡匹配效率和公平性，并开发了高效的近似算法。


<details>
  <summary>Details</summary>
Motivation: 随着在线约会服务和职位推荐等匹配平台的普及，设计既能增加匹配数量又能避免用户间不公平的互惠推荐系统变得至关重要。现有方法往往只关注最大化匹配数量，而忽视了推荐机会的公平分配问题。

Method: 论文从公平分配的角度定义了用户的推荐机会，并建立了嫉妒自由（envy-freeness）的公平概念。首先引入社会福利（SW）方法近似最大化匹配数量，然后提出纳什社会福利（NSW）方法通过交替优化两个NSW函数实现接近嫉妒自由的推荐。进一步将SW和NSW方法推广到α-SW方法，以平衡公平性和匹配率之间的权衡。基于Sinkhorn算法开发了计算高效的近似算法。

Result: 实验表明，SW方法虽然能近似最大化匹配数量，但会导致显著的推荐机会不公平，揭示了公平性和匹配率之间的权衡。NSW方法能够实现接近嫉妒自由的推荐，而α-SW方法可以在公平性和高匹配率之间取得平衡。在合成数据集和两个真实世界数据集上的广泛实验证明了方法的实际有效性。

Conclusion: 该研究为匹配平台中的互惠推荐系统提供了公平性分析框架和实用解决方案，通过社会福利和纳什社会福利方法有效平衡了匹配效率和公平性，提出的α-SW方法和高效算法具有实际应用价值。

Abstract: Matching platforms, such as online dating services and job recommendations, have become increasingly prevalent. For the success of these platforms, it is crucial to design reciprocal recommender systems (RRSs) that not only increase the total number of matches but also avoid creating unfairness among users. In this paper, we investigate the fairness of RRSs on matching platforms. From the perspective of fair division, we define the users' opportunities to be recommended and establish the fairness concept of envy-freeness in the allocation of these opportunities. We first introduce the Social Welfare (SW) method, which approximately maximizes the number of matches, and show that it leads to significant unfairness in recommendation opportunities, illustrating the trade-off between fairness and match rates. To address this challenge, we propose the Nash Social Welfare (NSW) method, which alternately optimizes two NSW functions and achieves nearly envy-free recommendations. We further generalize the SW and NSW method to the $α$-SW method, which balances the trade-off between fairness and high match rates. Additionally, we develop a computationally efficient approximation algorithm for the SW/NSW/$α$-SW methods based on the Sinkhorn algorithm. Through extensive experiments on both synthetic datasets and two real-world datasets, we demonstrate the practical effectiveness of our approach.

</details>


### [16] [Question-Focused Filtering for Knowledge-based VQA](https://arxiv.org/abs/2601.13856)
*Wei Ye,Yixin Su,Yueguo Chen,Longxiang Gao,Jianjun Li,Ruixuan Li,Rui Zhang*

Main category: cs.IR

TL;DR: 本文提出了一种基于问题聚焦的知识过滤方法，用于知识库视觉问答任务，通过可训练的问题聚焦过滤器和基于分块的动态多文章选择模块，在保持计算成本可控的同时显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的知识过滤方法存在两个主要问题：1）传统基于相似度度量的方法只能从单篇文章中定位相关部分，导致文章级和文章内部的信息选择错误；2）基于多模态大语言模型的过滤方法虽然具有更好的语义理解和跨文章过滤能力，但计算成本过高，限制了实际应用。

Method: 提出了一种问题聚焦的过滤方法，包含两个核心模块：1）可训练的问题聚焦过滤器（QFF），用于进行问题聚焦的跨文章过滤；2）基于分块的动态多文章选择（CDA）模块，共同缓解文章级和文章内部的信息选择错误。

Result: 实验结果表明，该方法在E-VQA数据集上比当前最先进模型高出4.9%，在InfoSeek数据集上高出3.8%，验证了其有效性，同时保持了与传统方法相当的计算成本。

Conclusion: 提出的问题聚焦过滤方法能够高效获取高质量过滤知识，有效解决了现有知识过滤方法在信息选择和计算效率方面的问题，为知识库视觉问答任务提供了更实用的解决方案。

Abstract: Knowledge-based Visual Question Answering (KB-VQA) aims to answer questions by integrating images with external knowledge. Effective knowledge filtering is crucial for improving accuracy. Typical filtering methods use similarity metrics to locate relevant article sections from one article, leading to information selection errors at the article and intra-article levels. Although recent explorations of Multimodal Large Language Model (MLLM)-based filtering methods demonstrate superior semantic understanding and cross-article filtering capabilities, their high computational cost limits practical application. To address these issues, this paper proposes a question-focused filtering method. This approach can perform question-focused, cross-article filtering, efficiently obtaining high-quality filtered knowledge while keeping computational costs comparable to typical methods. Specifically, we design a trainable Question-Focused Filter (QFF) and a Chunk-based Dynamic Multi-Article Selection (CDA) module, which collectively alleviate information selection errors at both the article and intra-article levels. Experiments show that our method outperforms current state-of-the-art models by 4.9% on E-VQA and 3.8% on InfoSeek, validating its effectiveness. The code is publicly available at: https://github.com/leaffeall/QKVQA.

</details>


### [17] [IF-GEO: Conflict-Aware Instruction Fusion for Multi-Query Generative Engine Optimization](https://arxiv.org/abs/2601.13938)
*Heyang Zhou,JiaJia Chen,Xiaolu Chen,Jie Bao,Zhen Chen,Yong Liao*

Main category: cs.IR

TL;DR: IF-GEO是一个用于生成引擎优化的"发散-收敛"框架，通过挖掘不同查询的优化偏好并合成全局修订蓝图，解决多查询优化中的冲突问题。


<details>
  <summary>Details</summary>
Motivation: 随着生成引擎通过检索源合成直接答案来革新信息检索，确保源可见性成为重要挑战。针对不同查询优化文档时，异质查询在有限内容预算下往往产生冲突和竞争的修订需求。

Method: 提出IF-GEO框架，包含两个阶段：(1)从代表性潜在查询中挖掘不同的优化偏好；(2)通过冲突感知的指令融合协调偏好，合成全局修订蓝图以指导编辑。引入风险感知稳定性指标来量化跨查询稳定性。

Result: 在多查询基准测试中，IF-GEO实现了显著的性能提升，同时在不同检索场景下保持了鲁棒性。

Conclusion: IF-GEO通过协调冲突的查询偏好，有效解决了生成引擎优化中的多查询约束优化问题，提高了文档在生成引擎中的可见性。

Abstract: As Generative Engines revolutionize information retrieval by synthesizing direct answers from retrieved sources, ensuring source visibility becomes a significant challenge. Improving it through targeted content revisions is a practical strategy termed Generative Engine Optimization (GEO). However, optimizing a document for diverse queries presents a constrained optimization challenge where heterogeneous queries often impose conflicting and competing revision requirements under a limited content budget. To address this challenge, we propose IF-GEO, a "diverge-then-converge" framework comprising two phases: (i) mining distinct optimization preferences from representative latent queries; (ii) synthesizing a Global Revision Blueprint for guided editing by coordinating preferences via conflict-aware instruction fusion. To explicitly quantify IF-GEO's objective of cross-query stability, we introduce risk-aware stability metrics. Experiments on multi-query benchmarks demonstrate that IF-GEO achieves substantial performance gains while maintaining robustness across diverse retrieval scenarios.

</details>


### [18] [Auditory Brain Passage Retrieval: Cross-Sensory EEG Training for Neural Information Retrieval](https://arxiv.org/abs/2601.14001)
*Niall McGuire,Yashar Moshfeghi*

Main category: cs.IR

TL;DR: 首次系统研究听觉EEG用于脑部段落检索，发现听觉EEG优于视觉EEG，跨感官训练显著提升性能，甚至超越BM25文本基线


<details>
  <summary>Details</summary>
Motivation: 现有脑部段落检索研究仅使用视觉刺激，无法回答关键问题：听觉EEG能否支持语音界面和视障用户？跨感官训练能否在数据稀缺情况下提升性能？

Method: 使用双编码器架构和四种池化策略（CLS、均值、最大、多向量），在Alice（听觉）和Nieuwland（视觉）数据集上对比纯听觉、纯视觉和组合训练

Result: 听觉EEG始终优于视觉EEG，跨感官训练（CLS池化）相比单独训练显著提升：MRR提高31%（0.474），Hit@1提高43%（0.314），Hit@10提高28%（0.858）。组合听觉EEG模型超越BM25文本基线（MRR: 0.474 vs 0.428）

Conclusion: 验证了听觉神经接口在信息检索任务中的有效性，跨感官训练既能解决数据稀缺问题，又优于单模态方法，使神经查询与传统检索相竞争，同时支持无障碍界面

Abstract: Query formulation from internal information needs remains fundamentally challenging across all Information Retrieval paradigms due to cognitive complexity and physical impairments. Brain Passage Retrieval (BPR) addresses this by directly mapping EEG signals to passage representations without intermediate text translation. However, existing BPR research exclusively uses visual stimuli, leaving critical questions unanswered: Can auditory EEG enable effective retrieval for voice-based interfaces and visually impaired users? Can training on combined EEG datasets from different sensory modalities improve performance despite severe data scarcity? We present the first systematic investigation of auditory EEG for BPR and evaluate cross-sensory training benefits. Using dual encoder architectures with four pooling strategies (CLS, mean, max, multi-vector), we conduct controlled experiments comparing auditory-only, visual-only, and combined training on the Alice (auditory) and Nieuwland (visual) datasets. Results demonstrate that auditory EEG consistently outperforms visual EEG, and cross-sensory training with CLS pooling achieves substantial improvements over individual training: 31% in MRR (0.474), 43% in Hit@1 (0.314), and 28% in Hit@10 (0.858). Critically, combined auditory EEG models surpass BM25 text baselines (MRR: 0.474 vs 0.428), establishing neural queries as competitive with traditional retrieval whilst enabling accessible interfaces. These findings validate auditory neural interfaces for IR tasks and demonstrate that cross-sensory training addresses data scarcity whilst outperforming single-modality approaches Code: https://github.com/NiallMcguire/Audio_BPR

</details>


### [19] [Rerank Before You Reason: Analyzing Reranking Tradeoffs through Effective Token Cost in Deep Search Agents](https://arxiv.org/abs/2601.14224)
*Sahel Sharifymoghaddam,Jimmy Lin*

Main category: cs.IR

TL;DR: 研究深度研究代理中推理预算分配问题，发现适度的列表重排序比增加搜索时推理更有效，能以更低成本获得可比精度


<details>
  <summary>Details</summary>
Motivation: 深度研究代理依赖迭代检索和推理回答复杂查询，但扩展测试时计算会带来显著的效率问题。需要研究如何在深度搜索流程中分配推理预算，特别关注列表重排序的作用。

Method: 使用BrowseComp-Plus基准，通过新颖的有效令牌成本（ETC）指标，分析模型规模、推理努力、重排序深度和总令牌成本之间的权衡关系。

Result: 重排序持续改进检索和端到端准确性；适度的重排序通常比增加搜索时推理带来更大收益，能以显著更低的成本实现可比的准确性。

Conclusion: 在深度研究代理中，适度的列表重排序是比增加搜索时推理更有效的预算分配策略，能在保持准确性的同时大幅降低成本。

Abstract: Deep research agents rely on iterative retrieval and reasoning to answer complex queries, but scaling test-time computation raises significant efficiency concerns. We study how to allocate reasoning budget in deep search pipelines, focusing on the role of listwise reranking. Using the BrowseComp-Plus benchmark, we analyze tradeoffs between model scale, reasoning effort, reranking depth, and total token cost via a novel effective token cost (ETC) metric. Our results show that reranking consistently improves retrieval and end-to-end accuracy, and that moderate reranking often yields larger gains than increasing search-time reasoning, achieving comparable accuracy at substantially lower cost. All our code is available at https://github.com/texttron/BrowseComp-Plus.git

</details>


### [20] [XR: Cross-Modal Agents for Composed Image Retrieval](https://arxiv.org/abs/2601.14245)
*Zhongyu Yang,Wei Pang,Yingfang Yuan*

Main category: cs.IR

TL;DR: XR是一个无需训练的多智能体框架，将检索重构为渐进式协调推理过程，通过三类智能体（想象、相似性、问题）协同工作，在组合图像检索任务上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于嵌入的组合图像检索方法存在视角狭窄、跨模态线索捕捉有限、缺乏语义推理等问题，无法满足智能体AI时代对多模态推理的需求。

Method: XR框架包含三类智能体：想象智能体通过跨模态生成合成目标表示；相似性智能体通过混合匹配进行粗筛选；问题智能体通过针对性推理进行事实一致性验证和细筛选。通过渐进式多智能体协调迭代优化检索结果。

Result: 在FashionIQ、CIRR和CIRCO三个基准测试上，XR比强基线方法提升高达38%，消融实验证明每个智能体都是必要的。

Conclusion: XR通过将检索重构为渐进式多智能体推理过程，有效解决了组合图像检索中的语义和视觉约束问题，为智能体AI时代的检索系统提供了新范式。

Abstract: Retrieval is being redefined by agentic AI, demanding multimodal reasoning beyond conventional similarity-based paradigms. Composed Image Retrieval (CIR) exemplifies this shift as each query combines a reference image with textual modifications, requiring compositional understanding across modalities. While embedding-based CIR methods have achieved progress, they remain narrow in perspective, capturing limited cross-modal cues and lacking semantic reasoning. To address these limitations, we introduce XR, a training-free multi-agent framework that reframes retrieval as a progressively coordinated reasoning process. It orchestrates three specialized types of agents: imagination agents synthesize target representations through cross-modal generation, similarity agents perform coarse filtering via hybrid matching, and question agents verify factual consistency through targeted reasoning for fine filtering. Through progressive multi-agent coordination, XR iteratively refines retrieval to meet both semantic and visual query constraints, achieving up to a 38% gain over strong training-free and training-based baselines on FashionIQ, CIRR, and CIRCO, while ablations show each agent is essential. Code is available: https://01yzzyu.github.io/xr.github.io/.

</details>
