{"id": "2511.05493", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.05493", "abs": "https://arxiv.org/abs/2511.05493", "authors": ["Hao Wang"], "title": "GreyShot: Zeroshot and Privacy-preserving Recommender System by GM(1,1) Model", "comment": null, "summary": "Every recommendation engineer needs to face the cold start problem when building his system. During the past decades, most scientists adopted transfer learning and meta learning to solve the problem. Although notable exceptions such as ZeroMat etc. have been invented in recent years, cold-start problem remains a challenging problem for many researchers. In this paper, we build a zeroshot and privacy-preserving recommender system algorithm GreyShot using GM(1,1) model by taking advantage of the Poisson-Pareto property of the online rating data. Our approach relies on no input data and is effective in generating both accurate and fair results. In conclusion, zeroshot problem of recommender systems could be effectively solved by grey system methods such as GM(1,1)."}
{"id": "2511.05495", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05495", "abs": "https://arxiv.org/abs/2511.05495", "authors": ["Tejas Pawar", "Sarika Patil", "Om Tilekar", "Rushikesh Janwade", "Vaibhav Helambe"], "title": "IMDMR: An Intelligent Multi-Dimensional Memory Retrieval System for Enhanced Conversational AI", "comment": "28 pages, 8 figures, submitted to arXiv for open access publication", "summary": "Conversational AI systems often struggle with maintaining coherent, contextual memory across extended interactions, limiting their ability to provide personalized and contextually relevant responses. This paper presents IMDMR (Intelligent Multi-Dimensional Memory Retrieval), a novel system that addresses these limitations through a multi-dimensional search architecture. Unlike existing memory systems that rely on single-dimensional approaches, IMDMR leverages six distinct memory dimensions-semantic, entity, category, intent, context, and temporal-to provide comprehensive memory retrieval capabilities. Our system incorporates intelligent query processing with dynamic strategy selection, cross-memory entity resolution, and advanced memory integration techniques. Through comprehensive evaluation against five baseline systems including LangChain RAG, LlamaIndex, MemGPT, and spaCy + RAG, IMDMR achieves a 3.8x improvement in overall performance (0.792 vs 0.207 for the best baseline). We present both simulated (0.314) and production (0.792) implementations, demonstrating the importance of real technology integration while maintaining superiority over all baseline systems. Ablation studies demonstrate the effectiveness of multi-dimensional search, with the full system outperforming individual dimension approaches by 23.3%. Query-type analysis reveals superior performance across all categories, particularly for preferences/interests (0.630) and goals/aspirations (0.630) queries. Comprehensive visualizations and statistical analysis confirm the significance of these improvements with p < 0.001 across all metrics. The results establish IMDMR as a significant advancement in conversational AI memory systems, providing a robust foundation for enhanced user interactions and personalized experiences."}
{"id": "2511.05496", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05496", "abs": "https://arxiv.org/abs/2511.05496", "authors": ["Hao Zhang", "Qinghua Lu", "Liming Zhu"], "title": "DOCUEVAL: An LLM-based AI Engineering Tool for Building Customisable Document Evaluation Workflows", "comment": null, "summary": "Foundation models, such as large language models (LLMs), have the potential to streamline evaluation workflows and improve their performance. However, practical adoption faces challenges, such as customisability, accuracy, and scalability. In this paper, we present DOCUEVAL, an AI engineering tool for building customisable DOCUment EVALuation workflows. DOCUEVAL supports advanced document processing and customisable workflow design which allow users to define theory-grounded reviewer roles, specify evaluation criteria, experiment with different reasoning strategies and choose the assessment style. To ensure traceability, DOCUEVAL provides comprehensive logging of every run, along with source attribution and configuration management, allowing systematic comparison of results across alternative setups. By integrating these capabilities, DOCUEVAL directly addresses core software engineering challenges, including how to determine whether evaluators are \"good enough\" for deployment and how to empirically compare different evaluation strategies. We demonstrate the usefulness of DOCUEVAL through a real-world academic peer review case, showing how DOCUEVAL enables both the engineering of evaluators and scalable, reliable document evaluation."}
{"id": "2511.05498", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05498", "abs": "https://arxiv.org/abs/2511.05498", "authors": ["Ilya Tyagin", "Saeideh Valipour", "Aliaksandra Sikirzhytskaya", "Michael Shtutman", "Ilya Safro"], "title": "Biomedical Hypothesis Explainability with Graph-Based Context Retrieval", "comment": "30 pages, 10 figures,", "summary": "We introduce an explainability method for biomedical hypothesis generation systems, built on top of the novel Hypothesis Generation Context Retriever framework. Our approach combines semantic graph-based retrieval and relevant data-restrictive training to simulate real-world discovery constraints. Integrated with large language models (LLMs) via retrieval-augmented generation, the system explains hypotheses with contextual evidence using published scientific literature. We also propose a novel feedback loop approach, which iteratively identifies and corrects flawed parts of LLM-generated explanations, refining both the evidence paths and supporting context. We demonstrate the performance of our method with multiple large language models and evaluate the explanation and context retrieval quality through both expert-curated assessment and large-scale automated analysis. Our code is available at: https://github.com/IlyaTyagin/HGCR."}
{"id": "2511.05500", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.05500", "abs": "https://arxiv.org/abs/2511.05500", "authors": ["Francis Gross"], "title": "Predicting Oscar-Nominated Screenplays with Sentence Embeddings", "comment": null, "summary": "Oscar nominations are an important factor in the movie industry because they can boost both the visibility and the commercial success. This work explores whether it is possible to predict Oscar nominations for screenplays using modern language models. Since no suitable dataset was available, a new one called Movie-O-Label was created by combining the MovieSum collection of movie scripts with curated Oscar records. Each screenplay was represented by its title, Wikipedia summary, and full script. Long scripts were split into overlapping text chunks and encoded with the E5 sentence em bedding model. Then, the screenplay embed dings were classified using a logistic regression model. The best results were achieved when three feature inputs related to screenplays (script, summary, and title) were combined. The best-performing model reached a macro F1 score of 0.66, a precision recall AP of 0.445 with baseline 0.19 and a ROC-AUC of 0.79. The results suggest that even simple models based on modern text embeddings demonstrate good prediction performance and might be a starting point for future research."}
{"id": "2511.05991", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05991", "abs": "https://arxiv.org/abs/2511.05991", "authors": ["Tiago da Cruz", "Bernardo Tavares", "Francisco Belo"], "title": "Ontology Learning and Knowledge Graph Construction: A Comparison of Approaches and Their Impact on RAG Performance", "comment": "12 pages, 8 Figures", "summary": "Retrieval-Augmented Generation (RAG) systems combine Large Language Models (LLMs) with external knowledge, and their performance depends heavily on how that knowledge is represented. This study investigates how different Knowledge Graph (KG) construction strategies influence RAG performance. We compare a variety of approaches: standard vector-based RAG, GraphRAG, and retrieval over KGs built from ontologies derived either from relational databases or textual corpora. Results show that ontology-guided KGs incorporating chunk information achieve competitive performance with state-of-the-art frameworks, substantially outperforming vector retrieval baselines. Moreover, the findings reveal that ontology-guided KGs built from relational databases perform competitively to ones built with ontologies extracted from text, with the benefit of offering a dual advantage: they require a one-time-only ontology learning process, substantially reducing LLM usage costs; and avoid the complexity of ontology merging inherent to text-based approaches."}
{"id": "2511.06668", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.06668", "abs": "https://arxiv.org/abs/2511.06668", "authors": ["Saeedeh Javadi", "Sara Mirabi", "Manan Gangar", "Bahadorreza Ofoghi"], "title": "When Evidence Contradicts: Toward Safer Retrieval-Augmented Generation in Healthcare", "comment": null, "summary": "In high-stakes information domains such as healthcare, where large language models (LLMs) can produce hallucinations or misinformation, retrieval-augmented generation (RAG) has been proposed as a mitigation strategy, grounding model outputs in external, domain-specific documents. Yet, this approach can introduce errors when source documents contain outdated or contradictory information. This work investigates the performance of five LLMs in generating RAG-based responses to medicine-related queries. Our contributions are three-fold: i) the creation of a benchmark dataset using consumer medicine information documents from the Australian Therapeutic Goods Administration (TGA), where headings are repurposed as natural language questions, ii) the retrieval of PubMed abstracts using TGA headings, stratified across multiple publication years, to enable controlled temporal evaluation of outdated evidence, and iii) a comparative analysis of the frequency and impact of outdated or contradictory content on model-generated responses, assessing how LLMs integrate and reconcile temporally inconsistent information. Our findings show that contradictions between highly similar abstracts do, in fact, degrade performance, leading to inconsistencies and reduced factual accuracy in model answers. These results highlight that retrieval similarity alone is insufficient for reliable medical RAG and underscore the need for contradiction-aware filtering strategies to ensure trustworthy responses in high-stakes domains."}
{"id": "2511.06803", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.06803", "abs": "https://arxiv.org/abs/2511.06803", "authors": ["Junpeng Zhao", "Lin Li", "Ming Li", "Amran Bhuiyan", "Jimmy Huang"], "title": "Learning to Fast Unrank in Collaborative Filtering Recommendation", "comment": null, "summary": "Modern data-driven recommendation systems risk memorizing sensitive user behavioral patterns, raising privacy concerns. Existing recommendation unlearning methods, while capable of removing target data influence, suffer from inefficient unlearning speed and degraded performance, failing to meet real-time unlearning demands. Considering the ranking-oriented nature of recommendation systems, we present unranking, the process of reducing the ranking positions of target items while ensuring the formal guarantees of recommendation unlearning. To achieve efficient unranking, we propose Learning to Fast Unrank in Collaborative Filtering Recommendation (L2UnRank), which operates through three key stages: (a) identifying the influenced scope via interaction-based p-hop propagation, (b) computing structural and semantic influences for entities within this scope, and (c) performing efficient, ranking-aware parameter updates guided by influence information. Extensive experiments across multiple datasets and backbone models demonstrate L2UnRank's model-agnostic nature, achieving state-of-the-art unranking effectiveness and maintaining recommendation quality comparable to retraining, while also delivering a 50x speedup over existing methods. Codes are available at https://github.com/Juniper42/L2UnRank."}
{"id": "2511.06905", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.06905", "abs": "https://arxiv.org/abs/2511.06905", "authors": ["Xiaokun Zhang", "Zhaochun Ren", "Bowei He", "Ziqiang Cui", "Chen Ma"], "title": "Have We Really Understood Collaborative Information? An Empirical Investigation", "comment": "This work has been accepted by WSDM 2026", "summary": "Collaborative information serves as the cornerstone of recommender systems which typically focus on capturing it from user-item interactions to deliver personalized services. However, current understanding of this crucial resource remains limited. Specifically, a quantitative definition of collaborative information is missing, its manifestation within user-item interactions remains unclear, and its impact on recommendation performance is largely unknown. To bridge this gap, this work conducts a systematic investigation of collaborative information. We begin by clarifying collaborative information in terms of item co-occurrence patterns, identifying its main characteristics, and presenting a quantitative definition. We then estimate the distribution of collaborative information from several aspects, shedding light on how collaborative information is structured in practice. Furthermore, we evaluate the impact of collaborative information on the performance of various recommendation algorithms. Finally, we highlight challenges in effectively capturing collaborative information and outlook promising directions for future research. By establishing an empirical framework, we uncover many insightful observations that advance our understanding of collaborative information and offer valuable guidelines for developing more effective recommender systems."}
{"id": "2511.06937", "categories": ["cs.IR", "cs.AI", "cs.LG", "cs.NI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.06937", "abs": "https://arxiv.org/abs/2511.06937", "authors": ["Yu Hou", "Hua Li", "Ha Young Kim", "Won-Yong Shin"], "title": "Fine-Tuning Diffusion-Based Recommender Systems via Reinforcement Learning with Reward Function Optimization", "comment": "14 pages, 12 figures, 9 tables", "summary": "Diffusion models recently emerged as a powerful paradigm for recommender systems, offering state-of-the-art performance by modeling the generative process of user-item interactions. However, training such models from scratch is both computationally expensive and yields diminishing returns once convergence is reached. To remedy these challenges, we propose ReFiT, a new framework that integrates Reinforcement learning (RL)-based Fine-Tuning into diffusion-based recommender systems. In contrast to prior RL approaches for diffusion models depending on external reward models, ReFiT adopts a task-aligned design: it formulates the denoising trajectory as a Markov decision process (MDP) and incorporates a collaborative signal-aware reward function that directly reflects recommendation quality. By tightly coupling the MDP structure with this reward signal, ReFiT empowers the RL agent to exploit high-order connectivity for fine-grained optimization, while avoiding the noisy or uninformative feedback common in naive reward designs. Leveraging policy gradient optimization, ReFiT maximizes exact log-likelihood of observed interactions, thereby enabling effective post hoc fine-tuning of diffusion recommenders. Comprehensive experiments on wide-ranging real-world datasets demonstrate that the proposed ReFiT framework (a) exhibits substantial performance gains over strong competitors (up to 36.3% on sequential recommendation), (b) demonstrates strong efficiency with linear complexity in the number of users or items, and (c) generalizes well across multiple diffusion-based recommendation scenarios. The source code and datasets are publicly available at https://anonymous.4open.science/r/ReFiT-4C60."}
{"id": "2511.07028", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.07028", "abs": "https://arxiv.org/abs/2511.07028", "authors": ["Huayang Xu", "Huanhuan Yuan", "Guanfeng Liu", "Junhua Fang", "Lei Zhao", "Pengpeng Zhao"], "title": "Wavelet Enhanced Adaptive Frequency Filter for Sequential Recommendation", "comment": null, "summary": "Sequential recommendation has garnered significant attention for its ability to capture dynamic preferences by mining users' historical interaction data. Given that users' complex and intertwined periodic preferences are difficult to disentangle in the time domain, recent research is exploring frequency domain analysis to identify these hidden patterns. However, current frequency-domain-based methods suffer from two key limitations: (i) They primarily employ static filters with fixed characteristics, overlooking the personalized nature of behavioral patterns; (ii) While the global discrete Fourier transform excels at modeling long-range dependencies, it can blur non-stationary signals and short-term fluctuations. To overcome these limitations, we propose a novel method called Wavelet Enhanced Adaptive Frequency Filter for Sequential Recommendation. Specifically, it consists of two vital modules: dynamic frequency-domain filtering and wavelet feature enhancement. The former is used to dynamically adjust filtering operations based on behavioral sequences to extract personalized global information, and the latter integrates wavelet transform to reconstruct sequences, enhancing blurred non-stationary signals and short-term fluctuations. Finally, these two modules work to achieve comprehensive performance and efficiency optimization in long sequential recommendation scenarios. Extensive experiments on four widely-used benchmark datasets demonstrate the superiority of our work."}
{"id": "2511.07295", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07295", "abs": "https://arxiv.org/abs/2511.07295", "authors": ["Tianrui Song", "Wen-Shuo Chao", "Hao Liu"], "title": "Hard vs. Noise: Resolving Hard-Noisy Sample Confusion in Recommender Systems via Large Language Models", "comment": "Accepted by AAAI2026", "summary": "Implicit feedback, employed in training recommender systems, unavoidably confronts noise due to factors such as misclicks and position bias. Previous studies have attempted to identify noisy samples through their diverged data patterns, such as higher loss values, and mitigate their influence through sample dropping or reweighting. However, we observed that noisy samples and hard samples display similar patterns, leading to hard-noisy confusion issue. Such confusion is problematic as hard samples are vital for modeling user preferences. To solve this problem, we propose LLMHNI framework, leveraging two auxiliary user-item relevance signals generated by Large Language Models (LLMs) to differentiate hard and noisy samples. LLMHNI obtains user-item semantic relevance from LLM-encoded embeddings, which is used in negative sampling to select hard negatives while filtering out noisy false negatives. An objective alignment strategy is proposed to project LLM-encoded embeddings, originally for general language tasks, into a representation space optimized for user-item relevance modeling. LLMHNI also exploits LLM-inferred logical relevance within user-item interactions to identify hard and noisy samples. These LLM-inferred interactions are integrated into the interaction graph and guide denoising with cross-graph contrastive alignment. To eliminate the impact of unreliable interactions induced by LLM hallucination, we propose a graph contrastive learning strategy that aligns representations from randomly edge-dropped views to suppress unreliable edges. Empirical results demonstrate that LLMHNI significantly improves denoising and recommendation performance."}
